{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize(input_str):\n",
    "    \"\"\"\n",
    "    e.g:\n",
    "    change \"/AAMAS/AAMAS2005/p1067-sukthankar.pdf\\n\" to \"AAMAS05-p1067-sukthankar\"\n",
    "    :param input_str: the conference info and paperID of the items in top-10 rank list\n",
    "    :return: paper ID that match the ground truth\n",
    "    \"\"\"\n",
    "    result = re.split(r'/', input_str)\n",
    "    paperInfo = result[-1].replace('.pdf\\n','')\n",
    "    if '20' in result[-2]:\n",
    "        conInfo = result[-2].replace('20','')\n",
    "    if '19' in result[-2]:\n",
    "        conInfo = result[-2].replace('19','')\n",
    "    item_name = conInfo + '-' + paperInfo\n",
    "    return item_name\n",
    "\n",
    "def rename_rank_item(data,num_user):\n",
    "    \"\"\"\n",
    "    rename all items name in the ranking result so as to check if the item in \n",
    "    the result list hit the ground truth.\n",
    "    :param data: a csv file that contain columns : ranking,R1,R2,R3...R50\n",
    "    :param num_user: number of columns in csv file, in this case num_user=50\n",
    "    : return ranking_df: the ranking dataframe with standard item name in each cell\n",
    "    \"\"\"\n",
    "    ranking = pd.read_csv(data)\n",
    "    ranking_df = pd.DataFrame()\n",
    "    # n denotes number of researchers, k denote top-k ranking list\n",
    "    for n in range(1,num_user+1,1):\n",
    "        r = 'R' + str(n)\n",
    "        ranking_ls = []\n",
    "        for k in range(len(ranking[r])):\n",
    "            input_str = ranking[r][k]\n",
    "            ranking_item = standarize(input_str)\n",
    "            ranking_ls.append(ranking_item)\n",
    "        ranking_df[r] = ranking_ls\n",
    "    return ranking_df\n",
    "\n",
    "def get_rank_matrix(ranking_df, ground_truth,num_user,k):\n",
    "    \"\"\"\n",
    "    :param ranking_df: dataframe\n",
    "    :param ground_truth: dataframe\n",
    "    :param num_user: number of users\n",
    "    :param k: recommend top k items for each user\n",
    "    :return rank_matrix: a matrix[k*num_user], cell value is either 1(relevant item) or 0(irrelevant item)\n",
    "    \"\"\"\n",
    "    # reverse all researchers to see if the top k recommended items hit the ground truth\n",
    "    rank_matrix = {}\n",
    "    for r in range(num_user):\n",
    "        hit_ls = []\n",
    "        # the ranking result of researcher r\n",
    "        rID = 'R' + str(r+1)\n",
    "        r_result_ls = ranking_df.iloc[:,r].tolist()\n",
    "        r_ground_truth_ls = ground_truth.iloc[r].tolist()\n",
    "        for i in range(k):\n",
    "            item_i  = r_result_ls[i]\n",
    "            # to see if item i hit the ground truth\n",
    "            if item_i in r_ground_truth_ls:\n",
    "                hit_ls.append(1)\n",
    "            else:\n",
    "                hit_ls.append(0)\n",
    "        rank_matrix[rID] = hit_ls\n",
    "    return rank_matrix\n",
    "\n",
    "def get_dcg(rank_list):\n",
    "    \"\"\"\n",
    "    :param rank_list: list, such as  [1,0,1,0,1,1,1], 1 denotes relevant item and 0 denotes irrelevant item\n",
    "    :return dcg: the dcg value of the input rank list\n",
    "    \"\"\"\n",
    "    n = len(rank_list)\n",
    "    dcg = 0\n",
    "    for i in range(n):\n",
    "        pos = i + 1\n",
    "        # here gains is 1 or 0\n",
    "        gains = rank_list[i]\n",
    "        discounts = np.log2(pos + 1)\n",
    "        if gains == 0:\n",
    "            cg = 0\n",
    "        else:\n",
    "            cg = (gains / discounts)\n",
    "        dcg += cg\n",
    "    return dcg\n",
    "\n",
    "def get_idcg(rank_list):\n",
    "    \"\"\"\n",
    "    :param rank_list: list, such as  [1,0,1,0,1,1,1], 1 denotes relevant item and 0 denotes irrelevant item\n",
    "    :return idcg: the ideal dcg value of the input rank list\n",
    "    \"\"\"\n",
    "    ideal_rank_list = sorted(rank_list, reverse=True)\n",
    "    idcg = get_dcg(ideal_rank_list)\n",
    "    return idcg\n",
    "\n",
    "def get_ndcg(rank_list):\n",
    "    \"\"\"\n",
    "    :param rank_list: list, such as  [1,0,1,0,1,1,1], 1 denotes relevant item and 0 denotes irrelevant item\n",
    "    :return ndcg: the ideal dcg value of the input rank list\n",
    "    \"\"\"\n",
    "    if get_dcg(rank_list) == 0:\n",
    "        ndcg = 0\n",
    "    else:\n",
    "        ndcg = get_dcg(rank_list)/get_idcg(rank_list)\n",
    "    return ndcg\n",
    "\n",
    "def get_avg_ndcg(rank_matrix):\n",
    "    \"\"\"\n",
    "    :param rank_list: list, such as  [1,0,1,0,1,1,1], 1 denotes relevant item and 0 denotes irrelevant item\n",
    "    :return (avg_ndcg,ndcg_ls): the average ndcg value of the input rank matrix for all users and ndcg list for all users\n",
    "    \"\"\"\n",
    "    ndcg_ls = []\n",
    "    for (k,v) in  rank_matrix.items():\n",
    "        ndcg = get_ndcg(v)\n",
    "        ndcg_ls.append(ndcg)\n",
    "    avg_ndcg = np.mean(ndcg_ls)\n",
    "    return avg_ndcg,ndcg_ls\n",
    "\n",
    "def get_precision(rank_list):\n",
    "    \"\"\"\n",
    "    :param rank_list: list, such as  [1,0,1,0,1,1,1], 1 denotes relevant item and 0 denotes irrelevant item\n",
    "    :return precision: the precision of the input rank list\n",
    "    \"\"\"\n",
    "    tp = 0 # truth positive\n",
    "    fp = 0 # flase positive\n",
    "    for i in range(len(rank_list)):\n",
    "        if rank_list[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    precision = tp/len(rank_list)\n",
    "    return precision\n",
    "\n",
    "def avg_precision(rank_matrix):\n",
    "    \"\"\"\n",
    "    :param rank_matrix: a dataframe or a ndarray contains top-10 rank result for all users\n",
    "    :return: average precision and precision list for all users\n",
    "    \"\"\"\n",
    "    p_ls = []\n",
    "    for (k,v) in  rank_matrix.items():\n",
    "        p = get_precision(v)\n",
    "        p_ls.append(p)\n",
    "    avg_p = np.mean(p_ls)\n",
    "    return avg_p,p_ls\n",
    "\n",
    "def reciprocal_rank(rank_list):\n",
    "    \"\"\" \n",
    "    :param rank_list: list, prediction [1,0,1,0,1,1,1], 1 denotes relevant item and 0 denotes irrelevant item\n",
    "    :return rr: reciprocal rank score of the input rank list\n",
    "    \"\"\"\n",
    "    rr = 0.0\n",
    "    for index,item in enumerate(rank_list):\n",
    "        if item == 1:\n",
    "            rr = 1.0 / (index + 1.0)\n",
    "            break\n",
    "    return rr\n",
    "\n",
    "def mean_reciprocal_rank(rank_matrix):\n",
    "    \"\"\"\n",
    "    :param rank_matrix: a dataframe or a ndarray contains top-10 rank result for all users\n",
    "    :return (mrr,rr_ls): MRR scores and RR scores list for all users\n",
    "    \"\"\"\n",
    "    rr_ls = []\n",
    "    for (k, v) in rank_matrix.items():\n",
    "        rr = reciprocal_rank(v)\n",
    "        rr_ls.append(rr)\n",
    "    mrr = np.mean(rr_ls)\n",
    "    return mrr,rr_ls\n",
    "\n",
    "def get_ndcg_p_mrr(rank_matrix):\n",
    "    \"\"\"\n",
    "    get final values for each metrics: NDCG@k,P@k and MRR\n",
    "    :param data: a csv file path that store the rank rusults\n",
    "    :param ground_truth: a csv file path that store the graound truth items of each user\n",
    "    :param num_user: number of users, there are 50 researchers in this experiment\n",
    "    :param k: recommend top k items for each user\n",
    "    :param rank_matrix_file: output file to save rank matrix\n",
    "    :return (average_ndcg, average_precision, mrr, metrics4each_user)  \n",
    "    \"\"\"\n",
    "    metrics4each_user = {}\n",
    "\n",
    "    # get mean ndcg for the ranking matrix of 50 researchers\n",
    "    average_ndcg = get_avg_ndcg(rank_matrix)\n",
    "    metrics4each_user['NDCG@10'] = average_ndcg[1]\n",
    "#     for r in range(1,num_user+1,1):\n",
    "#         print('the ndcg for researcher {} is: {}'.format(r, average_ndcg[1][r-1]))\n",
    "\n",
    "    # ge average precision for the ranking result of 50 researcher\n",
    "    average_precision = avg_precision(rank_matrix)\n",
    "    metrics4each_user['P@10'] = average_precision[1]\n",
    "#     for r in range(1,num_user+1,1):\n",
    "#         print('the precision for researcher {} is: {}'.format(r, average_precision[1][r-1]))\n",
    "\n",
    "    # get mean reciprocal rank for the ranking result of 50 researcher\n",
    "    mrr = mean_reciprocal_rank(rank_matrix)\n",
    "    metrics4each_user['RR'] = mrr[1]\n",
    "#     for r in range(1,num_user+1,1):\n",
    "#         print('the reciprocal rank  for researcher {} is: {}'.format(r, mrr[1][r-1]))\n",
    "\n",
    "#     print('the average ndcg for all researchers is: {}'.format(average_ndcg))\n",
    "#     print('the average precision for all researchers is: {}'.format(average_precision[0]))\n",
    "#     print('the mean reciprocal rank for all researchers is: {}'.format(mrr[0]))\n",
    "    \n",
    "    \n",
    "#     return(average_ndcg[0],average_precision[0],mrr[0],metrics4each_user)\n",
    "    return(average_ndcg[0],average_precision[0],mrr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ramdom ranking dataframe for all researchers\n",
    "def get_random_rank_result(data):\n",
    "    ranking_df = pd.DataFrame()\n",
    "    for i in range(1,51,1):\n",
    "        paperIDs = []\n",
    "        column_name = 'R'+ str(i)\n",
    "        # get paperID in the rank result\n",
    "        for index,row in data.sample(n=10).iterrows():\n",
    "            paperIDs.append(row[0])\n",
    "        ranking_df[column_name] = paperIDs\n",
    "    return ranking_df\n",
    "\n",
    "def get_mean_random_rank_result(n):\n",
    "    for i in range(data,n):\n",
    "        ranking_df = get_random_rank_result(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/Users/sherry/Downloads/candidate_papers.csv',sep=',')\n",
    "ground_truth = pd.read_csv('/Users/sherry/git_project/scholarly_paper_recommendatation/user_profiles/ground_truth.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_sampling(n,data,ground_truth):\n",
    "    metrics_ls = []\n",
    "    for i in range(n):\n",
    "        ranking_df = get_random_rank_result(data)\n",
    "        ranking_matrix = get_rank_matrix(ranking_df, ground_truth,50,10)\n",
    "        metrics_ls.append(list(get_ndcg_p_mrr(ranking_matrix)))\n",
    "    res = pd.DataFrame(metrics_ls)\n",
    "    return res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_sampling(10000,data,ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "version376env",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
