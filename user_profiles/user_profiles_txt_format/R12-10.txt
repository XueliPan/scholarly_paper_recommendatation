Parameter Free Bursty Events Detection in Text Streams

Gabriel Pui Cheong Fung†

Jeﬀrey Xu Yu†

Philip S. Yu‡

Hongjun Lu††

†The Chinese University of Hong Kong, Hong Kong, China, {pcfung,yu}@se.cuhk.edu.hk

‡T. J. Watson Research Center, IBM, USA, psyu@us.ibm.com

††The Hong Kong University of Science and Technology, Hong Kong, China, luhj@cs.ust.hk

Abstract

Text classiﬁcation is a major data mining task.
An advanced text classiﬁcation technique is
known as partially supervised text classiﬁca-
tion, which can build a text classiﬁer using a
small set of positive examples only. This leads
to our curiosity whether it is possible to ﬁnd
a set of features that can be used to describe
the positive examples. Therefore, users do not
even need to specify a set of positive exam-
ples. As the ﬁrst step, in this paper, we for-
malize it as a new problem, called hot bursty
events detection, to detect bursty events from
a text stream which is a sequence of chrono-
logically ordered documents. Here, a bursty
event is a set of bursty features, and is con-
sidered as a potential category to build a text
classiﬁer. It is important to know that the hot
bursty events detection problem, we study in
this paper, is diﬀerent from TDT (topic de-
tection and tracking) which attempts to clus-
ter documents as events using clustering tech-
niques.
In other words, our focus is on de-
tecting a set of bursty features for a bursty
event. In this paper, we propose a new novel
parameter free probabilistic approach, called
feature-pivot clustering. Our main technique
is to fully utilize the time information to de-
termine a set of bursty features which may
occur in diﬀerent time windows. We detect
bursty events based on the feature distribu-
tions. There is no need to tune or estimate
any parameters. We conduct experiments us-
ing real life data, a major English newspaper

Permission to copy without fee all or part of this material is
granted provided that the copies are not made or distributed for
direct commercial advantage, the VLDB copyright notice and
the title of the publication and its date appear, and notice is
given that copying is by permission of the Very Large Data Base
Endowment. To copy otherwise, or to republish, requires a fee
and/or special permission from the Endowment.
Proceedings of the 31st VLDB Conference,
Trondheim, Norway, 2005

in Hong Kong, and show that the parameter
free feature-pivot clustering approach can de-
tect the bursty events with a high success rate.

1 Introduction

In this paper, we study a new problem, called hot
bursty events detection in a text stream, where a text
stream is a sequence of chronologically ordered doc-
uments, and a hot bursty event is a minimal set of
bursty features that occur together in certain time win-
dows with strong support of documents in the text
stream. For example, SARS (Special Severe Acute
Respiratory Syndrome) is a bursty event that con-
sists of a set of bursty features such as sars, outbreak,
atypic, respire, pneumonia, inﬂect, etc. This bursty
event was reported in four hot periods, in a major Eng-
lish newspaper, South China Morning Post, in Hong
Kong: (1) from 3rd April 2003 to 26th June 2003, (2)
on 20th July 2003, (3) on 2nd October 2003; and (4)
on 11th January 2004. The ﬁrst hot period was the
period when it was identiﬁed as a dangerous new dis-
ease. The second hot period was the time that the
director of Health of Hong Kong announced that she
would resign her position and take up a senior posi-
tion at the World Heath Organization. The third hot
period was the period when an independent investi-
gation report against SARS was disclosed. The fourth
hot period was when there were some suspicious SARS
cases identiﬁed in Guangdong province of China. The
determination of such minimal set of bursty features,
to specify a burst event, assists text classiﬁcation, as
one major step ahead of current research activities on
text classiﬁcation. It is because that the set of bursty
features can be used as a set of features for positive
examples, and therefore helps partially supervised text
classiﬁcation [10, 6], which is a text classiﬁcation tech-
nique using positive examples only.
In other words,
with our techniques, users do not even need to spec-
ify a set of positive examples to build a text classiﬁer.
However, the focus of this paper is on the determina-
tion of bursty events, and is not on partially supervised
text classiﬁcation with a set of positive features.

The bursty events detection problem is given below.

Consider a text stream D = {d1, d2, · · · } where di is a
document, and the length of D is |D|. A document di
consists of a set of features, fi1 , fi2 , · · · , and is reported
at time ti.
In the text stream D, ti ≤ tj if i < j.
Dividing the text stream, D, into L non-overlapping
time windows, Wi of the same length, say per day. The
problem of hot bursty events detection is a problem
to ﬁnd a set of bursty events, where a bursty event
consists of a minimal set of bursty features, in time
windows Wi, Wj, · · · that together identiﬁes the event
with the largest number of documents that contain the
bursty features.

Our problem is diﬀerent from the existing event de-
tection problems such as TDT (Topic Detection and
Tracing) [2, 3, 14, 26, 21, 27, 25]. TDT is an unsu-
pervised learning task (clustering) that ﬁnds clusters
of documents matching the real events (sets of docu-
ments identiﬁed by human) by reducing the number of
missing documents in the clusters found and reducing
the possibility of false alarms. The key issue of our
hot bursty events detection is to ﬁnd the minimal sets
of bursty features automatically. In other words, the
emphasis of our problem is to identify sets of bursty
features, whereas the emphasis of TDT is to ﬁnd clus-
ters of documents.

The hot bursty events detection can be possibly
handled by clustering of documents followed by a step
of selecting features from the clusters found. We call it
a document-pivot clustering approach, because it ﬁrst
clusters similar documents into clusters, and then se-
lects features as bursty events from the clusters. The
related works include TDT [2, 3, 14, 18, 21, 26, 27],
text mining [9, 13, 14, 17, 19, 20, 22], and visualiza-
tion [7, 11, 24]. However, the main drawback of adapt-
ing these techniques for the new hot bursty events de-
tection problem is that they require many parameters
and it is very diﬃcult to ﬁnd an eﬀective way to tune
these parameters.
For example, [26, 27] propose a
divide-and-conquer version of the group-average clus-
tering approach [23] for event detection using six pa-
rameters, bucket size, clustering threshold, reducing
factor, number of iterations between re-clustering, fea-
tures per vector, and feature weighting schema. These
parameters are interrelated. Changing one parameter
may have great impacts on the selection of other pa-
[19] proposes a χ2 approach for extracting
rameters.
signiﬁcant time varying features from text, where ex-
tracting diﬀerent kinds of features requires diﬀerent
thresholds. It needs two χ2-thresholds for extracting
name entities and noun phrases in a text stream. [20]
proposes a χ2 based strategy for visualizing the major
events in a text stream. Similar to [19], [20] needs dif-
ferent thresholds for diﬀerent kinds of features includ-
ing two additional parameters, namely, the grouping
threshold and the stopping criteria, in order to identify
diﬀerent events. Without any prior knowledge about
the events in the text stream, it would be rather diﬃ-

cult to estimate these parameters. None of the previ-
ous reported studies discussed in details how to esti-
mate and tune the parameters, to our best knowledge.
The task of tuning parameters is time-consuming and
is diﬃcult, because these parameters are sensitive and
critical for event detection.

In this paper, we propose a new novel feature-pivot
clustering approach for hot bursty events detection.
By feature-pivot clustering, as a term to distinguish it
from document-pivot clustering, we mean that we do
not need to cluster documents in order to ﬁnd bursty
events. The uniqueness of our approach is as follows.
First, as the ﬁrst attempt, we identify hot bursty fea-
tures by feature distribution, as a time-series in time
windows. Second, we group bursty features into bursty
events. Third, we identify the hot periods of burst
events. The main advantage of our approach is pa-
rameter free. There are no parameters that need to
be tuned, and there is no need to use any weighting
schema as we do not need to weight the features. It is
also important to note that our approach can in turn
help TDT to select features for the existing event de-
tection problem [2, 3, 14, 26, 21, 27, 25].

The rest of the paper is organized as follows. Sec-
tion 2 discusses the document-pivot clustering and its
problems. Section 3 presents our novel parameter free
feature-pivot clustering approach. Section 4 shows
that the parameter free feature-pivot clustering ap-
proach can detect the bursty events with a high success
rate. The related works are discussed in Section 5. We
conclude this work in Section 6.

2 Document-Pivot Clustering and Its

Problems

In this section, we address the issues behind the
document-pivot clustering approach which makes hot
bursty events detection diﬃcult. For detecting hot
bursty events, the document-pivot clustering approach
ﬁrst assigns weights to the features based on the most
widely-used tf · idf schema [15]. Second, it performs
clustering to group similar documents into clusters.
Third, it selects features, as bursty features, from the
clusters of documents based on some feature selection
approaches [16]. The ﬁrst two steps are the main steps
used in TDT [2, 3, 14, 26, 21, 27, 25]. The limitations
of adopting this approach are given below.

• The task of hot bursty event detection is to ﬁnd a
minimal set of features that can represent a bursty
event. However, in the document-pivot clustering
approach, features as a whole need to be consid-
ered to measure the similarity between two docu-
ments. The similarity of documents can be biased
to the noisy features. Our early study reported
that the most similar documents often belong
to diﬀerent categories [6]. Therefore, with the
document-pivot clustering approach, most simi-

proach can cluster documents well as events. It
is still diﬃcult to determine bursty events, be-
cause it requires a ranking function to rank events.
However, without any prior knowledge, it is diﬃ-
cult to formulate a good ranking function. In ad-
dition, it is diﬃcult to determine its hot periods.
Another threshold may need to be introduced to
determine the hot periods, such that a hot period
is deﬁned as the number of documents that be-
long to the bursty event in a speciﬁc time period
is larger than the predeﬁned threshold. However,
there does not exist a single threshold for all dif-
ferent events.

3 Bursty Event Detection: A Feature-

Pivot Clustering Approach

All the above give us the motivation for considering
the feature distributions directly rather than on docu-
ment distributions during clustering, i.e. feature-pivot
clustering.

Our framework is outlined in Figure 2. There are
three major steps: (1) Bursty features identiﬁcation,
(2) Bursty features grouping, and (3) Hot periods of
the bursty events determination. Note that no weight-
ing schema is necessary in our framework. Details are
given in the following sections.

3.1 Bursty Features Identiﬁcation

Assume the number of documents that contain the fea-
ture fj in a window Wi, denoted as ni,j, follows a gen-
erative probabilistic model, which is a model based on
an unknown probability distribution. With the gener-
ative probabilistic model, we can compute the prob-
ability of the number of documents that contain the
feature fj in the time window Wi, denoted as Pg(ni,j ).
Pg(ni,j) can be modeled using a hyper-geometric
distribution. Recall the deﬁnition of hyper-geometric
distribution [12]: A sample of size n′ objects is se-
lected, at random (without replacement), from the N ′
objects, such that K ′ objects in n′ are classiﬁed as suc-
cess and N ′ − K ′ objects are classiﬁed as failure, then
the random variable X ′ that denotes the number of
successes in the sample has a hyper-geometric distrib-
ution. In our problem, N ′ is the number of documents
in the text stream, n′ is the number of documents in
a window, K ′ is the number of documents that con-
tain the speciﬁc feature in the particular window, and
n′−K ′ is the number of documents that do not contain
the speciﬁc feature in the particular time window. As
a result, the probability that the feature fj in the time
window Wi can be modeled by a hyper-geometric dis-
tribution. Note: hyper-geometric distribution is com-
putational expensive such that its computational time
growth quadratically with ni,j. We model the hyper-
geometric distribution using the binomial distribution,
since the computation of binomial distribution is far

Figure 1: Document Clustering

lar documents do not necessarily report the same
event.

• In the document-pivot clustering approach, the
tf · idf schema [15] is used for feature weightings.
However, the tf · idf schema is originally designed
for information retrieval, not for clustering, even
though it performs well in most text clustering
problems. There are many tf · idf schema varia-
tions, but the basic idea is the same such as fea-
tures that appear in a few documents are use-
ful, and should be assigned higher weights. The
tf · idf schema does not suit for our purposes for
hot bursty events detection, because we need to
ﬁnd the features that appear in a large number of
documents in certain hot periods, so as to distin-
guish the set of documents that contain the burst
features from the other documents.

• The document-pivot clustering approach is not
eﬀective in handling the cases where the same
events occurs as bursts several times in a long
time period [20, 27]. The reason is that such an
event will be broken into parts when some spe-
ciﬁc features do not occur frequently enough in
consecutive time windows. Figure 1 illustrates an
example. Suppose there are eight consecutive doc-
uments, from A to H, where the documents A,
D, E, G and H support the same event X. Dur-
ing document clustering, suppose that the docu-
ments A, D and G are initially grouped together
in a cluster, G1, and the documents E and H
are initially grouped together in another cluster,
G2. Recall a threshold (stopping criterion) is used
to maintain high intra-similarity within a cluster,
which prohibits further documents, that are not
related to the cluster, being assigned to it. As a
result, G1 and G2 may not be able to merge to
form a single cluster. In other words, a long run-
ning event may be broken down into several sep-
arated events (several diﬀerent clusters). It may
result in many small clusters, and can make it
diﬃcult to ﬁnd the major events which lasts long.

• Assume that the document-pivot clustering ap-

Figure 2: The Overview of Feature-Pivot Clustering Approach

more eﬃcient. Furthermore, both distributions will
eventually be the same when the database is large [12].
Hence, Pg(ni,j ) is modeled by a binomial distribution,
and is computed as follows.

Pg(ni,j ) =

N
ni,j(cid:19)

(cid:18)

pni,j
j

(1 − pj)N −ni,j

(1)

We explain N and pj below.

N is the number of documents in a time window.
It is worth noting that, although the number of doc-
uments, Ni,
in each time window can be diﬀerent,
we can re-scale it in all time windows, such that all
Ni become the same. We do it by adjusting the fre-
quencies of features in all time windows. For example,
suppose that there is a feature, fj, in two time win-
dows, W1 and W2. In W1, the number of documents is
N1 = 78, and the number of documents that contain
fj is n1,j = 24; and in W2, the number of documents
is N2 = 82, and the number of documents that contain
fj is n2,j = 35. We can re-scale Ni and Nj to 100 by
rescaling n1,j = 31 and n2,l = 43 accordingly. Note
that setting N does not aﬀect the quality of bursty
events detection, because the overall feature distribu-
tions are unaﬀected. As a result, N is not considered
as a parameter in our scheme.

In Eq (1), pj is the expected probability of the doc-
uments that contain the feature fj in a random time
window, and is therefore the average of the observed
probability of fj in all time windows containing fj:

pj =

Po(ni,j )

L′

Xi=0

1
L′
ni,j
N

Po(ni,j ) =

(2)

(3)

where L′ is the number of time windows containing fj.
Note that Pg(ni,j ) becomes maximum if ni,j /N = pj.
Figure 3 shows a typical binomial distribution,
Pg(ni,j), for the feature fj in a time window Wi. Note:
binomial distribution is asymmetric except when pj =
0.5. The shape of the binomial distribution depends

Figure 3: A typical binomial distribution

on pj only. A larger pj would shift the burst to the
right hand side. It is worth noting that if the frequency
of a feature that appears in every window is very high,
e.g. a stopword, then L′ and Po(ni,j), in Eq. (2) and
Eq. (3), would both be large, which result in a large
pj. In this case, the binomial distribution in a time
window is similar to the one shown in Figure 4. The
main diﬀerence between Figure 3 and Figure 4 is given
below. In Figure 3, there are three regions along the x-
axis (the number of documents): RA, RB and RC . RA
is from 0 to the x value where Pg(x) is the maximum,
RB is from the x value where Pg(x) is the maximum
to the x value where Pg(x) becomes zero again, and
RC is the region followed by RB.
In Figure 4, the
right hand side of the distribution can never reach 0.
Therefore, a feature fj will be taken as a stopword if
its binomial distribution becomes the one as shown in
Figure 4.

We discuss how likely an important feature fj will
be wrongly taken as a stopword below. Suppose that
fj is a bursty feature in a bursty event Ek, such that
fj only appears with high frequency in the hot peri-
ods of Ek. It implies ni,j is large in the time window
Wi where Ek is a bursty event. If it occurs, all the
observed probability in every sliding window, Po(ni,j )
(Eq. (3)) will be large, whereas the number of time
windows that contain fj will be small, i.e. L′ in Eq. (2)
will be small. Hence, pj will be large as well as shown
in Figure 4. Therefore, it is possible in theory that fj

Figure 4: The binomial distribution of a stopword

Figure 5: A Sigmoid Distribution

will be wrongly taken as a stopword. However, it is
most unlikely that fj will be wrongly taken as a stop-
word for the following reasons. Feature distributions
are sparse in nature. Even though fj may be only re-
lated to Ek as its bursty feature, it is most likely that
fj will appear in other time windows where Ek is not
a bursty window.
In other words, for the same fea-
ture fj, the number of documents that contain it, ni,j,
will be large in some time windows and small in some
other windows. On average, the observed probabil-
ity, Po(ni,j ), for such a feature fj, will not be large as
the one for a stopword, which is conﬁrmed by our ex-
tensive experimental studies. We will further conduct
analytical studies on this issue as our future work.

We decide the probability that a feature is bursty
based on the binomial distributions, Pg(ni,j), in Fig-
ure 3. Let Pb(i, fj) be the probability that the feature
fj is burst in the time window Wi. We consider three
cases below.

• When ni,j is in RA, it implies that Po(ni,j) ≤ pj.
It suggests that the probability of the feature fj
in Wi is less than or equal to the probability that
fj is drawn randomly. We consider fj as a non-
bursty feature in Wi, and let Pb(i, fj) = 0.

• When ni,j is in RC , it implies that Po(ni,j ) is
noticeably higher than the prior probability of the
feature fj (pj).
It suggests that fj exhibits an
abnormal behavior in Wi. We consider fj as a
bursty feature in Wi, and let Pb(i, fj) = 1.

• When ni,j is in RB, there are further three cases.
When ni,j approaches the boundary of RB and
RC , the corresponding feature fj will be a bursty
feature; when ni,j approaches the boundary of RB
and RA, fj will be a non-bursty feature; and when
ni,j is on the mid-point of region RB (the point q
in Figure 3), fj can either be bursty or not bursty.
Based on the case analysis, we use a sigmoid func-
tion to determine whether fj is bursty or not when
ni,j is in the region RB.

where q is the mid-point in the region RB of Fig-
ure 3, and θ is the slope of the sigmoid function
(Figure 5) which can be readily computed by re-
ferring to the range of RB.

3.2 From Bursty Features To Bursty Events

the bursty features

Let
identiﬁed be B =
{b0, b1, . . . , b|B|}. A bursty event is an event that con-
sists of bursty features. The selection of minimal num-
ber of features to form a bursty event is formulated
as follows. Let a bursty event Ek = {e0, e1, . . . , e|B|}
where ei = {0, 1}.
It suggests the following. When
ei = 0, the i-th bursty feature bi does not contribute
to the bursty event Ek; when ei = 1, the i-th fea-
ture bi is selected as the key feature for the bursty
event Ek. For example, suppose B = {database,
food, management, music}. Ek = {1, 0, 1, 0} implies
that the bursty event Ek contains two bursty fea-
tures, database and management. Here, the problem
of determining the minimal set of bursty features of
a bursty event can be solved as ﬁnding the optimal
Ek such that the probability of the bursty features
grouped together is maximum for the text stream D.
Let D = {D0, D1, . . . , D|B|} be a set of sets of docu-
ments where Di contains documents that contain the
bursty feature bi. Mathematically,

max P (Ek|D) =

P (D|Ek)P (Ek)

P (D)

Taking logarithm in Eq. (6), maximizing Eq. (6) is
equivalent to minimize:

min − ln P (Ek|D) = − ln P (D|Ek) − ln P (Ek)

+ ln P (D)

Note: ln P (D) is independent of Ek. Thus, minimiz-
ing Eq. (7) is equivalent to minimize the following cost
function:

(6)

(7)

Pb(i, fj) =

1

1 + e−x

x = Pg(ni,j) · θ − q

(4)

(5)

min c(Ek|D) = − ln P (D|Ek) − ln P (Ek)

(8)

We show how to compute P (D|Ek) and P (Ek) below.

• Computing P (Ek): Let the total number of time
windows be L. We consider that a bursty fea-
ture bj is a time-series of length L, such that
the i-th value in the time-series is the bursty
probability Pb(i, bj) in the time window Wi. We
solve the problem of computing the probability of
bursty features to be grouped together (P (Ek)) as
to compute the probability of the corresponding
time-series to be grouped together. This can be
achieved by computing the similarity among a set
of time-series given Ek [8, 5, 1]. In this paper, we
take a simple yet eﬃcient and eﬀective approach
to compute P (Ek), that is, by computing the over-
lapping areas among diﬀerent time-series.

P (Ek) = T

(9)

|B|
j=0
|B|
j=0

ej =1aj
(cid:12)
(cid:12)
ej =1aj
(cid:12)
(cid:12)

S

where aj is the area covered by the feature bj in
the time-series.

• Computing P (D|Ek): We assume the fea-
ture distribution is independent, and formulate
P (D|Ek) as follows. The idea behind it will be
explained after introducing the formulation.

P (D|Ek) =

ej

|Dj|
|M | (cid:19)

1 −

(cid:18)

|Dj|
|M | (cid:19)

1−ej

|B|

Yj=0 (cid:18)
|B|

[j=0
ej =1

M =

Dj

Here, M (Eq. (11)) is a set of documents that
contain the bursty event Ek.
In Eq. (10), the
ﬁrst component computes the probability of the
documents that contain the bursty feature bj in
M , whereas the second component computes the
probability of the documents that do not contain
the bursty feature bj in M .
In other words, if
ej = 1, it implies that the bursty feature bj be-
longs to the event Ek, and we compute the ﬁrst
component;
if ej = 0, we compute the second
component. Hence, P (D|E) computes the pro-
duction of the probability of D under M where
M is constructed by the given Ek.

Finally, the cost function (Eq. (8)) can be computed

as follows.

c(Ek|D) = −

ln(|Dj|) −

ln(|M | − |Dj|)

|B|

Xj=0
ej =1

+ l ln(|M |) − ln T

|B|

Xj=0
ej 6=1

|B|
j=0
|B|
j=0

S

ej =1aj
ej =1aj

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(10)

(11)

(12)

Some observations can be made on Eq. (12). First,
if the cost function c becomes smaller, then it sug-
gests that the selected bursty features (ej = 1) will be
strongly related for the bursty event Ek. Second, if the
bursty features are very similar, the cost function c be-
comes smaller, because the last component of Eq. (12)
becomes smaller, which makes these bursty features to
be grouped together. When the areas of two features
completely overlapped, then the forth component be-
comes 0. Third, if the documents do not share a high
degree of common bursty features, the cost function c
becomes larger, because the third component becomes
large. Fourth, if the bursty features are a subset of
another set of bursty features, the cost function c be-
comes large, because the last component becomes large
for the reason that the overlapping area of the two sets
of bursty features becomes small.

We further address two important issues in group-

ing bursty features for a bursty event below.

The ﬁrst issue is whether two bursty features fj and
fl will be wrongly grouped together in a bursty event
Ek, if the two features have the high similarity in their
feature distribution. For example, the bursty feature
Sars and the bursty feature Iraq are similar in the cor-
responding feature distributions (Figure 6 (a) and (g)).
Is it possible that the two features will be grouped
in the same bursty event? Below, we show that it
is unlikely that the irrelevant bursty features will be
grouped together. Consider the cost function (Eq. (8))
which has two components, P (D|Ek) and P (Ek).
If
two bursty features have high similarity in their fea-
ture distributions, as time-series data, P (Ek) (Eq. (8))
becomes large, because the common area of the two
feature distributions becomes large. In other words,
P (Ek) → 1, and therefore ln P (Ek) → 0. It makes the
cost (Eq. (8)) smaller as in favor of grouping these two
bursty features. However, if the two bursty features
are about two diﬀerent stories (events), it is unlikely
that they will appear in the same documents. For ex-
ample, it is unlikely that many documents will discuss
both Sars and Iraq together. Recall Dj is the set of
documents that contain a bursty feature fj, and Dl
is the set of documents that contain a bursty feature
fl.
If two bursty features appear in diﬀerent docu-
ments, P (D|Ek) becomes smaller (Eq. (10)), because
M (Eq. (11)) becomes larger, and therefore the cost
becomes larger. Consequently, it is unlikely that the
irrelevant bursty features will be wrongly grouped to-
gether. Detail information will be given in our exper-
imental studies.

The second issue is whether the resulting set of
bursty features for Ek will possibly include noises. The
quality of the set of bursty features grouped together is
guaranteed for the similar reasons we discussed for the
above ﬁrst issue. Consider P (D|Ek) again. If a bursty
event Ek contains many features that appear in diﬀer-
ent sets of documents, P (D|Ek) becomes small, which

Algorithm 1 HB-Event(B, D)
Input: A set of bursty features, B, and the set of
documents D that contains bursty features in B;
Output: A list of bursty events, {E1, . . . , Ek};

1: k ← 0;
2: repeat
3:
4:

for each ej ∈ Ek do

k ← k + 1;
compute Ek by minimizing Eq. (12), using B
and D;
5: B′ ← ∅;
6:
7:
8:
9:
10:
11: B ← B − B′;
12: until |B′| = 1
13: return {E1, . . . , Ek};

end if
end for

B′ ← B − {bj};

if ej = 1 then

makes it unlikely to group them together. A set of
bursty features are grouped under the condition that
they are contained in the similar documents (Eq. (11)).
Our HB-Event algorithm, for Hot-Bursty-Event de-
tection, is shown in Algorithm 1. The input is the set
of bursty features B and the set of documents D that
contains bursty features in B. The algorithm returns
hot bursty events by repeatedly selecting the bursty
events. Note:
in Algorithm 1, a bursty feature only
appears in one bursty event Ej. The main idea exhib-
ited here can be extended to the cases where a bursty
feature appears in multiple bursty events.

3.3 Hot Periods of the Bursty Events

The hot periods of a bursty event, Ek, are determined
below. Let Hk = {h0, h1, . . . , hn} for hi ∈ {0, 1} where
hi = 1 indicates that the bursty event Ek is hot in the
time window Wi. Because we formalize the bursty fea-
tures as time-series, we compute the probability of the
hot bursty event Ek in Wi, denoted Pb, by computing
the expected probability of the bursty event based on
the set of bursty features that belong to the bursty
event Ek:

Pb(i, Ek) =

ej · Pb(i, fj)

(13)

|B|

1

|Bk|

Xj=0

where |Bk| is the number of bursty features in Ek. In
this paper, we say a bursty event Ek is hot in Wi,
if Pb(i, Ek) > β, where β is simply set as 2 times of
the standard deviation above the expected value of
Pb(i, Ek) for i = 1, 2, · · · . We ﬁnd that the setting of
β value is eﬀective in our experimental study using a
real dataset. There is no need to tune β.

4 Experimental Studies

We have archived two-year news stories from a ma-
jor English news paper in Hong Kong, South China
Morning Post(www.scmp.com.hk), from 2003-01-01 to
2004-12-31. It consists of 66,300 news stories. We only
conducted simple document pre-processing to remove
punctuation, digits, stopwords, web page addresses
and email addresses. All features are stemmed and
converted to lower cases. The number of features after
stemming is 93,807.

We implemented our framework using JavaTM and
conducted our testing on Solaris. In the experimen-
tal studies, we concentrated on our novel feature-pivot
clustering approach, and do not show the results us-
ing document-pivot clustering, because there are no
reported studies providing details for us to ﬁne tune
parameters for grouping bursty features. Simply tun-
ing of parameters may result unfair results, and we
are reluctant to include such results in this paper, and
plan to study it more as our future work.

4.1 Identifying Bursty Features

Among the 93,807 features, we found 373 features as
bursty features in total. 12 bursty features are se-
lected for detail discussions, including Sars, Outbreak,
Disease, Iraq, Military, Saddam, Article, Law,
Rally, Gorge, Bush, and White, as shown in Figure 6.
In all the ﬁgures in Figure 6, the x-axis is the i-th date
starting from Jan. 1st, 2003. A time window is a sin-
gle day. There are two ﬁgures for each bursty feature,
fj, showing the percentages of news stories in a time
window Wi that contain the bursty feature fj, ni,j/N ,
and its bursty feature probability, Pb(i, fj). The 1st,
3rd, 5th, and 7th rows show the percentage of news
stories in a time window that contain the bursty fea-
ture in question, and the 2nd, 4th, 6th and 8th rows
show the bursty feature probabilities.

As shown in Figure 6, there are some noticeable
bursty features such as Sars, Outbreak, Iraq, and
Military, if we compare (a) vs (d) for Sars, (b) vs
(e) for Outbreak, and (g) vs (j) for Iraq. There are
also some bursty features, like Law ((n) vs (q)), which
appear everyday.

Our novel feature-pivot clustering approach can also
ﬁnd the hot periods where the bursty features occur.
It is important to notice that no bursty features can be
observed using document-pivot clustering approach, if
they appear continuously like Law in our example. It
shows the strength of the proposed feature-pivot clus-
tering approach. More discussions will be given later
when we discuss bursty events.

4.2 Identifying the Bursty Events and the Hot

Periods

A bursty event contains a set of bursty features. Total
28 bursty events are found using the HB-Event algo-

0

0

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

Day

(a) Sars (ni,j /N )

(b) Outbreak (ni,j /N )

Day

(c) Disease (ni,j /N )

Day

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(d) Sars (Pb(i, fj))

Day

(e) Outbreak (Pb(i, fj ))

Day

(f) Disease (Pb(i, fj))

Day

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

Day

(g) Iraq (ni,j /N )

(h) Military (ni,j /N )

Day

(i) Saddam (ni,j /N )

Day

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

Day

(j) Iraq (Pb(i, fj))

(k) Military (Pb(i, fj))

Day

(l) Saddam (Pb(i, fj ))

Day

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(m) Article (ni,j /N )

Day

Day

(n) Law (ni,j /N )

(o) Rally (ni,j /N )

Day

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(p) Article (Pb(i, fj))

Day

Day

(q) Law (Pb(i, fj))

(r) Rally (Pb(i, fj))

Day

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(s) George (ni,j /N )

Day

Day

(t) Bush (ni,j /N )

(u) White (ni,j /N )

Day

0

0

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

0

0

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(v) George (Pb(i, fj))

Day

(w) Bush (Pb(i, fj ))

Day

(x) White (Pb(i, fj ))

Day

Figure 6: 12 Bursty Features (The percentages of news stories in a window Wi that contain the bursty feature
fj, ni,j/N , are shown in (a)-(c), (g)-(i), (m)-(o), and (s)-(u), and the probabilities of bursty features, Pb(i, fj),
are shown in (d)-(f), (j)-(l), (p)-(r), and (v)-(x).)

Bursty Features
sars, outbreak, atypic, respire, pneumonia, inﬂect, . . .
article, Yip, law, rally, head
bird, ﬂu

Bursty Events
E1 (SARS)
E2 (Legislation)
E3 (Bird-Flu)
E4 (Taiwan Issue) Taiwan, Chen, Shu, Bian
E5 (Iraq-War)
E6 (Gas)

Iraq, war, military, Hussein, Saddam,
victim, might, accident, gas

Table 1: 6 Bursty Events

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(a) Bursty Event E1

Day

(b) Bursty Event E2

Day

(c) Bursty Event E3

Day

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(d) Bursty Event E4

Day

(e) Bursty Event E5

Day

(f) Bursty Event E6

Day

Figure 7: 6 Bursty Events (Probability for Bursty Events)

rithm (Algorithm 1) from the 373 bursty features. Ta-
ble 1 gives the top 6 bursty features. Recall: with the
HB-Event algorithm, the bursty event E1 is ﬁrst iden-
tiﬁed, from the total 373 bursty features. Those bursty
features that appear in E1 will be removed, and the
second bursty event E2 will be identiﬁed. The process
repeats until all bursty events are identiﬁed. The max-
imum, minimum, and average size of bursty events are,
12, 2 and 3.46. Note: the names of the bursty events in
Table 1 are named by human to match the real events.
The 6 bursty events are shown in Figure 7.

As shown in Figure 6, Sars, Outbreak and Iraq
have rather high similarity in their feature distribu-
tions (Figure 6 (d), (e) and (j)). However, Sars and
Outbreak should be grouped together as bursty fea-
tures for the bursty event SARS, and Sars and Iraq
should not be grouped together for any bursty event.
We explain why our feature-pivot clustering approach
can correctly group Sars and Outbreak together, but
not Sars and Iraq.

• Grouping bursty features Sars and Iraq: The to-
tal numbers of documents that contain the bursty
feature Sars and Iraq during the bursty period
are |DSars| = 3, 240 and |DIraq| = 2, 404, respec-
tively. In total, there are 153 documents reporting
both events at the same time, such as |DSars ∩
DIraq| = 153, and there are 5, 491 documents
that contain either Sars or Iraq such as |M | =
|DSars ∪ DIraq| = 5, 491 (Eq. (11)). Consider

whether Sars and Iraq shall be grouped. If they
are grouped together, with Eq. (10), P (D|Ek) =
(3240/5491) × (2404/5491) = 0.258. The cost c
becomes 0.190 + 0.588 = 0.778 where P (Ek) =
0.646. If they are not grouped, then with Eq. (10),
P (D|Ek) = (3240/5491) × (1 − 2404/5491) =
0.332. The cost c becomes 0+0.479 = 0.479 where
P (Ek) = 1. Therefore, the two bursty features
should not be grouped together.

• Grouping Sars and Outbreak: The total num-
bers of documents that contain the bursty fea-
ture Sars and Outbreak during the bursty period
are |DSars| = 3, 240 and |Doutbreak| = 2, 254, re-
spectively.
In total, there are 1, 854 documents
reporting both events at the same time, such as
|DSars ∩ DOutbreak| = 1, 854, and there are 3, 640
documents that contain either Sars or Outbreak
such as |M | = |DSars ∪ DOutbreak| = 3, 640
(Eq. (11). If Sars and Outbreak are grouped to-
gether, P (D|Ek) = (3240/3640) × (2254/3640) =
0.551. The cost c becomes 0.043 + 0.259 = 0.302
where P (Ek) = 0.906. If Sars and Outbreak are
not grouped together, P (D|Ek) = (3240/3640) ×
(1 − 2254/3640) = 0.338. The cost c becomes
0 + 0.471 = 0.471 where P (Ek) = 1. As a result,
we should group Sars and Outbreak together.

Figure 8 shows the hot periods of the same bursty
events (Figure 7). We highlight some observations in

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(a) Bursty Event E1

Day

(b) Bursty Event E2

Day

(c) Bursty Event E3

Day

100

200

300

400

500

600

700

100

200

300

400

500

600

700

100

200

300

400

500

600

700

(d) Bursty Event E4

Day

(e) Bursty Event E5

Day

(f) Bursty Event E6

Day

Figure 8: 6 Bursty Events (Hot Periods)

Table 1 and Figure 8 in details as case studies, refer-
ring to Figure 6. The bursty event E1 (SARS) includes
the bursty features Sars (Figure 6 (d)), Outbreak (Fig-
ure 6 (e)) and Disease (Figure 6 (f)), because all have
similar feature distribution.
In a similar fashion,
the bursty event E5 (Iraq-War) includes the bursty
features such as Iraq (Figure 6 (j)), Military (Fig-
ure 6 (k)) and Saddam (Figure 6 (l)) for having the high
similarity among their feature distributions.

associated with rally. The major diﬀerence between
the feature distribution of Rally and Article is that
Rally has a diﬀerent burst period on 2nd July 2004,
because on 1st July 2004, there was another massive
demonstration, which included over 300,000 people. In
short, all the bursty features are strongly interrelated
to each others. The similar observations can be ob-
served for E3 (Bird-Flu), E4 (Taiwan-Issue), and E6
(Gas).

The bursty event E2 (Legislation) includes the
bursty features such as Article (Figure 6 (p)) and
Law (Figure 6 (q)) in a less obvious way. Consider
the cost function (Eq. (8)) which has two components,
P (D|Ek) and P (Ek). For E2, although P (Ek) is small,
0.3, as the overlapping area between these features
is small), P (D|Ek) is large. Most documents con-
tain all these features during the period when the fea-
tures are bursty. Some details are given below for
E2. There was a massive demonstration against the
Hong Kong Basic Law Article 23 legislation on 1st
July 2003. In the aftermath of the demonstration, on
6th July 2003, the Hong Kong government announced
that the second reading of the law was to be post-
poned, and the head of security (Mrs. Yip) resigned
position on 16th July 2003 that political commenta-
tors attributed the resignation to the protests over the
Basic Law Article 23 legislation. Therefore, Article
and Law co-occurred together during the corresponding
bursty periods. Apart from the aforementioned peri-
ods, there are two major bursts for E2. One is on 5th
September 2003 and the other is on 23rd November
2003. On 5th September 2003, the Chief Executive
of Hong Kong announced that Article 23 legislation
would be withdrawn and there was no timetable for its
re-introduction. On 23rd, November 2003, it is the dis-
trict council election. It oﬀered the ﬁrst opportunity
for voters to express their opinions since July 1st. The
bursty feature Rally shows the similarity to the bursty
feature Article, because demonstration was usually

5 Related Work

Topic detection and tracking (TDT) is the major area
that tackles the problem of discovering events from a
stream of news stories [2, 18, 27, 26, 3, 4, 27, 26, 21].
They all use similar techniques for event detection,
that is to cluster similar documents together to form
events. We discussed in Section 2 that this approach
cannot be directly applied to our hot bursty events
detection. In addition to the quality issue whether it
can ﬁnd bursty events, there is an eﬃciency issue. The
size of the corpus usually makes the clustering problem
become diﬃcult. The work in [21, 27, 26] attempted to
improve the eﬃciency of clustering, however, it further
introduces more parameters to be tuned.

[9] shows how to extract bursty features from text
streams based on modeling the text stream using an
inﬁnite-state automaton, where bursts are modeled as
state transitions. Our work is diﬀerent, because we
do not only attempt to extract bursty features, but
also, as one step further, attempt to group the related
bursty features into bursty events, as well as to deter-
mine the hot periods of bursty events. Note: for the
state transition in [9], it needs to deﬁne the probabil-
ity for each state, whereas our feature-pivot clustering
approach is parameter free.

The work in [19, 20, 17, 14] studied bursty events
in a text stream using a similar model formulation.
For each feature, such as name entity and noun phase,
in the corpus, they performed a χ2 test to identify

days on which the occurrences yield a value above a
predeﬁned threshold, and group the consecutive days
that meet this criteria into events. Our approach is
diﬀerent. First, we do not need any complex parame-
ter tuning, whereas [19, 20] need to predeﬁne several
thresholds by the user. Second, the authors showed
that it is diﬃcult to construct an event which lasts for
a long period. The reason is that a period may be
broken into parts because the speciﬁc feature does not
occur frequently in every consecutive time windows.
We do not need to explicitly deﬁne whether a feature
is bursty or not in a time window. We model each of
the bursty features as time-series of probability, and
group the bursty features into bursty events.

[22] proposes methods for mining knowledge from
the query logs of the MSN search engine by build-
ing a time-series for each query term, where the ele-
ments of the time-series are the number of times that
a query is issued on a day. By observing the patterns
of the time-series, [22] attempts to ﬁnd the periods
where time-series becomes bursty. In transforming the
features into time-series, they adopted the techniques
of moving average, which is sensitive to the length of
time windows. They combined the features with sim-
ilar patterns only in the time-series, but did not pay
attention to the content. We ﬁnd the bursty events
based on both the time and content information.

The related works also include visualization tech-
niques [7, 24, 11]. Their focus is on the visualization
(how to present the information based on a set of given
events), rather than on the detection side (how to iden-
tify a set of events).

6 Conclusion

In this paper, we studied a new problem, called hot
bursty events detection in a sequence of chronologi-
cally ordered documents, where a bursty event is a set
of bursty features appearing in certain time windows.
Taken a set of bursty features as positive features in a
set of positive examples (labeled by the correspond-
ing bursty event). The new problem is important,
because, as the ﬁrst attempt, it attempts to ﬁnd a
complete solution to build text classiﬁers without any
human assistance, along the line of (1) identifying pos-
itive features, (2) enlargement of positive features, (3)
identiﬁcation/enlargement of negative examples, and
(4) text classiﬁcation building. Except for the ﬁrst
issue, which is the focus of this paper, the other is-
sues have been addressed in the recent papers, and
are known as partially supervised text classiﬁcation.

We proposed a parameter free probabilistic ap-
proach for eﬀectively and eﬃciently identifying bursty
events, called feature-pivot clustering approach. Our
algorithm, proposed in this paper, is an oﬀ-line al-
gorithm which has its potential to be extended to
an on-line algorithm, because it mainly uses distri-
butions, which can be handled using the up-to-date

data streaming mining techniques.
In the feature-
pivot clustering approach, we utilize both time and
content information in a very eﬀective way. We iden-
tify bursty features by their distributions, and group
strongly interrelated bursty features as bursty events.
Our approach groups the interrelated bursty features
together if they appear in the same documents fre-
quently enough, because our approach also pays atten-
tion to the content. In other words, it is unlikely that
a bursty event contains irrelevant bursty features to-
gether. It is important to know that it can be achieved
without parameter tuning and estimation.

We conduct experimental studies using a two-year
news stories archived from a major English news paper
in Hong Kong, South China Morning Post. The testing
results showed that the parameter free feature-pivot
clustering approach can detect the bursty events with
a high success rate.

References

[1] R. Agrawal, K.-I. Lin, H. S. Sawhney, and
K. Shim. Fast similarity search in the presence of
noise, scaling, and translation in time-series data-
bases. In Proceedings of 21th International Con-
ference on Very Large Data Bases (VLDB’95),
1995.

[2] J. Allan, R. Papka, and V. Lavrenko. On-line
new event detection and tracking. In Proceedings
of the 21st ACM SIGIR International Conference
on Research and Development in Information Re-
trieval (SIGIR’98), 1998.

[3] T. Brants and F. Chen. A system for new event
detection. In Proceedings of the 26th ACM SIGIR
International Conference on Research and De-
velopment in Information Retrieval (SIGIR’03),
2003.

[4] M. Connell, A. Feng, G. Kumaran, H. Ragha-
van, C. Shah, and J. Allan. UMass at tdt 2004.
In 2004 Topic Detection and Tracking Workshop
(TDT’04), 2004.

[5] C.

M.

Faloutsos,

Ranganathan,

and
Y. Manolopoulos.
Fast subsequence match-
ing in time-series databases. In Proceedings of the
1994 ACM SIGMOD International Conference
on Management of Data (SIGMOD’94), 1994.

[6] G. P. C. Fung, J. X. Yu, H. Lu, and P. S. Yu. Text
classiﬁcation without negative labeled examples.
In Proceedings of the 21st International Confer-
ence on Data Engineering (ICDE’05), 2005.

[7] G. J. F. Jones and S. M. Gabb. A visualisation
tool for topic tracking analysis and development.
In Proceedings of the 25th ACM SIGIR Interna-
tional Conference on Research and Development
in Information Retrieval (SIGIR’02), 2002.

[8] E. J. Keogh and P. Smyth. A probabilistic ap-
proach to fast pattern matching in time series
databases.
In Proceedings of the 3rd Interna-
tional Conference on Knowledge Discovery and
Data Mining (KDD’97), 1997.

[20] R. C. Swan and J. Allan. Automatic genera-
tion of overview timelines. In Proceedings of the
23rd ACM SIGIR International Conference on
Research and Development in Information Re-
trieval (SIGIR’00), 2000.

[9] J. M. Kleinberg. Bursty and hierarchical struc-
ture in streams. In Proceedings of the 8th Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD’02), 2002.

[21] D. Trieschnigg and W. Kraaij. Hierarchical topic
detection in large digital news archives. In Pro-
ceedings of the 5th Dutch Belgian Information Re-
trieval workshop, 2005.

[22] M. Vlachos, C. Meek, Z. Vagena, and D. Gunop-
Identifying similarities, periodicities and
ulos.
bursts for online search queries.
In Proceedings
of the 2004 ACM SIGMOD International Con-
ference on Management of Data (SIGMOD’04),
2004.

[23] P. Willett. Recent trends in hierarchic docu-
ment clustering: A critical review.
Informa-
tion Processing and Management, 24 (5):577–597,
1988.

[24] P. C. Wong, W. Cowley, H. Foote, E. Jurrus, and
J. Thomas. Visualizing sequential patterns for
text mining.
In Proceedings of the 2000 IEEE
Symposium on Information Visualization, 2000.

[25] Y. Yang, T. Ault, T. Pierce, and C. W. Lattimer.
A study on thresholding strategies for text catego-
rization. In Proceedings of the 23rd ACM SIGIR
International Conference on Research and De-
velopment in Information Retrieval (SIGIR’00),
2000.

[26] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T.
Archibald, and X. Liu. Learning approaches for
detecting and tracking news events. IEEE Intel-
ligent Systems, 14 (4):32–43, 1999.

[27] Y. Yang, T. Pierce, and J. Carbonell. A study
on retrospective and on-line event detection. In
Proceedings of the 21st ACM SIGIR International
Conference on Research and Development in In-
formation Retrieval (SIGIR’98), 1998.

[10] X. Li and B. Liu. Learning to classify texts using
positive and unlabeled data.
In Proceedings of
2003 International Joint Conference on Artiﬁcial
Intelligence (IJCAL’03), 2003.

[11] N. E. Miller, P. C. Wong, M. Brewster, and
H. Foote. Topic islands – a wavelet-based text
visualization system.
In Proceedings of the 9th
IEEE Visualization, 1998.

[12] D. C. Montogomery and G. C. Runger. Applied
Statistics and Probability for Engineers. John Wi-
ley & Sons, Inc., second edition, 1999.

[13] S. Morinaga and K. Yamanishi. Tracking dynam-
ics of topic trends using a ﬁnite mixture model.
In Proceedings of the 10th International Confer-
ence on Knowledge Discovery and Data Mining
(KDD’04), 2004.

[14] R. Papka and J. Allan. On-line new event detec-
tion using single pass clustering. Technical Re-
port IR–123, Department of Computer Science,
University of Massachusetts, 1998.

[15] G. Salton and C. Buckley. Term-weighting ap-
proaches in automatic text retrieval.
Informa-
tion Processing and Management, 24 (5):513–523,
1988.

[16] F. Seabastiani. Machine learning in automated
text categorization. ACM Computing Surveys,
34 (1):1–47, 2002.

[17] D. A. Smith. Detecting and browsing events in
unstructured text.
In Proceedings of the 25th
ACM SIGIR International Conference on Re-
search and Development in Information Retrieval
(SIGIR’02), 2002.

[18] M. Spitters and W. Kraaij. TNO at TDT2001:
In
Language model-based topic detection.
2001 Topic Detection and Tracking Workshop
(TDT’01), 2001.

[19] R. C. Swan and J. Allan. Extracting signiﬁcant
time varying features from text. In Proceedings of
the 7th International Conference on Information
and Knowledge Management (CIKM’98), 1998.

