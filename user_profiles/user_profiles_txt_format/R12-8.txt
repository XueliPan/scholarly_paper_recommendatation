6

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 1,

JANUARY 2006

Text Classification without Negative

Examples Revisit

Gabriel Pui Cheong Fung, Jeffrey X. Yu, Member, IEEE Computer Society,

Hongjun Lu, and Philip S. Yu, Fellow, IEEE

Abstract—Traditionally, building a classifier requires two sets of examples: positive examples and negative examples. This paper
studies the problem of building a text classifier using positive examples (P ) and unlabeled examples (U). The unlabeled examples are
mixed with both positive and negative examples. Since no negative example is given explicitly, the task of building a reliable text
classifier becomes far more challenging. Simply treating all of the unlabeled examples as negative examples and building a classifier
thereafter is undoubtedly a poor approach to tackling this problem. Generally speaking, most of the studies solved this problem by a
two-step heuristic: First, extract negative examples (N) from U. Second, build a classifier based on P and N. Surprisingly, most studies
did not try to extract positive examples from U. Intuitively, enlarging P by P 0 (positive examples extracted from U) and building a
classifier thereafter should enhance the effectiveness of the classifier. Throughout our study, we find that extracting P 0 is very difficult.
A document in U that possesses the features exhibited in P does not necessarily mean that it is a positive example, and vice versa.
The very large size of and very high diversity in U also contribute to the difficulties of extracting P 0. In this paper, we propose a labeling
heuristic called PNLH to tackle this problem. PNLH aims at extracting high quality positive examples and negative examples from U
and can be used on top of any existing classifiers. Extensive experiments based on several benchmarks are conducted. The results
indicated that PNLH is highly feasible, especially in the situation where jP j is extremely small.

Index Terms—Data mining, text categorization, partially supervised learning, labeling unlabeled data.

(cid:2)

1 INTRODUCTION

TEXT classification is a supervised learning task where its task

is to assign a Boolean value to each pair ðdj; kiÞ 2 D (cid:2) K,
where D is a domain of documents and K is a set of
predefined categories. The task is to approximate the true
function (cid:2) : D (cid:2) K ! f1; 0g by means of a function b(cid:2)(cid:2) :
D (cid:2) K ! f1; 0g such that (cid:2) and b(cid:2)(cid:2) coincide as much as possible
[20]. The function b(cid:2)(cid:2) is called a classifier. The coincidence is
measured by effectiveness, which is also known as the quality
of the classifier.

A classifier can be built by training it systematically using
a set of training documents D, where all of the documents
belonging to D are labeled according to K. Specifically, given
a category k 2 K, in order to build a classifier for k, we have
to determine which of the documents in D belong to k and
which of the documents in D do not belong to k. The
documents that belong to k are labeled as positive training
instances (P) and the documents that do not belong to k are
labeled as negative training instances (N ). Fig. 1a shows a
typical framework for building a classifier.

Note that the effectiveness of the classifier is strongly
affected by the precision of labeling. If the documents in (D)
are mislabeled, then the classifier built thereafter will be

. G.P.C. Fung and J.X. Yu are with the Department of Systems Engineering
and Engineering Management, The Chinese University of Hong Kong,
Shatin, New Territories, Hong Kong, China.
E-mail: {pcfung, yu}@se.cuhk.edu.hk.

. P.S. Yu is with the IBM T.J. Watson Research Center, 30 Saw Mill River

Road, Hawthorne, New York. E-mail: psyu@us.ibm.com.

Manuscript received 7 Feb. 2005; revised 9 June 2005; accepted 27 June 2005;
published online 18 Nov. 2005.
For information on obtaining reprints of this article, please send e-mail to:
tkde@computer.org, and reference IEEECS Log Number TKDE-0052-0205.

strongly biased, and poor quality must be resulted.
Unfortunately, labeling documents is a time consuming
and labor intensive task, and it
is the bottleneck of
constructing a reliable classifier.

if not

Due to the rapid growth of information available, the
task of accurately labeling all of the training documents as
positive examples and negative examples becomes very
difficult,
In order to overcome this
bottleneck, some researchers attempt to divide the training
documents D into three sets (a small set of positive
examples, a small set of negative examples, and a large set
of unlabeled examples), and then build a classifier using
these three sets of documents [1], [2], [6], [16], [26].

infeasible.

However, obtaining a set of negative examples is
sometimes still very expensive. Recently, a new direction
of building classifiers is recognized where the classifiers are
built using only a small set of positive examples (P ) and a
large set of unlabeled examples (U) [5], [9], [13], [11], [10],
[12], [23], [25], [24]. Note that no negative examples are
given. This kind of problem is sometimes known as partially
supervised learning [10]. The characteristics of this partially
supervised learning are summarized as follows: 1) The size
of the given positive examples (P ) is so small that it may not
the feature distribution of all
be possible to represent
positive examples, 2) the unlabeled examples (U) are mixed
with both positive and negative examples, and 3) no
negative example is given.

Generally speaking, the existing approaches that target
solving this problem use a two-step heuristics as shown in
Fig. 1b. In the first step, some negative examples (N) are
extracted from the unlabeled examples (U). In the second

1041-4347/06/$20.00 (cid:2) 2006 IEEE

Published by the IEEE Computer Society

FUNG ET AL.: TEXT CLASSIFICATION WITHOUT NEGATIVE EXAMPLES REVISIT

7

Fig. 1. A comparison of different learning frameworks.

step, the classifier is built1 using the given positive example,
P , and the extracted negative examples, N.

Surprisingly, none of the existing heuristics try to enlarge
the given positive examples P by extracting the positive
examples from U into P , with the one exception of our
previous work [5].2 Throughout our study, we found that
extracting P 0 from U is very difficult. The reason is that
simply comparing the differences of the feature distribu-
tions between P and U cannot help to extract P 0. A
document in U that possesses the features exhibited in P
does not necessarily mean that it is a positive example. The

1. Some of the papers (e.g., [9], [11], [13], [24]) modify the traditional
classification algorithm so as to build the classifier interactively. Details will
be discussed later.

2. Although [12] also builds a classifier by enlarging both the positive
documents, our work is very different from that because our whole learning
process is automatic, while [12] requires manual indexing.

very large size of and the very high diversity in U also
contribute to the difficulties of extracting P 0.

In this paper, we propose a labeling heuristic, called
PNLH (Positive examples and Negative examples Labeling
Heuristic) which is an extension of our preliminary work in
[5]. Fig. 1c shows the general framework of PNLH. It
consists of two steps: extraction and enlargement.

.

Step 1: Extraction. This step aims at extracting a set
of negative examples called reliable negative examples
(N) from the unlabeled examples (U). The extraction
is based on the concept of core vocabulary. This
concept is first introduced in our previous work [5].
Core vocabulary contains the features with feature
strengths greater than a threshold. However,
in
contrast
to [5] where this threshold must be
predefined, this paper proposed a new function to

8

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 1,

JANUARY 2006

compute the feature strength effectively such that
the threshold can be determined automatically. With
the help of the core vocabulary of positive examples (in
the following, we use positive features for short), we
then extract N from U.

Apart

from the issues related to computing
feature strengths effectively and determining a
threshold for the core vocabulary automatically, this
paper further addresses the question about whether
two documents should be considered as positive
examples, if both of them contain the same number of
but different positive features. Based on our extensive
experiments, we found that two documents that
have the same number of positive features do not
necessarily mean that both of them are positive
examples. In this paper, we propose a set of novel
functions to determine whether a document should
be considered as a positive example based on
ranking. Under these new proposals, we can
increase the number of reliable negative examples
and meanwhile maintain the quality of extraction.
Step 2: Enlargement. This step aims at extracting a
set of positive examples (P 0) and another set of
negative examples (N 0) from U 0 (U 0 ¼ U (cid:3) N), so as
to enlarge P by P 0 and N by N 0. It consists of
two steps:

First, we partition the reliable negative examples
(N) that are extracted in Step-1 into k clusters
(N1; N2; (cid:4) (cid:4) (cid:4) ; Nk). In contrast to [5] where the
number of partitions (k) is predefined and is the
same for all categories in the same domain, this
paper presents techniques for determining k
dynamically and automatically such that k may
vary among categories even when the categories
are within the same domain.
Second, we extract P 0 and N 0 from U 0 so as to
enlarge the P and N. In contrast to [5], where it
only considers the similarity among P , Ni, and
U 0 when performing the extraction, this paper
further takes the number of features within the
domain into consideration. We found that
reducing the number of features in this enlarge-
ment step can significantly reduce mislabeling
while still retaining as many positive and
negative examples extracted as before. The
reason why reducing the number of features
can improve the quality of extraction is due to
the fact that P and Ni are both very noisy.3
Reducing the number of features in P and Ni
can possibly diminish the noise and, therefore,
improve the quality of extraction.

.

1.

2.

Finally, we use P [ P 0 as the positive training instances
(P) and N [ N 0 as the negative training instances (N ) to
build the classifier. Note that applying PNLH in any domain
is independent of the classifier built. We have conducted
extensive experiments to address two issues: 1) how much
PNLH can achieve on top of several representative classifiers
and 2) how much PNLH can achieve, in comparison with

3. The definition of noisy will be discussed in Section 4.3

other heuristics, on the same classifier. Our results indicated
that our approach is highly feasible even when the set of
positive examples is extremely small. We do not need to
build any complex classifier interactively as those reported
in [9], [11], [13], [24].

The rest of the paper is organized as follows: Section 2
presents PNLH in detail. Section 3 reviews the major
heuristics that tackle the problem of partially supervised
learning. Section 4 evaluates our work. We summarize and
conclude this paper in Section 5.

2 PNLH: POSITIVE EXAMPLES AND NEGATIVE

EXAMPLES LABELING HEURISTICS

The overview of
the Positive examples and Negative
examples Labeling Heuristic (PNLH) is shown in Fig. 1c
and Algorithm 1. PNLH consists of two steps: Extraction
and Enlargement. The objective of Extraction is to extract a
set of reliable negative examples (N) from the unlabeled
examples (U) (line 1 of Algorithm 1). The objective of
Enlargement is to further extract positive examples (P 0) and
negative examples (N 0) from U (cid:3) N (line 3 of Algorithm 1),
so as to enlarge P and N.

Algorithm 1. PNLH(P , U).
Input: P (positive examples) and U (unlabeled examples)
Output: P (positive training examples) and N (negative

training examples)

1. N   ExtractReliableNegative(P , U);
2. U 0   U (cid:3) N;
3. obtain P 0 and N 0 by calling Partition(P , N, U 0);
4. P   P [ P 0;
5. N   P [ P 0;
6. return P and N

2.1 Extracting Reliable Negative Examples
In the absence of any prior knowledge about the character-
istics of the negative examples, the best way to extract a set
of negative examples is to use the differences of the feature
distributions between the given positive examples (P ) and
the unlabeled examples (U). In the following, we call the
negative documents extracted from U reliable negative
examples and denote them by N. We will show how reliable
N is in the experimental studies in Section 4. There are
two main procedures in this step, namely,
identifying
positive features and extracting reliable negative examples.

2.1.1 Identifying Positive Features
Positive features are the features that frequently appear in
P and can represent P well. We identify the positive
features based on a notion called core vocabulary. Note that
a document that belongs to P must possess some of the
features that are contained in the core vocabulary of P ,
denoted by VP (Remark 2.1). According to VP , we extract
the reliable negative examples (N) from the unlabeled
examples (U).

Remark 2.1. A document

that belongs to the positive
examples (P ) must possess some of the features that are
contained in the core vocabulary of P (VP ).

FUNG ET AL.: TEXT CLASSIFICATION WITHOUT NEGATIVE EXAMPLES REVISIT

9

We have to stress that we are not trying to identify all
negative examples in U simply using the concept of core
vocabulary. We only aim at identifying some negative
examples that are reliable in this step. In fact, it is impossible
to extract all negative examples by simply comparing the
feature distributions, as the feature distributions in the text
domain are sparse [19].

The core vocabulary of P , VP , is identified by comparing
the feature strengths among the features that are exhibited in
P . Without loss of generality, let HðfjÞ be a function to
compute the feature strength of a particular feature fj. A
feature fj is assigned to VP , i.e., fj 2 VP , if HðfjÞ is greater
than a threshold (cid:3). A feature with a higher feature strength
indicates that it is more effective in terms of classification. In
the following, we give the details of how to compute the
function HðfjÞ and show how to identify the threshold (cid:3). It
is important to know that users do not need to specify such
(cid:3) explicitly.

Remark 2.2. Let HðfjÞ be a function that computes the
feature strength of a particular feature fj, such that
HðfaÞ > HðfbÞ if and only if fa is more effective (higher
discriminative power) than fb in terms of classification.
Furthermore, fj 2 VP if and only if HðfjÞ > (cid:3) for (cid:3) 2 <.

Techniques for approximating feature strength, HðfjÞ,
can be found from probabilistic theory, such as information
gain, (cid:4)2, odd ratio, and mutual information [20]. Unfortu-
nately, none of them is suitable to solve our problem. The
reason is that we do not have any prior knowledge about
the feature distribution between the positive examples and
negative examples over the entire document domain.
Furthermore, it is impossible to obtain or estimate these
values. As a result, obtaining the feature strengths is a very
difficult and nontrivial task because we cannot use any of
the existing methods to compute it.

In this paper, we approximate HðfjÞ by proposing a
formula that can utilize the information presented in P and
U. Let nP ðfjÞ and nU ðfjÞ denote the number of documents
that contain fj in P and U, respectively. HðfjÞ is computed
by measuring the differences of the normalized document
frequency between P and U:4

HðfjÞ ¼

nP ðfjÞ (cid:3) minP
maxP (cid:3) minP

(cid:3)

nU ðfjÞ (cid:3) minU
maxU (cid:3) minU

;

ð1Þ

where maxP and minP are the maximum value and
minimum value of nP ðfjÞ for fj 2 P . A similar formula
applies to maxU and minU .

In (1), the first component and the second component
correspond to the normalized number of documents that
contain fj in P and U, respectively. Although some of the
existing works (e.g., [23], [24]) also compute HðfjÞ based on
some newly-defined formulas, none of them pays attention to
the significant of normalization. However, normalization is
critical. This is because the differences between jP j and jUj are
extremely large (jP j (cid:5) jUj), such that only computing

normalization in (1) makes it possible to compare the true
relative frequencies of features between P and U.

if

Moreover, (1) is robust since it captures the idea that a
feature is more important
its distribution is highly
skewed toward P . To be more specific, if the normalized
frequency of a feature in both P and U is similar, then its
feature strength will approach to zero (i.e., HðfjÞ ¼ 0). If it is
skewed toward one set, its feature strength will be either
strictly positive (if its distribution is skewed to P ) or strictly
negative (if its distribution is skewed to U).

Recall that VP is identified by extracting fj such that
HðfjÞ > (cid:3). Here, we determine (cid:3) as the average of HðfjÞ for
fj 2 P , i.e.:

(cid:3) ¼

1
NP

X

fj2P

HðfjÞ;

ð2Þ

where NP is the number of different features in P .

2.1.2 Extracting Reliable Negative Examples
We have shown how to compute HðfjÞ and determine (cid:3) so
as to select a set of positive features for formulating the core
vocabulary of the positive examples (VP ). Another impor-
tant issue is, given a document, how many positive features
should this document have in order to consider it a
potential positive example? In other words, what are the
number of positive features that a reliable negative example
in U should not exceed?

The choice of such a number is sensitive as it varies from
different documents in U. There should not exist such a
single number that applies to all documents in U and for all
different categories.
then,
although the precision of extracting N will be high, the
recall would be very low, i.e., the size of N would be very
small. This, in turn, makes the effort of this step ineffective.
In contrast, if we use a large number, then the recall would
be high, but the precision would be very low.

If we use a small number,

In sight of this, we propose a ranking-based approach for
determining whether a document in U should be regarded
as a reliable negative example. Consider two documents, di
and dj, such that both of them contain the same number of
features in VP . If the document di contains features with
higher rank in VP than dj does, we say that di is more likely
to be a positive example than dj. Based on the ranking of the
feature strengths using (1), we assign a unique value, (cid:5)ðfjÞ,
to every feature in VP . We call (cid:5)ðfjÞ the positive referencing
power. The positive referencing power is computed based
on the standard exponential distribution [15]:

(cid:5)ðfjÞ ¼

e(cid:3) 1

jVP jr; fj 2 VP ;

1

jVP j

ð3Þ

where r is the rank of
in VP . Note that
0 < (cid:5)ðfjÞ < 1. Given a document, di, let GðdiÞ be the positive
referencing power of di. It is computed by simply averaging
all of the (cid:5)ðfjÞ in di, i.e.:

feature fj

GðdiÞ ¼

(cid:5)ðfjÞÞ for all fj 2 di;

ð4Þ

1
m

X

j

4. It is based on the observation that the number of positive examples is

small when the diversity is high in a large U.

where m is the number of positive features in di. A
document, di, is considered a reliable negative example if

10

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 1,

JANUARY 2006

Fig. 2. When P is enlarged, the recall of the classifier would be increased. (a) Without enlarging P . (b) Enlarging P with P 0.

GðdiÞ < (cid:2). Here, (cid:2) is defined as the average of GðdiÞ for all
documents in P :

2.2 Enlarging the Sets of Positive Examples and

Negative Examples

(cid:2) ¼

GðdiÞ

Þ:

1
jP j

jP j
X
ð

i¼0

Remark 2.3. Using both Hð(cid:4)Þ and Gð(cid:4)Þ together will extract a
larger but higher quality set of reliable negative examples
than with Hð(cid:4)Þ alone. This is because Gð(cid:4)Þ allows some
documents which contain some higher ranked features in
VP to be negative examples while excluding those
documents which contains very many lower ranked
features in VP in the set of negative examples.
The ExtractReliableNegative algorithm, which aims at
extracting N, is outlined in Algorithm 2. It takes two inputs,
a small set of positive examples (P ) and a large set of
unlabeled documents (U). In Lines 1-3, it computes HðfiÞ (1)
for all features, fi, belonging to P . In Line 4, it computes the
threshold (cid:3) using (2). The core vocabulary of P (VP ) is
determined in Lines 5-10. In Line 11,
it computes the
threshold (cid:2) (5). Finally, the reliable negative examples are
extracted in Lines 13-17, and the result is returned in Line 18.

compute HðfjÞ using (1);

Algorithm 2. ExtractReliableNegative(P , U).
Input: P (positive examples) and U (unlabeled examples);
Output: N (reliable negative examples);
1. for all fj 2 P do
2.
3. end for
4. compute (cid:3) using (2);
5. VP   ;;
6. for each fj 2 P do
if HðfjÞ > (cid:3) then
7.
8.
end if
9.
10. end for
11. compute (cid:2) using (5);
12. N   ;;
13. for all di 2 U do
14.
15.
end if
16.
17. end for
18. return N;

if GðdiÞ < (cid:2) then

VP   VP [ ffjg;

N   N [ fdig;

ð5Þ

In the previous section, we discussed how to extract the
reliable negative examples (N) from the unlabeled exam-
ples (U) based on the positive examples (P ). In this section,
we present how to extract positive examples (P 0) and
another set of negative examples (N 0) from U 0, where
U 0 ¼ U (cid:3) N. By doing so, we can enlarge the given P by P 0
and the previously extracted N by N 0. The enlargement is
motivated by the following two observations:

.

.

If jP j is small, the number of positive examples for
training is inadequate. This is because P cannot
reflect the true feature distribution of all the positive
examples in the domain. Enlarging the total number
of positive examples will increase the recall of the
classifier. Fig. 2 illustrates this idea. When P is
enlarged, the situation where the positive examples
are wrongly being classified as negative examples
(Fig. 2a) can be possibly be corrected (Fig. 2b).
If jP j is large, jVP j will be large, and the number of
reliable negative examples (N) that is extracted in
the previous step will be small. Enlarging the total
number of negative examples will
increase the
precision of the classifier. Fig. 3 illustrates this idea.
When N is enlarged,
the situation where the
negative examples are wrongly being classified as
positive examples (Fig. 3a) can possibly be corrected
(Fig. 3b).

In text domains, the distribution of features are very
sparse. Typically, the number of features can be ranged
from 10,000 to 100,000, where the common features among
documents are very few. Hence, even if we only enlarged P
and N by few examples, the entire space occupied by the
positive and negative examples may change significantly. In
short, enlarging P and/or enlarging N properly will
improve the quality of classification, as the boundary
between the positive examples and negative examples will
be changed dramatically.

Unfortunately, properly extracting P 0 and N 0 from U 0 is a
very challenging task. The difficulty in extracting a proper
set of P 0 from U 0 is caused by the fact that a document in U 0
that possesses some features in VP may not necessarily be a
positive example. In addition, the proportion of P 0 in U 0 is
usually very small, which makes extracting P 0 even harder.

FUNG ET AL.: TEXT CLASSIFICATION WITHOUT NEGATIVE EXAMPLES REVISIT

11

Fig. 3. When N is enlarged, the precision of the classifier will be increased. (a) Without enlarging N. (b) Enlarging N with N 0.

We will discuss this issue in the following sections in more
detail. In the following sections, we first discuss some
possible but inappropriate approaches to extract P 0. Then, we
present our solution which is based on a partition strategy.

2.2.1 Possible but Inappropriate Approaches
In this section, we discuss three intuitively possible but
inappropriate approaches for extracting P 0 and N 0. We
summarize them below and discuss them in detail in the
following sections.

.

. Reverse Extraction Approach (RE): This approach
adopts the same idea as described in the previous
section, but implements in a reverse direction. It first
constructs a core vocabulary for the reliable negative
examples, VN , from N, and then extracts P 0 from U 0
such that none of the documents in P 0 possesses the
features that are listed in VN .
Simple Text Classifier Construction Approach
(TC): This approach first builds a traditional text
classifier based on P and N, and then classifies U 0.
The documents that are classified as positive are
regarded as P 0, and the documents that are classified
as negative are regarded as N 0. This approach is
reported in [11], [10], [13], [24] for extracting N 0, but
not P 0 as we claim that it cannot extract a high
quality of P 0.
Profile-Based Approach (PB): This approach first
constructs a profile for each class, P and N. Given a
document, d, in U 0, in order to classify whether d
belongs to P 0 or N 0,
it compares the distance
between d and the profile of P against the distance
between d and the profile of N. This technique is
adopted in [10], [11] for extracting N 0, but not P 0.

.

2.2.2 Reverse Extraction Approach (RE)
RE is inappropriate for extracting both P 0 and N 0. Note that
U consists of many different topics (categories) and does not
focus on one single topic.
text classification
problems, the level of diversity is always very high. Since
N (cid:6) U, N inherits this property. As every topic in N
contains its own set of core vocabulary, a large number of
different topics in N cancels the significance of each others’
core vocabulary. Eventually, none of the features in N can be
properly selected or the features selected are of poor quality.

In most

2.2.3 Simple Text Classifier Construction Approach (TC)
TC can possibly extract a high quality of N 0, but
is
impossible to extract a high quality of P 0. Our argument
is based on our observations on two major kinds of
classifiers: kernel-based classifier, like SVM and regression
models, and instance-based classifier, like kNN.

. Kernel-Based Classifier. Kernel-based approaches
may wrongly classify many negative examples into
P 0. The reasons are as follows: Recall that N consists
of diverse topics and covers a very large region in the
feature space. On the other hand, P only focuses on
one single topic and covers a much smaller region in
the same feature space. When jP j is small, P may be
inadequate for reflecting the true feature distribution
of all positive instances in the domain. To be more
specific, let us take SVM as an example. Figs. 4a and
4b show how SVM constructs its decision function by
maximizing the boundary between positive training
instances and negative instances and minimizing the
errors. In both figures, N (the crosses) are the same,
but P (the black dots) are different. Note that jP j in
Fig. 4a is smaller than that in Fig. 4b. Let us refer to
Fig. 4a first. When the number of positive training
instances is inadequate, SVM may generate a wrong
decision function (boundary) and cannot adjust it to
the correct one, as shown in Fig. 4b. This explains
why a kernel-based classifier always extracts P 0 with
high recall but very low precision, whereas it extracts
N 0 with high precision but low recall.
Instance-Based Classifier. Although instance-based
like kNN [21], [22], do not relay on
classifiers,
statistical distribution of the training data,
they
cannot extract a good set of positive examples, P 0.
Recall that a document containing some features that
are listed in VP cannot imply that it belongs to P .
According to the probabilistic nature of feature
distributions, any two documents may share a high
degree of similarity even though they belong to
different categories. Two documents can often be the
nearest neighbor but do not belong to the same
category. Table 1 shows the percentage of documents
that their k nearest neighbors belong to different
categories using three benchmarks as reference.5

.

5. We will discuss these benchmarks in Section 4. We use the cosine

measure for computing their similarity.

12

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 1,

JANUARY 2006

Fig. 4. The decision boundaries of SVM. (a) Small P (Inadequate). (b) Large P (Adequate)

In summary,

Although the percentage may vary from data set
to data set,
it does confirm the aforementioned
nearest neighbor behavior. This is why the thresh-
old k in kNN will never be set to a small value in
text classification. Typically, k is around 30-50 [7],
[21], [22].
in order to construct a
kNN classifier, we have first to define a proper k,
and then tune a proper threshold (cid:2) as the decision
function of
these
two issues are both difficult to answer in partially
supervised learning problem. First, a large k is not
good in our problem setting because jP j is always
small. A small k may obtain a poor result for kNN.
Second, if jP j is small, obtaining a validation data
set is impossible. Hence, tuning a proper (cid:2) is very
difficult as overfitting will occur.

the classifier. Unfortunately,

2.2.4 Profile-Based Approach (PB)
PB, such as Centroid-based [20] or Rocchio [17], can
possibly solve the problem of kNN. However, PB is still
inappropriate for extracting a high quality of P 0. To
formalize our arguments, let us consider two profiles, CP
and CN , which denote the documents in P and N,
respectively. Suppose that we use cosine coefficient for
measuring the similarity between a document, d, and a
profile, Ci for i 2 fP ; Ng:

Sðd; CiÞ ¼

d (cid:4) Ci

k d k (cid:4) k Ci k

for i 2 fP ; Ng:

ð6Þ

Note that, for most applications, d will be normalized to
unit length so as to account for the length of different
documents [20], i.e., k d k ¼ 1. For simplicity, let us use the
centroids of the positive examples and negative examples to
denote their corresponding profiles, i.e.:

CP ¼

di;

CN ¼

ð7Þ

1

X

j P j

di2P

1

X

j N j

di2N

di:

TABLE 1

Percentage of Documents whose First Five Nearest

Neighbors Belong to Different Categories

Let us consider Sðd; CN Þ; its numerator (6) now becomes:

d (cid:4) CN ¼

Sðd; diÞ:

ð8Þ

1

X

j N j

di2N

In short, the similarity between d and CN , Sðd; CN Þ, is
nothing more than the average similarity between d and all
documents in N divided by kCNk (k d k¼ 1). As N is large
and consists of many diverse topics, obviously, d can never
be similar to all documents in N. Thus, Sðd; CN Þ will be
extremely small. This explains why the precision of P 0
extracted from U 0
is very low. The same observation is
applied to other coefficients for computing similarity, such
as the Jaccard coefficient or the Dice coefficient.

Similarly, for Rocchio, it also suffers from the above
difficulties. In conclusion, it is difficult to extract P 0 from U 0
by building any kind of classifier simply using P and N
directly.

2.2.5 A Feasible Partition-Based Approach
In this section, we present the second step of PNLH: A
partition-based approach to extract P 0 and N 0 from U 0.
Recall that N consists of diverse topics. It is this diversity
that makes all of the approaches that we discussed so far
impractical to extract P 0 and/or N 0 from U 0. The key issue is
therefore finding a way to deal with the problem of
diversity. In the following, we will discuss how to solve
this problem using three main procedures, namely, parti-
tioning the reliable negative examples, assigning docu-
ments to P 0 and N 0, and iterative enlargement.

2.2.6 Partitioning the Reliable Negative Examples
The partition-based approach further extends the idea of the
profile-based approach by partitioning N into k partitions,
N1; N2; (cid:4) (cid:4) (cid:4) ; Nk, such that the documents in each partition
share a high degree of similarity. By doing so, each partition
focuses on a smaller set of more related features. In order to
extract P 0 and N 0 from U 0, we compare the similarity of the
documents in U 0 with the centroid of P (CP ) and the centroid
of Ni (CNi ) for i ¼ 1; 2; (cid:4) (cid:4) (cid:4) ; k.

For partitioning the reliable negative example, N, we
can apply any kind of the existing clustering algorithm. In
the paper, for simplicity, we adopt the classical k-means
clustering algorithm [3],
[8]. A common question
regarding clustering is: What
is the best number of
clusters, k? If k is too large, the problem that rises in the

[4],

FUNG ET AL.: TEXT CLASSIFICATION WITHOUT NEGATIVE EXAMPLES REVISIT

13

instance-based approach appears. If k is too small, the
problem that rises in the profile-based approach appears.
Suppose that an optimal number of partitions, k, does
exist. This optimal number should vary from one domain to
others. Furthermore, even in the same domain, different
categories should have different optimal values.

p

In this paper, we determine k based on the size of the
given positive examples (P ) and the reliable negative
examples (N) such that k ¼ ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
jNj=jP j
. The idea behind this
is that k needs to be controlled in a way as to maintain a
reasonable number of documents in clusters Ni. If k is large
as jNj=jP j, the number of documents in some cluster Ni may
be too small. Such a value is confirmed based on our
extensive testing results. Some observations are given below.
Consider two categories, x and y. x and y have Px and Py
positive examples and Nx and Ny extracted reliable negative
examples, respectively. Suppose jPxj (cid:7) jPyj. Then, the set of
extracted reliable negative examples, Nx, is much smaller
than Ny, i.e., jNxj < jNyj. Specifically, if jP j is small, then VP
will be small, which, in turn, makes Ni become larger.
Similarly, if jP j is large, then VP will be large, which, in turn,
makes jNij become smaller. Consequently, the diversity in
Nx is smaller than that of Ny. Therefore, we should set a small
k to partition Nx and a large k to partition Ny. It leads to the
following remark: If jP j is large and jNj is small, then k
should be small, whereas if jP j is small and jNj is large, then
k should be large. The above equation reflects the relation-
ship among jP j, jNj, and k, and is shown to be very effective
in our performance studies.

2.2.7 Assigning Documents to P 0 and N 0
Without loss of generality, suppose we have partitioned the
set of reliable negative examples, N, into k clusters, N1, N2,
(cid:4) (cid:4) (cid:4) ; Nk, as discussed above using a clustering algorithm. In
this section, we show how to assign documents from U 0 to
P 0 as in N 0, so as to enlarge the total positive training
instances P and the total negative training instances N
(Fig. 1c). Before we continue our discussion, let us stress
that, if a document is assigned to Ni, the document will be
assigned to N 0.

The assignment of a document d 2 U 0 to either P 0 or N 0
cannot be done by simply comparing the similarities
between d and CP and between d and CNi using (6). Recall
that CP and CNi are centroids for P and Ni. In other words,
we cannot say that d is a positive example if Sðd; CP Þ is less
than the closest of Sðd; CNi Þ for any cluster Ni using (6), and
we cannot say that d is a negative example if Sðd; CP Þ is
greater than the closest of Sðd; CNi Þ for any cluster Ni using
(6). The reason is that, in terms of similarity measurement, d
may be similar to both P 0 and some Ni. What we need is to
extract the document d which is significantly similar to either
P or some Ni. Documents which are similar to both P and
some Ni are regarded as ambiguous and will be ignored.

We propose the following strategy: A document d in U 0

belongs to P 0 if it satisfies the following conditions:

Sðd; CP Þ > (cid:6)P

MP ðdÞ ¼ Sðd; CP Þ (cid:3) max Sðd; CNi Þ > (cid:7)P ;

ð9Þ
ð10Þ

where (cid:6)P is the average similarity of the documents within
P and (cid:7)P is the average difference between the documents
in P and the closest Ni ð(cid:8) NÞ:

(cid:6)P ¼

Sðdi; CP Þ

1
jP j

1
jP j

X

di2P
X

(cid:3)

di2P

(cid:7)P ¼

Sðdi; CP Þ (cid:3) max
j¼1;(cid:4)(cid:4)(cid:4);k

Sðdi; CNj Þ

ð11Þ

ð12Þ

(cid:4)

:

Note that, from the above equations, Sðd; CP Þ > (cid:6)P can
guarantee d is similar to P , while MP ðdÞ > (cid:7)P can ensure d
is not similar to both P and N. In a similar fashion, a
document d in U belongs to N 0 if and only if the following
conditions hold:

Sðd; CNi Þ > (cid:6)Ni ; for some i

MNi ðdÞ ¼ Sðd; CNi Þ (cid:3) Sðd; CP Þ > (cid:7)N ; for some i;

where

(cid:6)Ni ¼

Sðdi; CNi Þ

1

X

jNij

di2Ni

(cid:7)N ¼

1
k

k

X

i¼0

1

X

(cid:3)

jNij

dj2Ni

Sðdj; CNi Þ (cid:3) Sðdj; CP Þ

ð16Þ

(cid:4)

:

ð13Þ
ð14Þ

ð15Þ

The above strategy works well when the size of the
positive examples (P ) is of reasonable size. However, the set
of positive examples may vary greatly:

.

.

jP j

is very small

(say, around 10),

If
then the
similarity among the documents in P will be very
high. Consider an extreme case. If jP j ¼ 1, then the
similarity will be 1, which implies (cid:6)P will be 1 and
(cid:7)P will be very large. As a result, no documents in U 0
will be selected as positive. The same observations
can be made for Ni.
If jP j is large, then P will contain many different
features because the distribution of features are
always sparse in text domains. The chance that a
document processes some positive features but not a
positive example increases. Moreover, the similarity
within P will be very low, which, in turn, makes
both (cid:6)P and (cid:7)P very small. Many documents in U 0
will be very similar to P and/or Ni. Eventually,
ambiguity appears. The same observations can be
made for Ni.

Mathematically, following (7)-(6), we have the following:

Sðd; CP Þ ¼

Sðd; djÞ

1

1

(cid:4)

1
jP j

X

dj2P

jjCP jj

1

X

(cid:4)

jjCNi jj

jNij

dj2Ni

Sðd; CNi Þ ¼

Sðd; djÞ:

ð17Þ

ð18Þ

It is important to notice that both jNij and jjCNi jj in the
above equations are dominate factors as the features are
always sparse in nature. As a result, the summation always
has a small value. Thus, the comparison between d and CP as
well as between d and CNi is most likely to be affected by
the size of each feature set, rather than the features
exhibited in P and Ni.

The above suggests that we cannot follow (7) to construct
the centroids of CP and CNi by simply averaging the
weights of the features in the corresponding P or Ni. In this

14

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 1,

JANUARY 2006

paper, we propose an efficient and effective approach to
construct the centroids, CP and CNi , by only using a set of
representative features in the corresponding centroid.
Consequently, these centroids will be more dense, and this
makes comparison of similarity between a document d and
the centroid CP or between a document d and the centroid
of CNi more efficient. Our strategy is given in the following
remark.

Algorithm 3 shows the steps for constructing CP and CNi .
First, in Line 1, it selects the top n representative features
from P [ N. In this work, we set n ¼ 3; 000 based on a series
of testing for selecting features in the range from 1; 000 to
5; 000. More details about the issues of feature selection
would be discussed in our experimental studies in Section 4.3.
The centroid CP is those top n features that exhibit in P
(Lines 3-6). The centroid CNi is those top n features that do not
belong to CP but belong to Ni (Lines 7-15).

Algorithm 3. Centroid(P , N0; N1; . . . ; Nk).
Input: P (positive documents), N0; N1; . . . ; Nk (negative

documents in different partitions);

Output: CP (centroid of the positive class) and

CN0 ; CN1 ; . . . ; CNk (negative centroids in different
partitions);

1. L   top n features ranked by information gain from

P [ N;

2. CP   ;;
3. for each fi 2 P do
4. CP   CP [ ffig
5. end for
6. construct the weights of CP by averaging the weights of

the features in P ;

for each fj 2 L do
if fj 2 Ni then

7. for all Ni; i 2 f1::kg do
8. CNi   ;;
9.
10.
11.
12.
13.
14.

end if

CNi   CNi [ ffjg;

end for
construct the weights of CNi by averaging the weights
of the features in N;

15. end for
16. return CP and CN0 ; CN1 ; . . . ; CNk ;

2.2.8 Iterative Enlargement
The whole enlargement process for extracting P 0 and N 0
from U 0 is outlined in the Partition algorithm (Algorithm 4).
This algorithm repeatedly extracts negative examples from
U 0 until no more documents in U 0 (Lines 2-13) can be
extracted. After it has extracted all N 0, it begins to extract P 0
from U 0 (Lines 15-19). We can obtain a set of positive
training instances (P) by merging the given P and the
extracted P 0, and a set of negative training instances (N ) by
merging the two sets of extracted negative documents,
N [ N 0. According to P and N , a classifier can be built.

Algorithm 4. Partition(P , N, U 0).
Input: P (positive documents), N (negative documents),

and U (unlabeled documents);

Output: P 0 (positive documents) and N 0 (negative

documents);

1. P 0   ;; N 0   ;;

2. repeat
3.
4.

5.
6.
7.
8.

partition N [ N 0 into k clusters, N0; N1; . . . ; Nk;
obtain CP and CN0 ; CN1 ; . . . ; CNk , by calling
Centroid(P ; N0; N1; . . . ; Nk);
U 0   U (cid:3) N 0;
i ¼ 0;
for all d 2 U 0 do

if d does not satisfy the conditions in (9) and (10)
and d satisfies the conditions in (13) and (14) then

N 0   fdg [ N 0;
i++;
end if

9.
10.
11.
end for
12.
13. until i ¼ 0
14. U 0   U (cid:3) N 0;
15. for all d 2 U 0 do
16.

P 0   fdg [ P 0;

17.
end if
18.
19. end for
20. return P 0 and N 0;

3 RELATED WORK

if d satisfies the conditions in (9) and (10) and d does
not satisfy the conditions in (13) and (14) then

This section briefly discusses the existing works that are
related to the extraction and enlargement of negative
examples. There are five major approaches for extracting
N from U given P , namely, Rocchio [10], [11], Spy [13],
PEBL [25],
[24], Weighted-Logistic Regression [9], and
Naive Bayes [12].

.

. Rocchio: This labeling heuristic is proposed in [10],
[11]. In order to extract N from U, it first builds a
traditional Rocchio classifier [17] using the given P
as the positive training example and U as the
negative training example. After that, it classifies
the documents in U using this Rocchio classifier. The
documents that are classified as negative form the
negative training examples N . In [11], Liu et al. also
show that using SVM iteratively to extract a set of
negative examples, N 0 from U 0 ¼ U (cid:3) N, can possi-
bly improve the effectiveness of the classifier.
Spy: Spy is proposed in [13]. It first randomly selects
a set of documents, S (cid:6) P , and puts them into U.
The default size of S is 15 percent of P . Here, S
serves as spy documents in U. It assumes that the spy
documents behave as similar as the unknown
positive examples in U, hoping to infer the behavior
of these unknown positive examples. It then runs an
I-EM (iterative-EM) algorithm using the set of S0
(S0 ¼ P (cid:3) S). After I-EM completes, the resulting
classifier uses a probabilistic approach based on S to
decide a threshold to identify N. In [11], Liu et al.
show that further extracting N 0 from U 0 using SVM
iteratively cannot improve the effectiveness of the
classifier.
PEBL: PEBL is proposed in [25], [24]. It first builds a
feature set, F , which contains features, fj, that occur
in P more frequently than in U. Any document in U

.

FUNG ET AL.: TEXT CLASSIFICATION WITHOUT NEGATIVE EXAMPLES REVISIT

15

that does not contain the features in F is classified as
N. In [11], Liu et al. show that further extracting N 0
from U 0 using SVM iteratively can improve the
effectiveness of the classifier when jP j is small but
deteriorate when jP j is large. However, in some
situations, iterative-SVM will improve the effective-
ness of the classifier when jP j is large but deteriorate
when jP j is small. Heterogeneous Learner (HL) [23]
uses a similar but more sophisticated approach for
Web page classification. However, we do not include
it in the experimental studies as we cannot get a
satisfactory result if we use HL.

. Weighted Logistic Regression: This heuristic is
proposed in [9]. It uses a logistic regression on the
weighted examples together with a performance
measure that is estimated from both the labeled
positive documents and the unlabeled documents so
as to select a regularization parameter for a valida-
tion set. The authors claim that this heuristic can be
applied in any data set, not just text data.

. Naive Bayes: This heuristic is proposed in [12].
Instead of labeling the documents, this heuristic
labels a set of representative words for each category
by human experts. It then uses these words to extract
a set of documents for each category from a set of
unlabeled documents to form the initial training set.
EM is then applied to build the final classifier. Since
this heuristic requires significant human indexing
and the evaluation is subjective, we do not include it
in our experimental studies.

4 EXPERIMENTAL STUDIES
Extensive experiments are conducted to verify the effective-
ness of PNLH. For the document preprocessing, punctuation,
numbers, Web page addresses, and e-mail addresses are
removed. All features are stemmed and converted to lower
cases. Features that only appear in one document are ignored.
All features are weighted using the traditional tf (cid:4) idf schema
[18] and normalized to unit length [20]. Different versions of
the tf (cid:4) idf schema are implemented and their influences are
insignificant or none. Due to the space limited, for evaluating
the effectiveness of the classifier, we only report the micro-F1
value [20], which is a balance between the precision and
recall. Unless otherwise specified, we used the following
settings by default:

1.
2.
3.

4.

For SVM, we used linear kernel with C ¼ 1:0:
For Naive Bayes, we use multinomial version.
For Rocchio, we turn the values of (cid:8) and (cid:9) to its
optimal values.
Feature selection is applied to Naive Bayes and
Rocchio, and we report the best F1 score.

4.1 Experiment Setup
The following benchmarks are used:

. Reuters-21578: Following the recent trend, we use the
ModApte split to separate the data into a training set
and a evaluation set. Note that Reuters-21578 is highly
skewed. The largest category contains 2,877 training
examples, while the smallest category contains
one training example. Thus, we decide to have the
following settings for this benchmark:

-

-

Reuters-21578 (10): We select the top 10 largest
categories for training and evaluation. This is
one of the most popular ways for text classifica-
tion evaluation [14], [7].
Reuters-21578 (90): We use all of the 90 cate-
gories for training and evaluation. This is the
standard method used in most
text mining
evaluation tasks.

. Newsgroup-20: There is a total of 20 different
categories. Each category contains about 1,000 mes-
sages. Note that the number of messages in each
category is nearly the same. There are 19,126 mes-
sages assigned to a single category and 320 messages
assigned to multiple categories. For each category,
we randomly select 80 percent of the messages as
training data and the remaining 20 percent as testing
data.

. Web-KB: There are 8,282 Web pages and seven cate-
gories. Each Web page belongs to one and only
one category. Web-KB is slightly skewed. The largest
category contains 3,764 Web pages, whereas the
smallest category contains 137 Web pages. For each
category, we randomly select 80 percent of the Web
pages as training data and the remaining 20 percent as
testing data.

For each benchmark, we conduct the following experiments:
For each category k in the target benchmark, we randomly
pick out x percent of the documents that belong to the
category k and use these documents to form the positive
examples, P . The remaining training documents in the
benchmark are regarded as unlabeled examples, U. The
experiment is repeated n number of times, and we report the
average of the results. Here, n is 30 and x is ranged from
1; 2; . . . ; 9 as small cases, and from 10; 20; . . . ; 100 as large
cases. Note that, when x ¼ 100 percent, all positive examples
are used, which is the benchmark case that the benchmark is
designed for. In total, there are 19 cases (nine small cases,
nine large cases, and one benchmark case).

4.2 PNLH versus without PNLH
In this section, we evaluate the effectiveness of PNLH in
labeling the unlabeled training examples using three popular
classifiers: SVM, Rocchio, and Naive Bayes (NB). Given a set
of positive examples (P ) and a set of unlabeled documents
(U), for each classifier, we compare two cases: using PNLH
versus not using PNLH. For the former, the positive examples
and negative examples to each of the three classifiers are P
and N . For the later, the positive examples and negative
examples to each of the three classifiers are P and U.

In each table,

Tables 2, 3, 4, and 5 show the results using these
the first column gives the
benchmarks.
number of percentage of the total number of positive
examples used. The second three columns show the
accuracy of the three classifiers when PNLH is not used,
and the last
three columns show the accuracy of the
three classifiers when PNLH is used. As shown in these
tables, the classifiers that use PNLH for labeling the training
examples show higher accuracy, in particular, when the
number of positive training examples becomes fewer.

All three classifiers with PNLH outperform the cases
without PNLH up to x ¼ 50 percent. The only exception is in
Table 5 with Newsgroup-20; when x ¼ 1 percent, Rocchio
obtained 0.001 without PNLH, in comparison to 0 with PNLH.

16

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 1,

JANUARY 2006

TABLE 2

Results of Reuters-21578 (10)

TABLE 4

Results of WebKB

TABLE 3

Results of Reuters-21578 (90)

TABLE 5

Results of Newsgroup-20

Note that the capability of Rocchio and NB to tolerant
noise6 is much better than SVM.
In general, Rocchio
performs acceptably in the large cases ((cid:9) 40 percent).
SVM’s accuracy deteriorates significantly even if
the
number of positive examples decreases slightly.

(all positive examples),

It is interesting to see that, with WebKB (Table 4), when
x ¼ 100 percent
the accuracy
obtained by SVM and Rocchio with PNLH perform even
better, which indicates that PNLH can possibly further
improve the classification results obtained by the traditional
classification. In traditional classification, the documents
that do not belong to a particular category are labeled as
negative training examples. However, documents that do
not belong to a particular category do not necessary fall into
the negative training set. The content of these documents

6. In general, there are two kinds of noises: noises in the feature and
noises in the training data. The former one is usually solved by applying
some feature selection techniques, such as information gain and (cid:4)2, whereas
the latest one is related to the precision of labeling the training data as
positive examples and negative examples. In this paper, unless otherwise
specified, the term of noise refers to the noise in the training data.

may be similar to the positive training examples. If we
simply put these documents into the negative training set,
the effectiveness of the classifier will possibly be affected.
When jP j is very large, the effectiveness of the classifiers
that do not use PNLH perform only marginally well. PNLH
can boost the effectiveness of the classifiers and seldom
harms it.

4.3 The Reliability of the Reliable Negative

Examples (Step 1)

Recall that Step 1 of PNLH is to extract a set of reliable
negative examples. To answer how reliable the extracted
reliable negative examples are, we use two measures,
precision and purity. By purity, we mean what percentage
of
the positive examples are mixed with the reliable
negative examples. Let P be the whole set of positive
examples. Note: P is the set of x percent sampled positive
examples and N is the set of extracted negative examples:

purity ¼ 1 (cid:3)

jP \ Nj

:

jPj

ð19Þ

FUNG ET AL.: TEXT CLASSIFICATION WITHOUT NEGATIVE EXAMPLES REVISIT

17

The Quality of the Reliable Negative Documents

TABLE 6

TABLE 7

The Significance of Step 2 of PNLH

Following the recent trend, we report the micro-Precision
and micro-Purity, which is to treat all of the categories as a
whole before computing this corresponding values. Table 6
shows the results with different benchmarks. Both precision
and purity are very high.

4.4 The Effectiveness of Enlargement (Step 2)
Recall that Step 2 of PNLH is to further extract a set of
example so as to enlarge the given positive examples and
the reliable negative examples. The extraction in Step 2 is
based on the distance between the positive examples and
the negative examples. To show the significance of the
enlargement, we compare PNLH with several cases. The
first three cases are given below.

. Case 1: Extraction only without enlargement.
. Case 2: Extraction and enlargement of positive

. Case 3: Extraction and enlargement of negative

examples only.

examples only.

Table 7 shows the results of Reuters-21578 (10) and
WebKB. All other benchmarks behave similarly. In Table 7,

the column SVM shows the result of SVM without PNLH as
the baseline for comparison. All the results of PNLH, as well
as the other three cases, are obtained using SVM after
extracting/enlarging examples. PNLH significantly outper-
forms the other three cases for the small cases. This
supports our claim and motivation for this paper, such as
whether the quality of a classifier can be enhanced by
including more positive documents in the training set,
especially when jP j is small. For Reuters-21578 (10), from
x ¼ 40 percent to x ¼ 100 percent, PNLH degrades slightly
in comparison with Case 1 (without enlargement). It is
because, as more processing is done, more errors can
possibly be introduced. However,
the differences are
marginal. Also, as shown in Table 7, when jP j is small
(say, 1 percent-50 percent), the influence of P 0
is more
important than N 0 because the Case 2 is superior to Case 3
for small cases. This suggests that enlarging P 0 is more
important than enlarging N 0 when P is small.

Furthermore, we show the significance of

feature

selection (Algorithm 3) with an additional case:

18

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 1,

JANUARY 2006

TABLE 8

The Necessity of Feature Selection

. Case 4: PNLH without feature selection. (L in line-1
of the algorithm Centroid (Algorithm 3) is the set of
all features in P [ N instead, without selection and
ranking.

We have experimented with three different kinds of the
most common feature selection techniques:
information
gain, (cid:4)2, and mutual information [20], and find that their
differences are insignificant or none. The results that we
reported in Table 8 are based on information gain. Referring
to Table 8, we can see that feature section is an important
procedure to ensure higher accuracy. Feature selection is
especially critical
in the two benchmarks: WebKB and
Newsgroup-20. The reason is that the numbers of features
in WebKB and Newsgroup-20 are significantly more than
Reuters-21578. In WebKB and Newsgroup-20, there are in
total around 50,000 and 65,000 different features, whereas
there are only around 23,000 features in Reuters-21578. The
large number of different features implies that the level of
noise presented in the data set can be higher.

4.5 Different Labeling Heuristics Comparison
The labeling heuristics are independent from the classifiers
to be used (refer to Fig. 1). In this section, we compare PNLH
with all the heuristics discussed in Section 3 using SVM as
the text classifier. The reasons that we choose SVM are three-
fold: 1) SVM does not require feature selection. Other
classifiers, such as Rocchio and NB, require feature selection
as the preprocessing step. The more preprocessing that is
done, the less we can conclude about the significance of a
specific labeling approach, as the result of that approach
may be improved/deteriorate by the preprocessing. 2) SVM
is very sensitive to noise (as shown before). Using SVM as
the classifier can give a clear picture about the precision of
different labeling heuristics. 3) SVM is the best reported
classification algorithm up-to-date. Using SVM can possibly
know how “best” a labeling heuristic can be.

In the following, we name each of the combinations as
follows: Rocchio+SVM (Roc-SVM), Spy+SVM (Spy-SVM),
PEBL+SVM (PEBL-SVM), Weighted-Logistic+SVM
(WL+SVM), and PNLH+SVM. Tables 9, 10, 11, and 12 show
the results.

TABLE 9

Results of Reuters-21578 (10)

TABLE 10

Results of Reuters-21578 (90)

FUNG ET AL.: TEXT CLASSIFICATION WITHOUT NEGATIVE EXAMPLES REVISIT

19

TABLE 11

Results of WebKB

TABLE 12

Results of Newsgroup-20

jP j ¼ 80 percent

PEBL-SVM is only acceptable (F 1 > 0:5) when jP j is large
(jP j ¼ 40 percent in Reuters-21578 (10), jP j ¼ 60 percent in
Reuters-21578 (90),
in WebKB, and
jP j ¼ 80 percent in Newsgroup-20). WL-SVM outperforms
PEBL-SVM, but is inferior to the others. Roc-SVM performs
the second best, although Spy-SVM outperforms Roc-SVM
in a few cases. PNLH-SVM outperforms all the others
significantly, even when jP j
is extremely small
((cid:10) 40 percent). It does not vary much regardless of the size
of P . In addition, PNLH-SVM performs the best on all cases
in Web-KB.

Finally, Table 13 shows some representative results of
macro-F1 for the most skewed data set: Reuters21578 (90).
Macro-F1 is usually used to evaluate a classifier against
skewed data sets. If the documents are evenly distributed
among categories, then both micro-F1 and macro-F1 would
be similar. Hence, we do not report the macro-F1 for
Newsgroup-20 and WebKB because the former one is
evenly distributed while the latter one is just slightly
skewed. As shown in Table 13, PNLH outperforms all other
approaches.

5 CONCLUSION

We present a partition-based heuristic to build a text
classifier using only positive examples (P ) and unlabeled
examples (U). No negative example is given explicitly.

Existing techniques that solve this problem only focus on
extracting negative examples (N) from U. They cannot
extract a high quality of positive examples (P 0) from U and,
therefore, cannot fully utilize the information in there.
Unfortunately, extracting P 0
from U is difficult. The
difficulty is due to the topic diversity in U. In order to
solve the diversity problem, we proposed a heuristic which
contains two steps: extraction and enlargement.

In Step 1 (extraction), we extract some negative examples
N from U based on a concept called core vocabulary and a
Gð(cid:4)Þ function. Core vocabulary is constructed based on a
concept called feature strength, which is computed based on
a Hð(cid:4)Þ function with a threshold (cid:3). The Gð(cid:4)Þ function,
together with a threshold (cid:2), is used to determine whether a
document can be considered as a potential positive example.
Note that both (cid:3) and (cid:2) are computed automatically.

In Step 2 (enlargement), we enlarge both the given P and
the extracted N by a partition-based heuristic. We partition
N into k clusters, so as to assist us extract some positive
examples (P 0) and more negative examples (N 0) from U 0
(U 0 ¼ U (cid:3) N). k is determined dynamically and automati-
cally, such that each category may have different k even in
the same domain.

We conducted extensive experiments using three
benchmarks: Reuters21578, Newsgroup20, and WebKB.
The results indicated that our approach is highly feasible
and significantly outperforms the others when jP j
is
extremely small.

Some Macro-F1 Results for Reuters21578 (90)

TABLE 13

20

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 1,

JANUARY 2006

REFERENCES
[1] D. Bennett and A. Demiritz, “Semi-Supervised Support Vector
Machines,” Advances in Neural Information Processing Systems,
vol. 11, 1998.
J. Bockhorst and M. Craven, “Exploiting Relations Among
Concepts to Acquire Weakly Labeled Training Data,” Proc. 19th
Int’l Conf. Machine Learning, 2002.
P. Bradley and U. Fayyad, “Refining Initial Points for k-Means
Clustering,” Proc. 15th Int’l Conf. Machine Learning, 1998.

[2]

[3]

[4] D.R. Cutting, D.R. Karger,

J.O. Pederson, and J.W. Tukey,
“Scatter/Gather a Cluster-Based Approach to Browsing Large
Document Collections,” Proc. 15th Int’l Conf. Research and Devel-
opment in Information Retrieval, 1992.

[5] G.P.C. Fung, J.X. Yu, H. Lu, and P.S. Yu, “Text Classification
without Negative Examples,” Proc. 21st Int’l Conf. Data Eng., 2005.
[6] R. Ghani, “Combining Labeled and Unlabeled Data for Multiclass
Text Categorization,” Proc. 19th Int’l Conf. Machine Learning, 2002.
[7] T. Joachims, “Text Categorization with Support Vector Machines:
Learning with Many Relevant Features,” Proc. 10th European Conf.
Machine Learning, 1998.

[8] B. Larsen and C. Aone, “Fast and Effective Text Mining Using
Linear-Time Document Clustering,” Proc. Fifth Int’l Conf. Knowl-
edge Discovery and Data Mining, 1999.

[9] W.S. Lee and B. Liu, “Learning with Positive and Unlabeled
Examples Using Weighted Logistics Regression,” Proc. 20th Int’l
Conf. Machine Learning, 2003.

[10] X. Li and B. Liu, “Learning to Classify Texts Using Positive and
Unlabeled Data,” Proc. 2003 Int’l Joint Conf. Artificial Intelligence,
2003.

[11] B. Liu, Y. Dai, X. Li, W.S. Lee, and P.S. Yu, “Building Text
Classifiers Using Positive and Unlabeled Examples,” Proc. Third
Int’l Conf. Data Mining, 2003.

[12] B. Liu, X. Li, W.S. Lee, and P.S. Yu, “Text Classification by
Labeling Words,” Proc. 19th Nat’l Conf. Artificial Intelligence, 2004.
[13] B. Liu, P.S. Yu, and X. Li, “Partially Supervised Classification of

Text Documents,” Proc. 19th Int’l Conf. Machine Learning, 2002.

[14] A. McCallum and K. Nigam, “A Comparison of Event Models for
Naive Bayes Text Classification,” Proc. 15th Nat’l Conf. Artificial
Intelligence Workshop Learning for Text Categorization, 1998.

[15] D.C. Montogomery and G.C. Runger, Applied Statistics and
Probability for Engineers, second ed. John Wiley & Sons, Inc., 1999.
[16] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell, “Text
Classification from Labeled and Unlabeled Documents Using
EM,” Machine Learning, vol. 39, 2000.
J. Rocchio, “Relevance Feedback Information Retrieval,” The Smart
Retrieval System—Experiments in Automatic Document Processing,
G. Salton, ed., pp. 313-323. Prentice Hall, 1971.

[17]

[18] G. Salton and C. Buckley, “Term-Weighting Approaches in
Automatic Text Retrieval,” Information Processing and Management,
vol. 24, no. 5, pp. 513-523, 1988.

[19] H. Schutze, D.A. Hull, and J.O. Pedersen, “A Comparison of
Classifiers and Document Representations for the Routing
Problem,” Proc. 18th Int’l Conf. Research and Development
in
Information Retrieval, 1995.

[20] F. Seabastiani, “Machine Learning in Automated Text Categoriza-

tion,” ACM Computing Surveys, vol. 34, no. 1, pp. 1-47, 2002.

[21] Y. Yang, “A Study on Thresholding Strategies for Text Categor-
in

ization,” Proc. 24th Int’l Conf. Research and Development
Information Retrieval, 2001.

[22] Y. Yang and X. Liu, “A Re-Examination of Text Categorization
Methods,” Proc. 22nd Int’l Conf. Research and Development in
Information Retrieval, 1999.

[23] H. Yu, K.C.-C. Chang, and J. Han, “Heterogeneous Learner for
Web Page Classification,” Proc. Second Int’l Conf. Data Mining,
2003.

[24] H. Yu, J. Han, and K.C.-C. Chang, “PEBL: Positive Example Based
Learning for Web Page Classification Using SVM,” Proc. Ninth
Int’l Conf. Knowledge Discovery and Data Mining, 2003.

[25] H. Yu, J. Han, and K.C.-C. Chang, “PEBL: Web Page Classification
without Negative Examples,” IEEE Trans. Knowledge and Data
Eng., vol. 16, 2004.

[26] T. Zhang, “The Value of Unlabeled Data for Classification

Problems,” Proc. 17th Int’l Conf. Machine Learning, 2000.

Gabriel Pui Cheong Fung received the BEng
degree (2001) and the MPhil degree (2003), both
in systems engineering and engineering man-
agement, from the Chinese University of Hong
Kong. He is currently a PhD candidate studying at
the same university. His research interests
include text mining, time series mining, Internet
applications and technologies, and database
systems. He has published various papers in
several related conferences,
including VLDB,
ICDE, and ICDM. He received the best student paper award at
PAKDD’05. He is a member of the ACM.

Jeffrey X. Yu received the BE, ME, and PhD
degrees in computer science from the University
of Tsukuba, Japan, in 1985, 1987, and 1990,
respectively. He was a research fellow (April
1990-March 1991) and a faculty member (April
1991-July 1992) at the Institute of Information
Sciences and Electronics, University of Tsuku-
ba, and a lecturer
in the Department of
Computer Science, Australian National Univer-
sity (July 1992-June 2000). Currently, he is an
associate professor in the Department of Systems Engineering and
Engineering Management, The Chinese University of Hong Kong. His
major research interests include data mining, data streaming mining and
processing, data warehouse, online analytical processing, XML query
processing, and optimization. He is a member of the IEEE Computer
Society

Hongjun Lu received the BSc degree from Tsinghua University, China,
and the MSc and PhD degrees from the Department of Computer
Science, University of Wisconsin-Madison. Before joining the Hong
Kong University of Science and Technology, he worked as an engineer
at the Chinese Academy of Space Technology, a principal research
scientist in the Computer Science Center of Honeywell Inc., Minnesota
(1985-1987), and a professor at the School of Computing at the National
University of Singapore (1987-2000). His research interests are in data/
knowledge base management systems with an emphasis on query
processing and optimization, physical database design, and database
performance. His recent research work includes data quality, data
warehousing and data mining, and management of XML data. He is also
interested in the development of Internet-based database applications
and electronic business systems.

Philip S. Yu received the BS degree in electrical
engineering from National Taiwan University,
the MS and PhD degrees in electrical engineer-
ing from Stanford University, and the MBA
degree from New York University. He is with
the IBM Thomas J. Watson Research Center
and is currently the manager of the Software
Tools and Techniques group. His research
interests include data mining, Internet applica-
tions and technologies, database systems,
multimedia systems, parallel and distributed processing, and perfor-
mance modeling. He has published more than 400 papers in refereed
journals and conferences and holds or has applied for more than 250 US
patents. He is an associate editor of the ACM Transactions on the
Internet Technology, is a member of the IEEE Data Engineering steering
the IEEE
committee, and is also on the steering committee of
Conference on Data Mining. He was the editor-in-chief of
IEEE
Transactions on Knowledge and Data Engineering (2001-2004) and
an editor, advisory board member, and guest coeditor of the special
issue on mining of databases. He has also served as an associate editor
of Knowledge and Information Systems. He has been a program chair,
cochair, or committee member on many international conferences and
has received several
including two IBM outstanding
innovation awards, an outstanding technical achievement award,
two research division awards, and the 84th plateau of
invention
achievement award. He received an outstanding contributions award
from the IEEE International Conference on Data Mining in 2003 and also
an IEEE Region 1 Award for “promoting and perpetuating numerous
new electrical engineering concepts” in 1999. He is an IBM Master
Inventor, a fellow of the ACM, and a fellow of the IEEE.

IBM honors,

