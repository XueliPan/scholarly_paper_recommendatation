Money, Glory and Cheap Talk: Analyzing Strategic

Behavior of Contestants in Simultaneous Crowdsourcing

Contests on TopCoder.com

New York University, Leonard N. Stern School of Business

Nikolay Archak

44 West 4th Street, Suite 8-185

New York, NY, 10012

narchak@stern.nyu.edu

ABSTRACT
Crowdsourcing is a new Web phenomenon, in which a ﬁrm takes
a function once performed in-house and outsources it to a crowd,
usually in the form of an open contest. Designing efﬁcient crowd-
sourcing mechanisms is not possible without deep understanding
of incentives and strategic choices of all participants. This paper
presents an empirical analysis of determinants of individual per-
formance in multiple simultaneous crowdsourcing contests using
a unique dataset for the world’s largest competitive software de-
velopment portal: TopCoder.com. Special attention is given to
studying the effects of the reputation system currently used by Top-
Coder.com on behavior of contestants. We ﬁnd that individual spe-
ciﬁc traits together with the project payment and the number of
project requirements are signiﬁcant predictors of the ﬁnal project
quality. Furthermore, we ﬁnd signiﬁcant evidence of strategic be-
havior of contestants. High rated contestants face tougher compe-
tition from their opponents in the competition phase of the contest.
In order to soften the competition, they move ﬁrst in the registra-
tion phase of the contest, signing up early for particular projects.
Although registration in TopCoder contests is non-binding, it de-
ters entry of opponents in the same contest; our lower bound esti-
mate shows that this strategy generates signiﬁcant surplus gain to
high rated contestants. We conjecture that the reputation + cheap
talk mechanism employed by TopCoder has a positive effect on al-
locative efﬁciency of simultaneous all-pay contests and should be
considered for adoption in other crowdsourcing platforms.

Categories and Subject Descriptors
H.4.m [Information Systems]: Miscellaneous; J.4 [Social and Be-
havioral Sciences]: Economics

General Terms
Measurement, Performance, Economics

Keywords
cheap talk, crowdsourcing, electronic markets, reputation, entry de-
terrence

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

1.

INTRODUCTION

Web, in general, and electronic markets, in particular, are still
evolving, periodically giving birth to new amazing market mech-
anisms like Amazon’s product review system, Google’s sponsored
search engine [8], mashups [9] and cloud computing [4]. A recent,
prominent and quite controversial example of such new mechanism
is crowdsourcing. The term “crowdsourcing” was ﬁrst used by Jeff
Howe in a Wired magazine article [14] and the ﬁrst deﬁnition ap-
peared in his blog:

Simply deﬁned, crowdsourcing represents the act of
a company or institution taking a function once per-
formed by employees and outsourcing it to an unde-
ﬁned (and generally large) network of people in the
form of an open call. This can take the form of peer-
production (when the job is performed collaboratively),
but is also often undertaken by sole individuals. The
crucial prerequisite is the use of the open call format
and the large network of potential laborers.

An important distinguishing feature of crowdsourcing, in addi-
tion to open call format and large network of contributors, is that
it blurs boundaries between consumption and production creating
a new proactive consumer type:
the “working consumer” [16].
Whether these individuals are writing a blog or a product review,
answering questions [21] or solving research problems [17], there
is a signiﬁcant economic value created by their actions; crowd-
sourcing platforms, that understand and successfully leverage this
economic value, thrive, while others vanish. Yet little we know
about incentives of “working consumers”, and even less we know
on how to shape them.

This paper presents an empirical analysis of determinants of indi-
vidual performance in crowdsourcing contests using a unique dataset
for the world’s largest competitive software development portal:
TopCoder.com. TopCoder is a crowdsourcing portal with strong
market orientation 1 : users compete to design and develop soft-
ware, which is later sold for proﬁt by the sponsoring ﬁrm; mone-
tary prizes are awarded to contestants with winning solutions. Not
amazingly, we ﬁnd that the prize amount is a strong determinant
of the individual performance in the contest after controlling for
the project complexity and the competition level. A very important
distinguishing feature of our empirical study is that we analyze ef-
fects of the reputation system currently used by TopCoder.com on
strategic behavior of contestants. In particular, we ﬁnd that strong

1TopCoder is not simply a marketplace but also a strong commu-
nity of interest. The social networking side of the platform is be-
yond the scope of this paper.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA21contestants (as measured by their reputations) face tougher compe-
tition in the contest phase, yet they strategically use cheap talk to
deter competitor entry in the contest in the project choice phase.

Our contributions are as follows:

1. The major contribution of this paper is in demonstrating that
online reputation of individuals in crowdsourcing contests
have signiﬁcant economic value and individuals may act strate-
gically in order to extract proﬁts from their online reputa-
tions.

2. The second contribution is in showing how individuals use
cheap talk in multiple simultaneous contests to enhance their
equilibrium payoffs and providing an estimate of the associ-
ated increase in user surplus.

3. The numerous minor contributions are in showing how indi-
vidual performance in a contest is related to individual and
project speciﬁc characteristics. For instance, we show that
the length of the requirements document has no signiﬁcant
effect on the contest outcome, once we control for the num-
ber of distinct requirements.

2. PRIOR RESEARCH

Although crowdsourcing attracted signiﬁcant attention in popu-
lar press, there are only few empirical studies of incentives and
behavior of individuals in crowdsourcing environments. Yang et
al. [21] examine behavior of users of the web-knowledge sharing
market Taskn.com. They ﬁnd signiﬁcant variation in the expertise
and productivity of the participating users: a very small core of suc-
cessful users contributes nearly 20% of the winning solutions on the
site. They also provide evidence of strategic behavior of the core
participants, in particular, picking tasks with lesser expected level
of competition. Huberman et al. [15] use data set from YouTube to
conclude that the crowdsourcing productivity exhibits a strong pos-
itive dependence on attention as measured by the number of down-
loads. Brabham [3] performed online survey of the crowdsourcing
community at iStockphoto. The survey results indicate that the de-
sire to make money, develop individual skills, and to have fun were
the strongest motivators for participation.

We emphasize that crowdsourcing is, by deﬁnition, related to a
single ﬁrm extracting economic value from the enterprise. In that
respect, crowdsourcing differs from open source (OSS) where the
product produced is a public good. While some driving forces be-
hind OSS phenomenon may have economic nature such as signal-
ing of skills to prospective employees, it is currently acknowledged
that intrinsic motivation is the crucial factor [2]. Another example
is knowledge sharing in community forums such as “Yahoo! An-
swers": Wasko and Faraj [20] show that individuals perceiving that
participation will enhance their reputation within the community,
tend to contribute more. Although it is not clear whether value of
such reputation is intrinsic or the individuals expect that the repu-
tation can be “cashed” in the future, scale of such contributions 2
deﬁnitely favors the ﬁrst explanation. As the example of “Yahoo!
Answers" shows, making a clear distinction between crowdsourc-
ing and other forms of peer production on the Web is often hard
when the mechanism produces both a public good (publicly avail-
able knowledge forums) and private beneﬁts (advertising proﬁts).

A very important distinguishing feature of our empirical study is
that we analyze effects of the reputation system currently used by
TopCoder.com on strategic behavior of contestants. In particular,

2For instance, as of December 2006, Yahoo!Answers had 60 mil-
lion users and 65 million answers.

we ﬁnd that strong contestants (as measured by their reputations)
face tougher competition in the contest phase, yet they strategically
use cheap talk to deter competitor entry in the contest in the project
choice phase. These results are novel to the Information Systems
literature due to the fact that the prior research on electronic mar-
kets predominantly concentrated on studying reputation effects for
ﬁrms rather than individuals [5]. To the best of our knowledge,
most empirical studies conﬁrmed signiﬁcant monetary value of on-
line ﬁrm reputation. For instance, there is evidence that sellers with
good reputation on eBay enjoy higher sales [18] and even price pre-
miums [19]. In another example, sellers on Amazon.com with bet-
ter record of online reputation can successfully charge higher prices
than competing sellers of identical products [11]. In our study, the
reputation premium is of different nature: contestants with higher
reputations enjoy more freedom in the project choice phase and can
successfully deter entry of their opponents in the same contest.

3. TOPCODER: THE WORLD’S LARGEST
COMPETITIVE SOFTWARE DEVELOP-
MENT COMMUNITY

TopCoder.com is a website managed by the namesake company.
The company hosts weekly online algorithm competitions as well
as weekly competitions in software design and software develop-
ment. The work in design and development produces useful soft-
ware, which is licensed for proﬁt by TopCoder. As of July 23, 2008
163,351 people have registered at the TopCoder website. 17.3% of
those registered have participated in at least one Algorithm compe-
tition, 0.3% in Design, 0.7% in Development 3. We are particularly
interested in Design and Development competitions as they have
tangible payments to competitors.

The business model underlying software Design and Develop-
ment competitions is brieﬂy summarized below. 4 TopCoder pro-
duces software applications for major clients. It interacts directly
with the client company to establish application requirements, time-
lines, budget etc. Once the application requirements are deﬁned,
the application goes to the Architecture phase, where it is split into
a set of components. Each component is supposed to have a rela-
tively small scope and precise set of technical requirements deﬁn-
ing the expected component behavior and interface for interacting
with other components. For instance, an “Address Book” com-
ponent can be required to implement certain address management
functionality, moreover, it should be written in Java and provide a
Web service interface. The set of requirements to each component
is summarized in a single document (Requirements Speciﬁcation)
and posted on the website as a single Design competition. Any reg-
istered member of the website satisfying minimum legal require-
ments can submit a UML 5 design to any posted design competi-
tion. Winning design submission goes as input into the Develop-
ment competition, which has similar structure, only the competi-
tors are required to submit actual code implementing the provided
UML design. Output from Development competitions is assem-
bled together into a single application, which is later delivered to
the customer.

Design and Development competitions are posted on TopCoder
website on a weekly basis, Figure 1 shows a sample list of weekly
Development competitions. Each competition belongs to a cer-
tain catalog. Four largest catalogs are Java (generic components

3http://en.wikipedia.org/wiki/TopCoder
4Due to space limitations, the model description is simpliﬁed and
we omit some of the recent changes to the business process such as
introduction of the Architecture and Speciﬁcation contests.
5Uniﬁed Modeling Language

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA22for Java platform), .Net (generic components for .Net platform),
Custom Java (custom components for Java platform), Custom .Net
(custom components for .Net platform), but there are other catalogs
such as catalogs for major clients (AOL can be seen in Figure 1).
Every competition has two associated deadlines: the registration
deadline and the submission deadline. The registration deadline
speciﬁes the time by which all individuals willing to participate
must register for the competition, it is usually two or three days
after the competition posting date. The submission deadline speci-
ﬁes the time by which all solutions must be submitted, it is usually
within ﬁve to seven day interval after the competition posting date.
Every competition has associated payment that is given to the com-
petition winner and 50% of this amount is given to the ﬁrst runner-
up. The registration information is public, so that competitors can
see who else has registered for the component. This is achieved
by clicking on items in the “Registrants Rated/Unrated” tab. The
result may look like shown in Figure 2. Moreover, the information
is updated instantly: as soon as one competitor has registered for
the contest, others can see that.

Important component of the Design and Development process is
its scoring and review system. Once the submission deadline has
passed, all submissions enter the review phase. Each submission
is graded by three reviewers according to a prespeciﬁed scorecard
on dimensions varying from technical submission correctness and
clarity of documentation to ﬂexibility and extendability of the so-
lution. After the review process is complete, submissions enter the
appeals phase where the competitors get a chance to appeal the
decisions made by the reviewers. Once all appeals have been re-
solved, the placement is determined by the average score across all
three reviewers. A sample of results is shown in Figure 3.

TopCoder implements policy of maximum observability 6. At
ﬁrst, competitors can always observe identities of their opponents,
i.e., other members registered for the same contest (see Figure 2).
Moreover, for every member TopCoder tracks all prior competition
history and summarizes it in a single rating number 7. The rating is
provided for members who have submitted at least one solution in
some contest. It is calculated via a relatively complex formula tak-
ing into account all prior submission history of the contestant and
relative performance compared to other contestants 8. Fortunately,
the exact formula for calculating the rating value is not important,
as, in fact, even more information is available for each rated com-
petitor, including all prior competition history. This information
can be revealed by clicking on the member’s handle; the sample is
shown in Figure 4. Thus, we will simply think of ratings as proxies
for the coder’s performance so far: the better the coder performed
in the past, the higher the rating will be, with more recent perfor-
mance having higher effect on the current rating.

Finally, there is also the reliability score, which is provided for
members who have been registered for at least 15 contests and is
equal to the fraction of the last 15 contests they registered for in
which they delivered a solution satisfying the minimum quality re-
quirement (received a score of at least 75 in the review phase).
Members with the reliability rating equal to or exceeding certain

6One important exception from this rule is that contestants cannot
see scores given by the reviewers to their opponents until after the
appeals phase is over.
7Ratings are different for every competition track. Thus, an indi-
vidual participating in both Design and Development competitions
will have two different ratings - one for Design and one for Devel-
opment.
8Detailed description of the rating system can be found at
http://www.topcoder.com/wiki/display/tc/
Component+Development+Ratings

threshold will receive a bonus for every prize they receive. Due to
space limitations, we will not give any more attention to the reli-
ability score, but only mention that we take it into account when
calculating the expected coder’s payment.

Dataset
We obtained historical contest data from the TopCoder website.
The dataset included 1,966 software Design contests and 1,722
software Development contests ran by TopCoder from 09/02/2003
to 08/23/2009. The total number of different TopCoder members,
who participated in at least one of the contests in our dataset, was
1,660. Among these members, 301 individuals participated in De-
sign competitions only, 1,106 individuals participated in Develop-
ment competitions only, and 253 participated in at least one Design
and at least one Development competition. Descriptive statistics of
the data are given in Table 1.

The ﬁrst two rows of this table summarize the design and de-
velopment rating distributions extracted from the competition data;
note that these distributions are different from the static snapshot of
the rating distributions for all TopCoder members as they weigh ac-
tive competitors more. The next two rows of Table 1 summarize the
distribution of review scores extracted from the contest data. Due
to the nature of the review scorecard, one can never get the score
higher than 100.0. 75.0 is the ofﬁcial reservation score - the min-
imum score required for submission to be accepted and the prize
(if any) to be paid to the competitor. Out of 5,113 design submis-
sions and 7,602 development submissions in our dataset, 4,247 and
6,046 submissions respectively received at least the score of 75.0.
The ﬁfth and the sixth row of Table 1 summarize the distribution
of the number of submissions per contest and the next two rows
summarize the distribution of the number of contests per active in-
dividual 9 as of the last date in our dataset (08/23/2009).

Additionally, by crawling the website, we obtained project re-
quirements data for 1,742 software Design projects from our sam-
ple. 10 For every Design contest, we obtained length in pages of
the project’s requirement speciﬁcation 11 as well as the number of
distinct project requirements and the target compilation platform.
The two most popular target languages were Java (994 projects) 12
and C# (605 projects), together they account for more than 90% of
all projects. The rest of the projects (Ruby, PHP, other languages,
and projects for which we failed to extract the target platform from
the Requirements Speciﬁcation) were classiﬁed as “other”.

Finally, we collected payment data for projects in our sample.
Table 1 lists the ﬁrst place prize amounts. Note that TopCoder also
awards second place prizes equal to exactly half of the ﬁrst place
prize for the corresponding project.

4. EMPIRICAL ANALYSIS: COMPETITION

PHASE

In this Section, we perform simple statistical analysis of the dataset

to determine what factors inﬂuence individual performance in a
software contest once the registration phase of the contest is closed.
The usual suspects are individual speciﬁc characteristics and skills,
project speciﬁc characteristics, experience with TopCoder compe-
tition platform, level of competition in a particular contest, current

9Competed in at least one contest
10The rest of the projects had missing or broken Requirements Spec-
iﬁcation document.
11Note that the same Requirements Speciﬁcation document, to-
gether with the design documentation produced during the design
contest, goes as input to the corresponding Development contest.
12including JavaScript

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA23Figure 1: Sample list of Development competitions from TopCoder.

Figure 2: Sample list of registrants for a Development competition from TopCoder.

reputation of the individual. We start by providing evidence that
individual’s rating is a signiﬁcant predictor of future performance.
Table 1 shows signiﬁcant variation in the distribution of ﬁnal
project scores. For instance, for Design contests the median review
score (88.6) is only slightly more than a standard deviation (10.1)
away from the perfect review score (100), as well as from the min-
imum passing score (75.0). To further understand the factors be-
hind the dispersion of scores, we ﬁrst grouped all competitors in
ﬁve different groups on the basis of their Design rating just before
the contest 13. The group boundaries were chosen in accordance
with the color group assignments by TopCoder: “grey” (rating 0-
899), “green” (rating 900-1199), “blue” (rating 1200-1499), “yel-
low” (rating 1500-2199), “red” (rating 2200+). Within each group,
we estimated the distribution of the review scores. Density and cu-
mulative density functions for each group are shown in Figure 5.
The plot suggests the ﬁrst order stochastic dominance ranking of
score distributions for different groups. We formally tested this
statement by Mann-Whitney-Wilcoxon stochastic dominance test
on pairs of adjacent groups; in all 4 cases (“red” vs. “yellow”,
“yellow” vs. “blue” etc.), the test rejected the null (no stochas-
tic dominance) hypothesis at 1% signiﬁcance level. Similar results
were obtained for the Development contests.

There might be several alternative explanations of why members
with high rating today are expected to deliver higher scores in the
future:

• Hypothesis Ia: Higher rated members are those with more
inherent skills and abilities and therefore they deliver better
solutions.

• Hypothesis Ib: Higher rated members are those who inher-
ently care more about their rating and therefore consistently

13Note that members ratings change over their lifetime, therefore
the same person may be classiﬁed to two different categories at
different moments of time.

Figure 5: Distribution of Software Design Review Scores Clus-
tered By Coder Rating

put more effort into the competition to keep the status high.

• Hypothesis II: Higher rated members are those with more ac-
crued experience and therefore they deliver better solutions.

• Hypothesis III: The rating is “addictive”, members that achieved

high rating today tend to contribute more in the future to keep
their status high (this is similar to Huberman et al. [15] state-
ment that users that get more attention on YouTube tend to
contribute more in the future).

• Hypothesis IV: Higher rated members experience less com-
petition in the project choice phase, therefore they can afford
to choose easier, better paying or less competitive projects
and deliver higher scores.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA24Figure 3: Sample contest results from TopCoder.

Figure 4: Sample Development competition history for a TopCoder member.

• Hypothesis V: Higher rated members expect ﬁercer competi-
tion from opponents and therefore have to deliver better so-
lutions in order to win.

Note that these hypotheses are not mutually exclusive.
In order
to test the hypotheses, we estimate a set of econometric models.
The ﬁrst set of econometric models, analyzed in this paper, relates
contestant’s performance on a particular project to the set of ob-
servable contestant and project characteristics. Results of the ﬁrst
set of models for Design contests are given in Table 2 14. To ensure
no “cold-start” effect for contestants who have not been rated yet
or are not familiar enough with the TopCoder Software methodol-
ogy, we dropped the ﬁrst ﬁve contests for every contestant from our
sample.

The ﬁrst column of Table 2 presents a simple OLS model which

speciﬁes that

scoreij = constant + β1paymentij + β2ratingi

+ β4 max rating−i + β5experiencei + β6opponentsj
+ β7 max spec. lengthj + β8num. req.j + β9is Javaj
+ β10is C#j + εij,

where

14Results for Development contests are quantitatively similar. We
omit them due to space limitations.

1. scoreij is the (ﬁnal) submission score for the contestant i in

the contest j on a scale of 0 to 100.

2. paymentij is the ﬁrst prize payment for the contest j mea-

sured in $1,000 15.

3. ratingi is the contestant i rating immediately before the con-

test j.

4. max rating−i is the maximum opponent rating for the con-

testant i in the contest j.

5. experiencei is the contestant i experience right before the
contest measured in the number of previous contests for this
contestant.

6. opponentsj is the number of opponents in the contest j.

7. spec. lengthj is the length of the Requirements Speciﬁcation

document for the contest j measured in pages.

8. num. req.j is the number of distinct requirement items in the

Requirements Speciﬁcation document for the contest j.

15Note the individual speciﬁc subscript i, indicating that we adjust
the payment to take into account individual speciﬁc bonuses, such
as the reliability bonus.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA25Variable
design rating
development rating
design submission score
development submission score
number of submission per design contest
number of submission per development contest
number of design contests per member
number of development contests per member
requirements speciﬁcation length
number of requirements per design contest
design contest 1st place payment
development contest 1st place payment

Min Max Mean Median
1376.0
1.0
124.0
1166.0
88.6
25.0
88.9
21.8
2.0
1.0
3.0
1.0
1.0
2.0
2.0
1.0
4.0
1.0
12.0
1.0
750.0
500.0

2794.0
2397.0
100.0
100.0
26.0
61.0
408.0
76.0
51.0
195.0
3000.0
3000.0

1406.6
1182.3
86.0
86.6
2.0
4.0
9.0
5.0
4.0
14.0
705.5
580.7

100.0
50.0

S.Dev
465.9
365.5
10.1
10.2
1.9
4.4
27.1
8.6
3.2
13.1
319.8
325.3

Table 1: Descriptive Statistics

9. is Javaj is the boolean indicator if the contest j had Java as

comparison purposes, the last column of Table 2 presents results of
the ﬁxed effects model without the rating variable.

the target platform.

target platform.

10. is C#j is the boolean indicator if the contest j had C# as the

4.1 Results

The second column of Table 2 (GMM 1) presents the GMM
(Generalized Method of Moments) [12] estimation results for Equa-
tion 1 controlling for potential endogeneity of the contest payment
(paymentj), as well as the coder’s rating (ratingi) and reliability
(reliabilityi). Endogeneity of the contest payment comes from the
fact that there might be unobservable project characteristics that af-
fect both the contest complexity as well the contest payment set
by TopCoder.com. For instance, it is plausible to assume that the
contest sponsor will set higher prizes for more complex contests,
however such contests will also have lower average solution quality
due to the complexity of the underlying project. OLS estimates will
capture both the direct effect of the contest payment on the contes-
tants’ performance and the projection of the unobserved complexity
variable, thus underestimating the causal effect of the contest prize.
This is a classic endogeneity problem in economics. In order to
account for endogeneity of the contest payment, we instrument this
variable with the average payment in the contemporaneous con-
tests 16. This is essentially the Hausman et al. [13] approach of
using prices from different markets as instruments for prices in the
given market. Furthermore, contestant’s rating might be correlated
with unobservable individual traits which affect project choice and
therefore, indirectly, contestant’s performance. To account for po-
tential endogeneity of the rating, we instrument it with 3 lags of
differences. The assumption here is that, although the rating might
be correlated with the individual speciﬁc characteristics, short term
ﬂuctuations of the rating are not; this is conceptually similar to the
Anderson and Hsiao [1] estimator for the dynamic panel data.

The third column of Table 2 (GMM 2) presents the GMM esti-
mation result for Equation 1 controlling additionally for potential
endogeneity of the maximum opponent rating (max rating−i) and
the number of opponents in the contest (opponentsj): both can be
correlated with unobservable project characteristics affecting the
individual performance. Again, we use the Hausman et al. [13]
approach and instrument the maximum opponent rating with the
average of the maximum contestant rating in the contemporane-
ous contests and the number of contestants with the average of the
number of contestants in the contemporaneous contests.

The fourth column of Table 2 extends the model by including
the coder speciﬁc ﬁxed effects in the previous model. Finally, for

16more precisely, average payment across the contests ran in the
previous two weeks (note it excludes the instrumented contest)

Payment is a strongly signiﬁcant determinant of the individual
contestant performance in all ﬁve models. From the ﬁrst column
of Table 2, we can see that the OLS signiﬁcantly underestimates
the effect of payment, as compared to the GMM 1 model, sug-
gesting the project payment is in fact correlated with unobservable
project characteristics, beyond the speciﬁcation length, the num-
ber of requirements and the target platform. Durbin-Wu-Hausman
endogeneity test using the average contemporaneous project pay-
ment as an instrument conﬁrms that the OLS model is inconsistent
(p < 0.001). Furthermore, endogeneity test for the maximum op-
ponent rating also rejects the null hypothesis (p < 0.001), there-
fore in the rest of the Section we will concentrate on analyzing the
results of the last three models (GMM 2, IV FE 1 and IV FE 2).

The last three columns of Table 2 suggest that the marginal ef-
fect of a $1,000 payment on the ﬁnal quality of submission in a
TopCoder Software Design contest lies somewhere between 6 to
10 project points. Speciﬁcation length is not a signiﬁcant determi-
nant of the ﬁnal project score, however the number of requirements
is: there is approximately 1.2 point loss in quality for every 10 ad-
ditional project requirements 17.

Java and C# contests seem to have lower average quality score
than “other” contests, although this effect is barely statistically sig-
niﬁcant. We acknowledge that these two variables are also po-
tentially endogenous as they depend on the project choice by the
contestant. While we did not instrument for the target platform
endogeneity, we performed a robustness check by dropping these
regressors from the model and verifying that coefﬁcient estimates
for the rest of the variables are not signiﬁcantly affected.

Experience of the contestant is not a signiﬁcant predictor of the
contestant’s performance 18, therefore we suggest that the Hypoth-
esis II does not hold in our sample.

While the number of opponents is a barely signiﬁcant predictor
of the contestant performance, the maximum rating of the opponent
is strongly statistically and economically signiﬁcant: the marginal
effect of an extra 1,000 points of the opponent’s rating is some-
where between 6 and 8 project points. Note that this is comparable
with an effect of an extra $1,000 of the project payment. In par-

17While this value might seem to be very low for readers famil-
iar with TopCoder Design contests, we should emphasize that we
count every bullet point in the section 1.2 of the Requirements
Speciﬁcation document as a separate requirement.
18Note that we drop the ﬁrst ﬁve observations for every contestant.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA26Table 2: Regressions of the ﬁnal project score for Software Design contests

payment (in $1,000)

rating (in 1,000 pts)

max opponent rating (in 1,000 pts)

experience (num contests)

number of opponents

speciﬁcation length

number of requirements

Java

C#

constant

(1)
OLS

2.044∗∗∗
(3.81)

9.306∗∗∗
(22.27)

(2)

GMM 1
3.351∗
(2.08)

3.789∗
(2.26)

-0.0435
(-0.10)

-0.0241
(-0.04)

(3)

GMM 2
6.345∗∗∗
(3.52)

3.408
(1.84)

7.485∗∗
(3.10)

(4)

IV, FE
6.425∗
(2.35)

3.942
(1.56)

6.248∗
(2.42)

(5)

IV, FE 2
10.11∗∗
(2.82)

8.834∗∗
(3.16)

0.0103∗∗
(3.01)

0.00158
(0.42)

-0.000881

(-0.20)

0.00729
(0.83)

-0.00134
(-0.13)

0.330∗∗
(2.92)

-0.0651
(-0.96)

-0.0247
(-1.78)

-1.895∗
(-2.07)

-1.083
(-1.17)

0.417∗∗
(3.22)

-0.135
(-1.39)

-0.0397∗
(-2.24)

-0.285
(-0.27)

0.793
(0.71)

0.524
(0.81)

-0.0944
(-0.65)

-0.0813
(-1.56)

-1.611
(-1.39)

-1.568
(-1.30)

1.165∗
(2.05)

-0.0722
(-0.49)

-0.109∗∗
(-2.66)

-2.612∗
(-2.11)

-2.997∗
(-2.23)

1.199
(1.84)

-0.151
(-0.89)

-0.122∗
(-2.56)

-2.778∗
(-1.99)

-3.517∗
(-2.31)

73.95∗∗∗
(57.84)
1488

80.81∗∗∗
(24.60)
1174

69.20∗∗∗
(16.36)
1174

69.56∗∗∗
(13.46)
1174

69.59∗∗∗
(12.79)
1174

N
t statistics in parentheses
∗ p < 0.05, ∗∗ p < 0.01, ∗∗∗ p < 0.001

ticular, this result means that, conditional on facing the same set
of opponents, a higher rated contestant will face ﬁercer competi-
tion from the opponents. We performed a robustness check of this
result by considering two separate cases: the highest rated oppo-
nent is rated lower than the coder (i.e., the coder is the favorite) and
the highest rated opponent is rated higher than the coder (i.e., the
coder is the underdog). In both cases, there was a (similar in mag-
nitude) positive effect of the opponent’s rating on the performance
of the coder. The news is not all bad for high rated coders: as the
next Section shows, higher rated coders face less competition in the
project choice phase.

We conclude this Section with discussion of the effect of rat-
ing on the contestant performance. As Figure 5 and the OLS re-
sults suggest, there is a lot of variation in performance between
contestants of different ratings. The result persists even if we in-
strument all variables except for the rating properly, but it signif-
icantly decreases and becomes statistically insigniﬁcant when one
puts contestant speciﬁc dummies in the model and instruments the
contestant’s rating by its short term ﬂuctuations. This suggest that
much if not all effect of ratings is due to inherent contestant speciﬁc
traits (Hypothesis Ia and Ib) and we do not see empirical evidence
for Hypothesis III that the rating is “addictive”. We acknowledge
that our current dataset does not allow us to test Hypothesis Ia and
Ib separately, i.e, although we know that some individuals consis-
tently perform better than others due to some individual character-
istics, we cannot conclude whether they are simply more skilled or
care more about their performance than other contestants.

5. EMPIRICAL ANALYSIS: REGISTRATION

PHASE

Empirical results of the previous section show that higher rated
contestants, ceteris paribus, face tougher competition from their
opponents in the competition phase of the contest. We have hy-
pothesized (Hypothesis IV) that this effect might be compensated
by a competitive advantage in the project choice phase: TopCoder
competition system allows contestants to register for projects in ad-
vance, thus letting strong contestants to signal their project choices
in order to deter entry of their opponents. In this Section, we for-
mally test this statement.

Our ﬁrst test is based on a simple observation that, in order to
deter entry of their opponents in the contest they like, higher rated
contestants should register early in the contest, while lower rated
contestants will prefer to wait until the higher rated opponents made
their choices. If this is true, one should empirically observe a corre-
lation between the contestant’s rating and the probability of being
the ﬁrst registrant in a contest. As Figure 6 shows, the correlation is
present in our dataset. The effect is also visible (although weaker)
if one plots contestant’s rating against the probability of NOT being
the last registrant in the contest (due to space limitations, we omit
this Figure).

Furthermore, we formally tested for presence of the “early reg-
istrant” effect by performing the Bernoulli (binomial probability)
test of the ﬁrst registrant being the highest rated coder in the contest
conditional on the number of contestants. If the coder registration
process is independent of the ratings, then the null hypothesis of

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA27resents the rating of the last registered coder (so far) and the to-
tal number of coders. The second column of Table 4 adds project
speciﬁc dummies to the regression. The next column additionally
includes the maximum rating of the coder who registered so far.
Finally, the last column includes the coder speciﬁc dummies (for
the last registered coder) in addition to the project dummies.

The simple OLS model (ﬁrst column of Table 4) shows strong
positive correlation between the rating of the last registered coder
and the rating of the coder who will register next for the same
project. We believe that this correlation picks up the fact that cer-
tain project types (like projects requiring speciﬁc set of skills) at-
tract higher rated coders while other project types attract lower
rated coders, therefore, as long as one does not control for the
project speciﬁc effects, there is a positive correlation between rat-
ings of the coders registered for the same project. Second column
of Table 4 shows that, as soon as project speciﬁc dummies are in-
cluded to the regression, the correlation turns negative. The next
column explains that it is the maximum rating of the coder who
registered so far, not the rating of the last registrant, that inﬂuences
future registration process. Adding coder dummies in the last col-
umn does not make a signiﬁcant difference.

We emphasize that registration for a contest in TopCoder is not
legally binding as the registered contestants may choose not to par-
ticipate; however, registering and not participating has a negative
effect of reducing the contestant’s reliability score, thus (poten-
tially) decreasing the amount of future winnings. In our sample,
only small fraction of contestants (about 20%) had reliability score
high enough (at least 80%) to qualify for a project related bonus.
We performed a robustness check by eliminating these contestants
from the sample and performing estimation only for coders who
had no signiﬁcant cost of registering but not participating. Our re-
sults were quantitatively similar, thus indicating that the cost of sig-
nal (registration) had no signiﬁcant effect on its value. We conjec-
ture that the registration phase in TopCoder contests works largely
as cheap talk [10] (communication between players which does not
directly affect the payoffs of the game) rather than costly signaling.
We conclude this Section, by providing a back-of-the-envelope
estimate of the effect of early registration of a high rated coder on
the expected surplus in the contest. In order to do this, we need
several other estimates:

1. We need an estimate of how early registration of a high rated
coder affects the maximum opponent rating in the contest.
We can infer this estimate from Table 4, which shows that
each rating point of the highest rated registrant in the con-
test (so far) decreases the rating of all future registrants by
approximately 0.5 rating point.

2. We need an estimate of how the level of effort in the compe-
tition phase decreases when the maximum opponent rating
decreases. While Table 2 reports this value, we take a con-
servative estimate and assume that coders do not reduce the
quality of their submission when the opponent ratings drop.
This assumption ensures that our estimate will provide a ro-
bust lower bound on the coder surplus whether or not the
coder behaves strategically in the competition phase.

3. We need an estimate of how the coder’s probability of win-
ning the ﬁrst prize in a contest depends on the maximum rat-
ing of the opponent. Note that we assume that the highest
rated coder in the contest is guaranteed to win at least the
second place prize; this assumption is consistent with our
dataset: we have only 9 observations where a member with
rating more than 1, 800 did not win the ﬁrst or the second

Figure 7: For three member contests, a fraction of all contests
in which the registration order of contestants represents a cer-
tain permutation of the rating order. 1 represents the highest
rated coder, 3 represents the least rated coder. Coders are or-
der by the registration time.

the “fair” coin-ﬂip model holds and, for contests with exactly N
members, the fraction of contests, where the ﬁrst registrant was the
highest rated member, should be approximately 1
N . In our sample,
out of 153 two member contests, in 92 contests the ﬁrst member
was the highest rated; out of 130 three member contests, in 67 con-
tests the ﬁrst member was the highest rated; out of 79 four member
contests, in 37 contests the ﬁrst member was the highest rated. 19
In all three cases, the null hypothesis of the fair coin ﬂip is rejected
(p < 0.001).

In fact, we observe even stronger effect in our data. Again, con-
sider all contests with exactly m participants. Encode every partici-
pant by a number from 1 to m in order of their ratings, with 1 being
the highest rated coder and m being the lowest rated coder (assume
no ties). Any registration sequence can be represented by a per-
mutation of numbers from 1 to m. For instance, in a three-person
contest, sequence 132 would represent that the highest rated coder
was the ﬁrst one to register and the lowest rated coder was the sec-
ond one to register. For any particular registration sequence, one
can count how many times did it occur in the data. Amazingly, not
only the ﬁrst registrant is most often the highest rated one but also
the ordering of registration sequences by contest shares is similar
to the lexicographic ordering. In case of m = 3 shown in Figure 7,
it is in fact identical to the lexicographic ordering.

Next, we formally test whether an early entry of a high rated
coder has a positive deterrence effect on entrance of other high
rated coders in the contest. If the effect exists, then, controlling
for the project complexity, there must be a negative correlation be-
tween the ratings of the coders who registered for the project so
far and the rating of the coder who is going to register next (as-
suming that there is such). Table 4 presents results of a series of
regressions, starting from a simple OLS model in the ﬁrst column:

next coder ratingj = constant

+ β1last coder ratingj
+ β2 the number of codersj + εj,

where the left hand side of the equation represents the rating of
the coder who is going to register next and the right hand side rep-

19In this sample, we have excluded unrated participants from con-
sideration.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA28Figure 6: By member probability of being the ﬁrst registrant in a Software Design competition. X axis represents the average lifetime
member rating (excluding the ﬁrst ﬁve contests). Y axis represents the sample probability estimate. The plot includes only members
that participated in at least 15 Software Design contests.

Table 3: Regressions of the next registrant rating for Software Design contests
(4)

(3)

(2)

the last registratnt rating

the number of registrants so far

the maximum registrant rating so far

(1)
OLS

0.0883∗∗
(2.73)

FE project
-0.195∗∗∗
(-3.71)

-0.00284
(-0.30)

-0.0101
(-0.70)

N
t statistics in parentheses ∗ p < 0.05, ∗∗ p < 0.01, ∗∗∗ p < 0.001

917

917

FE project

FE project coder

-0.0759
(-1.04)

0.0355
(1.88)

-0.476∗∗
(-2.95)

917

0.145
(1.04)

0.0590∗
(2.58)

-0.614∗∗
(-3.11)

917

prize in a contest where he was the highest rated participant.
In order to estimate the probability of winning the prize, we
estimate a series of logistic regressions; results are shown in
Table 5. In columns 2 to 4 of Table 5, we additionally control
for unobservable project speciﬁc characteristics by including
the project level random effects 20. Random effects and ad-
ditional regressors do not affect value of the coefﬁcient on
the maximum opponent rating variable signiﬁcantly, there-
fore we use more conservative estimate from the simple logit
model. The corresponding value of the marginal effect is
−0.54.

Putting it all together, for a sufﬁciently high rated member, ev-
ery additional rating point decreases the rating of the toughest op-
ponent by half a point and increases the probability of winning a
prize by 0.5 ∗ 0.001 ∗ 0.54 = 0.027%. For a $1, 000 contest, the
difference between the ﬁrst and the second prize is $500 and there-
fore the change in the expected winnings per single rating point is
13.5 cents. For large rating values, this amount can be quite sig-
niﬁcant. For instance, the difference between the smallest rating in
the “red” (the highest) rating category and the smallest rating in the
“yellow” (the second highest) rating category is 700 points, what
translates to the difference of $94.5 per a $1, 000 contest. Overall,
our empirical results support Hypothesis IV that higher rated mem-
bers face less competition in the project choice phase and behave
strategically to exploit this competitive advantage.

20We do not use ﬁxed effects to avoid the incidental parameters
problem.

6. SUMMARY

Crowdsourcing is a new Web phenomenon, in which a ﬁrm takes
a function once performed in-house and outsources it to a crowd,
usually in the form of an open contest. Designing efﬁcient crowd-
sourcing mechanisms is not possible without deep understanding
of incentives and strategic choices of all participants. This paper
presents an empirical analysis of determinants of individual per-
formance in multiple simultaneous crowdsourcing contests using
a unique dataset for the world’s largest competitive software de-
velopment portal (TopCoder.com). Special attention is given to
studying the effects of the reputation system currently used by Top-
Coder.com on behavior of contestants. We ﬁnd that individual spe-
ciﬁc traits together with the project payment and the number of
project requirements are signiﬁcant predictors of the ﬁnal project
quality. Furthermore, we ﬁnd signiﬁcant evidence of strategic be-
havior of contestants. High rated contestants face tougher compe-
tition from their opponents in the competition phase of the contest.
In order to soften the competition, they move ﬁrst in the registration
phase of the contest by signing up early for particular projects. Al-
though registration in TopCoder contests is non-binding, it deters
entry of opponents in the same contest; our lower bound estimate
shows that this strategy generates signiﬁcant surplus gain to high
rated contestants.

This study has a number of limitations. At ﬁrst, while we ﬁnd
that better performance of higher rated coders should be attributed
to some individual speciﬁc traits, we currently cannot say whether
such differentiation is purely skill based or some coders care more
about their reputation within community than others. Next, al-

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA29max opponent rating (in 1,000 pts)

Table 4: Logistic regressions for probability of winning the contest

(2)

(3)

(4)

logit + RE project

logit + RE project

logit + RE project

(1)
logit

-2.371∗∗∗
(-14.89)

-2.372∗∗∗
(-14.89)

rating (in 1,000 pts)

number of opponents

constant

N
t statistics in parentheses ∗ p < 0.05, ∗∗ p < 0.01, ∗∗∗ p < 0.001

3.247∗∗∗
(12.91)
1488

3.248∗∗∗
(12.92)
1488

-0.0344
(-0.10)
1488

-3.007∗∗∗
(-15.67)

2.871∗∗∗
(15.33)

-2.840∗∗∗
(-14.50)

2.846∗∗∗
(15.15)

-0.138∗∗∗
(-3.59)

0.120
(0.36)
1488

though we ﬁnd that strategic behavior of contestants in the regis-
tration phase increases the surplus of high rated coders, it is not
clear what effect does it have on the sponsor’s surplus and the over-
all social efﬁciency of the mechanism. We hypothesize that ability
of skilled contestants to signal their intention to perform a partic-
ular project should result in more efﬁcient allocation of the over-
all pool of contestants to particular contests, thus improving the
overall social surplus.
Indeed, imagine a situation in which the
reputation system is not in place and contestants choose competi-
tions in a setting of incomplete information. Theoretical analysis
of such “choice game” for simultaneous crowdsourcing contests
is provided in a recent paper of DiPalantino and Vojnovic [7] and
also, in a setting of simultaneous online auctions by sellers of dif-
ferent reputation, by Dellarocas [6]. Both papers identify a unique
symmetric equilibrium in which

1. Players (buyers) partition into several skill (value) zones, with
the zone l covering a certain interval range [vl+1, vl) of skills
(values).

2. Players (buyers) in the zone k randomly choose between projects

(auctions) with the top k highest rewards (seller reputation).

Such equilibrium is obviously inefﬁcient due to the absence of co-
ordination mechanism between selecting players; the inefﬁciency
is particularly stark when the number of potential contestants (buy-
ers) is relatively small compared to the number of offered contests
(auctions) as it increases the probability of having a contest (auc-
tion) with zero level of participation. Our empirical investigation
shows that a properly designed reputation system can, at least par-
tially, mitigate this inefﬁciency by allowing coordination between
individuals. Constructing a good structural model of individual be-
havior and using it to measure the social value of such signaling
mechanism is a valuable direction for future research. We conclude
the paper by conjecturing that the reputation + cheap talk mecha-
nism employed by TopCoder has a positive effect on allocative ef-
ﬁciency of simultaneous all-pay contests and should be considered
for adoption in other crowdsourcing platforms. 21

7. REFERENCES
[1] T. W. Anderson and C. Hsiao. Formulation and estimation of dynamic models

using panel data. Journal of the American Statistical Association,
76(375):598–606, Sep 1981.

[2] J. Bitzer, W. Schrett, and P. J. Schr¨oder. Intrinsic motivation in open source

software development. Journal of Comparative Economics, 35(1):160 – 169,
2007.

21This advice is provided “as is" without warranty of any kind.

[3] D. Brabham. Moving the crowd at istockphoto: The composition of the crowd

and motivations for participation in a crowdsourcing application. Working
Paper, Available at SSRN: http://ssrn.com/abstract=1122462, 2008.

[4] R. Buyya, C. S. Yeo, and S. Venugopal. Market-oriented cloud computing:
Vision, hype, and reality for delivering it services as computing utilities. In
Proceedings of the 10th IEEE International Conference on High Performance
Computing and Communications, pages 5–13, Aug 2008.

[5] C. Dellarocas. The digitization of word-of-mouth: Promise and challenges of

online reputation mechanisms. Management Science, 49(10):1407–1424, 2003.
[6] C. Dellarocas. Simultaneous auctions of imperfect substitute goods by sellers of

different reputations. Working Paper, Available at SSRN:
http://papers.ssrn.com/abstract=1115805, 2008.

[7] D. DiPalantino and M. Vojnovic. Crowdsourcing and all-pay auctions. In EC

’09: Proceedings of the tenth ACM conference on Electronic commerce, pages
119–128, 2009.

[8] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet advertising and the

generalized second-price auction: Selling billions of dollars worth of keywords.
The American Economic Review, 97(18):242–259, March 2007.

[9] R. J. Ennals and M. N. Garofalakis. Mashmaker: mashups for the masses. In

SIGMOD ’07: Proceedings of the 2007 ACM SIGMOD international
conference on Management of data, pages 1116–1118, 2007.

[10] J. Farrell and M. Rabin. Cheap talk. The Journal of Economic Perspectives,

10(3):103–118, 1996.

[11] A. Ghose, P. Ipeirotis, and A. Sundararajan. The dimensions of reputation in

electronic markets. Working Paper, New York University, 2006.

[12] L. P. Hansen. Large sample properties of generalized method of moments

estimators. Econometrica, 50(4):1029–54, July 1982.

[13] J. A. Hausman, G. Leonard, and J. D. Zona. Competitive analysis with

differentiated products. Annales d’Economie et de Statistique, (34):07, April
1994.

[14] J. Howe. The rise of crowdsourcing. Wired Magazine(http:

//www.wired.com/wired/archive/14.06/crowds_pr.html),
2006.

[15] B. Huberman, D. Romero, and F. Wu. Crowdsourcing, attention and

productivity. Working Paper, Available at SSRN:
http://ssrn.com/abstract=1266996, 2008.

[16] F. Kleemann, G. Voß, and K. Rieder. Un(der)paid innovators: The commercial

utilization of consumer work through crowdsourcing. Science, Technology &
Innovation Studies, 4(1), July 2008.

[17] K. R. Lakhani and J. A. Panetta. The principles of distributed innovation.
Innovations: Technology, Governance, Globalization, 2(3):97–112, 2007.

[18] P. Resnick and R. Zeckhauser. Trust among strangers in internet transactions:
Empirical analysis of eBay’s reputation system. Working Paper, University of
Maryland, 2001.

[19] P. Resnick, R. Zeckhauser, J. Swanson, and K. Lockwood. The value of
reputation on eBay: A controlled experiment. Experimental Economics,
9(2):79–101, jun 2006.

[20] M. Wasko and S. Faraj. Why should i share? examining social capital and
knowledge contribution in electronic networks of practice. MIS Quarterly,
29(1):35Â ˝U57, 2005.

[21] J. Yang, L. A. Adamic, and M. S. Ackerman. Crowdsourcing and knowledge

sharing: strategic user behavior on taskcn. In Proceedings of the 9th ACM
conference on Electronic commerce, pages 246–255, 2008.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA30