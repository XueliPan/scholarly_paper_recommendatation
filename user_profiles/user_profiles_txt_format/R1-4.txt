An Impact Analysis Method for Safety-
Critical User Interface Design

JULIA GALLIERS
City University London
ALISTAIR SUTCLIFFE
UMIST
and
SHAILEY MINOCHA
The Open University

We describe a method of assessing the implications for human error on user interface design of
safety-critical systems. In previous work we have proposed a taxonomy of influencing factors
that contribute to error. In this article, components of the taxonomy are combined into a
mathematical and causal model for error, represented as a Bayesian Belief Net (BBN). The
BBN quantifies error influences arising from user knowledge, ability, and the task environ-
ment, combined with factors describing the complexity of user action and user interface
quality. The BBN model predicts probabilities of different types of error—slips and mistakes—
for each component action of a task involving user-system interaction. We propose an Impact
Analysis Method that involves running test scenarios against this causal model of error in
order to determine user interactions that are prone to different types of error. Applying the
proposed method will enable the designer to determine the combinations of influencing factors
and their interactions that are most likely to influence human error. Finally we show how
such scenario-based causal analysis can be useful as a means of
focusing on relevant
guidelines for safe user interface design. The proposed method is demonstrated through a case
study of an operator performing a task using the control system for a laser spectrophotometer.

Categories and Subject Descriptors: D.2.1 [Software Engineering]: Requirements/Specifica-
tions—Methodologies (e.g., object-oriented, structured); D.2.2 [Software Engineering]: De-
sign Tools and Techniques—User interfaces; G.3 [Mathematics of Computing]: Probability
and Statistics; H.1.2 [Models and Principles]: User/Machine Systems—Human factors

General Terms: Design, Human Factors, Reliability

Additional Key Words and Phrases: Bayesian Belief Networks, human error, safety-critical,
scenario-based causal analysis

Authors’ addresses: J. Galliers, Centre for HCI Design School of Informatics, City University
London, Northampton Square, London EC1V 0HB, United Kingdom; email: jrg@csr.city.ac.uk;
A. Sutcliffe, Department of Computation, UMIST, P. O. Box 88, Manchester M60 1QD, United
Kingdom; email: a.g.sutcliffe@co.umist.ac.uk; S. Minocha, Faculty of Mathematics and Com-
puting, Walton Hall, The Open University, Milton Keynes MK7 6AA, United Kingdom; email:
S.Minocha@open.ac.uk.
Permission to make digital / hard copy of part or all of this work for personal or classroom use
is granted without fee provided that the copies are not made or distributed for profit or
commercial advantage, the copyright notice, the title of the publication, and its date appear,
and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to
republish, to post on servers, or to redistribute to lists, requires prior specific permission
and / or a fee.
© 2000 ACM 1073-0516/99/1200 –0341 $5.00

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999, Pages 341–369.

342

•

J. Galliers et al.

1. INTRODUCTION

This article introduces a method to assist with the identification of require-
ments for the user interface design of software-intensive safety-critical
systems. Of particular interest is the fact that most safety-critical systems
involve automated software control and human operation in a social
context. Many spectacular system failures are caused by human and user
interface design errors as well as failure in software functioning, e.g., the
much publicized London Ambulance Service and Therac-25 accidents
[Leveson 1995; Leveson and Turner 1993] were attributable to poor opera-
tor user interface design as well as unreliable control software.

Risk assessment methods such as HAZOP, Failure Modes and Effects
Analysis (FMEA), etc. have been adapted for a design orientation [Earthy
1995], but these methods do not deal with complex causes of human error
[Hollnagel 1993]. Human influences on failure modes are considered, but
the approach to predicting possible errors is still crude and takes little
account of cognitive psychology or social causes of failure. Several research-
ers, notably Reason [1990] and Hollnagel [1993], have called for a more
systematic and theoretically grounded approach to safety design for human
operation.

THERP (Technique for Human Error Rate Prediction) [Bell and Swain
1985] has been widely used to determine the probabilities of paths through
an event tree. More rigorous Bayesian methods for combining probabilities
are currently becoming more common in software reliability research and
safety-critical system design generally, but rarely with respect to determin-
ing predictions for human error rates. One exception to this is the work of
Phillips and Humphreys [1990] in which influence diagrams (Bayesian
networks augmented with decision variables and a utility function) are
applied to a study of human reliability in the context of pressurized
thermal shock events for two nuclear power stations in the United States.
Another formal approach is the WB-graph method [Ladkin 1998] which has
been applied to the retrospective modeling of accidents in which causal
factors are distinguished.

In our previous work [Sutcliffe et al. 1998a], we have proposed a
taxonomy of influencing factors that might contribute to human error. This
article shows how components of the taxonomy can be combined into a
causal model for error, represented as a Bayesian Belief Net (BBN) and
used to model the error influences arising from user knowledge and the
task environment. Firstly, human errors are distinguished as mistake-
errors and slip-errors [Reason 1990]. The method also makes error predic-
tions for user tasks according to different test scenarios with operator
characteristics. Finally, the method then directs the analyst or designer to
particular guidelines for safe user interface design based on the action-
type, and whether the requirement be to eliminate, reduce, or control the
risk of these human errors. These guidelines are a refinement of the
guidelines for safe HMI design proposed by Leveson [1995].

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

343

The article is organized in five sections. We start in Section 2 with the
background. Section 2.1 describes the different error types. Section 2.2
offers a short description of the nature of BBNs as a means of combining
generic influencing factors into a predictive model of human behavior.
Section 3 then introduces the Impact Analysis Method for assessing the
possible impact of human error on user interface design for safety-critical
systems. Section 3.1 has an overview of the method. Section 3.2 describes in
detail the causal analysis phase for predicting probabilities of mistake-
errors and slip-errors for a task, given a variety of test scenarios. Section
3.3 follows with the consequence analysis. In Section 4, we illustrate the
use of the method with a case study. This employs data from a task and
hazard analysis previously gathered and described in Sutcliffe [1998]
concerning the control system of a laser spectrophotometer. The article
concludes with a discussion in Section 5.

2. RELATED WORK AND BACKGROUND

2.1 Types of Human Error

Following a large body of research on human error by Reason [1990],
Hollnagel [1993], and Woods [1989], we distinguish between two different
types of error: slips and mistakes.

Slips (and lapses) are skill-based errors that happen when an action is
incorrectly performed during familiar work requiring little attention. In
our view, tasks of physical complexity, such as complex manipulations
involving precise movements and detailed coordination, are more prone to
slip-errors. Mistakes, on the other hand, are rule-based or knowledge-based
errors [Rasmussen 1983] and are associated with problem solving. Rule-
based mistakes can occur by the application of “bad” rules or the misappli-
cation of “good” rules, whereas knowledge-bound mistakes are rooted in
bounded rationality, incomplete or inaccurate knowledge. In this article,
tasks of high cognitive complexity are considered more prone to mistake-
errors.

2.2 Bayesian Belief Network Analysis

In this section, we introduce BBNs as a means of combining a set of generic
influencing factors into a more formal and predictive model of human error.
BBNs are graphical networks that represent probabilistic relationships
between variables. They offer decision support for probabilistic reasoning
in the presence of uncertainty and combine the advantages of an intuitive
representation with a sound mathematical basis in Bayesian probability
[Pearl 1988]. BBNs are useful for inferring the probabilities of events
which have not as yet been observed, on the basis of observations or other
evidence that have a causal relationship to the event in question.

BBNs have become increasingly popular as a means of predictive, proba-
bilistic reasoning [Fenton 1999]. Although the underlying notions of
Bayesian probability theory and propagation have been around for some

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

344

•

J. Galliers et al.

Fig. 1. Simple BBN fragment. The two prior nodes have a causal influence on the posterior
node of action complexity.

time, it is only recently that efficient algorithms, as well as the tools to
implement them, have enabled realistically sized problems to be solved.
BBN technology is now used in systems for medical and mechanical failure
diagnosis and, for example, underpins the interactive printer fault diagnos-
tic system on the Microsoft Web site [Fenton 1999].

One of the major advantages of the BBN is its intuitive, graphical
representation, but there are mathematical advantages too. Wright and Cai
[1994] present a detailed comparison of BBNs with more classical statisti-
cal representations of probability as well as Dempster-Shafer theory. They
take the view that BBNs greatly assist the understanding and representa-
tion of assumptions about conditional dependence relationships between
uncertain variables within a system. Pearl [1988] suggests that BBNs
provide a much richer language and better represention of useful indepen-
dencies than Markov networks. Modern BBN tools also allow for unob-
served variables, as is the case with hidden Markov models, and they
employ computationally very efficient Bayesian probability-updating algo-
rithms.

A BBN is made up of nodes and arcs. The nodes represent variables, and
the arcs represent (usually causal) relationships between variables. The
example in Figure 1 is a fragment of the net described in more detail in
Section 3 of this article. It shows the complexity of an action as a variable
that is affected causally by two factors: (1) the level of physical detail and
(2) the cognitive complexity of that action. Variables with either a finite or
an infinite number of states are possible in a BBN, so the choice of
measurement scale is left to the analyst’s discretion. As demonstrated in
the case study (see Section 4), we have assigned these variables to one of
the three possible states: high, medium, or low.

In the example of the net fragment in Figure 1, the nodes cognitive
aspects and physical aspects have no incoming arcs; they are root nodes. In
such cases and in the absence of any evidence one way or the other, it is
equally as probable that the physical aspects of any one action,
for
example, be high or medium or low; in other words we assume a uniform

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

345

Table I. Node Probability Table (NPT) for the Action Complexity Node

Cognitive

High

Medium

Physical

High Medium Low High Medium Low High Medium Low

Action

Complexity

medium

high

low

0.9
0.1
0.0

0.7
0.2
0.1

0.6
0.3
0.1

0.6
0.3
0.1

0.5
0.2
0.3

0.3
0.3
0.4

0.3
0.3
0.4

Low

0.2
0.2
0.6

0.0
0.1
0.9

prior. For the action complexity node, however, if both the cognitive and
physical aspects of a particular action are high, then the probability of the
overall complexity being high is greater than if the action has a low level of
physical detail and involves little cognitive ability. In the BBN we model
this by filling in a node probability table (NPT). Table I shows the NPT for
the action complexity node.

In Table I column 1, for example, the figures show, that, if the cognitive
aspect of the action is high, while the physical detail of the action is high,
then the probability of overall action complexity being high is 0.9, and
medium 0.1, with a zero probability of it being low. The table is configured
in this way, by estimating the probabilities for the output variables by an
exhaustive pairwise combination of the input variables.

Most frequently, the NPTs of BBNs are populated with probabilities
which are elicited as the subjective judgments of a domain expert. Some-
times they are the agreed consensus of several domain experts. If there are
data available, however, e.g., from accident or incident reports, values can
be obtained from such objective data.

Once the topology of the net and the NPTs are completed, Bayes theorem
is used to calculate the probability of each state of each node in the net. The
result of this calculation is a probability distribution for the states of each
node. Then, when evidence is available to determine the states of particular
nodes from particular scenarios, the values entered are propagated through
the network, updating the values of other nodes. These calculations are
entirely automated by a BBN tool, Hugin Explorer (see Hugin A/S Web site,
www.hugin.dk). The result is a network from which predictions can be
made regarding the probability of certain variable(s) being in particular
state(s), given the combination(s) of evidence entered.

Section 3.2 presents an example BBN which models a set of influencing
factors and how these factors impact upon the probabilities of mistake-
errors and slip-errors.

3. THE IMPACT ANALYSIS METHOD

3.1 Overview

The Impact Analysis Method for analyzing error-related requirements is
summarized in Figure 2.

First the application domain is analyzed to identify the important
influencing factors. This stage is not detailed in this article as standard

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

346

•

J. Galliers et al.

Fig. 2. Method stages. Input and output expressed in a dataflow diagram format.

techniques can be used, e.g., HAZOPs [CIA 1993; Earthy 1995]. The
taxonomy of influencing factors is also described elsewhere [Sutcliffe et al.
1998a; 1998b]. The output is a model of the domain, including the con-
trolled and controlling system, with a view on the important safety prob-
lems.

These are then used to construct the BBN model either by tailoring the
generic model described in this article or by building a new model from
scratch. The network probability tables of the BBN are configured based on
previous evidence of causal relationships or expert judgment. The output
from this stage is a configured BBN which could be reused for many
analyzes within the same domain.

The next stage is to gather evidence for input to the BBN model.
Information may be acquired by a variety of techniques and sources as
outlined in Table II. Questionnaire-like inventories can capture psycholog-
ical attributes of people, while environmental variables can be taken from
past histories or directly measured. Inevitably some variables will have to
be estimated if no measures or previous data are available. A set of
scenarios are then selected to be run in the outer layer of the BBN model
(see Figure 3). These scenarios may be directly based on historical evidence
gathered during the domain analysis; alternatively they may be fictitious
sensitivity analyses to investigate different combinations of the key influ-

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

347

Table II.

Inputs and Sources of Information for Each Method Stage

Inputs

Source

Method stage

Output

Domain knowledge,

safety history

Domain experts,
safety documentation

Domain analysis

List of key safety-
critical properties for
the domain

Causal model as BBN

Select influencing
factors

Method, domain
experts

Calibrate BBN

BBN model with
configured NPT tables

Influencing factor,

Method

taxonomy, Generic
BBN

Generic BBN 1 NPT

tables, prior
problems

Questionnaires,

experiments, expert
judgment

Users, domain
subjects, domain
experts

Measure/estimate
variables

Input values for outer
BBN scenarios

Influencing factor,

variables, scenarios
of domain

Estimates or
measures Domain
experts

Select domain
scenarios

1-N scenarios to be
run for outer layer
BBN

Scenario variables—

See stage 4

H/M/L

Input scenario
variables

Outer layer BBN for
selected scenario

Interviews and

observation of user
activities

Users, domain
experts,
documentation

Task model and
complexity estimates/
subgoal

List of task steps with
cognitive/physical
complexity

Subgoal complexity

Task complexity
measure or estimate

Input subgoal
complexity assessment

Configured BBN with

See stage 8

Run BBN

input variables

subgoal, safety
guidelines

Error probabilities for

Stage 9 method

Walkthrough subgoal User interface—safety
requirements for each
interaction step.

Inner layer BBN for
selected scenario/task
subgoal

mistake/slip
probabilities for task
subgoal

encing factors. The input variables are loaded for each scenario to model
variations in the domain-related influencing factors, so a complete analysis
takes place within several iterations. The BBN is run for N scenario
iterations and M iterations for each task subgoal within each domain
scenario.

The user’s task is analyzed using conventional techniques such as
knowledge acquisition for task (KAT) [Johnson 1992] or use cases in
object-oriented development [Graham 1996]. A list of task goals with
estimated physical and cognitive complexity is used as input for each run of
the inner layer model (see Figure 3). Optionally the quality of the user
interface may be added at this stage. The BBN for each scenario, thereby,
produces slip and mistake-error estimates for each subgoal in the user’s
task.

The analyst then assesses the error probabilities for each subgoal, with
the walkthrough model to analyze the generic requirements for safe inter-

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

348

•

J. Galliers et al.

Fig. 3. A generic BBN network for error analysis showing outer (domain) and inner (task
subgoal) layers.

action. The model indicates requirements that should be present in the
design to counteract or reduce mistakes or slip-errors at each stage of
interaction. The output from this stage is a generic list of requirements for
the user interface and system functions. Alternatively,
if a prototype
already exists, this stage can produce a critique of the current design.

Sections 3.2 and 3.3 describe the stages of the Impact Analysis Method in

more detail.

3.2 Causal Analysis

A domain-specific BBN model
is created by selecting a subset of the
influencing factors that apply to the domain and application under consid-
eration. This BBN represents a working hypothesis about combinations of
causal factors in the domain under consideration that is grounded in
previous work [Sutcliffe et al. 1998a; 1998b]. Many combinations of influ-
encing factors are possible, and each domain requires a particular model.
Here we propose a general modeling tool that can be instantiated with
domain-specific information.

The topology of the BBN shown in Figure 3 has been separated into an
outer and an inner net. The aspects of the BBN that model general
properties of the user and task context, make up the outer net. The inner
net describes design qualities of the user interface and complexity of each

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

349

user system interaction in the scenario. The combination of inner and outer
factors affects the probability of human errors as either mistakes or slips.
The user interface node represents estimated or measured usability of
the user interface design, whereas the cognitive and physical complexity
describe human actions either with the user interface or in a manual task.
We separate physical and cognitive attributes of actions because physical
actions require attentional and perceptual resources, and hence are more
prone to slip-errors [Norman 1990a] in comparison with cognitive actions.
Assigning complexity to physical and cognitive actions requires expert
judgment. Physical actions with many complex manipulations and sensori-
motor coordination, which have not been learned as skills will be complex
when judged by metric-based techniques, e.g., cognitive complexity theory
[Kieras and Polson 1985], or empirical evidence in ergonomics [Bailey
1982]. Similarly cognitive actions which have not been learned will be
complex, while known complex problems can be solved by retrieving the
appropriate memory. This complexity judgment interacts with task knowl-
edge in the outer layer, which has to be taken into account when setting up
the scenario for a BBN analysis. The outer layer takes a subset of the
influencing factors from our previous taxonomy [Sutcliffe et al. 1998a;
1998b]. The subset we selected represents a working hypotheses about the
factors that are likely to be more important causes of error, i.e., time
pressure and fatigue, following Reason’s [1990] work on frequency gam-
bling, with judgment, motivation, and knowledge, which are important
influences on mistakes and task performance [Bailey 1982].

In accordance with the views of Reason [1990], Hollnagel [1993], and
Rasmussen [1983], the BBN is configured so that the situation comprising
the relationships between external and internal factors on the operator
contributes to slips more strongly than to mistakes. User knowledge or
expertise, affected by domain and task knowledge, as well as a level of safety
awareness, contributes to both types of errors but more strongly to mis-
takes. General aptitude, caused by combinations of ability and judgment,
primarily influences mistakes.

The BBN model, once configured,

is run against a task script of a
particular user’s behavior with the system. In order to do this, each input
node is set to one of its alternative states, in this case high, medium, or low.
For example, of the outer BBN variables, an able user will have the nodes
judgment and ability set to high. Ability is measurable according to the
user’s level of qualification for the task at hand. Judgment can be deter-
mined by a questionnaire inventory such as the Need for Cognition (NFC)
scale [Cacioppo and Petty 1982]. Domain knowledge may be low, but task
knowledge and safety awareness will be high in a user familiar with
technical and safety procedures in his or her workplace, but unfamiliar
with the technical background. These three measures are obtainable via
questionnaires devised at the workplace and specific to the domain and
task(s) involved. Motivation, fatigue, and level of time/interrupts being set
to high, medium, or low has to be an estimate, but preferably based on
historical data of individual operators. Environmental conditions was ex-

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

350

•

J. Galliers et al.

panded for our case study (see Figure 7) as affected by temperature,
humidity, and dust levels, all of which can be easily measured. The
environmental conditions node may be set to good, average, or bad.

For the inner BBN variables, the following estimates have to be made,
for each action or subgoal

depending on the granularity of analysis,
comprising a particular task:

(1) The action has to be assessed as high/medium/low cognitive complexity.
Actions requiring complex decision making, problem solving, or judg-
ment are rated as high complexity. Conversely, simple physical actions
and computer I/O are rated low. A simplified version of cognitive
complexity theory [Kieras and Polson 1985] was used to make this
assessment. For example, an action with a high cognitive aspect was
considered as one with relatively more decision points than those
considered medium or low on our cognitive scale.
(An alternative
approach would be to use the Task Load Index of Hart and Staveland
[1988]).

(2) Physical complexity is rated for each action-step. Complex manipula-
tions involving precise movements and detailed coordination are rated
high, whereas single action-step, discrete actions (e.g., stop/start ma-
chine) are rated low. Again, a version of cognitive complexity theory
was employed here, in which a high physical activity had many skillful
actions involving sensorimotor coordination. Physical complexity is
taken to be a function of the precision, number, and diversity of
movements required, the degree of sensorimotor coordination, duration
of action, and number of different modalities and limbs involved.
Generally, actions which involve multiple limbs and complex sensori-
motor coordination are complex.

(3) If the action involves a human-computer interface then its usability is
assessed (U interface node). Measures for usability can be acquired
from evaluation of users’ observed problems using methods such as
Model Mismatch Analysis (MMA) or cooperative evaluation [Sutcliffe et
al. 1998a; Monk et al. 1993]. If a prototype of the user interface design
does not exist the usability score is set to medium for all potential
human-computer actions, and low (i.e., no design problems) when
actions are unlikely to involve human-computer interaction.
Once the outer and inner BBN variables are set, the BBN tool automat-
ically calculates the propagation of probabilities throughout the net-
work.

Note that some of the influencing factors representing the user and
task/domain can be set as constants in the application. For example, in our
case study in Section 4, the user being modeled is assumed to have little
domain knowledge. The value of the domain knowledge node was, there-
fore, set to low for all test scenario runs. Other aspects of the user were
varied, as were all aspects of the environment. The quality of the user

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

351

interface was set to medium. Mistake-error and slip-error probabilities for
each action and for the different test scenarios were then compared.

3.3 Consequence Analysis

The BBN model focuses on the probability of human error in user system
interaction. In this section we propose a set of heuristics that help analysis
of the consequences of human error and provide generic guidelines to
address these problems. So far we have only differentiated between physi-
cal and cognitive actions. Cognitive actions, generally, only involve people
and characterized decision making, problem solving, and judgment. Com-
plexity in cognition depends on the number of variables, complexity of the
problem, and how familiar the problem domain is. Known problems that we
have solved before are naturally simple, as the solution becomes part of our
routine skill. The BBN, therefore, predicts that complex problems judged
by experts to be difficult will create a high probability of mistakes if the
user’s knowledge of the task and domain is low. Simple problems, on the
other hand, are less prone to knowledge-bound errors, because simple
choices and decisions can be made from commonsense knowledge and
require less training to arrive at a solution.

Error prevention for purely cognitive actions requires training people in
problem-solving methods and avoiding typical reasoning pathologies such
as encysting (cannot see the wood for the trees), confirmation bias, or
partial mental model formation [Johnson-Laird 1983]. Consideration of
these topics is beyond the scope of this article, although we have dealt with
human reasoning errors elsewhere [Sutcliffe et al. 1998b]. Physical actions
are even more diverse than cognitive actions, and both mistakes and slips
are closely linked to training. Highly skilled operators will generally carry
out complex actions without mistakes, although they will still be prone to
slips. Novice operators will find complex physical actions daunting and will
make many mistakes as they generate inefficient plans for executing
actions. The error prevention advice here is to reduce the complexity of
physical action where possible by automated assistance or subdividing
tasks, and then train operators to acquire the necessary skill. However, the
distinction between purely cognitive and physical actions is an oversimpli-
fication. In most tasks an operator is interacting with a computer that
controls a safety-critical system, e.g., chemical plant, aircraft, power sta-
tion, etc. When interaction is considered the picture becomes more complex.
Interaction with safety-critical systems has to consider not only error
prevention but also (1) diagnosis of problems once a system failure has
happened, (2) containment of the hazard, and (3) remedial action to restore
the system to a safe state. Some problems arise during the normal course of
operation, but many problems are hidden and only become apparent when
failure in the controlled system (i.e., power plant, aircraft) has occurred.
Hence, the BBN analysis needs to be linked to a cycle of interaction that
accounts for normal operation and human reaction to system failure. To
provide such a perspective we have adapted Norman’s model of action

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

352

•

J. Galliers et al.

Fig. 4. Model of errors pertaining to safety-critical interaction based on Norman [1986].

[Norman 1986] to link error types to different phases of action. The BBN is
applied to a task subgoal which consumes one cycle of the model in a
normal course of operation. Heuristics direct the analyst toward the type of
error that may occur for different phases in interaction, summarized in
Figure 4.

Table III summarizes the link between interaction stages and generic

requirements for both mistake and slip-errors.

The model cycle commences with goal formation and then progresses to
“form intention” or a plan of action to achieve the goal. For skilled users
these two stages are automatic, so mistakes errors are unlikely. However,
even skilled operators may make erroneous plans, so the system should try
to prevent incorrect or dangerous plans from being formed. Slips may
occur, but they are unlikely to be observable at this cognitive stage of the
model. The implications for design are to provide all necessary control and
monitoring functions that the user needs to operate the system. Forming
intentions requires knowledge and awareness of the system; hence, an
interactive visual display of the system is advised to counteract low user
knowledge. Simulations and decision support models to answer “what if”
questions are useful
for counteracting mistakes for novice users, and
incidentally support learning. The more information that can be provided
about the state of the controlled systems, and facilities to forecast the effect

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

353

Table III. The Link between Interaction Stages and Generic Requirements

Action Cycle Stage

Slip Countermeasures

Mistakes Countermeasures

Form goal

n/a

Formulate intention

n/a

Specify action

Execute action

Recognize effect

Interpret change

Evaluate change

Difficult actions, preselect safe
actions
Safety locks, simple
manipulations, fail safe,
prevent dangerous actions,
input validations
Salient warnings, multimodal
warning, clear messages
Clear messages, repeat
warnings
n/a

n/a

Organization metaphors, system
model, procedure hints
Block unsafe action, aid memoirs,
system simulations, operational
metaphors, clear controls
Prompts cues, device affordances

Input validations

Clear feedback, set change in
context
System models to reflect change,
implications of change, situations
awareness models

of human action, the better the design will be in dissuading mistaken plans
(a real example being the planned experiment carried out in the Chernobyl
nuclear reactor).

Action specification may be prone to both mistakes by less well trained
users and slips by all operators. Action specification involves deciding the
detailed course of action, so physically complex goals will be especially
prone to errors. Well-designed representations of the controlled system
help action specification, with transparent detail of the parameters being
controlled (e.g., temperature, pressure in a chemical process) and effector
devices (e.g., valve, heater, pump, etc.). Mistaken and dangerous actions
should be prevented where the effects can be predicted, so warnings should
be built into simulations which forecast the effects of action before execu-
tion. Action specification, therefore, involves faithful presentations of de-
vices and predictable means of action in the human-computer interface.
This entails clear prompts, cues, commands, and operational metaphors for
devices. Slips can be counteracted by warnings and lock outs on undesir-
able actions (e.g., disabling a heater on switch when the temperature is too
high), and by making the effect of an operation immediately apparent and
easily reversible.

Action execution is a physical step in the model when the user issues a
command or enters data into the computer. The consequence of mistake-
errors in the early cognitive stages of the cycle may become visible at this
stage. Only at this stage is the user’s intent detectable, so prevention of
mistakes depends on the ability of the computer system to predict danger-
ous effects. Some effects can be discovered at design time and safety locks
or warnings added to the system (e.g., reducing engine power in an aircraft
while climbing leads to a stall warning). Potentially dangerous actions
should be made difficult. This stage may be prone to slips if the user

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

354

•

J. Galliers et al.

interface does not give sufficient guidance, or makes the action difficult in
terms of sensorimotor coordination. Examples of poor design which may
lead to action execution slips are poor obscure icons, or pointing targets
which are too small. The control device should be tested to ensure it is
within the limitations of human abilities. Slips at this stage can also be
counteracted by constructing devices that make the user aware of the
correct actions (see perceived affordances [Norman 1998]), and setting
actions in appropriate contexts (e.g., control switch icon is displayed on an
appropriate part of a chemical plant diagram).

The next three stages involve user perception and interpretation of
system output. Feedback of the effects of user action should be immediately
apparent. However, change may arise independently of human action via a
failure in the controlled system. Slips may occur when the user simply fails
to see or hear the warning message. Feedback has to be immediately
apparent to the user. Messages should ideally be given in two modalities
(i.e., audible and visual warnings). To alert users, the message has to be
salient and located where the user will see or hear it. Warning messages
have to tread a dividing line between giving messages for true alarms while
not giving too many false alarms. This requires careful design of sensors
and calibration of monitoring systems.

Once perceived, information has to be understood by the user. In the
normal course of the model, the change should confirm that the correct
action has taken place, and the effects on the controlled system should be
obvious. This follows the observability principle in HCI [Thimbleby 1990].
In the abnormal course at this stage, both slips and mistakes may happen.
Slips happen when people jump to conclusions and see what they want to
see rather than what has happened. This can lead to capture errors
[Norman 1990b] when the wrong course of action is triggered by an
interpretation slip. Mistakes involve some misinterpretation when the
user’s knowledge is insufficient or erroneous. Both types of errors can be
counteracted by providing clear, simple, but pertinent error messages,
using the user’s language [Neilsen 1993]. Interpretation is also helped by
setting the message in context (e.g., an excess pressure warning is high-
lighted in a vessel icon on the chemical plant diagram).

Evaluation of change in the normal course involves understanding the
effect of action on the controlled system, so the effect of change can be
assessed in terms of safe operation. At this stage additional forecasting
functions may be advisable to project future effects of change to allow the
user to evaluate possible impacts on safety, cost, efficient operation, etc.
The same advice applies in the abnormal course, but this stage also implies
an embedded diagnosis subtask. The user has to discover the reason for
failure and then take remedial action to return the system to a safe state.
Mistakes are especially serious at this stage, as many failures cannot be
anticipated, and hence the operator has to reason with partial knowledge.
Mistakes can be reduced by automated or semiautomated diagnostic tools,
which can reason about the causes for error and then recommend, or even
automatically execute, corrective action. The degree of automation depends

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

355

Table IV. Error Correction Cycle Stages and System Support Facilities to Prevent Mistakes

Error Cycle Stage

Recognize feedback
Interpret problem

Diagnose cause

Plan corrective action
Execute corrective action

and Slips

System Support Facilities

Salient warnings
Locate hazard, display hazard properties, show effect area,
indicate containment
List possible causes, display failure history, infer reasons,
assumption checks
List remedial procedures, warn of side-effects
Clear corrective controls, highlight appropriate controls,
automatically execute coorective action, action checklists

on the designer’s confidence in anticipating the probable causes of failure
and whether known solutions exist. If confidence in solutions is less than
100%, decision support tools can help users with lists of possible failure
causes, remedial treatments, as well as providing visualization and simu-
lation tools to allow treatments to be assessed before actual execution. Slips
in corrective action may also occur when people fail to complete procedures,
so semiautomatic checking on remedial procedures and reminders hints
can help. See Table IV.

The cycle of interaction then continues to the next task goal in the
normal course of action. In abnormal courses, there can be more complexity
in terms of containment actions to counteract the effect of system failure;
also, bringing the system back to a correct state may involve several
procedures. These issues are beyond the scope of this article. In the next
section we introduce a case study which is used to illustrate application of
the method.

4. CASE STUDY

4.1 Introduction

The case study system is a control system for a laser spectrophotometer.
For this case study the task analysis, hazard analysis, and gathering of
domain facts and scenarios are reported in detail in Sutcliffe and Rugg
[1998].

The laser spectrophotometer is a scientific instrument that analyzes the
visible spectra that are created by the laser ionization of a chemical
sample. In normal operation the laser should emit a high-energy light beam
that strikes the chemical sample, causing it to emit light energy that is
detected by the sensor. The light spectrum is analyzed, and results are
displayed on the computer VDU. The spectra have a characteristic pattern
for each chemical. The physical system model is illustrated in Figure 5.
Each laser emission cycle is controlled by software. There are two sub-
systems: one controls the operation of the laser, while the second detects
and analyzes emitted spectra. The controlled system operational sequence
is shown in Figure 5, below the model.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

356

•

J. Galliers et al.

Fig. 5. Physical model of the spectrophotometer system, plus operational sequence.

There are three types of system user. First are expert users, usually
research scientists who possess considerable domain expertise and become
skilled in system operation, i.e., task knowledge. They require the ability to
conduct complex analyses, with full control of the system for different
analyses, and presentation of complex results. The experts’ safety aware-
ness is variable, as not all laboratories have good safety practice. The
second group of experts consists of chief technicians who plan analyses in
public laboratories and companies. Accuracy and reliability of the results
are at a premium. These users need support for planning sessions and
investigation of the results. Their safety awareness is generally good.

The third user group contains skilled laboratory technicians whose
knowledge of the domain and system operation will be average or low.
From their viewpoint, system operation is a routine job. In multiple runs
they may make errors from lack of attention and boredom. Their safety
awareness depends on training. In this case study, we focus solely on the

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

357

Fig. 6. Task model for the system operator expressed in data flow diagram format.

third user group, hereafter referred to as the operator, and the activity of
planning a sample run, which is shown in Figure 6.

Task analysis identifies the normal operation and various fault scenar-
ios, such as if there should be a power failure or failures in the emission
detection apparatus [Sutcliffe and Rugg 1998]. Scenario analysis suggested
a sensitivity to temperature, humidity, and dust levels, which is of partic-
ular relevance given these machines are intended not only for the UK
market, but also for export around the globe. Excessive temperatures were
found to lead to inaccurate readings and increased operator fatigue, as
calibrations then have to be carried out more frequently. High humidity
increases condensation,
leading to possible sample contamination and
hence inaccurate readings. High levels of dirt or dust have the same effect
on machine reliability and caused increased operator workload due to
corrections and abandoned runs.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

358

•

J. Galliers et al.

Fig. 7. Environmental factors affecting the laser spectrophotometer.

Safety culture and level of operator training obviously also vary accord-
ing to the place of operation. The worst-case environment is a poorly
maintained laboratory with excessive ranges of temperature, humidity, and
contaminants, and no maintenance being carried out.

4.2 The Causal Analysis

The BBN shown in Figure 3 models four groups of “outer” BBN variables
including

—external factors such as the environmental conditions, and amount of

time and interrupts,

—internal factors such as fatigue and motivation,

—operator knowledge of the domain and task as well as safety issues, and

—operator judgment and ability.

The “inner” BBN variables are

and cognitive aspects and

—quality of the user interface.

—task complexity defined according to the relative emphasis on physical

As mentioned above, from the hazard analysis of the laser spectropho-
tometer system, it was found that the system is particularly sensitive to
temperature, humidity, and dust. The environmental conditions node of the
BBN can therefore be expanded as shown in Figure 7.

We report a detailed impact analysis of the operator task, planning the
sampling run. In this, the operator has to calibrate the machine against a
set of standard samples, then go through the cycle of loading samples for
each measurement as shown in Figure 6.

For this case study we assumed the operator to be of medium ability and
aptitude. We assumed only some knowledge of the domain and set the
domain knowledge node to medium. Finally, the quality of the user inter-
face (U interface node) was similarly set as to medium.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

359

Table V. Complexity Classification of Subgoals for the System Operator Task (see Figure 6)

Task Subgoal

Cognitive Attributes

Physical Attribute

1. Analyze measurement

Complex—requires reasoning
from domain knowledge

n/a

2. Plan analysis run

Complex—requires planning
but some knowledge reuse

Simple record plan

3. Calibrate Machine (also

recalibrate)

Complex—but set procedures
can simplify

4. Set parameters

Simple—takes parameters
from look up table based on
output from goal 2.

5. Load sample

n/a

6. Take Reading (omitted in

automatic mode)

Simple—find reading on LCD
display

7. Change sample

n/a

8. Analyze results

Complex—reasoning with
domain knowledge to
interpret results

n/a

Simple manipulations,
including load samples,
reading

Simple entry of values

Simple actions open cover,
place sample on holder,
replace cover

Simple—make note of reading

Simple actions, open cover,
remove sample

Thirty-two test scenarios were then run with different combinations of

the following operator attributes:

—safety awareness (high or low),

—level of task knowledge (high or low),

—internal factors of motivation (high or low) and fatigue (high or low),

—environmental conditions (good or bad) which refer to temperature,

humidity, and dust levels, and

—amount of time/level of interrupts (high or low).

Task action complexity was categorized according to the combination of

physical and cognitive aspects of the action as follows:

—Simple: low physical complexity and low cognitive complexity.

—Physical: high physical complexity and low cognitive complexity.

—Cognitive: low physical complexity and high cognitive complexity.

—Complex: high physical complexity and high cognitive complexity.

Each of the 32 scenarios was then run against the 9 subgoals which
composed the task. First we describe the BBN analysis at the subgoal level,
and then we give the consequence analysis walkthrough with one subgoal
to illustrate application of the guidelines. The complexity classification of
the task subgoals is given in Table V.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

s
r
o
r
r
e

 

 
f
o
y
t
i
l
i

b
a
b
o
r
p
%

 

70

60

50

40

30

20

10

0

360

•

J. Galliers et al.

mistake-errors

slip-errors

high

medium

low

sim ple

physical

cognitive

co m plex

action-type

sim ple

physical

cognitive

co m plex

Fig. 8(a). Test Scenario 1. Best case: operator is safety aware; has good task knowledge;
internal factors are good; the environmental conditions are good; and the level of time/
interrupts is low.

The safety implication for errors is primarily data accuracy for subgoals
1– 4 and 8. Data analysis errors can have safety-critical consequences if
mistakes are made in analysis,
for instance, of dangerous chemicals.
Subgoals 5–7 can have safety implications for the operator, as the laser
light is intense and parts of the body should not be exposed to it. The
design, therefore, needs to prevent laser exposure when the sample cover is
open.

4.2.1 Results of Causal Analysis. The interpretation of the resulting
probabilities for errors in the bands “high, medium,
low” was firstly
calibrated for the domain. In this example, low was taken to be an absolute
frequency which would not significantly impede normal system operation,
i.e., , 0.1% errors, or , 1 error per 1000 actions. Medium was estimated as
a range from 0.1% to 1.0% errors, and high was . 1.0% errors. Figures
8(a)–(e) illustrate the results for five of the 32 test scenarios.

Test scenarios 1 and 2 allow a direct comparison of the effects of the
external factors, i.e., environmental conditions (heat, humidity, and dust)
and level of time constraints/interrupts. In these two scenarios, both the
operators are safety aware and knowledgeable about the task. They are
also motivated and not tired. In scenario 2, however, the environmental
conditions are bad, and there is little time and plenty of interruptions. The
model predicts the probability of both mistake-errors and slip-errors will
rise in scenario 2, but the amount of increase is much greater for slip-errors
than mistake-errors. The probability of more than 1 slip in 100 operations
occurring while performing a complex action-type such as recalibration of
the machine, e.g., under test condition 1, is 39%. In test scenario 2 this
rises to 63%. For simple actions such as printing the results, the probabil-
ity of a high level of slip-errors also rises from 16% to 41%. In contrast, the
probability of high levels of mistake-errors only rises from 16% for a simple
action in scenario 1, to 20% in scenario 2.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

361

mistake-errors

slip-errors

s
r
o
r
r
e

 

 
f
o
y
t
i
l
i

b
a
b
o
r
p
%

 

70

60

50

40

30

20

10

0

s
r
o
r
r
e
 
f
o
 
y
t
i
l
i

b
a
b
o
r
p
%

 

70

60

50

40

30

20

10

0

high

medium

low

high

medium

low

sim ple

physical

cognitive

co m plex

action-type

sim ple

physical

cognitive

co m plex

Fig. 8(b). Test Scenario 2. Motivated operator is safety aware; has good task knowledge;
internal
factors are good; but the environmental conditions are bad; and the level of
time/interrupts is high.

mistake-errors

slip-errors

sim ple

physical

cognitive

co m plex

action-type

sim ple

physical

cognitive

co m plex

Fig. 8(c). Test Scenario 3. Well-trained operator with task knowledge is not safety aware.
The environmental conditions are good; however, internal factors such as motivation and
fatigue are bad, and the level of time/interrupts is high.

Test scenarios 4 and 5 similarly allow a comparison of the effects of the
external factors, but within the context of both representing operators with
low safety awareness and little task knowledge. They also differ in that the
operator in scenario 4 is motivated and not tired whereas these are not the
case in scenario 5. This lack of motivation and increased fatigue raises the
probability of high levels of slip-errors approximately 10% more than when
only external factors were bad. Both mistake-errors and slip-errors are at a
very high level for all action-types in these two scenarios, the probability of
high levels of mistake-errors ranging from 60% for simple action-types to
80% for complex action-types. Slip-errors are much lower in scenario 4,
and probabilities of high level range from 22% for simple action-types to
44% for complex action-types. This implies that the internal factors of
motivation and (lack of) tiredness make a substantial difference to slip-
errors.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

362

•

J. Galliers et al.

mistake-errors

slip-errors

s
r
o
r
r
e

 

 
f
o
y
t
i
l
i

b
a
b
o
r
p
%

 

90
80
70
60
50
40
30
20
10
0

s
r
o
r
r
e
 
f
o
y
t
i
l
i

 

b
a
b
o
r
p
%

 

90
80
70
60
50
40
30
20
10
0

high

medium

low

high

medium

low

sim ple

physical

cognitive

co m plex

action-type

sim ple

physical

cognitive

co m plex

Fig. 8(d). Test Scenario 4. Operator lacks safety awareness and task knowledge; however,
the environmental conditions and level of time/interrupts are both good, and the operator is
motivated and not tired.

mistake-errors

slip-errors

sim ple

physical

cognitive

co m plex

action-type

sim ple

physical

cognitive

co m plex

Fig. 8(e). Test Scenario 5. Worst case: operator is not safety aware; has little task knowledge;
internal factors are bad; the environmental conditions are bad; and the level of time/interrupts
is high.

Test scenarios 1 and 4 only differ in terms of levels of operator safety
awareness and task knowledge. In scenario 1 the operator is safety aware
and has good task knowledge; in scenario 4 the opposite is the case.
Otherwise, the environmental conditions and time/interrupts are favorable
in both test scenario, s and the internal factors of motivation and fatigue
are also positive. The results show that the predicted probability of high
levels of mistake-errors is much higher in scenario 4 than in scenario
1—from 16% for simple action-types in scenario 1 to 60% in scenario 4.
Similarly the probability of high levels of mistake-errors for complex
action-type rises from 44% for a complex action-type in scenario 1 to 78% in
scenario 4. In contrast, slip-errors are less affected. They only rise by a
probability of about 5% in scenario 4 in comparison with scenario 1, for all
action-types. This means that training in both task knowledge and safety
awareness is very important to reducing the potential for mistake-errors
rather than slip-errors. Actions involving greater cognitive activity, such as
analyzing results, are more at risk when task training and safety aware-
ness are reduced.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

363

Test scenario 3 gives predictions of high levels of mistake and slip-errors
in a context where the environmental conditions are good but motivation
and fatigue are a problem, as well as there being time constraints and
many interruptions. Safety awareness is low, but task knowledge is high.
The predictions of high levels of slip-errors are similar to those from
scenario 2 where the environmental conditions are bad but motivation and
fatigue are not a problem. The other difference is that in scenario 2 the
level of safety awareness is high. This has an impact on the results by
making the probability of high levels of mistake-errors about 10% less
likely for all action-types in scenario 2 than in scenario 3. Safety awareness
is shown, therefore, to impact slightly more upon mistake-errors than
slip-errors.

By examining the results of all 32 test scenarios, it was also found that of
the external factors, time/interrupts has a relatively greater impact on
particularly high levels of slip-errors than environmental conditions. First,
in general, the implications from these results are the importance of
adequate training of operators in task knowledge and safety awareness.
Second, good design of work environments and user interfaces should
enhance motivation, reduce fatigue, encourage sufficient time for each task,
and limit (if possible) any detrimental effects of environmental factors.

4.3 Consequence Analysis

In this section we apply the safety walkthrough to three different task
subgoals to illustrate how the BBN analysis informs safety-critical design.

4.3.1 Planning Analysis Run. This subgoal has high cognitive complex-
ity, although complexity could be reduced by training with standard
procedures. Mistake-errors are, therefore, likely to be critical when the
operators are not trained and when predictions of a high probability for
medium or high levels of mistake-errors in any scenario need to be viewed
with concern. Decomposing this goal with the walkthrough, the first stage,
formulate goal, is unlikely to present particular problems, although slip-
errors (i.e., forgetting to do this task) may arise from interruptions in the
environment. Forming intentions, or plans for scheduling the run, may be
prone to mistakes. The walkthrough model suggests generic requirements
for visualizations and aid memoirs of procedures, so the user interface
needs to supply checklists for planning and examples of typical analysis
runs. As most of this subgoal only involves human mental reasoning the
remainder of the cycle is less important; however, the selected run does
need to be recorded, and this requires another, separate cycle of human-
computer interaction to enter the run type into the control system. Slips
may occur at this stage, so prevalidation should be used so that the user
can only select from available run types in a menu list (run types are preset
according to number of samples, calibration, automatic, semiautomatic,
etc.). Mistakes in this subgoal will have data accuracy consequences later,
but these are difficult to trap without building an expert system with
considerable domain knowledge to validate the user’s analysis plans.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

364

•

J. Galliers et al.

4.3.2 Set Parameters. This subgoal has low cognitive and physical com-
plexity. Goal formation starts by deciding to set parameters and having the
output from subgoal 2 available as the necessary settings for each run (e.g.,
sample exposure timings from calibrations). Action specification involves
deciding how to find the settings from lookup tables and entering them into
the control user interface. Slips at this stage may involve looking in the
wrong place for the settings, and number transpositions, or omissions in
data entry. Mistakes may arise from misunderstanding the associations
between run types and lookup table entries. The walkthrough method
advises making the cues and prompts clear and prevalidation by displaying
only allowable actions. The valid parameters for the selected run type
should, therefore, be displayed, along with the usual ranges of parameters.
Action execution of entering the parameter values can be followed by
software validation to detect errors. Slips are a particular problem here and
can be trapped by making the entered value clear (recognize effect stage),
by validation routines to trap unreasonable values (interpret change), or
rules to check parameters against each other and known properties of the
run (evaluate change). Simple simulation/ forecasting tools may help, such
as displays that link the entered parameters to a selection of successful
previous run types to allow the user to cross-check their understanding.
Slips in the recognize effect to evaluate change stages imply the need for
clear and legible displays.

System malfunction is possible if the data entry mechanism encounters a
software error or if the laser control software malfunctions. Detection of
these alternative paths implies the need for a monitor system to assess the
operating state of the laser control software, and if failures are detected it
should signal warnings to the user. To prevent interpretation mistakes the
problem should be set in context— e.g., a diagram of the system can be
shown to highlight the fault in the laser control software—at the parameter
input stage, with simple error messages, possible diagnostic causes, and
repair suggestions.

4.3.3 Replace Sample. This subgoal was classified as a simple physical
action. Mistakes do not apply; however, action slips can have serious
consequences if the design does not prevent them. Once a sample run is
initiated the first two stages of this subgoal are simple, an intention to
replace the sample followed by the action specification of removing the
cover and loading the sample. Safety in the execute action stage can be
ensured by design guidelines for making dangerous actions difficult, so it is
hard to place one’s hand let alone eye in the laser beam; in addition, a
hardware/software lock prevents laser beam activation once the cover is
opened. Another possibility is to lock the cover while the beam is active.
The remaining part of the normal course for this subgoal raises few
implications beyond the fact that the effect of removing the sample should
be clear to the user. Note this may conflict with the safety requirement to
prevent dangerous actions that suggests the sample container area should

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

365

be difficult to access and not easily visible. Design frequently involves
trade-offs between safety and other nonfunctional requirements.

More interesting is the possible abnormal course walkthrough at this
stage. Taking the design of software laser lock that was activated by the
cover being opened, what would happen if this were to fail? The method
suggests making the consequences of failure clear to the user through
recognizable, interpretable messages or not allowing the failure to occur
through defence in depth. This suggests a status display should show which
component has failed, with an audible/visual warning when failure is
detected. Defence in depth could be implemented by a infrared beam
behind the cover, so should the safety lock fail, when the operator’s hand
intersects the beam a second safety lock is activated to turn off laser
emission.

This concludes the case study which illustrates how the BBN analysis
first highlights particular scenarios where errors may occur due to a
certain combination of operators and the task environment, then given a
set of tasks, how the BBN predictions of mistake or slip-errors can be
interpreted in a walkthrough of the user’s task to consider which safety-
critical guidelines might be recruited to the design. While this approach
does not claim to be comprehensive, as only a selection of available safety
guidelines has been included [Hollnagel 1993; Leveson 1995],
it does
provide a framework upon which more comprehensive, targeted advice can
be delivered.

5. CONCLUSIONS AND DISCUSSION

The main contribution of this article has been to propose a formal model of
causal
influences on human error and connect a means of estimating
human error to user interface design requirements. The BBN models we
have employed have the advantage of being easy to understand as causal
maps of influences while hiding the complexity of probabilistic reasoning
based on Bayes theorem. This approach has some similarities with John-
son’s work [Johnson 1993; 1996] on connecting informal (design rationale)
representations of safety arguments with formal (belief logic) techniques
for reasoning about such arguments. Our approach is based on a quantita-
tive, probabilistic approach rather than reasoning logics, and the connec-
tion to informality is via the BBN causal map and development of cognitive
walkthrough for safety assessment.

Several approaches have been proposed for formal reasoning for safety-
critical properties in interactive systems, for instance petri nets, temporal
logics [Palanque et al. 1997], and modal action logics [Fields et al. 1995].
While formal languages can represent problems precisely and offer power-
ful reasoning mechanism to uncover violation of system safety, their
precision is also their limitation. The user is rarely represented in such
models apart from being a source of events for the system model. As people
are a major source of system failure, reasoning about diverse contributions
of system failure arising from people and the system environment is

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

366

•

J. Galliers et al.

another contribution of this article. While the causal models we propose
could be formalized in, for example, deontic logics [Massonet and van
Lamsweerde 1998], we believe a quantitative, probabilistic approach pro-
vides decision support for exploring safety-critical requirements that may
then be refined by logic-based reasoning.

However, the model we have proposed is only a first step. It represents a
first cut synthesis of influencing factors based on previous approaches, e.g.,
THERP [Swain and Weston 1988] and psychological theories of human
error, notably Reason’s [1990] work. The process of examining different
sources of errors also builds on Hollnagel’s [1993] cognitive model of human
operational failure. In related work, we have developed Hollnagel’s concept
of error phenotypes (observable failures) and genotypes (causal factors) and
proposed taxonomies of error types at the event level [Sutcliffe et al. 1998a;
1998b]. The BBN analysis in this article enables a more accurate forecast of
the type of error at the cognitive level, and this is tied to error prevention
advice in the walkthrough analysis. The criticality of different errors has
not been explored here, but is another aspect of the problem to be explored
in future work.

BBNs may also be applied to assess the probability of errors in a design
(including human operators). For instance, a more sophisticated BBN
model could be developed from our proposal to model designed defences
against error. In this case a BBN model could be constructed that formal-
izes the walkthrough of a particular design, and the nodes become events/
actions in a specific design. Such BBN application models have been
constructed to assess vehicle safety [Fenton et al. 1999]. Defences against
different system and human error could be attached to BBN nodes and
subnets, as well as being suggested by the walkthrough process we de-
scribed in this article. The vision for our future work is to build several,
more complex BBN subnets that deal with different layers of system safety.
The baseline for more complex models is our taxonomy of causal factors
arranged in layers of managerial, operational, interaction, and physical
system problems [Sutcliffe and Rugg 1998], or a similar taxonomy of
multifaceted failure causality in the MORT method [Johnson-Laird 1984].
BBN subnets could be developed to investigate, for example, maintenance
failure in equipment or qualities of safety culture (poor monitoring, inade-
quate allocation of responsibility, etc). Scenario-based sensitivity analyses
could identify weak points; then defences could be planned to counteract
potential failure. No doubt that defences can be planned by direct applica-
tion of a comprehensive set of guidelines (e.g., Leveson [1995]); however all
safety defences have an implementation cost. BBN sensitivity analysis
provides a tool for designers to investigate a potential design considering
the likely contexts of use (as scenarios) and hence identify the points where
defences should be concentrated.

Validation of BBN models will require extensive study, as experiments
are very difficult to design for testing complex multivariate models. Hence,
we expect domain-specific models may be the way forward where more
evidence in a restricted context can be used to construct BBNs. Probabili-

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

367

ties of error, even when based on reliable models, are only partial guides to
safe designs, so the main value of BBNs may lie in the construction of the
net itself. Detailed consideration of the safety problem while constructing
the net and then discovering unexpected multivariate influences should
prove worthwhile in improving reasoning about safety arguments in de-
sign. The values of error probabilities are as merely guides toward further
analysis and recruiting requirements for error prevention.

The second main contribution of this article is the development of a
walkthrough method for safety assessment that extends Norman’s model of
action. This extends cognitive walkthrough [Wharton et al. 1994] with
safety-related questions cued by each stage of the interaction model and
the slip/mistake-error predictions output by the BBN analysis. Another
angle on connecting error probabilities with design advice that we intend to
explore is to use UAN (User Action Notation) [Hix and Hartson 1993] as a
means of framing human error in the context of its effect on machine
response, with the user interface display and internal system state. UAN
may complement cognitive walkthroughs but at a lower level of design
detail. The cognitive-walkthrough approach can be used for requirements
analysis and safety engineering early in design or for evaluation of proto-
types. The appeal of this approach is being able to quickly assess the users’
task, their working environment, and interface designs, and thereby pin-
pointing potential problems. So far our analysis has addressed errors in
human-computer interaction, and indirectly errors arising from the physi-
cal, controlled system. In the future, BBNs could be customized for predic-
tion of
for instance by a combination of
environment problems, design properties, and effectiveness of mainte-
nance. The walkthrough may also be extended to consider human errors in
judgment and problem solving that are not related to interaction. In this
approach, we are investigating how BBN analyses can be applied to
cognitive models of judgment and decision making, e.g., Payne et al. [1993].
In conclusion, the claims we make for this research are limited. We have
investigated an approach and demonstrated operational feasibility. Demon-
strating effectiveness will require case studies in industry. The power of
the approach lies in the formal model of error causation implemented in the
BBN, coupled with the utility of the guidelines provided by the walk-
through. Although the walkthrough could be used in “standalone” mode the
analysis would be impoverished because properties of the environment and
users would be ignored. Safety cases need to demonstrate that sources of
failure have been addressed at many levels. Validation of the models is part
of our future agenda; however, we are under no illusions about the
difficulty of that task. In spite of this, partially validated BBNs can deliver
utility as “tools for thought” in the design process, and sensitivity analyses
pose questions that direct further investigation.

failure in physical systems,

REFERENCES

River, NJ.

BAILEY, R. W. 1982. Human Performance Engineering. Prentice-Hall, Inc., Upper Saddle

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

368

•

J. Galliers et al.

BELL, B. J. AND SWAIN, A. D.

1985. Overview of a procedure for human reliability

analsis. Hazard Prev. (Jan.-Feb.), 22–25.

CIA. 1993. A Guide to Hazard Operability Studies. Chemical Industries Association.
CACIOPPO, J. T. AND PETTY, R. E. 1982. The need for recognition. J. Person. Soc. Psychol. 42,

1, 116 –131.

EARTHY, J. V. 1995. Full hazops of programmable electronic systems. Contesse Project Rep.

No. 7266-9-0-0.3. Lloyds Register, Croydon, UK.

FENTON, N. 1999. Applying Bayesian belief networks to critical systems assessment. Critical

Syst. Club Newslett. 8, 3 (Mar.), 10 –13.

FIELDS, R. E., WRIGHT, P. C., AND HARRISON, M. D. 1995. A task centered approach to
analysing human error tolerance requirements.
In Proceedings of the 2nd International
Symposium on Requirements Engineering (ICRE ’95), P. Zave and M. D. Harrison, Eds.
IEEE Computer Society Press, Los Alamitos, CA, 18 –26.

GRAHAM, I.

1996.

Task scripts, use cases and scenarios in object-oriented analysis.

Obj.-Oriented Syst. 3, 3, 123– 412.

HART, S. G. AND STAVELAND, L. E. 1988. Development of a NASA-TLX (Task Load Index):
Results of empirical and theoretical research. In Human Mental Workload, P. S. Hancock
and N. Meshkati, Eds. Elsevier, Amsterdam, The Nethlands, 139 –183.

HIX, D. AND HARTSON, H. R. 1993. Developing User Interfaces: Ensuring Usability through

Product and Process. John Wiley and Sons, Inc., New York, NY.

HOLLNAGEL, E. 1993. Human Reliability Analysis—Context and Control. Academic Press,

Inc., New York, NY.

JOHNSON, C. W. 1993. A probabilistic logic for the development of safety-critical, interactive

systems. Int. J. Man-Mach. Stud. 39, 2 (Aug. 1993), 333–351.

JOHNSON, C. W.

1996. The role of error tolerant design in minimising the impact of

risk. Safety Sci. 22, 1, 195–214.

JOHNSON, P. 1992. Human Computer Interaction. McGraw-Hill, London, UK.
JOHNSON-LAIRD, P. N. 1983. Mental Models. Cambridge University Press, New York, NY.
KIERAS, D. AND POLSON, P. G.

An approach to the formal analysis of user

1985.

complexity. Int. J. Man-Mach. Stud. 22, 4, 365–394.

LADKIN, P.

1998. Hazards, risk and incoherence. Article RVS-Occ-98-01. Faculty of

Technology, University of Bielefeld. Available via http://www.rvs.uni-bielefeld.de.

LEVESON, N. G. 1995. Safeware: System Safety and Computers. Addison-Wesley Longman

LEVESON, N. G. AND TURNER, C. S. 1993. An investigation of the Therac-25 accidents. IEEE

Publ. Co., Inc., Reading, MA.

Computer 26, 7 (July), 18 – 41.

MASSONET, P. AND VAN LAMSWEERDE, A. 1998. GRAIL/KAOS, analogical resue of requirements
In Proceedings of the 3rd IEEE International Conference on Requirements
IEEE Computer Society

frameworks.
Engineering (ICRE ’98), C. Heitmeyer and J. Mylopoulos, Eds.
Press, Los Alamitos, CA, 26 –37.

MONK, A., WRIGHT, P., HABER, J., AND DAVENPORT, L. 1993. Improving Your Human Computer

Interface. Prentice-Hall, Englewood Cliffs, NJ.

NIELSEN, J. 1993. Usability Engineering. Academic Press Prof., Inc., San Diego, CA.
NORMAN, D. A.

In User Centered System Design: New
Perspectives on Human Computer Interaction, D. Norman and S. Draper, Eds. Lawrence
Erlbaum Assoc. Inc., Hillsdale, NJ, 31– 62.

1986. Cognitive engineering.

NORMAN, D. A. 1990a. The Design of Everyday Things. Doubleday, New York, NY.
NORMAN, D. A.

interaction, not “over automation”.
Broadbent, J. Reason, and A. Baddeley, Eds. Clarendon Press, New York, NY, 137–145.

The “problem” with automation: Inappropriate feedback and
In Human Factors in Hazardous Situations, D. E.

1990b.

NORMAN, D. A. 1998. The Invisible Computer. MIT Press, Cambridge, MA.
PALANQUE, P., BASTIDE, R., AND PATERNO, F. 1997. Formal specification as a tool for the
objective assessment of safety critical interactive systems.
In Proceedings of the 6th IFIP
Conference on Human Computer Interaction (INTERACT ’97, Sydney, Australia, July
14 –18), S. Howard, J. Hammond, and G. Liindegaard, Eds.
IFIP, Laxenburg, Austria,
323–330.

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

An Impact Analysis Method for Safety-Critical User Interface Design

•

369

PAYNE, J. W., BETTMAN, J. R., AND JOHNSON, E. J. 1993. The Adaptive Decision Maker.

Cambridge University Press, New York, NY.

PEARL, J. 1988. Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan

Kaufmann, San Mateo, CA.

PHILLIPS, L. AND HUMPHREYS, P.

1990. A socio-technical approach to assessing human
reliability. In Influence Diagrams, Belief Nets and Decision Analysis, R. M Oliver and J. Q.
Smith, Eds. John Wiley and Sons, Inc., New York, NY.

RASMUSSEN, J.

1983.

Skills, rules, knowledge; signals, signs, and symbols; and other
IEEE Trans. Syst. Man Cybern. SMC-13, 3,

distinctions in human performance models.
257–266.

REASON, J. 1990. Human Error. Cambridge University Press, New York, NY.
SUTCLIFFE, A. G. 1998. Using scenarios for safety critical user interface design. Tech. Rep.

CHCID/98/7.

SUTCLIFFE, A. G. AND RUGG, G. 1998. A taxonomy of error types for failure anlaysis and risk

assessment. Int. J. Hum.-Comput. Interact. 10, 4, 381– 406.

SUTCLIFFE, A. G., MAIDEN, N. A. M., MINOCHA, S., AND MANUEL, D.

1998. Supporting
IEEE Trans. Softw. Eng. 24, 12 (Dec.), 1072–

scenario-based requirements engineering.
1088.

SUTCLIFFE, A. G., GALLIERS, J., AND MINOCHA, S.

1999. Human error and systems
In Proceedings of the 4th International Symposium on Requirements Engi-

requirements.
neering (ICRE ’99, Limerick, Ireland),

SWAIN, A. D. AND WESTON, L. M. 1988. An approach to the diagnosis and misdiagnosis of
abnormal conditions in post-accident sequences in complex man-machine systems. In Tasks,
Errors, and Mental Models, L. P. Goodstein, H. B. Andersen, and S. E. Olsen, Eds, Taylor
and Francis, Inc., Bristol, PA, 209 –229.

THIMBLEBY, H. 1990. User Interface Design. ACM Press, New York, NY.
WHARTON, C., RIEMAN, J., LEWIS, C., AND POLSON, P. 1994. The cognitive walkthrough method:
In Usability Inspection Methods, J. Nielsen and R. L. Mack, Eds.

A practitioner’s guide.
John Wiley and Sons, Inc., New York, NY, 105–140.

WOODS, D. D. 1989. Modeling and predicting human error. In Human Performance Models
for Computer-Aided Engineering, J. Hochberg, B. M. Huey, J. I. Elkind, and S. K. Card, Eds,
National Academy Press, Washington, DC, 248 –274.

WRIGHT, D. AND CAI, K.-Y. 1994. Representing uncertainty for safety-critical systems. PDCS2

Tech. Rep. 135. Center for Software Reliability, City University, London, UK.

Received: December 1998;

revised: July 1999 and October 1999; accepted: November 1999

ACM Transactions on Computer-Human Interaction, Vol. 6, No. 4, December 1999.

