2010 22nd International Symposium on Computer Architecture and High Performance Computing

Tree Projection-based Frequent Itemset Mining

on multi-core CPUs and GPUs∗

George Teodoro Nathan Mariano Wagner Meira Jr.

Renato Ferreira

Department of Computer Science

Universidade Federal de Minas Gerais

{george, nathanr, meira, renato}@dcc.ufmg.br

Abstract

Frequent itemset mining (FIM) is a core opera-
tion for several data mining applications as asso-
ciation rules computation, correlations, document
classiﬁcation, and many others, which has been
extensively studied over the last decades. More-
over, databases are becoming increasingly larger,
thus requiring a higher computing power to mine
them in reasonable time. At the same time, the ad-
vances in high performance computing platforms
are transforming them into hierarchical parallel
environments equipped with multi-core processors
and many-core accelerators, such as GPUs. Thus,
fully exploiting these systems to perform FIM tasks
poses as a challenging and critical problem that
we address in this paper. We present efﬁcient
multi-core and GPU accelerated parallelizations
of the Tree Projection, one of the most competitive
FIM algorithms. The experimental results show
that our Tree Projection implementation scales al-
most linearly in a CPU shared-memory environ-
ment after careful optimizations, while the GPU
versions are up to 173 times faster than standard
the CPU version.

1

Introduction

Frequent itemset mining (FIM) is a core op-
eration for various data mining applications, at-
tracting attention of the research and commercial
communities in the recent years [7, 20]. Given a
database of transactions, where each transaction
contains a set of items, it consists of ﬁnding all
itemsets that occurs more than a given user thresh-
old (support).

∗This work is partially supported by CNPq, CAPES, FINEP,
FAPEMIG, and INWEB- the Brazilian National Institute of
Science and Technology for the Web.

Due to its importance, a number of works have
focused on this problem and several FIM algo-
rithms have been proposed [2, 3, 11, 20]. These
algorithms may be divided into two classes. The
ﬁrst class includes those algorithms which are an
Apriori [3] variant [20] and ﬁnd frequent item-
sets based on a candidate-generation approach that
is levelwise and followed by the frequency count-
ing of the candidates. The second class of algo-
rithms, which includes Tree Projection [2] and FP-
Growth [11], is based on projection techniques that
employ a tree construction method based on preﬁx
trees [1] and perform mining recursively on pro-
gressively smaller databases. This second class of
algorithms has been shown to outperform the ﬁrst
class, being an order of magnitude faster in several
scenarios.

The current advances in computer architecture
are transforming traditional high performance dis-
tributed platforms into multi-core and many-core
hierarchical environments. Thus, creating efﬁ-
cient FIM algorithms that fully exploit this paral-
lelism is a very important contribution. A num-
ber of works have also developed parallel formu-
lations for ﬁnding frequent itemsets based on the
candidate-generation approach (Apriori − like),
both for shared- and distributed-memory comput-
ers [10, 14, 18, 19], including a GPU single node
based implementation of Apriori [8]. However,
the projection-based algorithms have received rel-
atively little attention [5, 13], while they have been
shown to be an order of magnitude faster than the
candidate-generation based. Moreover, to the best
of our knowledge, it does not exist a projection-
based FIM algorithm for GPUs.

In this paper we propose the ﬁrst projection-
based FIM implementation for GPUs. We par-
allelize the Tree Projection algorithm for multi-
core CPUs and GPUs. The Tree Projection was
chosen as the target of our implementation be-

1550-6533/10 $26.00 © 2010 IEEE
DOI 10.1109/SBAC-PAD.2010.15

47

cause its most compute intensive step is suitable
for data parallelism using a GPU or multi-core pro-
cessors. After careful analysis our implementation
has achieved almost linear speedups in a multi-
core shared memory machine, while its GPU-
version is up to 173 times faster than the CPU-
based sequential code.

2 Tree projection

The tree projection-based frequent itemset min-
ing algorithm (Tree Projection) employs a lexico-
graphic tree as its core data structure, which is
used to guide mining operations. Nodes in this tree
are the frequent itemsets and it assumes a lexico-
graphic ordering among items in a database.
In
this tree, a node on depth (or level) k corresponds
to an itemset of size k, while the root node has no
itemset.

Figure 1. The lexicographic tree with
absolute support value of 2.

In order to explain the algorithm we ﬁrst de-
ﬁne some concepts: The 1-extensions of a node
X is the set of itemset extensions that are gener-
ated from it, i.e. nodes whose itemsets have node
X itemset as preﬁx and so are called frequent
lexicographic tree extensions.
In our example,
Figure 1, the frequent lexicographic extensions of
node a are c and h; the active extensions of a
given node is the subset of its tree extensions that
can still generate frequent itemsets; a node is in-
active when it has no child that can generate more
frequent itemsets; the candidate branches refers
to the set composed by the last item of the item-

48

sets of brothers on the right of a given node, and a
node that is singleton has no candidate branches;
the set of active items are the same as its candi-
date branches if it has been just created, thus be-
ing at the tree boundary, or it is recursively deﬁned
as the union of the itemset of its active extensions
with the lists of active items of all nodes included
in these active extensions.

The projected transaction of a node is the in-
tersection of the transaction’s items with the active
items of this node. For example, if a transaction
is abcdef and the active items of a given node are
(b,c,e), the resulting projected transaction is bce. If
the itemset of the node is not in the transaction, the
corresponding projected transaction is null. For
further details please refer to [2].

Algorithm 1: BreadthFirst

Data: minsupport : s, Database : τ
Result: Frequent itemsets
f reqOnesAtts = f irstScan(support, T );
buildT opLevelT ree(f reqOnesAtts);
k = 1;
while kth level (cid:54)= ∅ do

createT riangularM atrices(kth − 1);
foreach transaction T ∈ τ do
addCounts(T, RootN ode);

checkSupport(kth − 1);
createN ewN odes(kth + 1);
deleteT riangularM atrices(kth − 1);
pruneT ree(kth + 1);
k++;

The Tree Projection algorithm extended in this
work is based on a breadth-ﬁrst search on the lex-
icographic tree, where all nodes at level k are cre-
ated before nodes at level k +1. In Algorithm 1 we
present its pseudocode. The ﬁrst step of this algo-
rithm is a scan in the database computing the fre-
quency of each attribute to determine the frequent
1-itemsets (f irstScan). This information is then
used to create the top level of the lexicographic tree
(buildT opLevelT ree), which consists of the root
and its frequent 1-itemsets children nodes.

After this initial phase,

the algorithm will
expand the tree levelwise, when it creates
level k − 1
counting matrices in nodes at
(createT riangularM atrices(kth − 1)), which
are then used to count the frequency of nodes at
level k + 1. Each node P at level k − 1 maintains
a matrix of dimensions |active extensions(P )| ∗
|active extensions(P )|, where a row and col-
umn exists in this matrix for each item i ∈
active extensions(P ), and each entry that ex-
ists in this matrix indicates the counting of itemset

P ∪ i, j (Since the matrix is symmetric, only one
lower or upper triangular part is maintained).

The algorithm then proceeds by incrementing
the appropriate entries in the existing counting ma-
trices at level k − 1, executing the addCounts for
each transaction. During this phase, each trans-
action is projected from the root node to nodes at
level k − 1, and, if the projected transaction is not
null, its items are combined to increment the ma-
trix entries as shown in Algorithm 2.
After processing all transactions,

the matrix
level k − 1 is veriﬁed re-
of each node at
garding its support (checkSupport(kth − 1)),
and a new node in the tree is then created at
level k + 1 for each entry with adequate sup-
port (createN ewN odes(kth + 1)). Then, the
level k − 1 are deleted
matrices of nodes at
(deleteT riangularM atrices(kth − 1)).

Algorithm 2: AddCounts

Data: transaction : T, N ode : P, Level : k
if P ∈ levelk − 1 then

foreach
(i, j) ∈ 2 − ascent − combination(T )
do

P.matrix[i, j] + +;

else

foreach active extension i ∈ P do

T (cid:48) =:
P roject transaction T onto i;
Call addCounts(T (cid:48));

The last step of the algorithm prunes the tree
(pruneT ree(kth + 1)), which is done by travers-
ing it in a bottom-up fashion, removing the in-
active nodes and updating the remaining nodes.
First, the singleton nodes created at level k + 1 are
removed, since they have no one to combine with.
It then continues pruning the tree and updating the
active items in each node. After removing the in-
active nodes of a level, the remaining nodes have
their active items lists updated following the re-
cursive deﬁnition already discussed. The program
ﬁnally ends when the newest level created has no
active nodes.

In Figure 1 we show an example lexicographic
tree (using absolute support value of 2) that has
been generated and examined up to level 2, but the
nodes at the level 3 have only been created.

3 Parallel Tree Projection

This section describes the Tree Projection im-
plementations for multi-core processors and accel-

erators. First, in Section 3.1 we discuss the par-
allelization opportunities of the algorithm and the
adopted strategy. Section 3.2 then details our im-
plementation to multi-core shared-memory envi-
ronments, where several approaches to avoid race
conditions when updating shared objects are stud-
ied. In Section 3.3 the GPU accelerated version of
Tree Projection is presented.

3.1 Parallelization strategy

As discussed in Section 2, the Tree Projection
algorithm performs the frequent itemsets count-
ing based on a lexicographic tree, which is built
at run-time and may be constructed through var-
ious strategies.
In this paper, as in the original
Tree Projection implementation [2], we employed
a breadth-ﬁrst strategy which, as shown in Algo-
rithm 1, builds each level of tree in consecutive
iterations of the algorithm.

This tree building method embeds two paral-
lelization opportunities: (i) the transactions pro-
cessing parallelism consists of processing the
transactions in parallel on the available hardware,
updating each counting matrix of the nodes at level
k − 1; (ii) the node level parallelism, where each
node being expanded is assigned to a different pro-
cessor, which processes all transactions updating
its node matrix.

While both strategies may have advantages
in different scenarios, we chose the transactions
processing parallelism due to the following rea-
sons: (i) the execution time is dominated by the
addCounts method that processes the transac-
tions, making it a very interesting candidate for
parallelization; (ii) if the node parallelism was em-
ployed, on the other hand, there would be a load
imbalance and the number of nodes may not be
enough to fully utilize all the processors during
the entire execution; (iii) the node level parallelism
also demands more memory, because the matrices
of all nodes being concurrently updated need to be
in memory, which may be unfeasible in many sce-
narios.

Although the transactions processing paral-
lelism has several advantages over node level par-
allelism, it shares the tree nodes that are being ex-
panded among the parallel processes, which may
try to concurrently update the same cell of a given
node matrix, requiring synchronized accesses. The
section below explains different methodologies to
deal with this problem on the CPU and GPU im-
plementations.

49

3.2 CPU parallel implementation

among threads accessing different nodes.

This section presents the CPU shared-memory
parallel Tree Projection. The proposed solution
employs transaction processing parallelism, dis-
cussed in the last section, dividing the transac-
tions among different CPU threads that perform
the addCounts operations independently. Other
phases of the algorithm, which were shown to be
inexpensive, are kept sequential.

The application’s parallel section is built based
on a work queue (transactions) that
is shared
among threads (AdderT hreads) [6], providing a
dynamic assignment that reduces load imbalance
among them. Our implementation also assigns
blocks of transactions in order to minimize the
overhead.

is applied to each input

The addCount operation, shown in Algo-
rithm 2,
transac-
tion updating the proper matrices. However,
when processing transactions in parallel, different
AdderT hreads may need to update the same cell
(i, j) of a given node (P ) matrix (P.matrix[i, j]+
+), demanding synchronized accesses. This prob-
lem, although easily solved in many scenarios, is a
critical aspect in achieving scalability in Tree Pro-
jection, as this shared object is at the core of the
algorithm.

A simple way to eliminate the overhead associ-
ated with avoiding these race conditions is Data
replication, where each process asynchronously
updates its local copy with a reduce at the end of
the parallel section. Unfortunately, in the Tree Pro-
jection it is an unacceptable solution as the size of
data to be replicated (matrices for each tree node)
may be very large and could then create an unfea-
sible memory requirement. Another way to avoid
such undesirable conditions is by creating critical
sections to prevent threads from updating matri-
ces’ cells concurrently. However, when solving
this problem it is important to analyze the size of
the critical sections, and the type of synchroniza-
tion primitive. For instance, in [14] the authors
assess the impact of the size of critical sections to
the performance of data mining tree based appli-
cations, which has a similar structure to Tree Pro-
jection, based on software synchronization prim-
itives. Next we discuss three different software-
based lock schemes and a new lock-free imple-
mentation using hardware based atomic instruc-
tions.

Tree level locking creates a critical section that
is associated with each level of the tree, thus only
one thread can increment matrices at a time. This
approach is simple and inexpensive in terms of
overhead to create locks, but it imposes a barrier

Node level locking employs a lock for each
Node at level k − 1 being updated. Despite its
higher lock management overhead, locking at the
node level will allow different threads to update
matrices of different nodes in parallel.

Cell level locking: employs a lock for each
cell, which may generate high memory require-
ments as the memory used by locks is larger than
the needed by nodes’ matrices. Moreover, each
cell update may generate extra cache misses when
reading its lock structure.

In some conditions, where the critical section
is very small and frequently executed by the par-
allel threads, using traditional software based syn-
chronization primitives may not be a good choice
as it may become a bottleneck to the application
scalability. Thus, in this paper, we evaluate the
use of Wait-free [12] instructions, which are im-
plemented at the hardware level and are avail-
able in most of the modern multi-core proces-
sors, to update such objects shared among differ-
ent threads. The Wait-free instructions guarantees
atomicity when updating memory and eliminates
the need for critical sections, as provided by mu-
texes and semaphores. Using a wait-free opera-
sync add and f etch(address, value),
tion as
included as a gcc built-in, a memory address is
guaranteed to be incremented by value even when
address is a shared object. In order to provide the
atomic add it loads the old value pointed by ad-
dress, adds value to it, and updates the address
with a new value if the old is still the same. To
avoid the ABA problem, one solution is to keep a
counter to make sure that the threads are updating
the correct data generation [4].

3.3 GPU implementation

The Tree Projection is a very computate-
intensive application which makes it an interest-
ing candidate for GPU acceleration. As other al-
gorithms for FIM [11, 20], however, it is irregular
in several dimensions, for instance, because of the
different costs for processing each transaction or
node matrix. This challenging behavior imposes a
barrier for the development of efﬁcient FIM algo-
rithms for GPU, as these processors are more suit-
able for very regular and parallel computations.

The proposed parallelization of Tree Projection
is implemented using CUDA [16], and, as in the
multi-core shared-memory implementation, it ex-
ploits the transactions processing parallelism. The
addCount function was then implemented as a
GPU kernel that process several transactions con-
currently.

50

Algorithm 3 shows the pseudo-code of the
modiﬁed breadth ﬁrst tree building strategy. As
discussed, the addCounts is now a kernel func-
tion that is executed on the GPU and processes a
set of T S transactions each time, exploiting the
GPU parallelism by assigning different transac-
tions to different GPU threads. Since transactions
are organized in lines, one dimension of blocks and
grids is enough.

In the addCounts step, a set of nodes is deliv-
ered to GPU, always respecting its memory con-
straints. In the GPU, transaction blocks are pro-
cessed for each node, where several transactions
of one block are projected in the active items of
the nodes and various entries of the node matrices
are updated simultaneously. After all transaction
blocks are processed, the nodes are brought back
to CPU and their matrices are used to build the next
tree level.

It is important to note that in this GPU version,
the projection of transactions are made directly in
the k − 1 level nodes instead of projecting them
all along from the root in a top-down fashion like
the original implementation. Although this strat-
egy goes against the beneﬁts of eliminating trans-
actions that will not generate frequent itemsets as
soon as possible, it is easier in the GPU.

Another critical problem we address is how to
efﬁciently represent a set of transactions on GPUs.
In the existing FIM algorithm for GPUs [8], the
authors use a m × n transaction-bitmap approach
to represent the database, where n is the number
of transactions and m is the number of items, and
bit (i,j) is set to 1 if item i occurs in transaction
j. Although this representation is interesting due
to its regularity and easy manipulation on GPUs,
it may not be acceptable to store sparse databases
due to the memory requirements.

Thus, we propose a novel compact and simple
representation of the database of transactions for
GPUs, which is not only useful to FIM algorithms
but may also, for example, be used to compute in-
verted indexes on these processors. The proposed
structure, shown in Figure 2, stores the transac-
tions using a vector, adding the size of each trans-
action just before its items. In addition, it creates
an auxiliary index to identify the beginning of each
transaction on the vector where they are stored.
Besides being a very simple and intuitive way to
represent sparse database, this approach provides
a very efﬁcient access pattern, as data elements
are successively located in the memory, a compact
data representation for sparse datasets.

Algorithm 3: BreadthFirstGrid (CPU)
Data: minsupport : s, Database : τ
Result: Frequent itemsets
f reqOnesAtts = f irstScan(support, T );
buildT opLevelT ree(f reqOnesAtts);
k = 1;
while kth level (cid:54)= ∅ do

createT riangularM atrices(kth − 1);
foreach transactionsSet T S ∈ τ do

grid := (T S/T h, 1, 1);
threads := (T h, 1, 1);
addCounts <<<
grid, threads >>> (T S, kth − 1);

checkSupport(kth + 1);
createN ewN odes(kth + 1);
deleteT riangularM atrices(kth − 1);
pruneT ree(kth + 1);
k + +;

Figure 2. Sparse data representation.

4 Empirical results

The experiments were performed using two
machines:
(i) a dual quad-core AMD Opteron
2.00GHz processor, 16GB of main memory, an
NVidia GeForce GTX260 GPU, and Linux oper-
ating system, and (ii) a second machine equipped
with an NVidia GeForce GTX470 GPU. We tested
the algorithm implementations using the three
databases presented in Table 1. The synthetic
datasets were generated using a data generator ob-
tained from IBM Almaden, and the procedure de-
scribed in [3]. In order to provide a fair compar-
ison among different versions of the application,
the programs were compiled using the ﬂag “-O3”.
The results shown in this section are an average
of 5 executions, the highest standard deviation for
CPU and GPU execution are, respectively: 0.75%
and 0.5%.

4.1 CPU multi-core analysis

This section analyzes the Tree Projection im-
plementations to multi-core shared-memory ma-
chines, as a function of the input datasets charac-
teristics. In Figure 3, we present the performance
of the proposed approaches to update shared ob-

51

Dataset

T25.I20.D1000K
T35.I20.D1000K
T45.I20.D1000K

#Items
10,000
10,000
10,000

Avg. Length

Transactions

Characteristics

Sparse repr. size

25
35
45

1,000,000
1,000,000
1,000,000

Synthetic
Synthetic
Synthetic
Table 1. Dataset characteristics.

Density
0.25%
0.35%
0.45%

∼ 49 MB
∼ 68 MB
∼ 87 MB

Bitmap size
∼ 1,192 MB
∼ 1,192 MB
∼ 1,192 MB

jects: Tree level locking (Level), Node level lock-
ing (Node), Cell level locking (Cell), and Wait-
free based (W-F).

In our ﬁrst evaluation the average transaction
length is varied, for a ﬁxed support value of 1%,
as shown in Figures 3(a), 3(b), and 3(c). As a
reference, the sequential execution times for the
databases: T25.I20.D100K, T35.I20.D1000K, and
T45.I20.D1000K, are respectively 68.96, 105.41,
and 296.82 seconds. For the ﬁrst dataset, Fig-
ure 3(a), all proposed solutions achieved good
scalability, being the W-F scheme slightly better
than the others. When using the second dataset:
T35.I20.D1000K, on the other hand, Level and
Node lockings strongly failed to exploit the avail-
able parallelism.
It occurred because as the av-
erage transaction length (T) increases there is a
higher pressure over the critical section, as the pro-
jected transactions are also larger, thus only ﬁne
grained locks could scale. These results are inter-
esting because although the synchronization sec-
tion refers to a very small piece of code (just in-
crement a variable), it is executed very frequently
and will scale poorly for coarse grained critical
sections.

When analyzing the T45.I20.D1000K dataset
(See Figure 3(c)), an even higher degradation of
coarse grained lock based solutions is observed,
while W-F and Node schemes still having good
speedups. For this database, however, the W-F
speedups is superlinear.
In order to explain this
behavior we measured the number of L2 cache
misses for this scheme, Figure 3(d), which shows
a reduction in misses as the number of threads in-
creases. The observed behavior is due to sharing
of read only objects among threads when projec-
tion transactions in nodes. The cache misses where
measured using oprof ile [15] and event counter
setted to 10,000.

Finally, in Figure 3(e), we evaluate the impact
of support to the application performance as it as-
sumes the following values: 0.5%, 0.75%, and 1%.
First, as shown, all schemes suffer with reduction
of support, which is due to larger resulting pro-
jected transactions at the tree nodes and more in-
crements on the critical section. For the shown
support, although, the W-F scheme is the most ef-
ﬁcient for all conﬁguration, achieving near 20%
higher performance over the Cell locking, on aver-
age.

4.2 Tree Projection on GPUs

The Tree Projection implementation to GPUs is
evaluated in this section. Firstly, Table 1 presents
the size in MB for 3 databases when using our
proposed sparse representation and the Bitmap
based currently used by other FIM algorithms on
GPUs [9]. As shown, the amount of memory used
by Bitmap representation is much higher than our
proposed strategy, making it unfeasible to repre-
sent low density and very large databases. This
problem is worsened for GPU accelerated appli-
cations, as data transfers among CPU and GPU is
one of the critical problems for achieving high per-
formance in such accelerators [17].

We also evaluate the impact of the support and
the GPU type for the Tree Projection performance,
comparing its accelerated version to the sequen-
tial CPU based implementation.
In Figure 4 we
present GPUs speedup over the sequential CPU
version as the support is varied, while Table 2
shows the absolute execution times for each pro-
cessor and the dataset T45.I20.D1000K.

Figure 4. GPUs speedup.

These results show that our proposed GPU
based Tree Projection is very efﬁcient when com-
pared to the CPU version, and is also capable of
scaling up the support decreases. The GTX470
and GTX260 achieved maximum speedup over
the sequential Tree Projection of 173× and 95×.
Moreover, the GTX470 near doubles the GTX260
performance for all values of support, what was
expected, as it represents a newer generation con-
taining more stream processors, bandwidth, etc.

Although gains of the GPUs version of Tree
Projection over the sequential CPU still high as the

52

(a) Speedup - T25.I20.D1000K

(b) Speedup - T35.I20.D1000K

(c) Speedup - T45.I20.D1000K

(d) W-F cache misses (L2) - T45.I20.D1000K

(e) Support variation - T45.I20.D1000K

Figure 3. Tree Projection on multi-cores.

Support

CPU

GTX260

GTX470

1.5%
1.25%

1%

0.75%
0.5%

164.068
203.293
296.827
332.128
379.314

1.730
2.509
3.906
6.179
10.003

0.945
1.405
2.221
3.543
5.865

Table 2. T45.I20.D1000K: execution
times (secs.) per processor.

support decreases, the relative performance among
them is reduced. This difference in performance is
the result of the CPU version’s ability to eliminate
the transactions which are not useful at lower lev-
els of the tree, while the GPU evaluates all trans-
actions of the candidates in each level, and conse-
quently is not capable of reducing the computation
cost for smaller supports as less transactions are
used at the lower levels.

The Tree Projection on GTX470 GPU relative
performance, over the sequential CPU-based ver-
sion, as the average transaction length varies for
25, 35 and 45 is presented in Figure 5. As shown,
the GPU relative performance is higher as the av-
erage transaction length increases. This behavior
is due to higher processing costs of larger trans-
actions and consequently more pressure over the
parallel section, boosting the GPU Tree Projection
as its parallel threads have more work per kernel
call.

Finally, in Figure 6, we present the relative ex-
ecution times as the database is scaled, having the
T45.I20.D1000K as the base case. As shown, the
CPU and GPU execution times increase almost
linearly according to the number of transactions,

Figure
speedup varying avg. trans. length.

TX.I20.D1000K: GPU

5.

showing a very good scalability of the algorithm
in both implementations.

Figure 6. Scaling the database.

5 Conclusions

In this paper we presented a multi-core shared
memory and a GPU-based parallelizations of the

53

[7] M.-S. Chen, J. Han, and P. S. Yu. Data mining:
IEEE
An overview from a database perspective.
Transactions on Knowledge and Data Engineering,
8:866–883, 1996.

[8] W. Fang, M. Lu, X. Xiao, B. He, and Q. Luo. Fre-
In
quent itemset mining on graphics processors.
DaMoN ’09: Proceedings of the Fifth International
Workshop on Data Management on New Hardware,
pages 34–42, New York, NY, USA, 2009. ACM.

[9] V. Guralnik and G. Karypis.

tree-
projection-based sequence mining algorithms. Par-
allel Comput., 30(4):443–472, 2004.

Parallel

[10] E.-H. Han, G. Karypis, and V. Kumar. Scalable
parallel data mining for association rules. In SIG-
MOD ’97: Proceedings of the 1997 ACM SIGMOD
international conference on Management of data,
pages 277–288, New York, NY, USA, 1997. ACM.
[11] J. Han, J. Pei, and Y. Yin. Mining frequent pat-
In SIGMOD
terns without candidate generation.
’00: Proceedings of the 2000 ACM SIGMOD inter-
national conference on Management of data, pages
1–12, New York, NY, USA, 2000. ACM.

[12] M. Herlihy. Wait-free synchronization. ACM
Trans. Program. Lang. Syst., 13(1):124–149, 1991.
[13] D. Jin and S. G. Ziavras. A super-programming
approach for mining association rules in parallel
on pc clusters. IEEE Trans. Parallel Distrib. Syst.,
15(9):783–794, 2004.

[14] R. Jin, G. Yang, and G. Agrawal. Shared mem-
ory parallelization of data mining algorithms: Tech-
niques, programming interface, and performance.
IEEE Trans. on Knowl. and Data Eng., 17(1):71–
89, 2005.

[15] J. Levon. OProﬁle Manual. Victoria University of

Manchester, 2004.

[16] NVIDIA. NVIDIA CUDA SDK, 2009.
[17] G. Teodoro, T. D. R. Hartley, U. V. Catalyurek,
and R. Ferreira. Run-time optimizations for repli-
cated dataﬂows on heterogeneous environments.
In Proc. of the 19th ACM International Sympo-
sium on High Performance Distributed Computing
(HPDC), 2010.

[18] M. J. Zaki. Parallel and distributed association
mining: A survey. IEEE Concurrency, 7(4):14–25,
1999.

[19] M. J. Zaki. Parallel sequence mining on shared-
J. Parallel Distrib. Comput.,

memory machines.
61(3):401–426, 2001.

[20] M. J. Zaki, S. Parthasarathy, M. Ogihara, and
W. Li. New algorithms for fast discovery of associ-
ation rules. In D. Heckerman, H. Mannila, D. Preg-
ibon, R. Uthurusamy, and M. Park, editors, In 3rd
Intl. Conf. on Knowledge Discovery and Data Min-
ing, pages 283–296. AAAI Press, 12–15 1997.

Tree Projection algorithm, which is, to the best of
our knowledge, the ﬁrst projection-based frequent
itemset mining implementation for GPUs. We also
conducted a detailed analysis of the algorithm’s
performance on multi-core processors, providing a
study of several strategies to mitigate potential race
conditions when processing transactions in paral-
lel.

The experimental results showed that we could
achieve almost
linear speedup on multi-core
shared memory platforms through the use of low
level wait-free atomic instructions, while tradi-
tional software based locking strategies failed to
provide scalability. Our GPU-based implementa-
tion of the Tree Projection, on the other hand, pro-
posed a new simple sparse data representation and
achieved speedups of up to 173 when compared to
sequential code compiled with “-O3” ﬂag. These
results are very relevant, as the sequential Tree
Projection is already an order of magnitude faster
than Apriori based implementation in several sce-
narios. As future work, we intend to develop a
distributed GPU accelerated Tree Projection, and
utilize CPU and GPU to concurrently execute its
most compute-intensive and parallel section.

References

[1] Preﬁxspan: Mining sequential patterns efﬁciently
In ICDE ’01:
by preﬁx-projected pattern growth.
Proceedings of the 17th International Conference
on Data Engineering, page 215, Washington, DC,
USA, 2001. IEEE Computer Society.

[2] R. C. Agarwal, C. C. Aggarwal, and V. V. V.
Prasad. A tree projection algorithm for generation
of frequent item sets. J. Parallel Distrib. Comput.,
61(3):350–371, 2001.

[3] R. Agrawal and R. Srikant. Fast algorithms for min-
ing association rules in large databases. In VLDB
’94: Proceedings of the 20th International Confer-
ence on Very Large Data Bases, pages 487–499,
San Francisco, CA, USA, 1994. Morgan Kaufmann
Publishers Inc.

[4] J. S. Berrios and M. E. Bermudez. Using wait-free
synchronization in the design of distributed applica-
tions. Future Gener. Comput. Syst., 22(1-2):46–56,
2006.

[5] G. Buehrer, S. Parthasarathy, S. Tatikonda, T. Kurc,
and J. Saltz. Toward terabyte pattern mining: an
In PPoPP ’07:
architecture-conscious solution.
Proceedings of the 12th ACM SIGPLAN sympo-
sium on Principles and practice of parallel pro-
gramming, pages 2–12, New York, NY, USA, 2007.
ACM Press.

[6] D. R. Butenhof. Programming with POSIX threads.
Addison-Wesley Longman Publishing Co., Inc.,
Boston, MA, USA, 1997.

54

