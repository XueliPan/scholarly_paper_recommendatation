Bypass Caching: Making Scientiﬁc Databases Good Network Citizens ∗

Tanu Malik, Randal Burns
Department of Computer Science

Johns Hopkins University

Baltimore, MD 21218

{tmalik, randal}@cs.jhu.edu

Amitabh Chaudhary

Department of Computer Science and Engineering

University of Notre Dame

Notre Dame, IN 46556

Amitabh.Chaudhary.1@nd.edu

Abstract

Scientiﬁc database federations are geographically dis-
tributed and network bound. Thus,
they could beneﬁt
from proxy caching. However, existing caching tech-
niques are not suitable for their workloads, which compare
and join large data sets. Existing techniques reduce paral-
lelism by conducting distributed queries in a single cache
and lose the data reduction beneﬁts of performing selec-
tions at each database. We develop the bypass-yield for-
mulation of caching, which reduces network trafﬁc in
wide-area database federations, while preserving paral-
lelism and data reduction. Bypass-yield caching is altruis-
tic; caches minimize the overall network trafﬁc generated
by the federation, rather than focusing on local perfor-
mance. We present an adaptive, workload-driven algo-
rithm for managing a bypass-yield cache. We also develop
on-line algorithms that make no assumptions about work-
load: a k-competitive deterministic algorithm and a ran-
domized algorithm with minimal space complexity. We
verify the efﬁcacy of bypass-yield caching by running work-
load traces collected from the Sloan Digital Sky Survey
through a prototype implementation.

1. Introduction

An increasing number of science organizations are pub-
lishing their databases on the Internet, making data avail-
able to the larger community. Applications such as Sky-
Query [37], PlasmoDB [31], and Distributed Oceano-
graphic Data System (DODS) [14] use the published
archives for comprehensive experiments that involve merg-
ing,
joining, and comparing Gigabyte and Terabyte
datasets. As these data-intensive scientiﬁc applications in-
crease in scale and number, network bandwidth constrains
the performance of all applications that share a net-
work.

We are particularly interested in the scalability and net-
work performance of SkyQuery [27]. SkyQuery is the

∗

This work was supported in part by NSF awards IIS-0430848 and
ACI-0086044, by DOE award P020685, and by the IBM Corporation.

mediation middleware used in the World Wide Tele-
scope (WWT) – a virtual telescope for multi-spectral and
temporal experiments. The WWT is an exemplar sci-
entiﬁc database federation, supporting queries across
vast amounts of freely-available, widely-distributed data
[15]. The WWT faces an impending scalability cri-
sis. With fewer than 10 sites, network performance limits
responsiveness and throughput already. We expect the fed-
eration to expand to more than 120 sites in 2006.

While caching is the principal solution to scalability
and performance, existing database caching solutions fail
to meet the needs of scientiﬁc databases. Caching is a dan-
gerous technology because it can reduce the parallelism and
data ﬁltering beneﬁts of database federations. Thus, caching
must be applied judiciously. A query against a federation is
divided into sub-queries against member sites, which are
evaluated in parallel. Parallel evaluation brings great com-
putational resources to bear on experiments that are initiated
from the weakest of computers. Caching can reduce par-
allelism by moving workload from many databases to few
caches. Running queries at the databases also ﬁlters results
[4], producing compact results from large tables. Many sci-
entiﬁc queries operate against a large amount of data. Bring-
ing the large data into cache and computing a small result
can waste an arbitrarily large amount of network bandwidth.
The primary goal in current database caching solutions
[3, 18, 22] is to maximize hit rate and minimize response
time for a single application. Minimizing network trafﬁc is
a secondary goal. Organizations have no direct motivation
to reduce network trafﬁc because they are not charged by
the amount of bandwidth they consume. However, it is im-
perative for data-intensive applications to focus on being
good “network citizens” and using shared resources consci-
entiously. If not, the workloads generated by these applica-
tions will make them unwelcome on public networks.

We propose bypass-yield caching, an altruistic caching
framework for scientiﬁc database workloads. As its princi-
pal goal, it adopts network citizenship: caching data in order
to minimize network trafﬁc. Bypass-yield caching proﬁles
workload to differentiate between data objects for which
caching saves network bandwidth and those which should
not be cached. The latter are routed directly to the back-end

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

database servers. Our experiments show that this framework
leads to an overall network trafﬁc reduction.

1.1. Our Contributions

In this paper, we model bypass-yield caching as a rent-
to-buy problem and develop economic algorithms that sat-
isfy the primary goal of network trafﬁc reduction. The al-
gorithms also meet the requirements of proxy database
caching, speciﬁcally, database independence and scalable
metadata. We implement these algorithms in the World-
Wide Telescope.

We introduce the concept of yield employed in
bypass-yield caching. The yield model differs from classi-
cal caching systems, such as page model and object model
caching, in that it considers the amount of data deliv-
ered to an application on a per request basis. In the page
model, a cache contains objects of a ﬁxed size (pages)
and a “cache hit” occurs when an application reads an en-
tire object. Memory hierarchies and operating systems
use the page model [21, 28, 30, 44]. The object model ex-
pands upon the page model to account for variable ob-
ject sizes and non-uniform distances to data sources.
Again, a cache hit
involves accessing the entire ob-
ject in cache. The object model applies to Web caching sys-
tems [6, 18, 22–24, 29, 42, 43] and distributed ﬁle stores
[17]. The yield model follows the object model in that ob-
jects vary in size and each has its own fetch cost. However,
applications that access objects in the cache see vari-
able beneﬁts, depending upon how many bytes of data the
request returns. In a yield cache, queries may return par-
tial results based on selectivity criteria, or they may return
an aggregate computed over an object.

We deﬁne yield-sensitive metrics for caching. Speciﬁ-
cally, we develop the byte-yield hit rate (BYHR), which gen-
eralizes the concept of hit rate in the yield model. BYHR
measures the rate at which a cached object reduces network
trafﬁc normalized to its size (amount of space consumed in a
cache). BYHR can be used to evaluate the utility of a cache
and cache management algorithms. We employ the metric
for eviction and loading decisions in our algorithms.

The yield model leads naturally to our formulation of
bypass caching in which cache management algorithms
make economic decisions that minimize network trafﬁc. We
choose between loading an object and servicing queries for
that object in the cache versus bypassing the cache and
sending queries to be evaluated at sites in the federation.
Network bandwidth is the currency of this economy. An al-
gorithm invests network trafﬁc to load an object in the cache
in order to realize long term savings. For any given request,
loading an object uses more network bandwidth than does
evaluating the query and shipping query results. The bypass
concept is closely related to hybrid shipping [39] and simi-
lar concepts are employed by query optimizers [41].

Based on bypass-yield metrics, we develop a workload-
driven, rate-based algorithm for cache management. The al-

gorithm proﬁles queries against data objects, evaluating the
rate of network trafﬁc reduction. We make the bypass ver-
sus load/eviction decision when a query occurs for an ob-
ject not present in the cache. The algorithm compares ex-
pected rate of savings of an outside object with the mini-
mum current rate of savings of all objects in the cache. Ob-
jects outside the cache are penalized by the network cost to
fetch them from the database federation. Queries to objects
for which the rate does not exceed the minimum are by-
passed. Aging and pruning techniques allow the algorithm
to adapt to workload shifts and to keep metadata compact
and computationally efﬁcient.

The rate-based algorithm works well in practice, but
lacks some desirable theoretical properties. In particular, it
depends upon some degree of workload stability; it uses
prior workload as an indicator of future accesses. Also, it
maintains metadata, in the form of query proﬁles, for ob-
jects in the federation.

We address the theoretical shortcomings of the rate-
based algorithms through on-line algorithms that make
no assumptions about workload patterns. We present a
k-competitive algorithm that uses the rent-to-buy prin-
ciple to load objects into the cache only after bypass
queries have generated network trafﬁc equal to the load
cost. We also present a randomized version of the al-
gorithm with minimum space complexity. It chooses to
load objects into the cache with a probability propor-
tional to the yield of the current query.

We have implemented all of the above algorithms and
experimentally evaluate their performance using a Terabyte
astronomy workload collected from the Sloan Digital Sky
Survey [36]. We also compare their performance against
competitive in-line caching (without bypass), optimal static
caching, and execution without caching. Results indicate
that bypass-yield algorithms approach the performance of
optimal static caching.

Experimental results also address the question of what to
cache for scientiﬁc workloads. We compare caching tables,
columns, and semantic (query) caching. Semantic caching
is attractive for database federations because it preserves
their ﬁltering beneﬁts. In fact, semantic caching lies out-
side the bypass-yield framework; i.e., bypass-yield depends
on query evaluation within the cache. However, we ﬁnd
that astronomy workloads do not exhibit the query reuse
and query containment upon which semantic caching relies
[25]. Rather, astronomy workloads exhibit schema reuse,
conducting queries with similar schema against different
data. For example, a common query iterates over regions of
the sky looking for objects with speciﬁc properties. Thus,
we employ tables and columns as objects to cache in the
bypass-yield framework.

2. Related Work

All caching systems address vital issues, such as cache
replacement, cached object granularity, cache consistency,

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

and cache initialization. In this section, we review related
work on cache replacement algorithms, the choice of object
to cache, and the concept of bypass caching.

In the general paging problem, pages have varying sizes
and fetch costs. The goal is to maintain a cache of ﬁxed
size so as to minimize the total fetch cost over cache misses
[3]. Paging, as used in memory and buffer management sys-
tems [10, 11, 19, 44], caches pages that have uniform size
and cost. Cache replacement policies that are commonly
used in such environments are LRU, LFU, and LRU-K [30],
among others. These algorithms use the single basic prop-
erty of the reference stream [22] in order to minimize the
number of page faults.

The Greedy-Dual (GD) algorithm [44] introduces vari-
able fetch costs for pages of uniform size. The Greedy-
Dual-Size (GDS) algorithm [6, 18] extends GD to the ob-
ject model in which objects have variable size and fetch
cost. On each access to an object, GDS assigns it a utility
value equal to its cost/size ratio. The utility value ages over
time, keeping objects with high temporal locality in cache.
GDS has been found to work well in practice [9, 18]. The
public-domain Squid Web proxy [42] incorporates a modi-
ﬁed version of GDS.

Proxy database caches also employ the object model
with variable size and fetch cost. Objects may be query
results [33–35] or database objects, such as relations, at-
tributes, and materialized views [7, 32]. Much attention has
been paid to cache organization and integration with query
optimization in different application environments [16, 20].
However, these systems use simple policies for cache evic-
tion, such as LRU, LFU, LFF, or heuristic combinations of
these simple policies.

Many caching algorithms use the reference information
in the workload to make better decisions. LRU-K [30] ex-
tends LRU to incorporate information for the last K refer-
ences to popular pages. For database disk buffering, LRU-
K outperforms conventional buffering algorithms in dis-
criminating between frequently and infrequently referenced
pages. GDSP [22] extends GDS to include a frequency
count in the utility measure. Their results show that it out-
performs GDS empirically on HTTP traces. Our rate-based
algorithm uses frequency count similar to GDSP for all ob-
jects in the reference stream, not just those in the cache cur-
rently.

Irani [18] gives algorithms for what can be considered a
bypass cache in the object model. They mention that the op-
tion of being able to bypass a request, i.e., serve a miss with-
out ﬁrst bringing the requested object into the cache, does
not help in reducing the cost of a request sequence. This,
however, is not true for database caches in which the size
of the query result may be much smaller than the size of
the object. Scheuermann et al [35] take advantage of vari-
able query result sizes in a cache admission policy that im-
proves performance by 32%. Their objective is to minimize
the execution cost of a query sequence. Neither apply di-

WAN 

LAN

Server

Cache

DL

BD

CD

Client

AD

Figure 1. Network ﬂows in a client/server pair
using bypass caching.

rectly to our objective of minimizing network cost.

There have been several industry initiatives to develop
caching systems for relational objects [1, 40] and for dy-
namic materialized views that may overlap with each other
[2]. The cache policies used in these systems are simple
compared to hierarchical and widely-distributed systems,
such as Squid [42], but they do underline the beneﬁts of
caching in database systems.

In this paper, we extend the Web caching problem and its
solutions to incorporate the concept of yield. We are con-
cerned not only with objects that have variable fetch costs
and sizes, but also with queries on these objects that return
results of variable size. Through bypass-yield caching and
the presented algorithms, we extend the beneﬁts of caching
in the Web environment to the (more complex) database en-
vironment.

3. The Bypass-Yield Caching Model

Our formulation of bypass caching includes both query
scheduling and network citizenship. In a database federa-
tion, we collocate a caching service with mediation mid-
dleware. When the mediator receives a query against the
federation, it divides it into sub-queries. Each sub-query is
a query against a single site in the federation. The cache
evaluates whether to service each sub-query locally, load-
ing data into the cache, versus shipping each sub-query to
be evaluated at a server in the federation. We term the lat-
ter a bypass query, because the query and its results cir-
cumvent the cache. A bypass cache is not a semantic cache
[12] and does not attempt to reuse the results of individ-
ual queries.

Caches elect to bypass queries in order to minimize the
total network cost of servicing all the queries. We assume
that the cache resides near the client on the network and
try to minimize data ﬂow from the servers to the cache(s)
and client(s) combined. Because each cache acts indepen-
dently, the global problem can be reduced to individual
caches (Figure 1). At this time, we do not consider hierar-
chies of caches or coordinated caching within hierarchies.
The network trafﬁc to be minimized (WAN) is the bypass
ﬂow from a server directly to a client DB, plus the traf-

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

ﬁc to load objects into the cache DL. The client application
sees the same query result data DA for all caching conﬁgu-
rations, i.e., DA = DB + DC, where DC is the trafﬁc from
the queries served out of the local cache. The local area net-
work is not a shared resource and is scalable. LAN trafﬁc
does not factor into network citizenship.

We develop the concept of yield to measure the effec-
tiveness of a bypass cache in the database environment. The
yield of a query is the number of bytes in the query result.
It measures both the network cost of a bypass query and the
network savings of a query served in the cache.

When composed with object sizes and load costs, yield
leads to metrics for bypass-yield caching. In the byte-yield
hit rate (BYHR), we deﬁne one such metric. BYHR measures
the beneﬁt of caching an object, i.e., the rate of network
bandwidth reduction per byte of cache. The metric is eval-
uated based on workload and object properties. Thus, ev-
ery object in the system has a BYHR, regardless of whether
it is being cached or not. BYHR is deﬁned as

BYHRi =

(cid:1)

j

pi,jyi,jfi

,

2

si

(1)

(cid:2)

(cid:2)

for an object oi of size si and fetch cost fi accessed by
queries Qi, with each query qi,j ∈ Qi occurring with prob-
ability pi,j and yielding yi,j bytes. BYHR can be decom-
posed into two components. The ﬁrst arises from yields of
j pi,jyi,j/si, and measures the po-
the different queries,
tential beneﬁt of caching an object. That is, the number of
j pi,jyi,j normalized to
bytes delivered to the application
the amount of cache space si. This component prefers ob-
jects for which the workload would yield more bytes out
of the cache, and, thus, more network savings per unit of
cache space occupied. The second component, fi/si, de-
scribes the variable fetch cost fi of different objects from
different sources, again, normalized by the object size si.
This component helps evict objects with lower fetch costs,
because re-loading these objects into the cache will be less
expensive.

Often, the fetch cost will be proportional to the object
size, fi = csi for some constant c. This is true when (1)
caching objects from a single server, (2) caching objects
from multiple collocated servers, or (3) when networks have
uniform performance. The proportional assumption also re-
lies on networks exhibiting linear cost scaling with object
size, which is true for TCP networks when the transfer size
is substantially larger than the frame size [38]. For these
environments, we use a simpliﬁcation of BYHR, called the
byte-yield utility, deﬁned as

rate to evaluate the cache management policies. BYU de-
generates to hit rate for objects of a constant size, with yield
equal to the object size. In the object model, BYHR degen-
erates to GDSP for yield equal to the object size. The gener-
alization extends to algorithms also; BYHR results in algo-
rithms that achieve the same bounds as competitive paging
algorithms in the page [44] and object [18] models.

A key challenge in employing the proposed metrics lies
in evaluating the probabilities of queries. One approach esti-
mates probabilities based on observed workload access pat-
terns. We employ such estimates in the rate-based algorithm
and develop techniques for aging and pruning. Aging allows
estimates to adapt to changing workloads and pruning limits
the amount of metadata. A different approach creates algo-
rithms that perform well on any access pattern. Rather than
predicting or estimating probabilities, algorithms use eco-
nomic principles to manage cache contents. We take this
approach in our on-line algorithms.

4. The Rate-Proﬁle Algorithm

In this section, we develop the Rate-Proﬁle algorithm
using the byte-yield utility (BYU) metric. The algorithm can
be trivially extended to use byte-yield hit rate (BYHR) for
multiple sources on non-uniform networks. We describe (1)
cache eviction policies based on the performance of objects
in the cache and (2) the bypass versus cache load decision
based on comparison of objects not in the cache with the
current cache state. When combined, these policies form an
algorithm for managing a bypass-yield cache.

The Rate-Proﬁle algorithm compares the expected net-
work savings of items not in the cache against the current
performance of cached items. Both quantities are expressed
as rate of savings: bytes (on the network) per unit time per
unit byte (of cache space occupied). Time is relative and
measured in number of queries in a workload, not seconds.
Rate-Proﬁle estimates access probabilities in BYU us-
ing workload as a predictor. Recency and frequency are im-
portant aspects of the estimates. For objects in the cache, we
maintain frequency counts, which are used as a probability
estimate. We enforce recency by only evaluating frequency
over the cache lifetime. For objects not in the cache, the is-
sue of recency is more complex. We use heuristics to divide
the past into episodes, which represent clustered accesses to
a data object. Within each episode, we use frequency count-
ing to estimate access probability; the division into episodes
enforces recency. When computing a rate of savings esti-
mate for items not in the cache, we consider all episodes,
weighing the contributions of recent episodes more heav-
ily.

The utility of an object in the cache is the measured rate
of network savings realized from queries against that object
over its cache lifetime. We construct a continuous time met-
ric that measures utility, which we call a rate proﬁle (RP).

BYUi =

(cid:1)

j

pi,jyi,j

.

si

4.1. Eviction

(2)

BYHR generalizes previous cache utility metrics in sim-
pler caching models. Page model caching systems use hit

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

The RP of object oi is deﬁned as
(cid:2)

RPi(t) =

j yi,j
(t − ti)si

cached for that episode. The load-adjusted rate (of savings)
for an episode is

(3)

LARi,e = maxt∈{tS (i,e)+1,tE (i,e)}LARPi,e(t)

(5)

in which an object oi of size si is accessed by queries Qi,
with each query qi,j ∈ Qi yielding yi,j bytes. Each of these
queries occur between time t and ti, where t can be evalu-
ated at all times during which an object is cached and ti is
the time at which the object was loaded into the cache. The
rate proﬁle measures the BYU of an object over its cache
lifetime. Accesses to an object increase the RP and time de-
cays it. Rate proﬁles are compact and easy to update in that
they simplify complex access patterns into an average rate
of network savings. For each query serviced in the cache,
we track and sum the yield, allowing us to evaluate the rate
proﬁle at any time.

Rate proﬁles of different items are compared to select
“victims”: items to be evicted from the cache. The item
with the lowest rate proﬁle has the lowest rate of savings.
Thus, to create space for new objects in the cache, we dis-
card items with the lowest current RP. Because time de-
cays the RP, unused items age out of the cache. The RP
does not retain the speciﬁc times of accesses and, thus, is
not weighted towards recency within the scope of an ob-
ject’s cache lifetime. However, a lower rate does represent
lower average savings.

4.2. The Bypass Decision

For objects not in cache, we compute utility based on
past performance, which estimates future network savings.
In the load-adjusted rate (LAR), we construct a metric based
on BYU that estimates the utility of an object were it to be
loaded into the cache. The LAR is expressed in savings rate:
the same units as cache RPs. Thus, the LAR may be used to
compare directly the expected performance of an object not
in the cache against the current performance of the cache
contents. In the LAR, we account for episodes of queries
and composing and aging the contributions of all episodes.
To compute the LAR, we require some intermediate
quantities. Items incur a network cost to be loaded into the
cache, which reduce the total network savings. We account
for this by reducing the rate proﬁle by the load cost of an
object, constructing a load-adjusted rate proﬁle for an indi-
vidual episode

LARPi,e(t) =

(cid:2)

j yi,j

(t − tS(i, e))si

− fi
si

in which and tS(i, e) is the start time of the current episode
and t is any time in episode e subsequent to tS(i, e). This
quantity is a proﬁle, a continuous time metric, and must
be distilled into a single value. For each episode, we take
the maximum value, which represents the maximum rate of
savings that would have been realized were the object to be

in which tE(i, e) is the end time of episode e. The max-
imum value describes the balance point between network
savings overcoming the initial load cost and reduced usage
of the object decreasing its utility. Finally, we consider con-
tributions over all episodes to generate an expected rate of
savings

(cid:2)

LARi =

e∈Ei LARi,ewe
(cid:2)

e∈Ei we

(6)

in which we is a function that weights the contributions of
episode e drawn from all episodes Ei of object oi.

To make the bypass decision when the cache receives a
query, we compare the LAR of the requested object against
the minimum RPs of items in the cache. If enough cached
objects have lower RPs (to make space for the requested
object), the requested object will be loaded into the cache.
Otherwise, the query will be bypassed.

Rate-Proﬁle employs simple economic principles. It in-
vests in the future savings when loading an object and in-
curs a load penalty when the expected savings of an ob-
ject not in the cache exceeds the current savings of objects
in the cache. However, when using the RP to evaluate ob-
jects already in the cache, we do not include a load penalty,
because it is a sunk cost. This ensures that the cache is con-
servative in its evictions, which is an important aspect of al-
gorithms in the bypass-yield model. Objects must reside in
the cache long enough to recover the load investment. All of
this decision making is based on comparing expected ver-
sus current network savings using a single currency.

4.3. Episodes

We employ simple heuristics that divide the workload
against an object into disjoint episodes. Each episode repre-
sents a clustered set of accesses to an object. There are haz-
ards in choosing episodes incorrectly. If episodes are too
long, then the utility of an object gets reduced by averag-
ing over too long an interval. If episodes are too short, then
the object does not get used enough to overcome the load
penalty. An episode begins on the ﬁrst access to an object
and we terminate the current episode and start a new episode
when either

(4)

1. LARPi,e < c · LARi,e or
2. the object has not been accessed during the last k

queries.

In our experiments, we chose c = 0.5 and k = 1000. The
ﬁrst rule extends episodes as long the rate increases and
allows for some decrease in rate in order to survive short
idle periods between bursts of trafﬁc. The second rule en-
sures that the episodes of lightly used objects do not last

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

for long periods, observing that the rate will always be in-
creasing until the load penalty has been overcome, i.e., un-
til LARPi,e > 0. The parameters of these heuristics have
not been tuned carefully, nor do we support that this is the
only or best technique for dividing workload. Our experi-
ence dictates that episodes are mandatory to deal with bursts
in workload and that our results are robust to many param-
eterizations.

5. On-line Algorithms

We present OnlineBY and SpaceEﬀBY, the second and
third in our suite of algorithms for bypass-yield caching.
OnlineBY achieves a minimum level of performance for
any workload. In particular, we show its cost is always at
most O(lg2 k) times that of the optimal algorithm, where k
is the ratio of the size of the cache to the size of the small-
est possible object in the cache. Further, to achieve this per-
formance, OnlineBY does not need to be trained with a
representative workload. However, we expect OnlineBY to
under-perform Rate-Proﬁle in practice, because it forgoes
workload information.

SpaceEﬀBY is a space efﬁcient algorithm. Both
Rate-Proﬁle and OnlineBY need to store informa-
tion for objects in the federation, whether
they are
in the cache or not. This may prove to be impracti-
cal. SpaceEﬀBY uses the power of randomization to do
away with the need to store object metadata. It has, how-
ever, no accompanying performance guarantees.

OnlineBY is an amalgamation of algorithms for two on-
line sub-problems: (1) the on-line ski rental problem and
(2) the bypass-object caching problem. The next subsection
describes these sub-problems and their known algorithms.
The subsection after that describes OnlineBY and proves
the bound on its performance. Finally, there is a subsection
describing SpaceEﬀBY.

5.1. Related Sub-problems

On-line ski rental is a classical rent-to-buy problem [13].
A skier, who doesn’t own skis, needs to decide before every
skiing trip that she makes whether she should rent skis for
the trip or buy them. If she decides to buy skis, she will not
have to rent for this or any future trips. Unfortunately, she
doesn’t know how many ski trips she will make in future,
if any. This lack of knowledge about the future is a deﬁn-
ing characteristic of on-line problems [5]. A well known
on-line algorithm for this problem is rent skis as long as the
total paid in rental costs does not match or exceed the pur-
chase cost, then buy for the next trip. Irrespective of the
number of future trips, the cost incurred by this on-line al-
gorithm is at most twice of the cost incurred by the optimal
ofﬂine algorithm.

If there was only one object to cache, the bypass-yield
problem would be nearly identical to on-line ski rental. By-
passing a query corresponds to renting skis and loading
the object into the cache corresponds to buying skis. The

one difference is that renting skis always costs the same,
whereas the yield from queries differs. However, the same
algorithm applies to the bypass-yield problem, again at cost
no more than twice optimal.

Bypass-object caching can be viewed as a restricted ver-
sion of the bypass-yield caching in which queries are lim-
ited to those that return a single object in its entirety. For-
mally, in bypass-object caching, we receive a request se-
quence σobj = o1, . . . , on for objects of varying sizes. Let
the size of object oi be si. If oi is in the cache, we ser-
vice the request at cost zero. Otherwise, we can either (1)
bypass the request to the server or (2) ﬁrst fetch the object
into the cache and then service the request. Both cases in-
cur cost fi. In the former, the composition of the cache does
not change. In the latter, if the cache does not have enough
space to store oi, we evict some objects from those currently
cached to create the required space. The objective is to re-
spond to each request in a manner that minimizes the to-
tal cost of servicing the sequence σobj without knowledge
of future requests.

Irani [18] gives an O(lg2 k)-competitive algorithm for
bypass-object caching. She calls it optional multi-size pag-
ing under the byte model. Recall that an on-line algorithm
Aobj is said to be α-competitive if there exists a constant b
such that for every ﬁnite input sequence σ

cost(Aobj on σ) ≤ α · cost(OPT on σ) + b

in which OPT is the ofﬂine optimal. Again, k is the ratio
of the size of the cache to the size of the smallest possible
object.

OnlineBY extends the algorithm for bypass-object
caching to the bypass-yield caching problem. It does so
by running an instance of the on-line ski rental algo-
rithm for every object that is being queried. When enough
queries for an object have arrived such that the cumu-
lative yield matches or exceeds the size of the object,
OnlineBY treats the situation as if a request for the en-
tire object arrives in bypass-object caching. The next sub-
section describes OnlineBY formally and shows that it is
O(lg2 k)-competitive.

5.2. OnlineBY

OnlineBY is an on-line algorithm for the bypass-yield
caching problem. OnlineBY receives a query sequence
σ = q1, . . . , qn. Each query qj refers to a single object oi
and yields a query result of size yi,j. If oi is in the cache, we
service the query at cost zero. Otherwise, we can either (1)
bypass the query to the server at cost c(qj ) or (2) fetch ob-
ject oi into the cache at cost fi and service the query in
cache. We deﬁne c(qj) equal to (yi,j/si) · fi, in which si
is the size of oi. In the former case, the composition of the
cache does not change. In the latter case, the cache evicts
objects, as necessary, to create storage space for oi. The ob-
jective is to respond to each query in a manner that min-

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

OnlineBY (qj )

SpaceEﬀBY (qj )

/* qj is the next query in the input sequence.

qj refers to object oi and has yield yi,j.
For all i, BYUi is initially set to 0.
Aobj is an algorithm for bypass-object caching. */

/* qj is the next query in the input sequence;

qj refers to object oi and has yield yi,j.
Aobj is an algorithm for bypass-object caching. */

BYUi ← BY Ui + yi,j/si.
If (BYUi ≥ 1)

BYUi ← BY Ui − 1.
oi is generated as the next input for Aobj.

The cache is maintained according to Aobj.
If (oi is in the cache)

Service qj from the cache.

Else

Bypass qj to the server.

Figure 2. The OnlineBY Algorithm.

imizes the total cost of servicing the sequence σ without
knowledge of future queries.

OnlineBY uses any on-line algorithm for the bypass-
object caching Aobj as a sub-routine. This generates a fam-
ily of on-line algorithms for bypass-yield caching based
on different on-line algorithms for bypass-object caching.
OnlineBY is described in Figure 2. The algorithm employs
the byte-yield utility metric (BYUi, Section 4). As with
Rate-Proﬁle, on-line algorithms can be extended trivially
to BYHR. OnlineBY maintains a cache according to what
Aobj does. In other words, OnlineBY loads and evicts the
same objects as Aobj at the same times (in response to the
object sequence presented by OnlineBY).

The main result of this section is the following.

Theorem 5.1 For every α-competitive on-line algorithm
Aobj for bypass-object caching, OnlineBY creates a cor-
responding on-line algorithm for bypass-yield caching that
is (4α + 2)-competitive.

Corollary 5.2 There exists an on-line algorithm for
bypass-yield caching that is O(lg2 k)-competitive, where k
is the ratio of the size of the cache to the size of the small-
est object.

Proof: The corollary follows from the on-line algorithm
given by Irani [18]. (cid:1)

Towards proving Theorem 5.1, we state deﬁnitions and a
lemma. Given the input sequence σ = q1, . . . , qn, let σi =
qj1 , . . . , qjni be the sub-sequence consisting of all queries
that refer to oi. Divide σi into a sequence of groups such
that each group gk consists of consecutive queries from σi
and

(cid:1)

yi,jl
si

= 1.

qjl

∈gk

(7)

The idea is that the cost of bypassing queries in gk should
be equal to the fetch cost fi. If all queries are assigned to

With probability yi,j/si,

oi is generated as the next input for Aobj.

The cache is maintained according to Aobj.
If (oi is in the cache)

Service qj from the cache.

Else

Bypass qj to the server.

Figure 3. The SpaceEffBY Algorithm.

groups integrally, i.e., each group either contains the whole
query or no part of it, it may not be possible to satisfy Con-
dition 7 exactly. So, when necessary, we assign a fraction
of a query to one group and the rest to the next and di-
vide the yield proportionately. A group is said to end at the
last query that belongs to it. If we rearrange the queries in
σ such that all queries that belong to a group are consecu-
tive, then within a group the queries are in their original or-
der, and the groups are ordered according to the query at
which they end. We call this the grouped sequence, denoted
by grouped(σ). All queries in σ may not be able to form
a group; this happens when there aren’t enough queries
left for the yield to equal the object size. All such queries
are dropped from grouped(σ). Let the sub-sequence of just
those queries be dropped(σ). By dropping the queries in
dropped(σ) from σ, we create the trimmed sub-sequence,
denoted by trimmed(σ). This trimmed sequence contains
queries in the same order as in σ, but some of them may
be fractional. If we replace each group in grouped(σ) with
the object to which the queries refer, we obtain an equiv-
alent object sequence, denoted by object(σ). object(σ) is
the very sequence sent to Aobj by OnlineBY. OPTobject
and OPTyield are the optimal ofﬂine algorithms for bypass-
object and bypass-yield caching respectively.

The following lemma states a relationship between the
costs of OPTobject and OPTyield in terms of object se-
quences and trimmed sequences.

Lemma 5.1 Given any input sequence σ = q1, . . . , qn of
queries, the cost of OPTobject on object(σ) is at most 2
(cid:1)
times the cost of OPTyield on trimmed(σ).

The proof of Lemma 5.1 appears in the companion techni-
cal report [26].

Among dropped(σ),

let droppedN (σ) be the sub-
sequence of dropped queries that refer to objects not in
object(σ). Let trimmedN (σ) refer to the sub-sequence re-
maining when the queries in droppedN (σ) have been
dropped from σ. Lastly, let the cost of droppedN (σ) be the
sum of the cost to bypass each query in it to the server. Sim-
ilarly, deﬁne cost for dropped(σ).

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

Observation 5.3 The cost of OPTyield on σ is the cost of
OPTyield on trimmedN (σ) plus the cost of droppedN (σ).
In other words, there is no beneﬁt in fetching objects re-
ferred to by queries in droppedN (σ), which have a total by-
pass cost less than the fetch cost.

The remainder of the proof of Theorem 5.1 appears
in the companion technical report. In it, we assemble the re-
lations we have established between OPTobject and
OPTyield on the division of σ to complete the bound.

5.3. SpaceEffBY

SpaceEﬀBY is quite similar to OnlineBY (Figure 3).
Instead of maintaining BYUi values to decide when to create
oi for Aobj SpaceEﬀBY simulates a similar effect by ran-
domly creating oi with probability yi,j/si. The extra space
taken by SpaceEﬀBY is O(1).

6. Experiments

We develop bypass-yield caching within the SkyQuery
[27] federation of astronomy databases. Caches are placed
at mediators within the federation, which are the sites that
receive user queries and resolve them into sub-queries for
each member database. Because mediators are placed near
clients, the network cost of communicating between clients
and mediator sites is insigniﬁcant compared to that between
clients and database servers. Thus, mediator sites act as
proxy caches. We have built a prototype implementation
of the above system. This allows us to evaluate the perfor-
mance of the various algorithms. We base our implementa-
tion of OnlineBY and SpaceEﬀBY on the GDS algorithm
[6, 18], because of its widespread use and known effective-
ness.

Our cache is a binary heap of database objects in
which heap ordering is done based on utility value. For
Rate-Proﬁle, the utility value is the RP value of each ob-
ject. For OnlineBY, the utility value equals the BYUi
value. The heap implementation makes insertions in
O(log k) time, for a heap of k objects. Evictions re-
quire O(1) time. By maintaining an additional hash ta-
ble on cached objects, the cache resolves hits and misses in
O(1) time.

We evaluate the yield of each query by re-executing the
traces with the server. In case of joins and when caching
columns, yields for individual objects are calculated by de-
composing the yield of the entire query into component
parts, corresponding to the cached objects. We demonstrate
yield estimation using a typical astronomy query.

select p.objID, p.ra, p.dec, s.z as redshift from SpecObj s,
PhotoObj p where p.objID = s.objID and s.z < 0.01 and
s.zConf > 0.95

When caching tables, yield for each table or view in a
joined query is divided in proportion to the table’s con-
tribution to the number of attributes in the query. For the

above query, yield is divided into half for each table. For at-
tribute caching, yield is proportional to the storage of each
attribute as a fraction of the total storage of all columns ref-
erenced in the query. In the above query, the total storage of
all columns is 40 bytes. Storage of p.objID is 8 bytes, so its
yield is 8/40 * Y , where Y is the yield of the entire query.
Cache consistency issues do not arise with respect to
the database objects. The workload has been taken from
the Sloan Digital Sky Survey (SDSS), a participant in the
SkyQuery federation. Once published, an SDSS database is
immutable. Changes or re-calibrations of data are admin-
istered by the organization and distributed only through a
new data release, i.e., a new version of the database. User
queries are read-only and specify a database version. How-
ever, meta-data inconsistencies might arise, especially when
materialized views and indices are modiﬁed. We use the
SkyQuery Web services, by which the server notiﬁes the
mediator and the cache of any changes to metadata. The
cache uses this event to update metadata.

To test the effectiveness of our algorithms, we use traces
gathered from the logs of the federating databases. Specif-
ically, we use traces from two data releases – EDR and
DR1 – of the largest federating node of the SkyQuery sys-
tem. Each trace consists of more than 25,000 SQL re-
quests amounting to about 1000 GB of network trafﬁc.
The SDSS traces include variety of access patterns, such as
range queries, spatial searches, identity queries, and aggre-
gate queries. Preprocessing on the traces involves removing
queries that query the logs themselves.

6.1. Comparing Cache Objects

We analyze the workload traces to answer the ques-
tion “What class of objects perform well in a bypass-yield
cache?” and to determine the preferred object granular-
ity. We consider query (semantic) caching versus caching
database objects, such as relational tables, attributes, and
materialized views. Luo et al [25] state that it is impera-
tive for the workload to show both locality and containment
for query caching to be viable.

The SDSS workload exhibits little query containment,
which renders semantic caching ineffective. The degree of
query containment is the number of queries that can be re-
solved from previous queries due to reﬁnement. While de-
termining actual query containment is NP-complete [8], an
upper bound can be evaluated experimentally. Most queries
in the workload look at celestial objects and their prop-
erties in a region of sky. These objects are denoted with
unique identiﬁers. We examine queries from a continuous
sub-sequence of the EDR trace in order to evaluate contain-
ment. A necessary, but not sufﬁcient, condition for contain-
ment is that object identiﬁers of a subsequent query must be
satisﬁed by object identiﬁers of a previous query. For clar-
ity in presenting the data, we look at a window of 300 such
queries (Figure 4). Results over larger windows are simi-
lar. Points in the chart indicate reuse of an object identi-

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

 50

 100

 150

 200

 250

 300

Query Number

Figure 4. Query containment

Figure 5. Table locality

D

I
 
t
c
e
j
b
O

 2e+16

 1.8e+16

 1.6e+16

 1.4e+16

 1.2e+16

 1e+16

 8e+15

 6e+15

 4e+15

 2e+15

 0

 0

ﬁer in different queries, and, thus, a potential cache hit in a
query cache. Our experiments indicate that few objects ex-
perience reuse in any portion of the trace over a large uni-
verse of objects. The problem is that there are few candidate
celestial objects to cache, indicating few candidate queries.
Schema locality describes the reuse of (locality in) data
columns and tables; the reuse of schema elements rather
than speciﬁc data items. Figures 5 and 6 evaluate schema
locality over the EDR trace. The x-axis corresponds to each
query. On the y-axis, we enumerate columns and tables re-
spectively. Each table is enumerated by giving a unique
number between 1 and maximum number of tables in the
database. Columns are enumerated by x.y in which x is
unique table number to which the column belongs and y is
the column number in the table. Data points on the same
horizontal line indicate reuse of a column or table. Both
columns and tables show heavy and long lasting periods
of reuse. Reuse is localized to a small fraction of the to-
tal columns or tables in the system, indicating that these
columns or tables could be placed in a cache and could ser-
vice many future queries as cache hits. Our algorithmic re-
sults verify this ﬁnding, showing large network trafﬁc re-
ductions when caching schema elements.

6.2. Performance Comparison of Algorithms

In this set of experiments, we compare the rate-based
algorithm and the two on-line algorithms. We also con-
trast the performance of these algorithms against the base
SkyQuery system (without caching) and a system that uses
Greedy-Dual-Size (GDS) caching without bypass. In all ex-
periments, we evaluate the algorithms using network cost
as a metric: the total number of bytes transmitted from the
databases to the proxy cache and client. Because clients and
proxy caches are collocated, we do not factor trafﬁc be-
tween them into network costs.

Rate-Proﬁle outperforms signiﬁcantly in-line (GDS)
caching and SkyQuery without caching. Figures 7 and 8
show the network costs of each algorithm for columns
and tables respectively. The graphs show the cumula-

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

Figure 6. Column locality

tive network usage for all queries in the trace. Bypass-yield
caching reduces network load by a factor of ﬁve to twenty
when compared with GDS and no caching. The “no
cache” results show the sum of the size of all query re-
sults shipped from the servers. GDS performs poorly
because it caches all requests, loading columns (resp. ta-
bles) into the cache and generating query results in the
cache. We include results of static table caching for com-
parison in which a cache is populated with the opti-
mal off-line set of tables: no cache loading or eviction
occurs. While static table caching is not optimal (dy-
namic ofﬂine algorithms could perform better), we expect
bypass-yield caches to be relatively stable when com-
pared with caching in other models, and static table caching
provides a sanity check for performance. Bypass-yield al-
gorithms approach the performance of static table caching.
Results indicate that bypass-yield algorithms realize the
beneﬁts of caching, frequent reuse and reduced net-
work bandwidth. At the same time, they avoid the hazards
of caching in database federations, preserving the data ﬁl-
tering beneﬁts of evaluating queries at the servers. Bypass
is the essential feature for caching the SDSS workload suc-
cessfully, and the economic algorithms provide a frame-

DR1

24567

1980.40

Table 1. Cost breakdown for table caching

Data
Set
EDR

Number
of Queries

27663

Sequence
Cost (GB)
1216.94

Algorithm

Bypass

Fetch

Total

Cost (GB) Cost (GB) Cost (GB)

Rate-Proﬁle
OnlineBY
SpaceEﬀBY
Rate-Proﬁle
OnlineBY
SpaceEﬀBY

41.08
41.06
83.40
126.54
130.10
145.28

Rate-Proﬁle
OnlineBY
SpaceEﬀBY
Rate-Proﬁle
OnlineBY
SpaceEﬀBY

4.12
1.09
3.89
73.65
98.40
112.71

52.84
63.38
42.86
75.13
68.43
87.35

80.12
86.97
90.71
43.91
48.20
52.90

93.92
104.44
126.26
201.67
198.53
232.63

84.24
88.06
94.60
117.56
146.60
175.61

Data
Set
EDR

Number
of Queries

27663

Sequence
Cost (GB)
1216.94

Algorithm

Bypass

Fetch

Total

Cost (GB) Cost (GB) Cost (GB)

DR1

24567

1980.40

Table 2. Cost breakdown for column caching

)

B
G

(
 
t
s
o
C

 1800

 1600

 1400

 1200

 1000

 800

 600

 400

 200

 0

 0

)

B
G

(
 
t
s
o
C

 1400

 1200

 1000

 800

 600

 400

 200

 0

 0

 5000

 10000

 15000

 20000

 25000

 30000

 5000

 10000

 15000

 20000

 25000

 30000

Query Number

Rate-Profile
GDS

Static Table
No Cache

Query Number

Rate-Profile
GDS

Static Column
No Cache

Figure 7. Network cost for table caching

Figure 8. Network cost for column caching

work for making the bypass decision.

We also compare the performance of our three algo-
rithms. Tables 1 and 2 show the total network costs over
an entire trace and divide those costs into a bypass com-
ponent, the cost of queries served at the databases, and a
load component, the costs to bring objects into the cache. In
most cases, Rate-Proﬁle outperforms the on-line algorithm
(OnlineBY), indicating that observed workload is a sound
predictor of future access patterns. However, OnlineBY
performs surprisingly well, which is promising, given that
it reduces state and offers competitive bounds. The on-line
randomized algorithm (SpaceEﬀBY) always lags behind,
indicating that some amount of state aids in making the by-
pass decision.

The ﬁne granularity of column caching offers beneﬁts in

all experiments when compared with table caching. In col-
umn caching, the cache is much more active; more data are
fetched into the cache and fewer queries are bypassed. This
results in lower overall costs when caching columns. The
coarse granularity of table caching also leads to poor results
for in-line (GDS) caching, which evicts and loads large ta-
bles (Figure 8).

6.3. Inﬂuence of Cache size

We examine the variability in network cost at a variety of
cache sizes in order to determine the size requirements of a
bypass-yield cache. Figures 9 and 10 show the performance
of all algorithms as the cache size varies between 10% and
100% of the database size.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

Rate-Profile
OnlineBY
SpaceEffBY
GDS
Static Table

Rate-Profile
OnlineBY
SpaceEffBY
GDS
Static Column

 20

 30

 40

 50

 60

 70

 80

 90

 100

Cache Size (% of DB Size)

Figure 9. Performance of table caching for an
increasing cache size

)
e
l
a
c
s
 
g
o
l
(
 
)

B
G

(
 
t
s
o
C

)
e
l
a
c
s
 

g
o
l
(
 
)

B
G

(
 
t
s
o
C

 1e+06

 100000

 10000

 1000

 100

 10

 1

 10

 1e+06

 100000

 10000

 1000

 100

 10

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

Cache Size (% of DB Size)

Figure 10. Performance of column caching
for an increasing cache size

We draw two conclusions from these results. First,
the rate-based (Rate-Proﬁle) algorithm performs poorly
at very small cache sizes. The algorithm exchanges ob-
jects for those with higher rates, often evicting objects
before the load cost is recovered. We expect that this arti-
fact can be removed by tuning the algorithm. Second, by-
pass caches need to be relatively large, 20% to 30% of
the database, to be effective. We attribute this partly to
the fact that scientiﬁc databases are populated with large
data items. However, we ﬁnd this result to be inconclu-
sive. The SDSS data is only about 700 MB and, thus, small
relative to the amount of data in the federations we tar-
get with our technology.

Determining how the needs of cache size scale with
database size remains an issue for further study. We expect
that the cache size needs will not grow with database size.
Rather, we expect cache size to be a function of workload.
In the case of the static table caching, the load cost at small
cache sizes is much more than bypass cost leading to an in-
crease in the total cost. At 100% cache size it loads more
tables that are required by the sequence.

SpaceEﬀBY shows a beneﬁt of randomized algorithms
in Figure 9 at a cache size of 100% of the database size. Be-
cause the algorithm loads objects randomly, with probabil-
ity proportional to the yield of an individual query, the al-
gorithm may load tables earlier or later – if at all – when
compared with its deterministic relatives OnlineBY and
Rate-Proﬁle. In this case, SpaceEﬀBY “gets lucky” when
it does not load a very large table for which future work-
load does not overcome the load cost. The experiment is
run with a cold cache and over a ﬁnite trace, which exacer-
bates this effect. However, the example does show how ran-
domized algorithms are robust to adversarial workloads.

7. Conclusions

We have presented the bypass-yield architecture for al-
truistic caching and network citizenship in large-scale sci-
entiﬁc database federations. Our treatment contains several
algorithms for caching within this framework, including a
workload-based predictive algorithm, a competitive on-line
algorithm, and a minimal-space on-line randomized algo-
rithm. Experimental results show that all algorithms provide
the beneﬁts of caching, while preserving the ﬁltering and
parallelism beneﬁts of database federations. Bypass-yield
caching and the associated algorithms are well suited to sci-
entiﬁc workloads, which exhibit schema locality, rather than
query locality. Bypass-yield algorithms allow a cache to dif-
ferentiate between queries that can be evaluated in cache to
realize network savings from those that are better shipped to
the servers of the federation in order to be evaluated in par-
allel at the data sources.

References
[1] M. Altinel, Q. Luo, S. Krishnamurthy, C. Mohan, H. Pira-
hesh, B. G. Lindsay, H. Woo, and L. Brown. DBCache:
Database caching for Web application servers. In SIGMOD,
2002.

[2] K. Amiri, S. Park, and R. Tewari. A self-managing data
cache for edge-of-network Web applications. In Proc. of the
Conference on Information and Knowledge Management,
2002.

[3] M. Arlitt, L. Cherkasova, J. Dilley, R. Friedrich, and T. Jin.
Evaluating content management techniques for Web proxy
caches. In Proc. of the Workshop on Internet Server Perfor-
mance, 1999.

[4] M. D. Beynon, T. Kurc, U. Catalyurek, C. Chang, A. Suss-
man, and J. Saltz. Distributed processing of very large
datasets with DataCutter. Parallel Computing, 27(11), 2001.
[5] A. Borodin and R. El-Yaniv. On-line Computation and Com-

petitive Analysis. Cambridge University Press, 1998.

[6] P. Cao and S. Irani. Cost-aware WWW proxy caching algo-
rithms. In Proc. of the USENIX Symposium on Internet Tech-
nology and Systems, 1997.

[7] B. Y. Chan, A. Si, and H. V. Leong. A framework for cache
management for mobile databases: Design and evaluation.
Distributed Parallel Databases, 10(1), 2001.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

[28] N. Megiddo and D. Modha. ARC: A self-tuning, low over-
head replacement cache. In Proc. of the USENIX File and
Storage Technologies Conference, 2003.

[29] N. Niclausse, Z. Liu, and P. Nain. A new efﬁcient caching
policy for the World Wide Web. In Proc. of the Workshop on
Internet Server Performance, 1998.

[30] E. O’Neil, P. O’Neil, and G. Weikum. The LRU-K page re-
In ACM

placement algorithm for database disk buffering.
SIGMOD, 1993.
[31] PlasmoDB: The

plasmodium

resource.

genome

http://www.plasmodb.org, 2002.

[32] R. Pottinger and A. Y. Levy. A scalable algorithm for an-

swering queries using views. In VLDB, 2000.

[33] Q. Ren and M. H. Dunham. Semantic caching and query
processing. Technical report, Department of CSE, Southern
Methodist University, 1998.

[34] N. Roussopoulos and H. Kang. Principles and techniques in

the design of ADMS. IEEE Computer, 19(12), 1986.

[35] P. Scheuermann, J. Shim, and R. Vingralek. Watchman: A

data warehouse intelligent cache manager. In VLDB, 1996.

[36] http://www.sdss.org.
[37] http://www.skyquery.net.
[38] R. Stevens. TCP/IP Illustrated Volume 1: The Protocols.

Addison-Wesley, 1994.

[39] M. Stonebraker, P. M. Aoki, R. Devine, W. Litwin, and
M. Olson. Mariposa: A new architecture for distributed data.
In ICDE, 1994.

[40] The Times Ten Team. In-memory data management in the

application tier. In ICDE, 2000.

[41] G. Valentin, M. Zuliani, D. Zilio, and G. Lohman. DB2 Ad-
visor: An optimizer smart enough to recommend its own in-
dexes. In ICDE, 2000.

[42] D. Wessels and K. C. Claffy. ICP and the Squid Web cache.
IEEE Journal on Selected Areas in Communications, 16(3),
1998.

[43] R. Wooster and M. Abrams. Proxy caching that estimates
page load delays. In Proc. of the International WWW Con-
ference, 1997.

[44] N. E. Young. On-line caching as cache size varies. In Proc.

of the Symposium on Discrete Algorithms, 1991.

[8] A. K. Chandra and P. M. Merlin. Optimal implementation of
conjunctive queries in relational databases. In ACM Sympo-
sium on Theory of Computing, 1977.

[9] L. Cherkasova and G. Ciardo. Role of aging, frequency, and
In Proc. on High

size in Web cache replacement policies.
Performance Computing and Networking, 2001.

[10] E. Coffman and P. Denning. Operating Systems Theory.

Prentice Hall, Inc, 1973.

[11] E. Cohen and H. Kaplan. LP-based analysis of Greedy-Dual-
Size. In Proc. of the ACM-SIAM Symposium on Discrete Al-
gorithms, 1999.

[12] S. Dar, M. J. Franklin, B. T. J`onsson, D. Srivastava, and
M. Tan. Semantic data caching and replacement. In VLDB,
1996.

[13] H. Fujiwara and K. Iwama. Average-case competitive anal-
In Intl. Symposium on Algo-

yses for ski-rental problems.
rithms and Computation, 2002.

[14] G. Gallagher. Data transport within the Distributed Oceano-
In Proc. of the WWW Conference,

graphic Data System.
1995.

[15] J. Gray and A. Szalay. Online science: The World-Wide Tele-
scope as a prototype for the new computational science. Pre-
sentation at the Supercomputing Conference, 2003.
[16] J. M. Hellerstein. Practical predicate placement.

In SIG-

MOD, 1994.

[17] J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols,
M. Satyanarayanan, R. N. Sidebotham, and M. J. West. Scale
and performance in a distributed ﬁle system. ACM Transac-
tions on Computer Systems, 6(1), 1988.

[18] S. Irani. Page replacement with multi-size pages and appli-
cations to Web caching. In Proc. of the ACM Symposium on
the Theory of Computing, 1997.

[19] J. Jeong and M. Dubois. Cost-sensitive cache replacement
algorithms. In Proc. of the High-Performance Computer Ar-
chitecture. IEEE, 2003.

[20] A. Jhingran. A performance study of query optimization al-
In

gorithms on a database system supporting procedures.
VLDB, 1988.

[21] S. Jiang and X. Zhang.

LIRS: An efﬁcient low inter-
reference recency set replacement policy to improve buffer
cache performance. In ACM SIGMETRICS, 2002.

[22] S. Jin and Z. Bestavros. Popularity-aware Greedy Dual-Size
Web proxy caching. In Proc. of the Intl. Conference on Dis-
tributed Computing Systems, 2000.

[23] S. Jin and Z. Bestavros. Greedydual* Web caching algo-
rithms: Exploiting two sources of temporal locality in Web
request streams. Computer Communications, 24(2), 2001.

[24] D. Li, P. Cao, and M. Dahlin. WCIP: Web Cache Invalida-

tion Protocol. Internet Draft, IETF, 2000.

[25] Q. Luo and J. F. Naughton. Form-based proxy caching for

database-backed Web sites. In VLDB, 2001.

[26] T. Malik, R. Burns, and A. Chaudhary. Bypass caching:
Making scientiﬁc databases good network citizens. Techni-
cal Report HSSL-2004-01, Storage Systems Lab, Johns Hop-
kins University, 2004.

[27] T. Malik, A. S. Szalay, A. S. Budavri, and A. R. Thakar.
SkyQuery: A Web service approach to federate databases.
In Proc. of the Conference on Innovative Data Systems Re-
search, 2003.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

