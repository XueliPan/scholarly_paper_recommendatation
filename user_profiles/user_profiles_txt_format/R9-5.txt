Start Globally, Optimize Locally, Predict Globally:

Improving Performance on Imbalanced Data

Department of Computer Science and Engineering, University of Notre Dame, USA

David A. Cieslak, Nitesh V. Chawla

{dcieslak,nchawla}@cse.nd.edu

Abstract

a common approach to counter the curse of imbalance in
many domains.

Class imbalance is a ubiquitous problem in supervised
learning and has gained wide-scale attention in the lit-
erature. Perhaps the most prevalent solution is to apply
sampling to training data in order improve classiﬁer per-
formance. The typical approach will apply uniform lev-
els of sampling globally. However, we believe that data
is typically multi-modal, which suggests sampling should
be treated locally rather than globally.
It is the purpose
of this paper to propose a framework which ﬁrst identi-
ﬁes meaningful regions of data and then proceeds to ﬁnd
optimal sampling levels within each. This paper demon-
strates that a global classiﬁer trained on data locally sam-
pled produces superior rank-orderings on a wide range of
real-world and artiﬁcial datasets as compared to contem-
porary global sampling methods.

1 Introduction

Mining imbalanced datasets continues to be a pervasive
problem in a large variety of applications [8, 22] including
medicine, ﬁnance, and security. Traditional objective met-
rics, such as accuracy, often fail to describe performance
properly under these circumstances as a trivial classiﬁer
may produce high accuracy but perform poorly on the more
interesting rare class. Instead, Area Under the ROC Curve
(AU ROC) is typically used as a measure of the capabil-
ity of a classiﬁer to effectively rank the minority or posi-
tive class instances above the majority or negative class in-
stances. AU ROC captures the trade-off between true posi-
tives and false positives, producing a robust metric for even
the most imbalanced datasets.

The most signiﬁcant direction of research on imbalanced
datasets has focused on sampling strategies for countering
the problem of class imbalance [3, 8]. Sampling methods
focus on either adding minority (positive) class points or
removing majority (negative) class points to improve accu-
racy or AU ROC of the positive class [3, 15]. Sampling is

)

|

X
+
=
y
(
P

X2

X1

Figure 1. Data example for which the class
distributions are generated through piece-
wise, regional functions.

Sampling methods consider the class skew and proper-
ties of the dataset as a whole. However, it is the contention
of this paper that machine learning and data mining often
face nontrivial datasets, which often exhibit characteristics
and properties at a local, rather than global level. As an
example, we consider Figure 1 where the class probability
generating function is different within each region of the
data, dictating that the relationship between classes is sub-
stantially different within each discrete area. Even when
two regions exhibit the same class skew, it is possible for
them to exhibit other signiﬁcant and disjoint characteristics.
While a particular sampling level induces a strong classiﬁer
for the ﬁrst region, it may well induce a poor classiﬁer on
the second region. This suggests that the typically global
task of selecting sampling levels to improve classiﬁer per-
formance should in many contexts be considered locally.

With these concerns in mind, the primary contributions
of this paper are as follows. 1) An analytic framework for
supporting locally optimal mixtures of sampling . 2) An
effective data partitioning or segmentation method based on
the distribution invariant Hellinger distance [9, 10, 11, 21].
3) Identifying optimal levels and types of sampling within
each data segment. 4) A comprehensive evaluation using 20
real-world and UCI datasets.

For benchmarking, we will consider two popular sam-
pling methods from the related work: Synthetic Minority
Oversampling TechniquE (SMOTE) [6] and random under-
sampling. In addition, we include Classiﬁcation using lOcal
clusterinG Over Sampling (COG-OS) [23]. SMOTE injects
synthetic positive class examples to emphasize class bound-
aries, while COG-OS uses k-means clustering to partition
the complex majority class into simpler sub-classes and mi-
nority class replication.

2 Start Globally, Optimize Locally

In the supervised learning task a labeled training data
sample D is available for model training, presumably drawn
randomly from some distribution p(Y |X), for which each
xi ∈ X is a single feature vector and yi ∈ Y is the
associated class label.
In addition, there is an available
unlabeled sample T for testing purposes, which for the
sake of this study is presumed from the same distribution
p(Y |X). The goal of this task is then to train some clas-
siﬁer f : X (cid:3)−→ Y to estimate the probability for each
xi in T to belong to a class in Y . The ideal for this clas-
siﬁer is strong performance, either in minimizing loss or
cost or maximizing accuracy, proﬁt, or AU ROC. Thus, the
model should maximize some objective function, O, such
that E(X,Y )[O(f (X), Y )] is maximized [4].
In the ideal
case, f : X (cid:3)−→ Y represents the “true model” of the data
p(Y |X). In practice, this is difﬁcult if not impossible due
to the limited representation D provides of p(Y |X). Thus,
the learning algorithms often approximate the true function.
Due to the bias inherent in each learning algorithm’s rep-
resentation and the particular emphasis of each objective
function, it is often the case that D∗, a resampling of D
not necessarily drawn from p(Y |X), will actually enhance
the performance of the determined classiﬁer. The task of
ﬁnding some S : D (cid:3)−→ D∗ that improves a classiﬁer’s
performance is called sampling.

A primary question when employing sampling is how
does one tune S : D (cid:3)−→ D∗ to yield optimal results?
A suggested method is to use a heuristic wrapper [7] to
set sampling levels for each class. The wrapper greedily
explores the search space of sampling parameters until the
“potentially optimal” amounts are discovered. An alternate
method stems from cost sensitive learning. For instance,
Elkan discusses a simple method to calculate optimal sam-
pling levels based on the ratio of misclassiﬁcation for each
class [14]. However, evaluation typically occurs without
explicit costs. In this case, Elkan’s calculation simply indi-
cates to sample the classes to a balance point, which can be
limiting. It is often necessary to explore a much larger sam-
pling space, to address the question of how do the proper-
ties of data (aside from class skew) affect optimal sampling
levels? The wrapper exploits the properties of the data as

it utilizes the performance of the learning algorithm as a
guide.

We consider this question by constructing some artiﬁcial
datasets, seen in Figure 2, as follows. To begin, there ex-
ist two classes of examples with a skew ratio (+ : −) of
(50 : 1000), positive examples depicted as +’s and nega-
tive examples as ·’s. Each class is centered at a ﬁxed point
¯x+ and ¯x− with radius r+ and r−, respectively. Since the
number of points for each class is ﬁxed, the densities ρ+
and ρ− are a product of the radii, reducing radius length in-
creases density. The ﬁnal consideration for this dataset is
δ, the distance between ¯x+ and ¯x−. The higher the δ, the
lower the class overlap. Of interest is the interaction be-
tween the density ρ+, relative overlap between the regions,
and end ability to discriminate between + and − as quan-
tiﬁed by AU ROC. Since maintaining the relative skew is
desired, ρ+ will be affected by changing r+ and the resul-
tant region.

This experiment considers three discrete increments of
both r+ and δ: High, Medium, and Low to determine the
importance of degree of r+ and δ on end performance.
We say that ρ+ is High when r+ = r−, Medium when
r+ = 0.75r−, and Low when r+ = 0.5r−. Once r+ is de-
termined, overlap is High when δ = 0.5r+, Medium when
δ = 0.75r+ and Low when δ = r+. When overlap is High,
nearly all of the + region is engulfed within the − region.
However, when overlap is Low, over half the + region is
outside the larger − area. We note that ¯x− and ρ− (and
therefore r−) remain ﬁxed through all constructions. Based
on these constructions, we now address how, if at all, do
these constructed properties affect discriminative ability?

As we have conserved the relative skew ratio between
classes at (50 : 1000), Elkan’s solution would apply the
same level of over and undersampling as a universal opti-
mal solution to each of the nine scenarios. However, the re-
sults shown in Table 1 indicate otherwise, also establishing
three general trends. First, increased minority class density
improves AU ROC, since increased density allows C4.5
to develop a more conﬁdent decision region. Second, in-
creased separation between the two class centers (reduced
class overlap) increases AU ROC, since this enables C4.5
to construct a more deﬁnitive border between the classes.
Third, the optimal levels of sampling for each scenario is
substantially different (and does not generate a class bal-
anced sample), which is in contradiction with Elkan’s so-
lution. Thus, it is observed that data properties aside from
class skew may have a substantial effect on optimal sam-
pling level.

However, datasets are often more complex than those
in Figure 2, perhaps containing multiple points of inter-
section.
In such a scenario, even an optimized sampling
of data may represent an incomplete solution from a local
perspective. Thus, we would like to address what effect

(High,High)

(High,Medium)

(High,Low)

(Medium,High)

(Medium,Medium)

(Medium,Low)

(Low,High)

(Low,Medium)

(Low,Low)

Figure 2. Demonstration of the different two-class radial distributions, with pairs (ρ, δ) specifying
the level of density ρ and centroid separation δ for each. As the rows descend, ρ decreases and δ
likewise decreases from the left to right columns. For example, (High,Medium) displays the dataset
with High density and Medium class separation. The (+ : −) skew in this situation is (50 : 1000). Each
+ represents a positive example, while each · represents a negative example.

does considering sampling locally have on global perfor-
mance? We consider the data examples highlighted in Fig-
ure 3. We note that the left and middle datasets are similar
to the previous: they represent two-class overlapping radi-
als, with varying distance between centers and density. The
left dataset is signiﬁcantly easier to learn than the middle,
since there is less overlap between the regions. This is re-
ﬂected by the determined AU ROC for left and middle of
0.868 and 0.531, using C4.5. Using sampling on the left
dataset, the AU ROC can be improved to 0.906 using levels
of (U ndersample, SM OT E) of (50, 350) found through
the wrapper method in [7], while the middle can improve
to 0.812 using (20, 250). However, there is some difﬁculty
when the left and middle dataset concatenate to form the
rightmost dataset in the ﬁgure. By default, C4.5 obtains
an AU ROC of 0.817, which can be improved to 0.845 us-
ing (100, 50). This is somewhat problematic. The com-
ponents dictated different sampling levels, but when they
were considered as a single dataset a distinct third sam-

pling combination was used, indicating that there is a loss
in efﬁciency of optimizing AU ROC via sampling globally.
This is conﬁrmed, since using the regional sampling level of
(50, 350) for the leftmost dataset and (20, 250) for the cen-
ter dataset improves global performance further to 0.874.
Thus, our current methods of global sampling may be in-
sufﬁcient in some applications, since we observe a consid-
erable improvement when local sampling is considered.

Rather, the task of ﬁnding some S : D (cid:3)−→ D∗ to im-
prove performance might be divided into components. It is
our belief that much of data operates as a set of modalities
guiding the construction of feature vectors and their associ-
ated class labels, in accordance with P (Y |X).

Presuming at least some data exists in such a modal
fashion, it is therefore meaningful to derive samplings lo-
cally. Such a procedure begins by decomposing D into
m meaningful segments, Di. In Figure 3, the two compo-
nents (D1, D2) for the right dataset are the left and middle
datasets. Once such segments have been determined, sam-

Figure 3. Two separate two class radials with different δ,ρ+, and skews. The third reﬂects their
combination into a single dataset.

Table 1. AU ROC values for C4.5 and C4.5
with Sampling, in addition to optimal sam-
pling levels, on each of the nine scenarios in
Figure 2.

AU ROC

(C45, C45 + S)

(U ndersampling,

(ρ+, δ)

(High,High)

(High,Medium)

(High,Low)

(Medium,High)

(Medium,Medium)

(Medium,Low)

(Low,High)

(Low,Medium)

(Low,Low)

(0.926, 0.968)
(0.909, 0.942)
(0.898, 0.904)
(0.915, 0.961)
(0.878, 0.927)
(0.814, 0.831)
(0.892, 0.940)
(0.825, 0.847)
(0.705, 0.736)

SM OT E)
(60, 450)
(40, 250)
(40, 100)
(50, 300)
(60, 250)
(30, 250)
(100, 150)
(30, 350)
(80, 500)

(cid:2)m

i=1 D∗

pling optimizes locally to ﬁnd each D∗
i . This is recombined
as D∗ =
i , which is then used to formulate a global
classiﬁer. As can be seen from this example, a key aspect
to local optimization is constructing a reliable partitioning
which will be discussed in the next section.

3 Partitioning Data

This section introduces a supervised method for split-
ting the data into segments or components. The partitioning
builds a tree of maximum height 2 (see Algorithm 1), re-
sulting in at most 4 segments. Our initial experimentation
demonstrated that using a height greater than 2 resulted in
much sparser segments, given the high imbalance of some
of the datasets used in our paper. Thus, to be consistent
we constrained the tree to the height of 2 for all datasets.
This type of segmentation is used instead of unsupervised
learning, such as k-means, since the known class labels pro-
vide potentially useful information. The data will also be
presumably be highly imbalanced, and we want to exploit
the class skew information while constructing the partitions
as well. To that end, we propose using Hellinger distance

sZ

Ω

(cid:3)

[9, 10, 11, 21] to guide the segmentation.

Hellinger distance is a measure of distributional diver-
gence. Let (Θ, λ) denote a measurable space with A and B
as two continuous distributions measured with respect to λ.
Let a and b be the densities of A and B with respect to λ.
The deﬁnition of Hellinger distance can be given as:

dH(A, B) =

√

√

(

a −

b)2dλ

(1)

This is equivalent to:

dH (A, B) =

2(1 −

abdλ)

(2)

(cid:4)

√

Ω

√

(cid:5)
Ω

(cid:6)

(cid:7)

abdλ is the Hellinger integral. Note the
where
Hellinger distance does not depend on the choice of param-
eter λ. It can also be deﬁned for a countable space Φ, as
B(φ))2. The Hellinger
dH (A, B) =
distance carries these following properties: 1) dH (A, B) is
in [0,
2]. 2) dH is symmetric and non-negative, imply-
ing dH (A, B) = dH (B, A). Moreover, squared Hellinger
distance is the lower bound of KL divergence.

A(φ) −

Σφ∈Φ(

(cid:7)

√

Here, the A and B in Equation 1 are assumed to be
the normalized frequencies of feature values across classes.
This allows us to capture the notion of “afﬁnity” between
the probability measures on a ﬁnite event space. If A = B,
then distance = 0 (maximal afﬁnity) and if A and B are com-
(2) (zero afﬁnity). This
pletely disjoint then distance =
dictates the partition splitting criterion for separability be-
tween classes. We want to select a feature that carries the
minimal afﬁnity between the classes.

(cid:7)

We assume a countable space, so we discretize all contin-
uous features into p partitions or bins. Then, we are essen-
tially interested in calculating the “distance” between the
relative class frequencies for each feature value, normalized
by the overall class frequency to reduce the effect of class
skew. Thus, we apply Hellinger distance as

Algorithm 1 Hellinger Partitioning Procedure
Input: Dataset D with feature set F

Current height h of the binary tree (called with 0)

return

1: if h == 2 or |D| == 0 then
2:
3: end if
4: f = argmaxi∈F dH (T+, T−)
5: for each value v of f do
6: HellingerP artition(Tf =v, h + 1)
7: end for

dH (+, −) =

 s

v
u
u
u
t

pX

j=1

P (+|X = xj)

P (−|X = xj)

P (+)

P (−)

s

−

!2

(3)
We note that bounds on this metric are unaffected by the
class skew, making Hellinger distance ideal for separating
regions of disjoint class distribution within highly imbal-
anced data. To construct partitions, we use the procedure
given in Algorithm 1. The Hellinger distance is applied to
each feature with the highest distance used to generate a
split in data to generate a two-level binary tree, as initial
experimentation indicated this produced the best results. In
the case of continuous features each potential feature split
is explored, meaning p = 2. The procedure is then applied
on each partition until a desired maximum depth is reached.
With this partitioning method, we now propose a general-
ized framework for local sampling aimed to leverage the
observations of Section 2 into practical application.

4 Applying Local Sampling

Local Sampling (LS) provides a general framework into
which it is possible to incorporate any sort of classiﬁer,
data partitioning, or sampling mechanism. This ﬂexible
structure allows for versatility, allowing a practitioner to
readily adapt this framework to many application settings.
In essence, this is a three step extension to any sampling
scheme.

Algorithm 2 outlines the LS pseudo-code. While we use
the proposed Hellinger distance based partitioning method,
in principle any arbitrary partitioning method P is applied
to divide the data. This phase may apply nearly any type of
supervised or unsupervised method. With the set of discov-
ered data partitions Z, the procedure moves to the sampling
phase. Here, the chosen sampling method S is applied to
each component. The algorithm checks the range of param-
eters θ for S and determines the optimal performer accord-
ing to the objective function O. Once optimal sampling pa-
rameters are applied to each component or segment, all the
components are merged to create the new training set. Then,

Algorithm 2 The Local Sampling (LS) Framework
Input: D: Labeled Training Set

S Sampling Method with associated parameters θ
P Partitioning Method ﬁnding m partitions
f Learning Algorithm
O Objective Function

Output: M : the model built

Z is the data segments or partitions discovered via P .

z = argmaxθi∈θ O(S(z, θi))

1: Z = P (D)
2: for each partition z ∈ Z do
3: D∗
4: end for
5: D∗ =
6: M = f (D∗)

i=1 D∗
i

(cid:2)m

a classiﬁer is learned. Thus, we go from global view of the
data to local view (partitioning) and then again to a global
view for learning the classiﬁer.

We follow the wrapper framework [7] to discover the po-
tentially optimal amounts of undersampling and SMOTE
for S. Note that the same wrapper framework is used for
the benchmarks used in this paper – COG-OS and combi-
nation of undersampling and SMOTE on the entire dataset
(without LS). The sampling levels for each form the pa-
rameters θ and are {0, 10, ...100} for undersampling and
{0, 50, 100, ..., 500} for SMOTE. For P , we apply the par-
titioning method from Section 3. The wrapper then pro-
ceeds as follows. First, a 5-fold cross-validation (cv) is ap-
plied on the training set to determine the baseline AU ROC
performance. Since, we use 5x2 cross-validation frame-
work, this 5-fold cv is applied to each of the training sets
of 5x2, making it an independent validation framework.
Then, the majority class examples are randomly removed
10% at a time, as long as the performance remains above
the baseline. Once an undersampling level is selected, it
results in the new improved baseline performance. Sub-
sequently, SMOTE is applied in progressive steps, adding
synthetic points in increments of 50% of the minority class
size. Again, this is repeated as long as it beats the baseline
performance. This procedure is slightly adapted for local
sampling by exploring the parameter space exhaustively for
each partition in sequence. The order of optimization is the
partition with the most majority class points to the one with
the fewest, allowing us to gradually shift the bias towards
the minority class. It is also likely that not all partitions will
require either or both undersampling and SMOTE. The par-
titions are merged after each optimization to determine the
AU ROC via the 5-fold cv, allowing for measurement of the
global relevance to any optimization done on the partition.
Figure 4 depicts the practical application and beneﬁts of
LS on the Pima dataset [2], represented in two dimensions.
While the + and − classes have disjoint regions, a signiﬁ-

a)

Att 2

c)

C

D

b)

C

D

Att 2

d)

A

B

8
 
t
t

A

8
 
t
t

A

8
 
t
t

A

A

B

8
 
t
t

A

Att 2

Att 2

Figure 4. (a) The Pima dataset, illustrated on two dimensions. + represents positive examples,
while (cid:6) represents negative examples. (b) Using Hellinger distance, a two-level tree is trained and
then combinations of sampling levels are explored.
(c) Each component optimizes to a different
sampling level, removing some negative examples and injecting SMOTE examples represented by
(cid:7). The concatenation of components is used to form a single training set. This is juxtaposed with
(d), which presents the effects of global undersampling and SMOTE.

cant class overlap is also observable in other regions. Using
C4.5 alone yields an AU ROC of 0.765.

data.

In Figure 4(b), Hellinger distance (discussed in Section
3) is used to form a four-region split of the dataset. Re-
gion B is entirely comprised of − examples and A exhibits
− example dominance. However, regions C and D have
isolated regions in which + examples actually form the ma-
jority of examples. Using AU ROC, each region is opti-
mized with (U ndersample, SM OT E) levels of (40, 50),
(90, 100), (40, 250), and (50, 500) for A, B, C, and D,
respectively, seen in Figure 4(c). Doing so produces an
AU ROC of 0.817, which outperforms a global undersam-
pling and SMOTE combination, which ﬁnds an AU ROC of
0.790. The global sampling uses (70, 400), while the local
sampling effectively applies (66, 303) (based on the num-
ber of examples actually added and removed for each class
across all partitions). We have therefore observed a practi-
cal case similar to Figure 3, where LS improves AU ROC.
In this case, LS is more judicious in terms of requiring
fewer examples for better results as it is able to focus use
of undersampling and SMOTE to particular regions of the

5 Empirical Evaluation

In this section, we present experimental results to vali-
date and understand the performance of LS on imbalanced
datasets, particularly in comparison with other contempo-
rary methods.

5.1 Performance Analysis

We use 5x2-fold cross-validation (cv)

[13] with
AU ROC as the evaluation criterion. We chose 5x2 cv over
10-fold cv as the use of 10-fold cv, while resulting in in-
dependent testing sets, carries sufﬁcient overlap in train-
ing data across folds. This makes application of typical t-
tests assuming independent trials questionable. Dietterich
[13] notes that this results in an elevated level of Type I er-
ror, which can be corrected for by his 5x2 cv. This relies
on the idea that learning curves rarely cross for algorithms
as training set size varies. The elevated Type I error be-

comes of particular concern when evaluating on imbalanced
datasets because of the trade-off between false positive and
false negative. Signiﬁcance will be tested by the F-test as
described by Alpaydin [1] at 95% conﬁdence. We note
that two datasets, Compustat and Intrusion, possess natural
train/test split which are maintained to preserve the sanity
of evaluation. Signiﬁcant differences in results will not be
determined on these datasets.

We also use the statistical analysis framework to com-
pare classiﬁer ranks outlined by Demsar [12]. This method
uses the Holm procedure of the Friedman test to estab-
lish the signiﬁcance of classiﬁer rankings across multiple
data sets. This is a specialized, non-parametric procedure
for testing the signiﬁcance of differences between multiple
means. Demsar notes that the Friedman test is “appropriate
since it assumes some, but limited commensurability [12]
(page 27).” It is “safer than parametric tests since it does
not assume normal distributions or homogeneity of vari-
ance. As such, it can be applied to classiﬁcation accuracies,
error ratios or any other measure for evaluation of classiﬁers
[12] (page 27).” Both 5x2 cv and Demsar’s analysis are in-
creasingly embraced as standards for classiﬁer comparison
in the data-mining community.

5.2 Methods and Benchmarks

We compare the proposed local sampling framework
to other contemporary methods, notably the combination
of undersampling and SMOTE, which has been shown to
signiﬁcantly improve performance on imbalanced datasets,
and COG-OS [23], which is a recent method for countering
class imbalance using clustering analysis. COG-OS uses
local clusterings of majority class points and minority class
replication to enhance linear separability. These clusterings
are derived using k-means. Since this algorithm is partially
dependent on the initial random seedings of centroids, we
used 10 different clusterings, and the clustering resulting in
the lowest distance to centroid was preserved.

Each of the sampling methods and COG-OS utilize the
same framework for discovering the potentially optimal pa-
rameters allowing for fair comparisons across the board.
That is, for the vanilla SMOTE and undersampling combi-
nation with C4.5, we used a 5-fold cv on the training fold of
5x2 cv to ﬁrst determine the levels of sampling. For COG-
OS, we empirically determined the appropriate number of
clusters k, using the same 5-fold cv on top of 5x2 cv frame-
work. Finally, for our LS approach we also used the 5-fold
cv to determine the levels of sampling in each partition. The
5x2 training and testing sets remained the same across all
the approaches. This ensured that all approaches were inde-
pendently optimized, in the same fashion, to achieve their
best performances.

We use two base classiﬁers: C4.5 [19] and SVM

MinClass

18,916

4%/4%

E-State

Examples

FourClass

3,505
569

10,358/3,258

4%
33%
5%

Calmodoulin
Compustat

Dataset
Boundary
Breast-W

Table 2. All the datasets used in this pa-
per. The Examples column for Compustat
and Intrusion also includes the number of
both training and testing examples.
No.
Features
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

174
30
131
20
12
2
24
41
16
6
49
10
16
5
479
8
36
19
60
4

19%
2%
4%
10%
10%
21%
5%
35%
10%
16%
4.8%
35%

937
5,473
10,992
5,404
11,411

768
6,435
2,310
1,000
3,089

Satimage
Segment
Splice

Pendigits
Phoneme

5,322
862
1,000

German.Numer

12%
36%
30%

Mammography

20,000
11,183

17,600/17,400

PhosS
Pima

SVMGuide1

Oil
Page

Intrusion

Letter

0.7%/0.8%

[5]. Here, decision trees are unpruned and use Laplace-
smoothing at the leaves in keeping with the observations
of Provost & Domingos [18] that this provides improve-
ments for AU ROC. We train the SVM using the param-
eters −t 0 −b 1 and build a probabilistic estimator. Since,
SVM training has a much higher time complexity than de-
cision tree induction, which is exacerbated to a prohibitive
computational cost when combined with the wrappers for
Undersample/SMOTE and Local Sampling, we restrict our
experiments with Undersample/SMOTE and LS with C4.5,
which is also the most commonly used classiﬁer with sam-
pling for imbalanced datasets in the literature. Throughout
the remainder of this paper, we will abbreviate undersam-
pling/SMOTE as U nSM .

The results include AU ROC for all ﬁve approaches —
LS, U nSM , COG-OS, C4.5, and SVM. For complete-
ness and repeatability, we also show the parameter values
(amount of sampling for U nSM and LS, and number of
clusters and replications of the minority class for COG-OS).
As will be described later, we use robust statistical measures
to evaluate the performance and difference among the dif-
ferent methods.

5.3 Datasets

Table 2 describes the characteristics of the datasets used
in our experimental analysis of local sampling. Several
datasets were acquired from various real-world scenarios
originally considered in [6]. E-State contains electrotopo-

Table 3. AU ROC values averaged over 5x2 folds for all methods, along with standard deviations.
Statistical signiﬁcance is calculated using F-test at 95% conﬁdence using the Alpaydin method. If
a classiﬁer performance is in bold, it implies C4.5 + LS signiﬁcantly improved over that classiﬁer.
Italicized values indicate the corresponding method signiﬁcantly improved over C4.5 + LS. Note that
C4.5+LS achieves a higher performance than all the other methods for 17 of the 20 datasets.
In
addition, the average rank for each classiﬁer is also listed, showing C4.5+LS achieves the best rank
and signiﬁcantly so (the (cid:2) shows that C4.5+LS achieves statistically signiﬁcantly lower ran than the
corresponding classiﬁer). We use the Friedman test to compare the ranks at 95% conﬁdence interval
[12].

C4.5 + COGOS

SV M + COGOS

Dataset
Boundary
Breast-w

Calmodoulin
Compustat

E-State
Fourclass

German.numer

Mammography

Intrusion

Letter

Oil
Page

Pendigits
Phoneme

PhosS
Pima

Satimage
Segment
Splice

SVMguide1
Avg Rank

Friedman α = .05

C4.5

0.509 ± 0.006
0.922 ± 0.014
0.675 ± 0.008

0.792

0.555 ± 0.056
0.973 ± 0.006
0.736 ± 0.016

0.970

0.964 ± 0.003
0.887 ± 0.005
0.757 ± 0.015
0.962 ± 0.001
0.987 ± 0.002
0.897 ± 0.003
0.647 ± 0.026
0.742 ± 0.023
0.910 ± 0.010
0.973 ± 0.007
0.954 ± 0.012
0.982 ± 0.004

4.85
(cid:2)

SV M

0.758 ± 0.016
0.938 ± 0.001
0.724 ± 0.012

0.584

0.567 ± 0.009
0.808 ± 0.008
0.753 ± 0.001

0.926

0.678 ± 0.028
0.895 ± 0.013
0.543 ± 0.007
0.930 ± 0.005
0.984 ± 0.003
0.799 ± 0.003
0.692 ± 0.005
0.801 ± 0.014
0.690 ± 0.031
0.977 ± 0.002
0.841 ± 0.014
0.980 ± 0.001

4.90
(cid:2)

0.667 ± 0.034
0.956 ± 0.015
0.684 ± 0.011

0.844

0.620 ± 0.007
0.984 ± 0.002
0.706 ± 0.024

0.982

0.954 ± 0.002
0.903 ± 0.008
0.807 ± 0.014
0.979 ± 0.004
0.995 ± 0.001
0.910 ± 0.003
0.631 ± 0.015
0.779 ± 0.014
0.906 ± 0.006
0.992 ± 0.002
0.958 ± 0.008
0.990 ± 0.001

3.65
(cid:2)

0.776 ± 0.004
0.940 ± 0.008
0.821 ± 0.008

0.738

0.623 ± 0.019
0.881 ± 0.019
0.753 ± 0.001

0.976

0.857 ± 0.005
0.906 ± 0.015
0.690 ± 0.038
0.955 ± 0.001
0.995 ± 0.001
0.861 ± 0.002
0.691 ± 0.013
0.797 ± 0.006
0.927 ± 0.001
0.980 ± 0.001
0.849 ± 0.011
0.982 ± 0.001

3.53
(cid:2)

C4.5 + U nSM
0.671 ± 0.035
0.961 ± 0.018
0.686 ± 0.006

0.867

0.624 ± 0.006
0.978 ± 0.005
0.744 ± 0.010

0.973

0.969 ± 0.001
0.909 ± 0.012
0.807 ± 0.036
0.983 ± 0.002
0.993 ± 0.001
0.913 ± 0.006
0.695 ± 0.003
0.787 ± 0.010
0.921 ± 0.004
0.986 ± 0.004
0.962 ± 0.005
0.991 ± 0.000

2.75
(cid:2)

C4.5 + LS
0.694 ± 0.039
0.973 ± 0.015
0.698 ± 0.005

0.870

0.636 ± 0.004
0.988 ± 0.003
0.762 ± 0.007

0.988

0.975 ± 0.001
0.924 ± 0.008
0.858 ± 0.026
0.986 ± 0.002
0.994 ± 0.002
0.920 ± 0.005
0.710 ± 0.004
0.818 ± 0.009
0.927 ± 0.004
0.993 ± 0.001
0.976 ± 0.006
0.992 ± 0.001

1.33
—

logical state descriptors for a series of compounds from the
National Cancer Institute’s Yeast AntiCancer drug screen.
Mammography is highly imbalanced and records informa-
tion on calciﬁcation in a mammogram. Oil is provided by
[17]; it is relatively small and very noisy. The Phoneme
dataset originates from the ELENA project and is used
to distinguish between nasal and oral sounds. Compustat
North America is a ﬁnancial database of U.S. and Canadian
publicly held companies, accumulated since 1995. Intru-
sion is a subset of data acquired from the KDD’99 Chal-
lenge on intrusion detection with u2r as the minority class
and normal as the majority class [16]. Boundary, Calmod-
oulin, and PhosS are various biological datasets [20]. Four-
Class, German.Numer, Splice, and SVMGuide1 all are
available with LIBSV M [5]. The remaining datasets all
originate from the UCI repository [2]. Some of these are
originally multiple class datasets and were converted into 2-
class problems by keeping the smallest class as minority and
the rest as majority. The exception is Letter, for which each
vowel became a member of the minority class, set against
all of the consonants. Aside from stated modiﬁcations, each
dataset is used “as is.”

5.4 Results

The AU ROC values for each dataset are presented in
Table 3. When C4.5 + LS signiﬁcantly improves over a
classiﬁer, then that classiﬁer’s value is in bold. When a
classiﬁer’s value is in italics, it signiﬁcantly improves over
C4.5 + LS. From these results, the following conclusions
about the performance of each classiﬁer may be drawn.

From Table 3, we note that LS improves C4.5 and
C4.5 + U nSM categorically. This indicates that iden-
tifying local rather than global sampling levels is a very
In fact, the AU ROC values produced
effective strategy.
through C4.5 + LS are quite compelling, producing the
best result on 17 of 20 datasets, with one result a tie with
SV M + COGOS on Satimage. C4.5 + LS is outper-
formed by both SV M + COGOS and SV M on Boundary
and Calmodoulin, datasets for which SV M naturally out-
performs C4.5 by a wide margin. Both datasets are highly
dimensional, and as such SV M naturally lends itself to the
high dimensional datasets. Despite the high dimensional-
ity, Local Sampling is still able improve over all other C4.5
based methods. For Pendigits, we note that there is not quite

Table 4. Parameters optimizing each classiﬁer. For COG-OS, they are given as (k, r), where k
is the number of k-means clusters found for the majority class and r is the number of replica-
tions of the minority class, while for Undersample/SMOTE and Local Sampling, they are given as
(U ndersample, SM OT E).
Dataset
Boundary
Breast-w

SV M + COGOS

C4.5 + COGOS

Calmodoulin
Compustat

E-State
Fourclass

German.numer

Mammography

Intrusion

Letter

Oil
Page

Pendigits
Phoneme

PhosS
Pima

Satimage
Segment
Splice

SVMguide1

(4.6, 8.0)
(4.8, 2.9)
(3.4, 6.7)

(2, 8)

(4.8, 7.4)
(3.8, 2.1)
(3.2, 3.9)

(6, 2)

(6.8, 3.3)
(5.0, 1.0)
(4.0, 2.5)
(2.0, 4.5)
(5.2, 3.3)
(5.8, 6.6)
(4.0, 7.2)
(4.4, 8.1)
(2.4, 8.7)
(7.4, 4.4)
(2.6, 3.3)
(2.0, 5.9)

(4.6, 2.7)
(4.6, 6.7)
(2.4, 3.7)

(2, 7)

(6.6, 7.0)
(6.6, 6.2)
(2.0, 6.0)
(8, 10)
(8.0, 3.4)
(4.0, 5.5)
(4.0, 4.0)
(3.0, 6.4)
(8.0, 2.3)
(3.6, 1.4)
(2.8, 1.9)
(2.0, 3.4)
(7.0, 2.8)
(6.2, 6.3)
(5.2, 2.0)
(5.2, 2.0)

C4.5 + U nSM C4.5 + LS
(28, 240)
(54, 217)
(42, 155)
(34, 79)
(36, 247)
(41, 231)
(56, 208)
(43, 244)
(49, 170)
(37, 195)
(42, 151)
(49, 234)
(56, 256)
(56, 170)
(29, 205)
(57, 211)
(40, 397)
(37, 170)
(70, 84)
(42, 166)

(28, 325)
(64, 180)
(35, 285)
(30, 50)
(38, 255)
(55, 285)
(48, 280)
(30, 300)
(60, 205)
(27, 330)
(47, 300)
(50, 290)
(59, 280)
(56, 250)
(30, 160)
(32, 260)
(44, 345)
(45, 250)
(73, 185)
(43, 280)

as large a discrepancy in base classiﬁer performance. We
note that COG-OS improves both C4.5 and SV M above
Local Sampling. Since it does so for both classiﬁers, we
presume that COG-OS is quite effective in extracting a sub-
class for each digit forming the majority class, which is
more effective to improve C4.5 than injecting synthetic
examples. Overall, on the 18 datasets for which signiﬁ-
cance is tested, there is a signiﬁcant improvement on 14
datasets when comparing to C4.5, 13 when comparing to
SV M , 8 when comparing to C4.5 + COGOS, 10 when
comparing to SV M + COGOS, and 10 when comparing
to C4.5 + U nSM . Considering the conservative nature
of the 5x2 cv, these results are noteworthy. Finally, when
we compare the performance ranking of different methods,
C4.5 + LS emerges as a clear winner, at 95% conﬁdence
per the Friedman tests using the Holm procedure. This is a
compelling result, clearly establishing the performance im-
provements achieved by local sampling.

We also point out that dividing the majority class into
sub-classes through COG-OS is effective in improving the
AU ROC values over the base classiﬁer. While it often un-
derperforms, as compared to C4.5 + LS (and statistically
signiﬁcantly so), it improves the AU ROC of C4.5 on 16
of 20 datasets and SV M on 18. COG-OS, using k-means
clustering, is able to effectively exploit the inherent struc-
tures in the majority class space to improve performance
over the base classiﬁer. However, we show that it is more
impactful to decompose the minority class space using a su-
pervised partitioning method.

Table 4 shows the sampling parameters producing opti-
mal results. For COG-OS, the k value selected for C4.5
and SV M are moderately similar, while r values are quite
divergent. The Pearson correlation comparison for k values
between C4.5 and SV M is 0.46, showing moderate corre-
lation; however, r values are weakly negatively correlated at
-0.35. C4.5 and SV M will tend to select r values at oppo-
site ends of the spectrum. Fisher’s z-test suggests that these
two correlations are signiﬁcantly different. We note that the
undersamplings produced by U nSM and LS are more sim-
ilar with a Pearson of 0.74 than the determined SMOTE lev-
els, which correlate at 0.61. However, both sampling levels
are highly correlated and Fisher’s z-test suggests that these
correlations are similar with 95% conﬁdence. Therefore,
we conclude the strength of Local Sampling derives from its
ability to manipulate speciﬁc regions of samplings. These
parameters will allow for the reproducibility of our results.
We will also readily provide all the real-world datasets and
source code to the interested readers.

6 Conclusions

There are various key conclusions to be drawn from this
work. We elucidate the key observations on the behavior
of classiﬁers in the presence of class imbalance, which also
helped develop the thesis for the proposed framework of
Local Sampling (LS). We observed how properties of the
data, aside from class skew itself, affect end classiﬁer per-
formance and that different sampling levels may produce

[13] T. G. Dietterich. Approximate statistical test for comparing
supervised classifﬁcation learning algorithms. Neural Com-
putation, 10(7):1895–1923, 1998.

[14] C. Elkan. The Foundations of Cost-Sensitive Learning. In
International Joint Conference on Artiﬁcial Intelligence (IJ-
CAI), pages 973–978, 2001.

[15] N. Japkowicz. Class Imbalance Problem: Signiﬁcance &
Strategies. In International Conference on Artiﬁcial Intelli-
gence (ICAI), pages 111–117, 2000.

[16] The Third International Knowledge Discovery and Data

Mining Tools Competition, 1999.

[17] M. Kubat, R. C. Holte, and S. Matwin. Machine Learn-
ing for the Detection of Oil Spills in Satellite Radar Images.
Machine Learning, 30:195–215, 1998.

[18] F. Provost and P. Domingos. Tree Induction for Probability-
Machine Learning, 52(3):199–215,

Based Ranking.
September 2003.

ing, 1:81–106, 1986.

[19] J. R. Quinlan. Induction of Decision Trees. Machine Learn-

[20] P. Radivojac, N. V. Chawla, A. K. Dunker,

and
Z. Obradovic. Classiﬁcation and knowledge discovery
Journal of Biomedical Informatics,
in protein databases.
37:224–239, 2004.

[21] C. Rao. A Review of Canonical Coordinates and an Alter-
native to Corresponence Analysis using Hellinger Distance.
Questiio (Quaderns d’Estadistica i Investigacio Operativa),
19:23–63, 1995.

[22] G. Weiss. Mining with Rarity: A Unifying Framework.

ACM SIGKDD Explorations, 6:7–19, 2004.

[23] J. Wu, H. Xiong, P. Wu, and J. Chen. Local Decomposi-
In International Conference
tion for Rare Class Analysis.
on Knowledge Discovery and Data Mining (KDD), pages
814–823, 2007.

optimal performances on similar datasets, even when the
class skew is identical. We also noted that a classiﬁer im-
proved through global sampling levels may be insensitive
to the peculiarities of different components or modalities in
the data, resulting in a suboptimal performance. We demon-
strated that by ﬁrst discovering components or segments in
data, and then applying sampling locally to each of the seg-
ments, resulted in the best global performance — starting
globally, optimizing locally, and predicting globally.

The main contribution of the paper general framework
for Local Sampling (LS) and a class skew insensitive parti-
tioning method, effective in this application. This frame-
work is able to achieve signiﬁcantly better performances
than other contemporary methods. As per the Friedman test,
it achieves best performance, and statistically so, across 20
datasets derived from different domains.

References

[1] E. Alpaydin. Combined 5x2cv F Test for Comparing Super-
vised Classiﬁcation Learning Algorithms. Neural Computa-
tion, 11(8):1885–1892, 1999.

[2] A. Asuncion and D. Newman. UCI Machine Learning

Repository, 2007.

[3] G. Batista, R. Prati, and M. Monard. A Study of the Be-
havior of Several Methods for Balancing Machine Learning
Training Data. SIGKDD Explorations, 6(1):20–29, 2004.

[4] S. Bickel, M. Br¨uckner, and T. Scheffer. Discriminative
Learning for Differeing Training and Test Distributions. In
ICML, pages 81–88, 2007.

[5] C. Chang and C. Lin.

port vector machines, 2001.
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.

LIBSVM: a library for sup-
Software available at

[6] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer. SMOTE: Synthetic Minority Over-sampling
Journal of Artiﬁcial Intelligence Research,
Technique.
16:321–357, 2002.

[7] N. V. Chawla, D. A. Cieslak, L. O. Hall, and A. Joshi. Au-
tomatically countering imbalance and its empirical relation-
ship to cost. Utility-Based Data Mining: A Special issue of
the International Journal Data Mining and Knowledge Dis-
covery, 2008.

[8] N. V. Chawla, N. Japkowicz, and A. Kolcz. Editorial:
Learning from Imbalanced Datasets. SIGKDD Explorations,
6(1):1–6, 2004.

[9] D. A. Cieslak and N. V. Chawla. Detecting fractures in clas-
In IEEE International Conference on

siﬁer performance.
Data Mining (ICDM), 2007.

[10] D. A. Cieslak and N. V. Chawla. A Framework for Moni-
toring Classiﬁers’ Performance: When and Why Failure Oc-
curs? Knowledge and Information Systems, 2008.

[11] D. A. Cieslak and N. V. Chawla. Learning Decision Trees
for Unbalanced Data. In European Conference on Machine
Learning (ECML), pages 241–256, 2008.

[12] J. Demsar. Statistical Comparisons of Classiﬁers over Multi-
ple Data Sets. Journal of Machine Learning Research, 7:1–
30, 2006.

