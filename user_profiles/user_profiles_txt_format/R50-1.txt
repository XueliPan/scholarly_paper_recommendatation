Equip Tourists with Knowledge Mined from Travelogues 

Qiang Hao†*, Rui Cai‡, Changhu Wang‡, Rong Xiao‡, Jiang-Ming Yang‡, Yanwei Pang†, Lei Zhang‡ 

†Tianjin University, Tianjin 300072, P.R. China 

‡Microsoft Research Asia, Beijing 100190, P.R. China 

†{qhao, pyw}@tju.edu.cn; ‡{ruicai, chw, rxiao, jmyang, leizhang}@microsoft.com 

 

ABSTRACT 
With  the  prosperity  of  tourism  and  Web  2.0  technologies,  more 
and more people have willingness to share their travel experiences 
on  the  Web  (e.g.,  weblogs,  forums,  or  Web  2.0  communities). 
These so-called travelogues contain rich information, particularly 
including  location-representative  knowledge  such  as  attractions 
(e.g., Golden Gate Bridge), styles (e.g., beach, history), and activ-
ities  (e.g.,  diving,  surfing).  The  location-representative  informa-
tion  in  travelogues  can  greatly  facilitate  other  tourists’  trip  plan-
ning,  if  it  can  be  correctly  extracted  and  summarized.  However, 
since most travelogues are unstructured and contain much noise, it 
is difficult for common users to utilize such knowledge effective-
ly. In this paper, to mine location-representative knowledge from 
a large collection of travelogues, we propose a probabilistic topic 
model,  named  as  Location-Topic  model.  This  model  has  the  ad-
vantages of (1) differentiability between two kinds of topics, i.e., 
local topics which characterize locations and global topics which 
represent other common themes  shared by  various locations, and 
(2)  representation of  locations  in  the  local  topic  space  to  encode 
both  location-representative  knowledge  and  similarities  between 
locations.  Some  novel  applications  are  developed  based  on  the 
proposed model, including (1) destination recommendation for on 
flexible queries, (2) characteristic summarization for a given des-
tination  with  representative  tags  and  snippets,  and  (3)  identifica-
tion of informative parts of a travelogue and enriching such high-
lights with related images. Based on a large collection of travelo-
gues,  the  proposed  framework  is  evaluated  using  both  objective 
and subjective evaluation methods and shows promising results. 

Categories and Subject Descriptors 
I.5.1  [Pattern  Recognition]:  Models  –  statistical;  H.3.3  [Infor-
mation Storage and Retrieval]: Information Search and Retriev-
al – information filtering 

General Terms 
Algorithms, Experimentation. 

Keywords 
Travelogue mining, probabilistic topic model, recommendation. 

1.  INTRODUCTION 
Travel, as an integral part of human history, has become more and 
more  popular  in  people's  everyday  lives  in  recent  years,  partly 
owing to the increasing amount of travel-related information and 
services on the Web, which provide people with efficient ways to 

* This work was performed at Microsoft Research Asia. 

Copyright  is  held  by  the  International  World  Wide  Web  Conference 
Committee (IW3C2). Distribution of these papers is limited to classroom 
use, and personal use by others. 
WWW 2010, April 26-30, 2010, Raleigh, North Carolina, USA. 
ACM  978-1-60558-799-8/10/04. 

online plan and prepare for their trips. Meanwhile, Web 2.0 tech-
nologies facilitate and encourage people to contribute rather than 
just  obtain  information,  leading  to  a  huge  amount  of  user-
generated  content  (UGC)  on  the  Web.  In  the  tourism  domain, 
more and more people have willingness to record and share their 
travel  experiences  on  weblogs,  forums  or  travel  communities,  in 
the form of textual travelogues and photos taken during the trips. 

Since travel-related UGC not only underlies the communities and 
social network among travelers but also provides other web users 
with rich information related to travel, how to leverage it has at-
tracted  extensive  attention  in  the  literature.  For  instance,  a  lot of 
work has been proposed to mine knowledge from user-contributed 
photos on Flickr [6] to support various applications such as land-
mark  discovery  and  recognition  [22],  landmark  image  selection 
[11][18], location explorer [1][5], and image tag suggestion [15]. 
By contrast, fewer research efforts have been dedicated to know-
ledge mining from travelogues. One related work is [8], in which 
the authors proposed to generate overviews for locations by min-
ing representative tags from travelogues. However, to the best of 
our knowledge, the complete framework of travelogue mining and 
its applications has not been specially investigated. 

We  claim  that  travelogues  can  serve  as  a  promising  resource  of 
travel-related  knowledge,  which  is  complementary  to  user-
generated photos because travelogues cover various travel-related 
aspects,  including  not  only  landmarks  and  natural  things  which 
correspond  to  specific  visual  descriptions  in  photos,  but  also  ab-
stract  aspects  (e.g., history,  culture,  genius loci)  which  are  infor-
mative  to  tourists  but  difficult  to  visualize  using  photos.  With 
such  rich  information,  travelogues  could  support  more  compre-
hensive  descriptions  of  locations  and  comparisons  between  loca-
tions  than  user-generated  photos,  and  thus  could be  leveraged  to 
recommend  locations  according  to  various  queries.  In  addition, 
travelogues contain rich textual contexts of locations to meet vari-
ous  information  needs.  For  example,  representative  snippets  can 
be extracted to describe a location’s characteristics and linked to 
original travelogues as detailed context. 

Furthermore,  in  spite  of  the  access  to  a  great  deal  of  structured 
travel-related information (e.g., vacation packages, flights, hotels) 
offered by travel websites and travel agents, many people who are 
planning a trip prefer to learn experience and guidance from other 
travelers.  Travelogues  supplement  this  structured  information 
with unstructured but personal descriptions of tourist destinations 
and  services.  Although  the  information  in  a  single  travelogue  is 
possibly  noisy or biased, numerous travelogues as a whole  could 
reflect  people’s  overall  preference  and  understanding  of  travel 
resources, and thus can serve as a reliable knowledge source. 

However, acquiring the knowledge in travelogues is a non-trivial 
task,  especially  for  common  users.  Actually,  there  is  a  gap  be-
tween raw travelogues and the information needs of tourists due to 
the data’s intrinsic limitations listed as follows: 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA401Travelogues

We went back to the hotel, packed our bags 
and took the shuttle back to the airport for our 
flight from Oahu to Kauai…

Since we had a few hours to kill before our big 
sunset dinner cruise, we went and checked out 
Spouting Horn near Poipu Beach. It's a pretty 
cool thing so see because as water rushes under 
a lava shelf, it spurts through a small opening at 
the surface. A very unique sight! ….

After a few photos, we continued on our drive. 
What a scenic ride! We drove along the eastern 
coastline and up to Kilauea, Princeville and 
Hanalei...just beautiful. We stopped along the 
way for lots of pictures, one place had at least 
a dozen chickens in the parking lot...

Oahu, Kauai,
Poipu Beach,

Kilauea, Princeville …

locations

surface, water …

lava...

coastline, sunset ...

local topics

hotel, shuttle ...

airport,
flight ...

picture,
photo ...

global topics

Figure 1. Different topics in travelogues, where local topics are 
shown in italic and blue; global topics are shown in green; and 
locations are shown in underline and red. 
·  Noisy  topics:  As  other  UGC,  most  travelogues  are  unstruc-
tured and contain much noise. For instance, the depictions of 
destinations  and  attractions,  in  which  the  common  tourists 
are most interested, are usually intertwined with topics com-
mon in travelogues related to various locations. 

·  Multiple viewpoints: For each destination, there are usually 
various  descriptions  coming  from  many  previous  travelers. 
When  trying  to  comprehensively  know  about  a  destination, 
users usually confront a dilemma that the viewpoint in each 
single travelogue may be biased, while it is time-consuming 
to  read  and  summarize  a  number  of  related  travelogues  to 
outline an overview of the destination’s characteristics. 

·  Lack  of  destination  recommendation:  Although  a  large 
collection of travelogues can cover most of popular destina-
tions in the world, the depictions in a single travelogue usual-
ly focus on only one or a few destinations. Hence, for tourists 
who have particular travel intentions (e.g., go to a beach, go 
hiking)  and  need  to  determine  where  to  go,  there  is  no 
straightforward  and  effective  way  to  obtain  recommended 
destinations, except for surveying a lot of travelogues. 

·  Lack of destination comparison: In travelogues, besides the 
explicit  comparison  made  by  the  authors,  there  is  little  in-
formation about the similarity between destinations, which is 
helpful  for tourists  who  need  suggestions  about destinations 
similar (or dissimilar) to the ones that they are familiar with. 

To overcome these limitations of raw  travelogue  data and bridge 
the  gap  to  real  information  needs,  several  kinds  of  information 
processing  techniques  need  to  be  leveraged.  (1)  For  the  issue  of 
noisy  topics,  we  need  to  discover  topics  from  travelogues  and 
further  distinguish  location-related  topics  with  other  noisy  ones. 
(2) For the issue of multiple viewpoints, we need to find a repre-
sentation of  locations  that  summarizes  all  the  useful  descriptions 
of a location to capture its location-representative knowledge (i.e., 
local  characteristics  such  as  attractions,  activities,  styles).  (3)  To 
provide  destination  recommendation,  a  metric  of  relevance  is 
necessary  to  suggest  locations  most  relevant  to  tourists’  travel 
intentions.  (4)  For  destination  comparison,  a  location  similarity 
metric  is  necessary  to  compare  locations  from  the  perspective  of 
travel. We believe that the first two points should be given prima-
ry  importance  because  the  location-representative  knowledge 
mined from location-related topics underlies the ranking and simi-
larity measurement of locations. 

In  this  paper,  we  consider  the  above  issues  and  investigate  the 
problem  of  mining  location-representative  knowledge 1  from  a 
                                                                 
1 Knowledge  useful  for  tourists  (e.g.,  accommodation,  expense) 
but not specific to locations is beyond our objective in this paper. 

Location
Extraction

Travelogue
Modeling

Location-

Representative

Knowledge

Destination

Recommendation

Destination

Summarization

Travelogue
Enrichment

Travelogues

User

 

Knowledge Mining

 
Figure 2. The overview of the proposed framework, in which 
location-representative knowledge is first mined from a trave-
logue corpus, and then used to support three applications. 

Applications

large  amount  of  travelogues  to  facilitate  tourists  to  fully  utilize 
such knowledge. 

In recent years, probabilistic topic models, such as latent Dirichlet 
allocation (LDA) [2], have been successfully applied to a variety 
of  text  mining  tasks  [3,  4,  13,  14,  16,  17,  19,  20].  This  kind  of 
models are suitable for the task of travelogue mining owing to its 
powerful  capability  of  discovering  latent  topics  from  text  and 
representing documents with such topics. However, to the best of 
our  knowledge,  the  existing  models  are  not  applicable  for  our 
objective because none of them can be used to address the limita-
tions  of  travelogue  data.  Specifically,  although  documents  under 
these models are represented as mixtures of the discovered latent 
topics,  the  entities  appearing  in  the  documents  (e.g.,  locations 
mentioned in travelogues) either lack of representation in the topic 
space, or are represented as mixtures of all the topics, rather than 
the  topics  appropriate  to  characterize  these  entities.  Considering 
the  noisy  topics  in  travelogues,  the  representation  of  locations 
using all the topics would be contaminated by the noise and thus 
is unreliable for further relevance and similarity metrics. 

Therefore, we propose a probabilistic topic model, i.e., Location-
Topic (LT) model, to discover topics from travelogues and simul-
taneously represent locations with appropriate topics. Specifically, 
we define two different types of topics (as illustrated in Figure 1), 
i.e.,  local  topics  which  characterize  specific  locations  from  the 
perspective of travel (e.g., lava, coastline), and global topics (e.g., 
hotel,  airport)  which  do  not  characterize  certain  locations  but 
rather extensively co-occur with various locations in travelogues. 
Since each local topic corresponds to some specific locations and 
travel-related  characteristics,  a  location’s  overall  characteristics 
can be generally represented in the local topic space, as a mixture 
of (i.e., a multinomial distribution over) local topics. 

Based on the LT  model, the aforementioned  limitations of trave-
logue data are handled to some extent because: 
·  By decomposing travelogues into local and global topics, we 
can  obtain  location-representative  knowledge  from  local  top-
ics, with other semantics captured by global topics filtered out. 
·  By  representing  each  location  using  local  topics  mined  from 
the  entire  travelogue  collection,  multiple  viewpoints  of  each 
location can be naturally summarized. 

·  Based  on  the  representation  of  locations  in  the  local  topic 
space, both the relevance of a location to a given travel inten-
tion and the similarity between locations can be measured. 

As  shown  in  Figure  2,  given  a  collection  of  travelogues  (in  the 
implementation, either of two data sets: 100K English travelogues, 
or 94K Chinese ones), we first extract the locations mentioned in 
the  text.  Then  a  LT  model  is  trained  on  the  collection  to  learn 
local and global topics, as well as the representation of locations 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA402Document

Local Topic

Global Topic

c Location

Document

×

i

p
o
T

l

a
c
o
L

×

n
o
i
t
a
c
o
L

+ (1-λ) ×

m
r
e
T

Document

×

i

c
p
o
T

l

a
b
o
G

l

m
r
e
T

=

λ ×

m
r
e
T

(I)
(cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1856)(cid:4667) (cid:3) (cid:3404) (cid:3)(cid:2019) (cid:3400) (cid:3533) (cid:3533) (cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667)(cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1856)(cid:4667)

(cid:3) (cid:3397) (cid:3) (cid:4666)(cid:883) (cid:3398) (cid:2019)(cid:4667) (cid:3400) (cid:3533)

(II)
(cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:1314)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:1314)(cid:513)(cid:1856)(cid:4667)

 

Figure 3. An illustration of the travelogue decomposition with (I) Tloc local topics and (II) Tgl global topics. It should be noted that 
this figure mainly serves as an illustrative interpretation of the ideas, but does not exactly accord with the model details. 

(cid:1838)

(cid:1864)(cid:1867)(cid:1855)

(cid:1846)

(cid:1864) (cid:3404) (cid:883)

(cid:1878) (cid:3404) (cid:883)

 

(cid:1859)(cid:1864)

(cid:1846)

(cid:1878)(cid:1314) (cid:3404) (cid:883)

in  the  local  topic  space.  Based  on  the  learnt  knowledge,  we  can 
fulfill different application tasks. Specifically, we consider a sce-
nario where a user learns online knowledge to plan a trip in three 
steps: 1) selecting a destination from some recommended ones, 2) 
browsing  the  characteristics  of  the  selected  destination  to  get  an 
overview, and 3) browsing some travelogues to figure out detailed 
travel  route.  To  facilitate  these  three  steps,  the  following  three 
applications are implemented, respectively: 
·  Destination Recommendation: We recommend destinations 
to users, in terms of either similarity to a given destination or 
relevance to a given travel intention. 

·  Destination  Summarization:  Each  destination  is  presented 
as  an  overview  by  summarizing  its  representative  aspects 
with textual tags. Representative snippets are also offered as 
further  descriptions  to  verify  and  interpret  the  relation  be-
tween each tag and the destination. 

·  Travelogue  Enrichment:  To  help  a  user  better  browse  and 
understand travelogues, we identify the informative parts of a 
travelogue and highlight them with related images. 

The paper is organized as follows. In Section 2, we introduce the 
proposed Location-Topic model. Then we describe three applica-
tions of the LT  model in Section 3. Experimental and evaluation 
results are shown in Section 4. Section 5 presents the related work. 
In Section 6, we give the conclusion and future work. 

2. TRAVELOGUE MODELING 
In this section, we present the Location-Topic (abbreviated as LT) 
model  and  its  usage  for  further  applications.  By  modeling  the 
generative process of travelogues, the model could discover topics 
from travelogues and represent locations with the learnt topics. 

2.1  Basic Idea 
Following  the  existing  work  on  probabilistic  topic  models,  we 
treat each travelogue document as a mixture of topics, where each 
topic  is  a  multinomial  distribution  over  terms  in  vocabulary  and 
corresponds to some specific semantics. As discussed in Section 1, 
we  further  assume  that  travelogues  are  composed  of  local  and 
global  topics,  and  each  location  is  represented  by  a  multinomial 
distribution over local topics. Thus, the proposed LT model aims 
at  discovering  local  and  global  topics,  as  well  as  each  location’s 
distribution over local topics, from a travelogue collection. 

We  use  Figure  3  to provide  an  illustrative  and  intuitive  explana-
tion  how  we  decompose  travelogue  documents  into  local  topics 
and global topics. A travelogue collection can be represented by a 
th doc-
Term-Document matrix where the 
ument’s  distribution  over  terms,  as  illustrated  at  the  top  left  of 
Figure  3.  Based  on  this  representation,  our  goal  is  equivalent  to 
decomposing  a  given  Term-Document  matrix  into  multiple  ma-

th column encodes the 

(cid:1862)

(cid:1862)

trices,  including  Term-LocalTopic,  Term-GlobalTopic,  and  Lo-
calTopic-Location  matrices.  In  Figure  3,  there  are  another  two 
matrices that we should learn, i.e., GlobalTopic-Document matrix 
and Location-Document matrix. The former is the same as that of 
common topic models, whereas the latter is specific and important 
to our objective, and thus need particular discussion. 

For Location-Document matrix, we have some observed informa-
tion,  namely  the  user-provided  location  labels  associated  with 
each travelogue. However, such document-level labels are not fit 
for  our  scenario  because  they  are  usually  too  coarse  and  incom-
plete to support knowledge mining for all the locations described 
in  travelogues,  and  sometimes  they  are  even  labeled  incorrectly. 
Hence,  we  rely  on  the  locations  extracted  from  text  instead  of 
these  labels.  There  are  several  methods  for  location  extraction, 
e.g., looking up a gazetteer, or applying a Web service like Yahoo 
Placemaker2. As such pre-processing is not our focus, we employ 
a beforehand implemented location extractor based on a gazetteer 
and location disambiguation algorithms  handling geographic hie-
rarchy and context of locations, which can achieve high accuracy 
by considering all the candidate locations in a document simulta-
neously. We will detail this location extractor elsewhere. 

Intuitively, the extracted locations can serve as strong indications 
of  locations  described  in  travelogues.  However,  these  extracted 
locations are improper to be taken as the real Location-Document 
matrix,  due  to  an  observed  gap  between  them  and  the  locations 
actually described. For instance, a series of locations may be men-
tioned only as a trip summary, but without (or with quite unequal) 
descriptions  in  the  contextual  text.  Besides,  we  also  observe  that 
in a typical travelogue, the author usually concentrates on depict-
ing some locations in consecutive sentences. That is, consecutive 
words tend to correspond to the same locations. Considering these 
observations, we assume that all the words in a text segment (e.g., 
a document, paragraph, or sentence) share a multinomial distribu-
tion over locations, which is affected by a Dirichlet prior derived 
from the extracted locations in the segment. In this way, the Loca-
tion-Document  matrix  is  kept  variable  to  better  model  the  data, 
while also benefiting from the extracted locations as priors. 

(cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1856)(cid:4667)

As  the  decomposition  of  likelihood 
 shown  in  Figure  3, 
each word in a document is assumed to be “written” by making a 
binary decision between two paths, i.e., (1) selecting a location, a 
local  topic,  and  a  term  in  sequence,  and  (2)  selecting  a  global 
topic and a term in sequence. Once decomposed as above, a trave-
logue  collection  preserves  its  location-representative  knowledge 
in  LocalTopic-Location  matrix,  and  topics  in  Term-LocalTopic 
and Term-GlobalTopic matrices. 
                                                                 
2 http://developer.yahoo.com/geo/placemaker/ 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA403Figure 4. Graphical representation of the proposed LT model. 

(cid:7)

(cid:1868)(cid:3435)(cid:1876)(cid:1861) (cid:3404) (cid:1859)(cid:1864)(cid:481) (cid:1878)(cid:1861) (cid:3404) (cid:1878)(cid:3627)(cid:1875)(cid:1861) (cid:3404) (cid:1875)(cid:481) (cid:2206)(cid:819)(cid:1861)(cid:481) (cid:2208)(cid:819)(cid:1861)(cid:481) (cid:2205)(cid:819)(cid:1861)(cid:481) (cid:2009)(cid:481) (cid:2237)(cid:481) (cid:2015)

(cid:3439)

 

(cid:574)

(cid:15)

(cid:134)(cid:481)(cid:149)
(cid:588)

(cid:590)

(cid:134)(cid:481)(cid:149)
(cid:134)
(cid:22)

(cid:142)

(cid:154)

(cid:156)

(cid:134)

(cid:153)
(cid:17)

(cid:142)(cid:145)(cid:133)
(cid:142)(cid:145)(cid:133)(cid:596)
(cid:23)(cid:3)

(cid:142)(cid:145)(cid:133)

(cid:580)

(cid:137)(cid:142)

(cid:596)
(cid:23)(cid:3)

(cid:137)(cid:142)

(cid:137)(cid:142)

 

(cid:580)

(cid:134)(cid:481)(cid:149)
(cid:598)

(cid:576)

(cid:573)

2.2  Generative Process of Travelogues 
In  the  LT  model,  each  location 
,  a  multi-
nomial  distribution  over  local  topics,  with  symmetric  Dirichlet 
prior
,  a  multinomial 
distribution over global topics, with symmetric Dirichlet prior

 is  associated  with

 is  represented  by

;  each  document 

(cid:3)(cid:2032)(cid:1864)

. 

(cid:1864)

(cid:3)(cid:2010)

(cid:1856)

 as a set of 

We extend the extensively used bag-of-words assumption to treat 
each document 
 non-overlapping segments, where 
a segment could be a sentence, a paragraph, or a sliding window 
(cid:1845)(cid:1856)
in the document. Each segment 
 is associated with (1) a bag-of-
words, (2) a binomial distribution over global topics versus local 
topics
, and (3) a multinomial 
distribution 

(cid:1871)
’s corresponding location set 

, with Beta prior

 over segment

(cid:3)(cid:2009)

(cid:1864)(cid:1867)(cid:1855)

(cid:1856)

(cid:1859)(cid:1864)

(cid:3)(cid:2016)(cid:1856)

(cid:3)(cid:2024)(cid:1856)(cid:481)(cid:1871)

(cid:3)(cid:2011) (cid:3404) (cid:4668)(cid:2011)

(cid:481) (cid:2011)

(cid:4669)

with Dirichlet prior parameterized by 

(cid:2022)(cid:1856)(cid:481)(cid:1871)
 defined as 
(cid:2278)(cid:1856)(cid:481)(cid:1871) (cid:1565) (cid:4668)(cid:1864)(cid:513)(cid:1864)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:149)(cid:135)(cid:137)(cid:143)(cid:135)(cid:144)(cid:150)(cid:3)(cid:1871)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1856)(cid:4669)

(cid:3)(cid:1871)

, 

(cid:2031)(cid:1856)(cid:481)(cid:1871)

 , 

” is short for “the number of times” and coefficient 

(cid:2031)(cid:1856)(cid:481)(cid:1871) (cid:1565) (cid:3419)(cid:2012)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:1864) (cid:3404) (cid:2020) (cid:942) (cid:851)(cid:4666)(cid:1864)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:149)(cid:135)(cid:137)(cid:143)(cid:135)(cid:144)(cid:150)(cid:3)(cid:1871)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1856)(cid:4667)(cid:3423)(cid:1864)(cid:1488)(cid:2278)(cid:1856) (cid:481)(cid:1871)

where “
 
denotes  the  precision  of  the  prior.  In  the  implementation,  each 
(cid:2020)
paragraph in a travelogue is treated as a raw segment, with further 
merging to ensure that each segment contains at least one location. 

(cid:851)(cid:4666)(cid:942)(cid:4667)

,  which  consists  of 

 documents  covering 

the  generative  process  of  a 

 unique terms, is defined as follows: 

The graphical representation of the LT model is shown in Figure 
4.  Accordingly, 
travelogue 
tion
 unique  loca-
tions and 
·  For  each  local  topic
tribution over terms, 
·  For each global topic
tribution over terms, 

,  draw  a  multinomial  dis-
. 
, draw a multinomial dis-

(cid:1849)

(cid:3)(cid:1829)

(cid:1830)

(cid:1864)(cid:1867)(cid:1855)

(cid:1864)(cid:1867)(cid:1855)

(cid:1864)(cid:1867)(cid:1855)

(cid:1838)

·  For each location
over local topics, 

·  For each document 

(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:4666)(cid:2015)

, draw a multinomial distribution 
. 
(cid:3)(cid:1864) (cid:1488) (cid:4668)(cid:883)(cid:481) (cid:485) (cid:481) (cid:1838)(cid:4669)
(cid:2032)(cid:1864)(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:4666)(cid:2010)(cid:4667)

: 

(cid:4667)

topics, 

o  Draw  a  multinomial  distribution  over  global 

(cid:3)(cid:1878) (cid:1488) (cid:4668)(cid:883)(cid:481) (cid:485) (cid:481) (cid:1846)
(cid:2030)(cid:1878)
(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:4666)(cid:2015)
(cid:3)(cid:1878) (cid:1488) (cid:4668)(cid:883)(cid:481) (cid:485) (cid:481) (cid:1846)
(cid:1859)(cid:1864)
(cid:1859)(cid:1864)
(cid:2030)(cid:1878)

(cid:4669)
(cid:4667)
. 
(cid:1859)(cid:1864)
(cid:4669)

. 

(cid:1856) (cid:1488) (cid:4668)(cid:883)(cid:481) (cid:485) (cid:481) (cid:1830)(cid:4669)

o  For each segment 

 of document 

: 

(cid:2016)(cid:1856) (cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:4666)(cid:2009)(cid:4667)
§  draw a binomial distribution over global topics versus lo-

cal topics, 

(cid:1871)

; 

(cid:1856)

§  draw  a  multinomial  distribution  over  locations  in 

, 

o  For each word 

(cid:2024)(cid:1856) (cid:481)(cid:1871)(cid:817)(cid:1828)(cid:1857)(cid:1872)(cid:1853)(cid:4666)(cid:2011)(cid:4667)

. 
 in segment 

§  draw a binary switch 
§  if 

(cid:2022)(cid:1856)(cid:481)(cid:1871)(cid:817)(cid:1830)(cid:1861)(cid:1870)(cid:3435)(cid:2031)(cid:1856)(cid:481)(cid:1871)(cid:3439)
(cid:1875)(cid:1856)(cid:481)(cid:1866)
,  draw  a  location 

; 
(cid:1856)
(cid:1876)(cid:1856)(cid:481)(cid:1866) (cid:817)(cid:1828)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2024)(cid:1856) (cid:481)(cid:1871)(cid:3439)

(cid:1871)

and then draw a local topic 

, draw a global topic 

(cid:1876)(cid:1856)(cid:481)(cid:1866) (cid:3404) (cid:1864)(cid:1867)(cid:1855)

§  if 
§  draw word 
(cid:1876)(cid:1856)(cid:481)(cid:1866) (cid:3404) (cid:1859)(cid:1864)

; 
(cid:1864)(cid:1856)(cid:481)(cid:1866) (cid:817)(cid:1839)(cid:1873)(cid:1864)(cid:1872)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2022)(cid:1856)(cid:481)(cid:1871)(cid:3439)

(cid:1878)(cid:1856) (cid:481)(cid:1866) (cid:817)(cid:1839)(cid:1873)(cid:1864)(cid:1872)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2032)(cid:1864)(cid:1856) (cid:481)(cid:1866)

(cid:3439)

. 

(cid:1878)(cid:1856) (cid:481)(cid:1866) (cid:817)(cid:1839)(cid:1873)(cid:1864)(cid:1872)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:4666)(cid:2016)(cid:1856) (cid:4667)

(cid:1876)(cid:1856)(cid:481)(cid:1866)
(cid:1875)(cid:1856)(cid:481)(cid:1866) (cid:817)(cid:1839)(cid:1873)(cid:1864)(cid:1872)(cid:1861)(cid:1866)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864) (cid:4672)(cid:2030)(cid:1878)(cid:1856) (cid:481)(cid:1866)

(cid:4673)

(cid:1871)

, 

; 

(cid:3)(cid:1829)
, 

(cid:1859)(cid:1864)

(cid:1859)(cid:1864) (cid:481)(cid:1878)
(cid:1866)(cid:1856)(cid:481)(cid:819)(cid:1861)
(cid:1859)(cid:1864)
(cid:1866)(cid:1856) (cid:481)(cid:819)(cid:1861)
(cid:1864)(cid:1867)(cid:1855)

2.3  Parameter Estimation 
To estimate the parameters of the LT model, we need to estimate 
the latent variables conditioned on the observed variables, namely 
 are  vectors  of  assignments 
of  global/local  binary  switches,  locations,  and  topics  for  all  the 
(cid:1868)(cid:4666)(cid:2206)(cid:481) (cid:2194)(cid:481) (cid:2208)(cid:513)(cid:2205)(cid:481) (cid:2238)(cid:481) (cid:2009)(cid:481) (cid:2010)(cid:481) (cid:2237)(cid:481) (cid:2015)(cid:4667)
words  in  travelogue  collection
,  respectively.  We  use  the  col-
lapsed Gibbs sampling [7] with the following updating formulas. 

,  where 

(cid:2206)(cid:481) (cid:2194)(cid:481) (cid:2208)

For global topic

(cid:3)(cid:1878) (cid:1488) (cid:4668)(cid:883)(cid:481) (cid:485) (cid:481) (cid:1846)

(cid:4669)

 

(cid:1859)(cid:1864)

(cid:1859)(cid:1864) (cid:481)(cid:1878)
(cid:1866)(cid:1875)(cid:481)(cid:819)(cid:1861)
(cid:1859)(cid:1864) (cid:481)(cid:1878)
(cid:1314)
For local topic
(cid:963) (cid:1866)(cid:1875)
(cid:481)(cid:819)(cid:1861)

(cid:1503)

(cid:1875)

(cid:1314)

(cid:1859)(cid:1864)

(cid:3397) (cid:2015)

(cid:1859)(cid:1864) (cid:942)

(cid:3397) (cid:1849)(cid:2015)

(cid:3397) (cid:2009)
(cid:1859)(cid:1864)

(cid:1859)(cid:1864)

(cid:942) (cid:4672)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

 and location
(cid:3397) (cid:1846)

(cid:2009)

(cid:1859)(cid:1864)

(cid:3397) (cid:2011)
, 

(cid:4673)(cid:484)

(cid:3)(cid:1878) (cid:1488) (cid:4668)(cid:883)(cid:481) (cid:485) (cid:481) (cid:1846)

(cid:4669)

(cid:3)(cid:1864) (cid:1488) (cid:2278)(cid:1856) (cid:481)(cid:1871)

 

(cid:1864)(cid:1867)(cid:1855)
, 
(cid:1868)(cid:3435)(cid:1876)(cid:1861) (cid:3404) (cid:1864)(cid:1867)(cid:1855)(cid:481) (cid:1864)(cid:1861) (cid:3404) (cid:1864)(cid:481) (cid:1878)(cid:1861) (cid:3404) (cid:1878)(cid:3627)(cid:1875)(cid:1861) (cid:3404) (cid:1875)(cid:481) (cid:2206)(cid:819)(cid:1861)(cid:481) (cid:2194)(cid:819)(cid:1861)(cid:481) (cid:2208)(cid:819)(cid:1861)(cid:481) (cid:2205)(cid:819)(cid:1861)(cid:481) (cid:2010)(cid:481) (cid:2237)(cid:481) (cid:2015)

(cid:1864)(cid:1867)(cid:1855) (cid:481)(cid:1878)

(cid:1864)(cid:1867)(cid:1855)

(cid:1864)

(cid:3439)

(cid:1503)
where 

(cid:1875) (cid:1314)

(cid:1864)(cid:1867)(cid:1855) (cid:481)(cid:1878)
(cid:963) (cid:1866)(cid:1875) (cid:1314) (cid:481)(cid:819)(cid:1861)
 is  the  number  of  times  term 

(cid:1866)(cid:1864)(cid:481)(cid:819)(cid:1861)(cid:3397)(cid:1846)

(cid:1866)(cid:1856) (cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:3397)(cid:1849)(cid:2015)

(cid:2010) (cid:942)

(cid:1864)(cid:1867)(cid:1855)

(cid:1864)(cid:1867)(cid:1855)

(cid:942)

(cid:1866)(cid:1864)(cid:481)(cid:819)(cid:1861)

(cid:3397)(cid:2010)
(cid:1864)(cid:1867)(cid:1855)

(cid:1866)(cid:1856) (cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:3397)(cid:2031)(cid:1856) (cid:481)(cid:1871)(cid:481)(cid:1864)
(cid:3397)(cid:2031)(cid:1856) (cid:481)(cid:1871) (cid:942) (cid:3435)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:1864)(cid:1867)(cid:1855)

 is  assigned  to  global 

(cid:3397) (cid:2011)

(cid:3439)

(cid:1864)(cid:1867)(cid:1855) (cid:481)(cid:1878)
(cid:1866)(cid:1875) (cid:481)(cid:819)(cid:1861)

(cid:3397)(cid:2015)

(cid:1864)(cid:1867)(cid:1855)

(cid:1864)(cid:1867)(cid:1855) (cid:481)(cid:1878)
(cid:1866)(cid:1875) (cid:481)(cid:819)(cid:1861)

(cid:1859)(cid:1864) (cid:481)(cid:1878)
,  and  similarly 
(cid:1866)(cid:1875) (cid:481)(cid:819)(cid:1861)

topic 
number of times a word in document 

 is  that  for  local  topic

. 

(cid:1875)

 is  the 
 is assigned to global topic 
 is 

(cid:1859)(cid:1864) (cid:481)(cid:1878)
(cid:1866)(cid:1856)(cid:481)(cid:819)(cid:1861)

(cid:3)(cid:1878)

(cid:1878)

,  while 

 is  the  number  of  times  a  word  in  document 

(cid:1856)

(cid:1859)(cid:1864)
(cid:1866)(cid:1856) (cid:481)(cid:819)(cid:1861)

(cid:1864)(cid:1867)(cid:1855) (cid:481)(cid:1878)

(cid:1866)(cid:1864)(cid:481)(cid:819)(cid:1861)
 in total. 

 is  assigned  to  local  topic 

assigned  to  a  global  topic. 
(cid:1878)
assigned  to  location 
words assigned to location 
a  word in segment 
consequently to a local topic. 
of times a word in segment 
and  to  local  topics,  respectively.  For  all  the  counts,  subscript 
indicates that the

 is  the  number  of  times  a  word 
 
 is the number of times 
(cid:1866)(cid:1864)(cid:481)(cid:819)(cid:1861)
, and 
 denote the number 
 is assigned to global 
 

 is assigned to location 
(cid:1866)(cid:1856) (cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)
 and 

-th word is excluded from the computation. 

(cid:1856)
 of document 
(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:1864)
 of document 

,  out  of 

(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:1864)(cid:1867)(cid:1855)

(cid:1856)

(cid:1856)

(cid:1878)

(cid:1859)(cid:1864)

(cid:1871)

(cid:1871)

(cid:1864)

(cid:1864)

(cid:1864)

(cid:819)(cid:1861)
After such a Gibbs sampler reaches burn-in, we can harvest sever-
al samples and count the assignments to estimate the parameters: 

(cid:3)(cid:1861)

, 

(cid:1876)

(cid:1876)
(cid:2030)(cid:1878)(cid:481)(cid:1875)

(cid:1876)

(cid:1876)(cid:481)(cid:1878)
(cid:1503) (cid:1866)(cid:1875)
(cid:3397) (cid:2015)
(cid:2032)(cid:1864)(cid:481)(cid:1878) (cid:1503) (cid:1866)(cid:1864)

(cid:1864)(cid:1867)(cid:1855) (cid:481)(cid:1878)

(cid:481) (cid:1876) (cid:1488) (cid:4668)(cid:1859)(cid:1864)(cid:481) (cid:1864)(cid:1867)(cid:1855)(cid:4669)(cid:481) (cid:1878) (cid:3404) (cid:883)(cid:481) (cid:485) (cid:481) (cid:1846)

. 

(cid:1864)(cid:1867)(cid:1855)

(cid:3397) (cid:2010)(cid:481) (cid:1878) (cid:3404) (cid:883)(cid:481) (cid:485) (cid:481) (cid:1846)

2.4  Utilizing the Model 
Once estimated, the parameters of the LT model can support sev-
eral  applications  by  providing  the  data  representations  and  simi-
larity metrics for both locations and terms. 
2.4.1  Location Representation and Similarity Metric 
Each  location 
-dimensional 
-dimensional  term  space.  For  the  for-
local  topic  space  or  the 
mer, location 
 namely its correspond-
ing  multinomial  distribution  over  local  topics.  For  the  latter,  we 
derive  a  probability  distribution  over  terms  conditioned  on  loca-
tion 
 directly from the raw Gibbs samples, by counting the words 
assigned to location

 can  be  represented  in  either  the 

(cid:1864)
 is simply represented by

, as 

(cid:3)(cid:2032)(cid:1864)

(cid:1849)

(cid:1864)(cid:1867)(cid:1855)

(cid:1846)

(cid:1864)

(cid:1864)

(cid:3)(cid:1864)
(cid:1875)
(cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1864)(cid:4667) (cid:1503) (cid:1866)(cid:1864)

(cid:481)(cid:3)(cid:3)(cid:3)(cid:1875) (cid:3404) (cid:883)(cid:481) (cid:485) (cid:481) (cid:1849)

, 

(cid:1875)
(cid:1866)(cid:1864)

According  to the location  representation  in the local  topic  space, 
the symmetric similarity between two locations 
 is meas-
ured  by  the  distance  between  their  corresponding  multinomial 
distributions over local topics 

 and 

 and

, as 

(cid:1864)(cid:884)

(cid:1864)(cid:883)

(cid:1875)

(cid:3)(cid:1864)

(cid:2032)(cid:1864)(cid:883)

(cid:3)(cid:2032)(cid:1864)(cid:884)

, 

(cid:1838)(cid:1867)(cid:1855)(cid:1845)(cid:1861)(cid:1865)(cid:4666)(cid:1864)(cid:883)(cid:481) (cid:1864)(cid:884)(cid:4667) (cid:3404) (cid:1857)(cid:1876)(cid:1868)(cid:3419)(cid:3398)(cid:2028)(cid:1830)(cid:1836)(cid:1845) (cid:3435)(cid:2032)(cid:1864)(cid:883)

(cid:3630)(cid:2032)(cid:1864)(cid:884)(cid:1)(cid:3439)(cid:3423)

 of document 

: 

where 

 is the number of times term 

 is assigned to location

. 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA404(cid:1875)(cid:883)

. 

 is location 

(cid:1845)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:1875)(cid:1869) (cid:4666)(cid:1864)(cid:4667) (cid:3404) (cid:3398)(cid:1830)(cid:1837)(cid:1838) (cid:4672)(cid:2012)(cid:1875)(cid:1869) (cid:1313)(cid:2032)(cid:1864)(cid:1)(cid:4673) (cid:3397) (cid:2021) (cid:142)(cid:145)(cid:137) (cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:481)

where 
’s distribution over the local topics. Actual-
ly,  with the  above query  expansion strategy,  it is straightforward 
to support multi-word queries for more complex travel intentions. 

(cid:1864) (cid:1488) (cid:2278)(cid:481) (cid:2021) (cid:3410) (cid:882)

(cid:2032)(cid:1864)

(cid:1864)

(cid:1868)(cid:3397)(cid:1869)
(cid:884) (cid:1)(cid:4673)

,  while 

(cid:1830)(cid:1836)(cid:1845) (cid:4666)(cid:1)(cid:942)(cid:513)(cid:513)(cid:942)(cid:1)(cid:4667)

(cid:1868)(cid:3397)(cid:1869)
(cid:884) (cid:1)(cid:4673) (cid:3397)

(cid:1830)(cid:1836)(cid:1845) (cid:4666)(cid:1868)(cid:1313)(cid:1869)(cid:1)(cid:4667) (cid:3404)

(cid:883)
(cid:884) (cid:1830)(cid:1837)(cid:1838) (cid:4672)(cid:1868) (cid:4699)

(cid:883)
(cid:884) (cid:1830)(cid:1837)(cid:1838) (cid:4672)(cid:1869) (cid:4699)

 denotes  the  Jensen-Shannon  (JS)  divergence  de-

where 
fined  as 
denotes  the  Kullback-Leibler  (KL)  divergence;  coefficient 
is used to normalize different numbers of local topics. 
2.4.2  Term Representation and Similarity Metric 
In addition to that of locations, we also need a representation and 
corresponding  similarity  metric  of  terms,  so  as  to  measure  the 
relevance of a location (or a snippet) to a given query term in the 
application  of  destination  recommendation  (or  summarization). 
Hence, we expand each term 
 in the vocabulary into a probabili-
, as 
ty distribution over the learnt 

 
 
(cid:1830)(cid:1837)(cid:1838)(cid:4666)(cid:1)(cid:942)(cid:513)(cid:513)(cid:942)(cid:1)(cid:4667)
(cid:2028) (cid:3408) (cid:882)

 local topics, denoted by

(cid:1864)(cid:1867)(cid:1855)

(cid:1875)
(cid:1846)

(cid:3)(cid:2012)(cid:1875)

 , 

(cid:1864)(cid:1867)(cid:1855)

(cid:1846)

(cid:2012)(cid:1875) (cid:3404) (cid:4668)(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1875)(cid:4667)(cid:4669)(cid:1878)(cid:3404)(cid:883)

where 
Accordingly, the symmetric similarity between two terms 

 is the total number of words assigned to local topic 

(cid:1864)(cid:1867)(cid:1855)
(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1875)(cid:4667) (cid:1503) (cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:4667) (cid:1503) (cid:2030)(cid:1878)(cid:481)(cid:1875)

. 
 and 

(cid:4682)(cid:3)(cid:3)

 is measured based on their distributions over local topics, as 

(cid:1878)

(cid:1864)(cid:1867)(cid:1855)
(cid:1866)(cid:1878)

(cid:1864)(cid:1867)(cid:1855) (cid:1)
(cid:1866)(cid:1878)

(cid:1846)(cid:1857)(cid:1870)(cid:1865)(cid:1845)(cid:1861)(cid:1865)(cid:4666)(cid:1875)(cid:883)(cid:481) (cid:1875)(cid:884)(cid:4667) (cid:3404) (cid:1857)(cid:1876)(cid:1868)(cid:3419)(cid:3398)(cid:2028)(cid:1830)(cid:1836)(cid:1845) (cid:3435)(cid:2012)(cid:1875)(cid:883)

(cid:1875)(cid:884)
2.4.3  Inference 
Given the estimated parameters
 , we can infer hidden variables 
for unseen travelogues. Specifically, a Gibbs sampler is run on the 
unseen document 

 using the following updating formulas: 

(cid:3630)(cid:2012)(cid:1875)(cid:884)(cid:1)(cid:3439)(cid:3423)

(cid:3)(cid:563)

(cid:1856)

(cid:1859)(cid:1864) (cid:481)(cid:1878)
(cid:1866)(cid:1856) (cid:481)(cid:819)(cid:1861)
(cid:1859)(cid:1864)

(cid:3397)(cid:2009)
(cid:1859)(cid:1864)

(cid:1868)(cid:3435)(cid:1876)(cid:1861) (cid:3404) (cid:1859)(cid:1864)(cid:481) (cid:1878)(cid:1861) (cid:3404) (cid:1878)(cid:3627)(cid:1875)(cid:1861) (cid:3404) (cid:1875)(cid:481) (cid:2206)(cid:819)(cid:1861)(cid:481) (cid:2208)(cid:819)(cid:1861)(cid:482) (cid:563)(cid:3439)

(cid:1859)(cid:1864)
(cid:1503) (cid:2030)(cid:1878)(cid:481)(cid:1875)

(cid:942)

(cid:1866)(cid:1856) (cid:481)(cid:819)(cid:1861)

(cid:3397)(cid:1846)

(cid:2009) (cid:942) (cid:4672)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:3397) (cid:2011)

(cid:4673) (cid:481) (cid:1878) (cid:3404) (cid:883)(cid:481) (cid:485) (cid:481) (cid:1846)

 

(cid:1859)(cid:1864)

(cid:1859)(cid:1864)

, 

(cid:1859)(cid:1864)

 

(cid:1868)(cid:3435)(cid:1876)(cid:1861) (cid:3404) (cid:1864)(cid:1867)(cid:1855)(cid:481) (cid:1864)(cid:1861) (cid:3404) (cid:1864)(cid:481) (cid:1878)(cid:1861) (cid:3404) (cid:1878)(cid:3627)(cid:1875)(cid:1861) (cid:3404) (cid:1875)(cid:481) (cid:2206)(cid:819)(cid:1861)(cid:481) (cid:2194)(cid:819)(cid:1861)(cid:482) (cid:563)(cid:3439)

(cid:1864)

(cid:1864)(cid:1867)(cid:1855)

(cid:1864)(cid:1867)(cid:1855)
(cid:1503) (cid:2030)(cid:1878)(cid:481)(cid:1875)

(cid:942) (cid:2032)(cid:1864)(cid:481)(cid:1878) (cid:942)

(cid:1866)(cid:1856) (cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:1864)(cid:1867)(cid:1855)

(cid:3397)(cid:2031)(cid:1856) (cid:481)(cid:1871)(cid:481)(cid:1864)
(cid:3397)(cid:2031)(cid:1856) (cid:481)(cid:1871) (cid:942) (cid:3435)(cid:1866)(cid:1856)(cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:1864)(cid:1867)(cid:1855)

(cid:1866)(cid:1856) (cid:481)(cid:1871)(cid:481)(cid:819)(cid:1861)

(cid:3397) (cid:2011)

After collecting a number of samples, we can infer a distribution 
over locations for each term 
 by count-
ing the number of times term 
(cid:1875)
(cid:1875)

 appearing in document 
 is assigned to each location 

(cid:1856)
. 
(cid:851)(cid:4666)(cid:1875) (cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149) (cid:3)(cid:139)(cid:144) (cid:3)(cid:1856)(cid:3)(cid:131)(cid:144)(cid:134) (cid:3)(cid:139)(cid:149) (cid:3)(cid:131)(cid:149)(cid:149)(cid:139)(cid:137)(cid:144)(cid:135)(cid:134) (cid:3)(cid:150)(cid:145) (cid:3)(cid:1864)(cid:4667)

 as 

(cid:3439)(cid:481) (cid:1878) (cid:3404) (cid:883)(cid:481) (cid:485) (cid:481) (cid:1846)

(cid:1864)

. 

(cid:1864)(cid:1867)(cid:1855)

(cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667) (cid:3404)

3.  APPLICATIONS 
In this section, we introduce how to leverage the learnt LT model 
to enable three interesting applications: destination recommenda-
tion, destination summarization, and travelogue enrichment. 

(cid:851)(cid:4666)(cid:1875) (cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149) (cid:3)(cid:139)(cid:144) (cid:3)(cid:1856)(cid:4667)

3.1  Destination Recommendation 
The  first  question  raised  by  a  tourist  is:  where  should  I  go? 
Meanwhile, a tourist has some preferences about the travel desti-
nations, which are usually expressed in terms of two criteria: 
·  Being similar to a given location 

¾  “I quite enjoyed the trip to Honolulu last year. Is there 

any other destination with similar style?” 

·  Being relevant to a given travel intention 

¾  “I plan to go hiking next month. Could you recommend 

some destinations good for hiking?” 

3.1.1  Similarity-Oriented Recommendation 
, each 
Given a query location 
 in  the  local  topic  space, 
destination 
defined as 
in Section 2.4.1. Besides, each destination has 
a query-independent popularity which should also be considered. 
The ranking score for recommendation is computed as 

 and a candidate destination set

 has  a  similarity  to 

(cid:1864) (cid:1488) (cid:2278)
(cid:1838)(cid:1867)(cid:1855)(cid:1845)(cid:1861)(cid:1865)(cid:3)

(cid:3)(cid:1864)(cid:1869)

(cid:1864)(cid:1869)

(cid:3)(cid:2278)

  

, 

where  coefficient 

(cid:1845)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:1864)(cid:1869) (cid:4666)(cid:1864)(cid:4667) (cid:3404) (cid:142)(cid:145)(cid:137) (cid:1838)(cid:1867)(cid:1855)(cid:1845)(cid:1861)(cid:1865)(cid:3435)(cid:1864)(cid:1869) (cid:481) (cid:1864)(cid:3439) (cid:3397) (cid:2020) (cid:142)(cid:145)(cid:137) (cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:481)

 controls the influence of the static popularity 
 is  simply  defined  as  the  occur-

in  ranking.  Here, 

(cid:1864) (cid:1488) (cid:2278)(cid:481) (cid:2020) (cid:3410) (cid:882)

rence frequency of location 
(cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:3)

(cid:2020)

(cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)

 in the travelogue collection

, as 

(cid:1864)

(cid:851)(cid:4666)(cid:1864)(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149) (cid:3)(cid:139)(cid:144) (cid:3)(cid:1829)(cid:4667)

(cid:1314)

. 

(cid:3)(cid:1829)

(cid:963)

(cid:851)(cid:4666)(cid:1864)

(cid:1864)(cid:1314) (cid:1488)(cid:2278)

(cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149) (cid:3)(cid:139)(cid:144) (cid:3)(cid:1829)(cid:4667)

(cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667) (cid:3404)

3.1.2  Relevance-Oriented Recommendation 
 (e.g.,  “hiking”), 
Given  a  travel  intention  described  by  a  term 
.  Since  a  travel 
we  rank  destinations  in  terms  of  relevance  to
intention  usually  contains  more  comprehensive  semantics  than  a 
single term, we expand 
 (a dis-
tribution over the local topics, as introduced in Section 2.4.2). In 
this way, the relevance of  each location 
 can be  meas-
ured using KL-divergence. The ranking score is thus computed as 

 in the local topic space as 

(cid:1875)(cid:1869)
(cid:3)(cid:1875)(cid:1869)

 to the 

(cid:3)(cid:2012)(cid:1875)(cid:1869)

(cid:1875)(cid:1869)

(cid:1864)

(cid:1875)(cid:1869)
  

, 

3.2  Destination Summarization 
Once  a  destination  has  been  determined,  a  tourist  would  like  to 
know more details of the destination, like 
¾  “What are the most representative things in San Francisco? 

Can you tell me with a few words or sentences?” 

To summarize the representative aspects of a destination, we first 
generate a few representative tags, and then identify related snip-
pets for each tag to further describe and interpret the  relation be-
, we can 
tween the tag and the destination. For a given location
obtain  its  probability  distribution  over  terms 
 as 
described  in  Section  2.4.1,  and  simply  select  those  terms  with 
highest probabilities in this distribution as the representative tags. 
, we generate its corresponding 
Then, given a representative tag
snippets by ranking all the sentences 
 in the travelogue collec-
tion  according  to  the  query  “
”.  Specifically,  a  sentence
(cid:4668)(cid:1871)(cid:4669)
consisting  of  a  (mentioned)  location  set 
rated  in  terms  of  the  geographic  relevance  to  location 
semantic relevance to tag

 
 is 
(cid:3)(cid:1871)
 and  the 

(cid:1864)(cid:1869) (cid:3397) (cid:1875)(cid:1869)
, as 

(cid:3419)(cid:1868)(cid:3435)(cid:1875)(cid:3627)(cid:1864)(cid:1869) (cid:3439)(cid:3423)(cid:1875) (cid:3404)(cid:883)(cid:483)(cid:1849)

 and  a  term  set 

(cid:3)(cid:1875)(cid:1869)

(cid:2289)(cid:1871)

(cid:2278)(cid:1871)

(cid:3)(cid:1864)(cid:1869)

(cid:3)(cid:1875)(cid:1869)

(cid:1864)(cid:1869)

, where 

(cid:1845)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:1864)(cid:1869) (cid:481)(cid:1875)(cid:1869) (cid:4666)(cid:1871)(cid:4667) (cid:3404) (cid:1833)(cid:1857)(cid:1867)(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1864)(cid:1869) (cid:4666)(cid:1871)(cid:4667) (cid:3400) (cid:1845)(cid:1857)(cid:1865)(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1875)(cid:1869) (cid:4666)(cid:1871)(cid:4667)

, and 

 

, 

(cid:3)

(cid:513)(cid:942)(cid:513)

(cid:1875) (cid:1488)(cid:2289)(cid:1871)

(cid:142)(cid:145)(cid:137)(cid:4666)(cid:883) (cid:3397) (cid:513)(cid:2289)(cid:1871)(cid:513)(cid:4667)

(cid:1846)(cid:1857)(cid:1870)(cid:1865)(cid:1845)(cid:1861)(cid:1865)(cid:3435)(cid:1875)(cid:1869) (cid:481) (cid:1875)(cid:3439)

(cid:1845)(cid:1857)(cid:1865)(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1875)(cid:1869) (cid:4666)(cid:1871)(cid:4667) (cid:3404) (cid:963)

(cid:1833)(cid:1857)(cid:1867)(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1864)(cid:1869) (cid:4666)(cid:1871)(cid:4667) (cid:3404) (cid:851)(cid:3435)(cid:1864)(cid:1869) (cid:3)(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:2278)(cid:1871)(cid:3439) (cid:513)(cid:2278)(cid:1871)(cid:513)
(cid:932)
 denotes  the  cardinality  of  a  set,  and 
(cid:932)

where 
 is  the 
pair-wise term similarity defined in Section 2.4.2. Note that all the 
terms  in  sentence 
 contribute  to  the  semantic  relevance  more  or 
less, according to their similarities to the query tag. 
3.3  Travelogue Enrichment 
Besides  the  brief  summarization,  a  tourist  would  also  like  to 
browse through some travelogues written by other tourists. Given 
a travelogue, a reader is usually interested in the places visited by 
the author and how these places look like. 
¾  “Where  did  Jack  visit  when  he was  in  New  York  City? And 

(cid:1846)(cid:1857)(cid:1870)(cid:1865)(cid:1845)(cid:1861)(cid:1865)

(cid:1871)

how do those places look like?” 

To  facilitate  browsing,  we  extract  the  highlights  of  a  travelogue 
and enrich them with images to provide additional visual descrip-
tions.  Given  a  travelogue 
, 

 which  refers  to  a  set  of  locations

(cid:1856)

(cid:3)(cid:2278)(cid:1856)

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA405we treat the informative depictions of locations in 
lights. As described in Section 2.4.3, each term 
has a probability 
the  highlight  corresponding  to  location 
(cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667)(cid:3)
dimensional term-vector

to be assigned to location
(cid:1875)

 in travelogue 
(cid:2278)(cid:1856)

 
. Hence, 
(cid:1856)
-

 is  represented  as  a 

, where 

(cid:3)(cid:1864) (cid:1488) (cid:2278)(cid:1856)

 as the high-

(cid:1864)

(cid:1849)

. 

(cid:3)(cid:2203)(cid:1864) (cid:3404) (cid:3435)(cid:1873)(cid:1864)(cid:481)(cid:883)(cid:481) (cid:485) (cid:481) (cid:1873)(cid:1864)(cid:481)(cid:1849)(cid:3439)
To visually enrich every identified highlight
from  a  candidate  image  set 
location
and  is  also  represented  as  a 

.  Each  image 

(cid:1873)(cid:1864)(cid:481)(cid:1875) (cid:3404) (cid:851)(cid:4666)(cid:1875)(cid:3)(cid:1853)(cid:1868)(cid:1868)(cid:1857)(cid:1853)(cid:1870)(cid:1871)(cid:3)(cid:139)(cid:144)(cid:3)(cid:1856)(cid:4667) (cid:3400) (cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667)(cid:481) (cid:1875) (cid:3404) (cid:883)(cid:481) (cid:485) (cid:481) (cid:1849)

, we select images 
 that  is  geographically  relevant  to 
 is  annotated  with  a  set  of  tags
, 

(cid:3)(cid:2203)(cid:1864)

-dimensional  vector

(cid:3)(cid:1864)

, where 

(cid:2284)(cid:1864)
(cid:1870) (cid:1488) (cid:2284)(cid:1864)

(cid:1849)

(cid:3)(cid:2286)(cid:1870)
(cid:3)(cid:2204)(cid:1870) (cid:3404)

. 

(cid:3435)(cid:1874)(cid:1870)(cid:481)(cid:883)(cid:481) (cid:485) (cid:481) (cid:1874)(cid:1870)(cid:481)(cid:1849)(cid:3439)
Then, the relevance of image 

(cid:1872)(cid:1488)(cid:2286)(cid:1870)

(cid:1874)(cid:1870)(cid:481)(cid:1875) (cid:3404) (cid:963)

 to highlight 

(cid:1846)(cid:1857)(cid:1870)(cid:1865)(cid:1845)(cid:1861)(cid:1865)(cid:4666)(cid:1872)(cid:481) (cid:1875)(cid:4667)(cid:481)(cid:3)(cid:3)(cid:3)(cid:1875) (cid:3404) (cid:883)(cid:481) (cid:485) (cid:481) (cid:1849)
, 

is computed as 

(cid:1870)

(cid:2203)(cid:1864)(cid:3)

(cid:883)

where 
 denotes  inner  product,  and  the  second  term  is  used  to 
(cid:1845)(cid:1855)(cid:1867)(cid:1870)(cid:1857)(cid:2203)(cid:1864) (cid:4666)(cid:1870)(cid:4667) (cid:3404) (cid:1731)(cid:2203)(cid:1864) (cid:481) (cid:2204)(cid:1870) (cid:1732) (cid:942)
normalize  images  with  different  numbers  of  tags.  Moreover,  to 
diversify the resulting images, we select images one by one. Once 
the  kth  image 
 to 
decay the semantics already illustrated by the selected images, as 

 is  chosen,  we  update 

(cid:142)(cid:145)(cid:137) (cid:4666)(cid:883)(cid:3397)(cid:513)(cid:2286)(cid:1870) (cid:513)(cid:4667) (cid:481) (cid:1870) (cid:1488) (cid:2284)(cid:1864)

(cid:1731)(cid:942)(cid:481)(cid:942)(cid:1732)

(cid:4666)(cid:1863)(cid:4667)

(cid:4666)(cid:1863)(cid:4667)
(cid:3404) (cid:4672)(cid:1873)(cid:1864)(cid:481)(cid:883)

(cid:4666)(cid:1863)(cid:4667)
(cid:481) (cid:485) (cid:481) (cid:1873)(cid:1864)(cid:481)(cid:1849)

(cid:4673)

(cid:2203)(cid:1864)

(cid:1870)(cid:1863)

 

(cid:4666)(cid:1863)(cid:3398)(cid:883)(cid:4667)

(cid:4666)(cid:1863)(cid:4667)
(cid:1873)(cid:1864)(cid:481)(cid:1875)
where 

(cid:3404) (cid:4682)

(cid:3400) (cid:1857)(cid:1876)(cid:1868)(cid:3435)(cid:3398)(cid:2028) (cid:942) (cid:1874)(cid:1870)(cid:1863) (cid:481)(cid:1875) (cid:3439)(cid:3)(cid:481) (cid:1863) (cid:3410) (cid:883)
(cid:1873)(cid:1864)(cid:481)(cid:1875)
(cid:1875) (cid:3404) (cid:883)(cid:481) (cid:485) (cid:481) (cid:1849)
 is a coefficient to control the strength of decay. 
(cid:1873)(cid:1864)(cid:481)(cid:1875) (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:481) (cid:1863) (cid:3404) (cid:882)

(cid:1)(cid:3)

 , 

, 

(cid:2028) (cid:3408) (cid:882)

4.  EXPERIMENTAL RESULTS 
In  this  section,  we  present  experimental  results  of  the  LT  model 
and  its  applications.  Both  objective  and  subjective  evaluation 
methods are used to evaluate the effectiveness of the framework. 

4.1  Data Set 
There  are  many  sources  of  travelogues  on  the  Web,  either  from 
Weblogs such as  Windows Live Spaces, or dedicated travel web-
sites  like  TravelPod3,  IgoUgo4,  and  TravelBlog5.  We  collected 
approximately 100,000 travelogues written in English and related 
to  tourist  destinations  in  the  United  States,  to  form  an  English 
corpus. A location extractor was applied to extract locations men-
tioned in these travelogues,  yielding 18,000 unique locations. As 
some subjective evaluations require participators’ knowledge, we 
also built a Chinese corpus from Ctrip6, consisting of 94,000 Chi-
nese travelogues related to around 32,000 locations in China. 

4.2  Travelogue Modeling 
After pre-processing including stemming and stop-word removal, 
we trained a LT model on each corpus to learn a number of local 
topics  and  global  topics.  The  numbers  of  local  and  global  topics 
were  set  empirically  to  100  and  50,  respectively.  The  training 
procedure for each corpus included 2,000 iterations of Gibbs sam-
pling  and  lasted  for  approximately  40  hours  on  a  server  with  an 
AMD Opteron quad-core 2.4GHz processor. 

To  illustrate  the  topics  learnt by  the  LT  model,  we  show  the  top 
terms (i.e., terms with the highest probabilities) of some topics in 
Table  1.  We  can  see  that  local  topics  characterize  some  tourism 
                                                                 
3 http://www.travelpod.com/ 
4 http://www.igougo.com/ 
5 http://www.travelblog.org/ 
6 http://www.ctrip.com/ 

Table 1. Top terms of example local and global topics. 

local #23 

local #62 

local #66 

desert 
cactus 
canyon 
valley 

hot 
west 
heat 
spring 

flight 
airport 

fly 

plane 
check 
bag 
air 

travel 

local #57 
museum 

art 

collect 
gallery 
exhibit 
paint 
work 

sculpture 
global #19 

great 
best 
fun 

beautiful 

enjoy 

wonderful 

love 
amaze 

dive 

snorkel 

fish 

aquarium 

sea 
boat 
whale 
reef 

kid 

family 

old 

children 

fun 
love 
young 

age 

casino 
gamble 

play 
slot 
table 

machine 

game 
card 

local #69 
mountain 

peak 
rocky 
snow 
high 
feet 
lake 

summit 

room 
hotel 
bed 
inn 

breakfast 
bathroom 

night 
door 

rain 

weather 

wind 
cold 

storm 
sun 
warm 

temperature 

global #8 

global #22 

global #26 

global #37 

(a) Local topic #57 (museum, …) 

(b) Local topic #62 (dive, …) 

Figure  5.  Geographic  distributions  of  two  local  topics.  The 
darker a region is, the higher correlation with the topic it has. 

 

styles  and  corresponding  locations,  including  both  natural  styles 
like  seaside  (local  #62)  and  cultural  styles  like  museum  (local 
#57);  whereas  global  topics  correspond  to  common  themes  such 
as  accommodation  (global  #26)  and  opinion  (global  #19),  which 
tend to appear in travelogues related to almost any destination. 

(cid:1864)

 is  measured  by  the  conditional  probability

To exemplify the relationships between local topics and locations, 
we utilize, following [13], the Many Eye visualization service7 to 
visualize  the  spatial  distribution  of  some  local  topics.  Based  on 
 and a loca-
the LT model, the correlation between a local topic 
tion 
,  which  is 
equal to
 in the LT model. In Figure 5, we plot two local top-
ics  (#57  museum  and  #62  seaside  in  Table  1)  on  the  U.S.  state 
map respectively, where a darker state indicates a higher 
 it 
has.  Both  maps  show  uneven  geographic  distributions  of  local 
topics, indicating high dependence between local topics and loca-
tions. From Figure 5 (a) we see that New York, Illinois, and Okla-
homa  are  famous  for  {museum,  art,  …};  while  in  Figure  5  (b) 
Hawaii  shows  the  highest  correlation  with  {dive,  snorkel,  …}. 
This  demonstrates  the  learnt  relationships  between  local  topics 
and locations are reasonable and consistent with prior knowledge. 

(cid:3)(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667)

(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667)

(cid:3)(cid:2032)(cid:1864)(cid:481)(cid:1878)

(cid:1878)

4.3  Destination Recommendation 
4.3.1  Similarity-Oriented Recommendation 
Since  the  effectiveness  of  similarity-oriented  destination  recom-
mendation highly relies on the pair-wise similarity metric of loca-
tions, we directly evaluate this metric’s capability of discovering 
similar locations from a given set. 

                                                                 
7 http://manyeyes.alphaworks.ibm.com/manyeyes/ 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA406(a) LT model 

(b) Baseline method 

 

Figure  6.  Location  similarity  graphs  generated  by  the  LT 
model  and  the  baseline  method,  where  different  colors  and 
shapes stand for different location categories. 

We first collected the top destinations recommended by TripAdvi-
sor8 for four travel intentions including Beaches & Sun, Casinos, 
History  &  Culture,  and  Skiing.  After  filtering  out  locations  not 
appearing  in  our  corpus,  we  built  a  location  set  consisting  of  36 
locations,  based  on  which  pair-wise  location  similarities  were 
computed (as describe in Section 2.4.1) to form a location similar-
ity  graph.  To  demonstrate  how  well  the  graph  is  consistent  with 
the  ground-truth  similarity/dissimilarity  between  four  categories 
of locations, we use the NetDraw9 software to visualize this graph 
where similar locations tend to be positioned close to each other, 
as  shown  in  Figure  6  (a).  As  a  comparison,  we  implemented  a 
baseline method which formed a pseudo document for each loca-
tion by concatenating all the  travelogues referring to it, and then 
measured the pair-wise location similarity using the common TF-
IDF-based cosine similarity. Comparing the two graphs in Figure 
6 (a) and (b), we can see that different categories of locations are 
roughly  differentiated  by  our  similarity  metric,  while  under  the 
baseline metric some of them are coupled together. This is owing 
to  one  advantage  of  the  LT  model,  namely  preserving  the  infor-
mation  that  characterizes  and  differentiates  locations  when  pro-
jecting the travelogue data into a low-dimensional topic space. 
4.3.2  Relevance-Oriented Recommendation 
To evaluate the relevance-oriented recommendation, we collected 
the  top destinations  recommended  by  TripAdvisor  for  five  travel 
intentions,  i.e.,  Beaches  &  Sun, Casinos,  Family  Fun,  History  & 
Culture  and  Skiing,  as  the  ground-truth  for  five  queries,  respec-
tively. For the sake of uniformity, all the queries are truncated into 
unigrams. Besides the LT model-based method presented in Sec-
tion 3.1.2, we also set up a baseline method, which ranks locations 
for  a  query  term  in  decreasing  number  of  travelogues  containing 
both a location and the query term. The resulting location ranking 
lists  of  both  methods  are  evaluated  by  the  number  of  locations, 
within  the  top  K  ones,  matching  the  ground-truth  locations.  The 
evaluation results are shown in Table 2, while Table 3 lists some 
top destinations recommended by our approach. 

From Table 2 we observe that the locations recommended by  the 
LT model generally match more ground-truth ones than the base-
line;  whereas the baseline exceeds our approach at the top 5 and 
top 10 results for the query family. This observation can be inter-
preted as the two sides of a coin. On one hand, our method meas-
ures each location’s relevance to the query term in the local topic 
space  to  naturally  expand  the  query  with  similar  terms,  and  thus 
enable partial match and improve the relevance  measurement for 
queries well captured by local topics (e.g., beach, casino). On the 
                                                                 
8 http://www.tripadvisor.com/Inspiration/ 
9 http://www.analytictech.com/Netdraw/netdraw.htm 

Table  2.  Comparison  of  the  relevance-oriented  destination 
recommendation results for five queries.  

Query 

#Ground-

truth 

Methods 

#Matches at top K 

K=5  K=10  K=15  K=20 

beach 

casino 

family 

history 

skiing 

35 

6 

38 

12 

20 

baseline 
LT model 
baseline 
LT model 
baseline 
LT model 
baseline 
LT model 
baseline 
LT model 

1 
4 
2 
4 
4 
3 
4 
5 
2 
3 

4 
9 
2 
5 
6 
5 
6 
8 
4 
5 

7 
12 
3 
5 
8 
8 
8 
9 
4 
10 

9 
13 
3 
5 
11 
11 
8 
10 
6 
12 

 
Table  3.  Top  destinations  recommended  by  the  LT  model-
based method, where those in the ground-truth shown in bold. 

Query 

Top 10 recommended destinations 

beach 

Myrtle Beach, Maui, Miami, Santa Monica, Destin, Hilton 
Head  Island,  Virginia Beach,  Daytona  Beach,  Key  West,  
San Diego 

casino 

Las Vegas, Atlantic City, Lake Tahoe, Biloxi, Reno, Dead-
wood, New Orleans, Detroit, Tunica, New York City 

family 

history 

skiing 

Orlando,  Las  Vegas,  New  York  City,  Washington,  D.C., 
New  Orleans,  Charleston,  Myrtle  Beach,  Chicago,  San 
Francisco, Walt Disney World 
New  Orleans,  Charleston,  Williamsburg,  Washington, 
D.C.,  New  York  City,  Chicago,  Las  Vegas,  Philadelphia, 
San Francisco, San Antonio 
Lake  Tahoe,  Park  City,  South  Lake  Tahoe,  Jackson  Hole, 
Vail,  Breckenridge,  Winter  Park,  Salt  Lake  City,  Beaver 
Creek, Steamboat Springs 

 
other  hand,  for  queries  mainly  captured  by  global  topics  (e.g., 
family, a top term of the global topic #22 shown in Table 1), this 
query expansion mechanism is less reliable, due to the low confi-
dence of these terms’ distributions over local topics. 

4.4  Destination Summarization 
4.4.1  Representative Tag Generation 
To  compare  with  the  location-representative  tag  generation  ap-
proach  described  in  Section  3.2,  we  implemented  three  baseline 
methods.  The  first  one  (“TF”)  is  to  generate  a  pseudo  document 
for  each  location  by  concatenating  all  the  travelogue  paragraphs 
referring to it, and then rank terms in decreasing frequency in the 
pseudo document.  The  second  one  (“TF-IDF”)  is to  further  mul-
tiply each term’s frequency with the Inverse Document Frequency 
(IDF) to penalize common terms. The third baseline is similar to 
the LT model-based approach but disable the global topics. 

As  there  is  no  existing  ground-truth  of  location-representative 
tags,  we  built  one  by  borrowing  people’s  knowledge.  For  each 
location, we first formed a tag pool by merging the top tags gen-
erated by the proposed method and three baselines, and then asked 
20 graduate students to select the top 10 most representative tags. 
Finally,  each  tag  was  rated  according  to  the  number  of  times  it 
was selected, to generate a ranking list of tags as the ground-truth. 
Considering  the  participators’  background  knowledge,  we  used 
the  Chinese  corpus  in  this  experiment  and  involved  20  popular 
tourist destinations in China to form the questionnaire. 

Based on the ground-truth, the tag ranking list generated by each 
method is evaluated using the Normalized Discounted Cumulative 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA407K
@
G
C
D
N

0.90

0.80

0.70

0.60

0.50

0.40

0.30

TF

TF-IDF

LT (only local topics)

LT (local + global topics)

K

5.00

4.00

3.00

2.00

1.00

4.03

4.17

Baseline

4.09

3.78

LT-Model

3.97

3.64

4.05

3.67

5

1

3

 
Figure  7.  NDCG@K  results  of  location-representative  tags 
generated by (a) TF, (b) TF-IDF, (c) LT model with only local 
topics, and (d) LT model with both local and global topics. 

10

15

20

geographic relevance

semantic relevance

 
Figure  8.  A  subjective  evaluation  of  representative  snippets 
generated by the LT model-based method and the baseline. 

comprehensiveness

overall satisfaction

Table 4. Representative tags generated by the LT model-based 
method for example destinations in the United States. 
Destination 

Top 10 representative tags 

Anchorage 

Boston 

Chicago 

Las Vegas 

Los Angeles 

Maui 

New York 

City 

Orlando 

San Francisco 

Washington, 

D.C. 

bear, moose, alaskan, glacier, fish, cruise, salmon, wild-
life, trail, mountain 
fenway, whale, historic, sox, cape, england, red, history, 
revere, church 
michigan,  institute,  field,  lake,  museum,  cta,  tower, 
loop, windy, cub 
strip,  casino,  show,  hotel,  bellagio,  gamble,  fountain, 
venetian, mgm, slot 
hollywood,  star,  studio,  universal,  movie,  boulevard, 
theatre, china, getty, sunset 
island,  beach,  snorkel,  whale,  ocean,  luau,  volcano, 
dive, fish, surf 
subway,  broadway,  brooklyn,  zero,  avenue,  island, 
yorker, manhattan, village, greenwich 
disney,  park,  universal,  resort,  world,  theme,  studio, 
kingdom, magic, epcot 
bay,  cable,  alcatraz,  chinatown,  wharf,  bridge,  prison, 
bart, fisherman, pier 
museum, memorial, monument, national, metro, capitol, 
war, smithsonian, lincoln, president 

Gain at top K (NDCG@K) [9], which is commonly used in the IR 
area to measure the accuracy of ranking results. The results aver-
aged  over  all  the  20  locations  are  shown  in  Figure  7.  It  can  be 
seen that our method significantly outperforms the baselines con-
sistently at top K ranking positions. Out of the baselines, the TF-
IDF method outperforms the TF method consistently, owing to the 
penalty  to  noisy  tags  commonly  co-occurring  with  various  loca-
tions.  However,  this  frequency-based  penalty  mechanism  is  too 
coarse to filter out all the noisy tags. Our approach properly filters 
out these tags using global topics. When global topics are disabled 
and all the information is modeled by local topics as in the third 
baseline, the performance is even worse than the TF-IDF method. 

In addition to the above quantitative evaluation, we also generated 
representative  tags  for  some  U.S.  destinations  based  on  the  Eng-
lish corpus. As exemplified in Table 4, the generated tags include 
not only landmarks (e.g.,  bellagio, alcatraz) but also styles (e.g., 
historic, beach) and activities (e.g., gamble, dive). 
4.4.2  Representative Snippet Generation 
As it is quite subjective to evaluate the extent to which a  textual 
snippet is informative for something at somewhere, we resorted to 
user study to evaluate the generated representative snippets. Based 
on the Chinese corpus, we prepared 20 groups of data, each con-
sisting  of  a  query  in  the  form  of  “location  +  term”,  5  snippets 
generated by the proposed method, and another 5 snippets from a 

baseline  snippet  ranking  method  based  on  the  number  of  occur-
rences of the query in a snippet. 

Twenty  graduate  students  were  asked  to  assess  the  two  snippet 
sets  (presented  in  random  order)  in  each  group  using  1  to  5  rat-
ings, from four aspects namely (1)  geographic relevance (i.e., to 
what  extent  the  snippets  are  describing  the  query  location),  (2) 
semantic relevance (i.e., describing the query term), (3) compre-
hensiveness (i.e., providing rich information about the query), and 
(4)  overall  satisfaction.  Using  these  aspects  we  want  to  demon-
strate whether the proposed method can suggest snippets not only 
relevant to the query but also informative and comprehensive. For 
each snippet set, we averaged all the users’ evaluations as its rat-
ings  on  the  four  aspects.  The  two  methods  are  compared  using 
pair-wise t-test on the 20 groups and exhibit significant differenc-
es  (p<0.01)  in  all  the  four  aspects.  As  depicted  in  Figure  8,  al-
though  the  difference  in  the  geographic  relevance  is  relatively 
small  due  to  the  straightforward  measurement  in  both  methods, 
our  method  shows  significant  advantages  in  other  three  aspects 
due to the query term expansion mechanism. 

Besides,  some  examples  generated  based  on  the  English  corpus 
are illustrated in Table 5, where words relevant to the query term 
(shown in bold and italic) provide informative and comprehensive 
descriptions for the queries. 
4.5  Travelogue Enrichment 
For the evaluation of travelogue enrichment, we conducted a user 
study  based  on  the  Chinese  corpus.  The  materials  presented  to 
users consist of 10 travelogue segments, each referring to at least 
one  location  and  related  characteristics.  For  each  segment,  there 
are  two  image  sets  (each  with  three  images)  generated  by  our 
method  and  a  baseline  method which  simply  uses  the  mentioned 
locations as queries to retrieve geo-tagged photos from Flickr. 

We  asked  20  graduate  students  to  assess  both  image  sets  (pre-
sented in random order) of each segment from four aspects includ-
ing (1) geographic  relevance (i.e., to what extent the images are 
depicting the main locations in the  segment), (2) semantic relev-
ance  (i.e.,  depicting  the  objects  mentioned  in  the  segment),  (3) 
diversity (i.e., depicting different objects in the segment), and (4) 
overall satisfaction (well highlighting and enriching the text). The 
results  are  depicted  in  Figure  9,  and  pair-wise  t-test  on  the  10 
pairs of image sets exhibits significant differences (p<0.01) in all 
the four aspects.  It indicates that the images  selected by our me-
thod are more favorable compared with the baseline. Note that in 
the aspect of geographic relevance, the difference of two methods 
is small because the baseline is actually geo-based, while in other 
three  aspects,  our  method  exhibits  larger  advantages  due  to  the 
learnt  location-representative  knowledge  and  query  expansion 
enabled by the LT model. 

Another two English examples are illustrated in Figure 10, where 
each  travelogue  segment  is  enriched  by  three  images  that  depict 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA408Query 

Alaska 

+ wildlife 

Las Vegas  
+ casino 

Waikiki 
Beach 
+ beach 

3 

1 

2 

3 

1 

2 

 

5.00

4.00

3.00

2.00

1.00

Table 5. Representative snippets generated by the LT model-based method for several “location + term” queries, where locations 
are shown in underline and words relevant to the query term are shown in bold and italic. 

Top snippets  

1 

This time, the bus stopped at the Alaska Wildlife Conservation Center, where we saw musk oxen, bison, brown bears, black bears, eagles, owls, foxes, rein-
deers, and a few other animals. 

2  Along the way we stopped at the Alaska Wildlife Recreation Center to check on the musk ox, Moose, Brown and Black Bears, and many other Arctic animals. 

The rest of my week in Alaska was filled with more wildlife sightings, including numerous brown bears and cubs, caribou, and a rare black wolf in Denali; 
and a moose with a calf and several wild trumpeter swans. 
With the ringing of slot machines it finally hit me, I was rolling into a Las Vegas casino! Sitting at the first row of slot machines we experienced one of the 
most amazing aspects of common casino etiquette. 
Being more inclined to stick my hard earned cash in a money market fund than a slot machine, Las Vegas and casinos have never held much allure for me, 
but I have to admit that the Las Vegas Strip with all the big casinos seemed pretty glamorous and exciting. 

We have played in the Casinos of Las Vegas! They want you to stay in the Casinos so they have waitresses coming around giving you free drinks and all you 
have to do is tip her $1 or $2 dollars and she just keeps coming back. 
The famous Waikiki Beach is very beautiful and is packed with swimmers from the early hours, it has lovely clean sand but the beach is getting smaller by 
the year and is already at the base of a couple of hotels! The dreaded global warming! 

Since Waikiki Beach faces southwest, we were able to enjoy an extraordinary sunset right from the beach in front of our hotel, then we retired to our room 
where we spent the evening sitting out on the lanai, having some cocktails, and listening to the surf. 

3  If you are near the Waikiki Beach area, enjoy the day at the beach, relax, and stay for the beautiful sunset. 

4.23

3.77

Baseline

4.07

3.22

LT-Model

3.92

4.05

3.18

3.27

geographic relevance

 
Figure 9. A subjective evaluation of travelogue enrichment by 
the LT model-based method and the baseline method. 

semantic relevance

overall satisfaction

diversity

its most informative parts. We also present each image’s original 
tags and the words in text it corresponds to. For instance, the pre-
sented  images  in  Figure  10  (a)  depict  representative  and  diverse 
semantics described in the text, i.e., ocean, volcano, and beach. 

5.  RELATED WORK 
Some related  work has been dedicated to organizing information 
on  the  Web  to  provide  online  travel  assistant  services.  For  in-
stance,  Jing  et  al.  [10]  proposed  a  travel  plan  assistant  system 
which  provided  high-quality  images  relevant  to  given  locations 
based on tourist sight extraction and image retrieval. Wu et al. [21] 
proposed  a  system  to  generate  personalized  tourism  summary  in 
the form of text, image, video, and news. In [12], a trip planning 
system  was  presented  for  place  recommendation  according  to 
users’ previous choices and tag-based place similarity. 

Recently,  leveraging  user-contributed  photo  collections  (e.g., 
Flickr  [6])  has  attracted  lots  of  research  efforts.  Some  of  them 
[11][18]  selected  representative  photos  to  visually  summarize  a 
given landmark or scene. Ahern et al. [1] analyzed the tags asso-
ciated with photos to identify and visualize representative tags for 
arbitrary areas in the world; while Crandall et al. [5] utilized geo-
tagged  photos  to  discover  worldwide  popular  places  and  their 
representative images. In [22], geo-tagged photos were leveraged 
to discover landmarks and build a world-scale landmark recogni-
tion engine. Moxley et al. [15] proposed an image tag suggestion 
tool based on mining of location tags from Flickr photos. 

In [8], the authors proposed to generate overviews for locations by 
mining representative tags from travelogues and retrieving related 
images. Each travelogue is assumed to be related to only one loca-

tion; neither similarity between locations nor the representation of 
locations in the learnt topic space is considered. 

Probabilistic  topic  models,  such  as  latent  Dirichlet  allocation 
(LDA)  [2]  and  its  extensions,  have  been  successfully  applied  to 
many  text  mining  tasks.  Rosen-Zvi  et  al.  [17]  extended  LDA  by 
incorporating  authors  of  documents  as  observed  variables  and 
representing authors with mixtures of topics. Some models [4][19] 
aimed to discover topics in different granularity levels other than 
document-level. In [16][20], locations (entities) appearing in doc-
uments  were  explicitly  modeled  as  generated  by  topics,  while  in 
[13][14] locations served as labels associated with documents. In 
a  very  recent  work  [3],  the  model  was  sensitive  to  both  entities 
and  relationships  between  entities,  given  textual  data  segmented 
beforehand. In spite of the success in their respective scenarios, all 
the above models are not applicable to the travelogue mining sce-
nario in this paper, as discussed in the section of Introduction. 

6.  CONCLUSION AND FUTURE WORK 
Travelogues contain abundant location-representative knowledge, 
which is informative for other tourists, but difficult to extract and 
summarize manually. In this paper, we have investigated the min-
ing  of  location-representative  knowledge  from  travelogues  to 
facilitate tourists to utilize such knowledge. We proposed a prob-
abilistic topic model, i.e., Location-Topic model, to discover local 
and  global  topics  from  travelogues  and  characterize  locations 
using local topics. With this model, we could effectively  (1) rec-
ommend destinations for flexible queries; (2) summarize destina-
tions  with  representative  tags  and  snippets;  and  (3)  enrich  the 
highlights  of  travelogues  with  images.  The  proposed  framework 
was evaluated based on two large travelogue collections, showing 
promising results on the above tasks. 

For  the  future  work,  we  plan  to  incorporate  prior  knowledge  of 
locations to improve the unsupervised knowledge mining. Anoth-
er  direction  is  to  leverage  more  types  of  information  in  travelo-
gues  (e.g.,  opinions,  travel  routes,  and  temporal  information)  to 
meet more practical information needs such as itinerary planning. 

7.  REFERENCES 
[1]  S. Ahern, M. Naaman, R. Nair, and J. Yang. World Explorer: 
visualizing  aggregate  data  from  unstructured  text  in  geo-
referenced collections. In Proc. JCDL, 2007. 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA409This was our first trip to Hawaii, let alone Maui. The
beaches, activities,
types of accommodations, and
restaurants make it a great choice for a first visit to the
islands. 1) The beaches! There are so many all over the
island, and all different types: white, black, even red.
Large, busy, and with amenities and activities, or small,
private, and rustic (no facilities). 2) The activities! Go
snorkeling,
fishing,
golfing, hiking up an old volcano, biking down the
volcano, four-wheeling on unpaved, virtually vacant dirt
roads through old lava flows, driving on narrow, curvy,
crowded roads through tropical forests, and helicopter
rides around the island. 3) The restaurants! There are so
many fine-dining choices with all types of menus, as
well as sandwich shops and the more familiar chains.

parasailing,

surfing,

diving,

Lobster,
lobster everywhere, and very reasonably
priced. The city was a GREAT place to find seafood. We
went to a Red Sox game, which was great fun and we
also went to Cape Cod, and Plymouth. This was a highly
recommended vacation that we all enjoyed. Take a
whale watching tour or visit the aquarium and cruise
around the harbor. You can see Old Iron Sides, where
the English lost their tea. We also recommend The Ghost
and Graves tour which was fun, and a way to get a feel
for Boston while getting some local history.

ocean, life, blue, sea, brown, 
green, beach, water, animal, 
coral, hawaii, sand, marine, 

underwater, turtle, shell, 
diving, maui, snorkeling, 

travel, vacation, mountain, 

cold, tourism, island, 

volcano, hawaii, islands, 
nationalpark, paradise, 
pacific, horizon, maui, 

reef, creature, flipper

haleakala, crater, summit, …

sky, beach, water, clouds, 
hawaii, sand, surf, maui, 

palmtrees

(a) A segment of a Maui travelogue titled as “Our Maiden Journey to Magical Maui” 

(http://www.igougo.com/journal-j23321-Maui-Our_Maiden_Journey_to_Magical_Maui.html) 

 

 

park, new, york, red, boston, 

d50, nikon, baseball, 
stadium, sox, fenway, 

pitcher, yankees, ballpark, ...

blue, boston, aquarium, 

lobster

ocean, boston, island, 
harbor, boat, whale

(b) A segment of a Boston travelogue titled as “The home of the Lobster” 

(http://www.igougo.com/journal-j16396-Boston-The_home_of_the_Lobster.html) 

Figure  10.  Two  example  travelogue  segments  visually  enriched  by  the  proposed  method,  where  each  image’s  original  tags  are 
shown below the image; locations are shown in underline and informative words/tags are shown in bold and italic. 

[2]  D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allo-

[13] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic modeling with 

cation. J. Mach. Learn. Res., 3:993-1022, 2003. 

network regularization. In Proc. WWW, 2008. 

[3]  J.  Chang,  J.  Boyd-Graber,  and  D.  M.  Blei.  Connections  be-
tween  the  lines:  augmenting  social  networks  with  text.  In 
Proc. KDD, 2009. 

[4]  C.  Chemudugunta,  P.  Smyth,  and  M.  Steyvers.  Modeling 
general and specific aspects of documents with a probabilis-
tic topic model. In Proc. NIPS, 2006. 

[5]  D.  Crandall,  L.  Backstrom,  D.  Huttenlocher,  and  J.  Klein-

berg. Mapping the World’s Photos. In Proc. WWW, 2009. 

[6]  Flickr. http://www.flickr.com/ 
[7]  T.  Griffiths  and  M.  Steyvers.  Finding  scientific  topics.  In 

PNAS, 101:5228–5235, 2004. 

[8]  Q.  Hao,  R.  Cai,  X.-J.  Wang,  J.-M.  Yang,  Y.  Pang,  and  L. 
Zhang. Generating location overviews  with images and tags 
by  mining  user-generated  travelogues.  In  Proc.  ACM  Multi-
media, 2009. 

[9]  K.  Jarvelin  and  J.  Kekalainen.  IR  evaluation  methods  for 

retrieving highly relevant documents. In Proc. SIGIR, 2000. 

[10] F.  Jing,  L.  Zhang,  and  W.-Y.  Ma.  VirtualTour:  an  online 
travel assistant based on high quality images.  In Proc. ACM 
Multimedia, 2006. 

[11] L.  Kennedy  and  M.  Naaman.  Generating  diverse  and  repre-
sentative image search results for landmarks. In Proc. WWW, 
2008. 

[12] J. Kim, H. Kim, and J. Ryu. TripTip: a trip planning service 

with tag-based recommendation. In Proc. CHI, 2009. 

 

[14] Q. Mei, C. Liu, H. Su, and C. Zhai. A probabilistic approach 
to spatiotemporal theme pattern mining on weblogs. In Proc. 
WWW, 2006. 

[15] E.  Moxley,  J.  Kleban,  and  B. S. Manjunath.  SpiritTagger:  a 
geo-aware  tag  suggestion  tool  mined  from  Flickr.  In  Proc. 
MIR, 2008. 

[16] D. Newman, C. Chemudugunta, P. Smyth, and M. Steyvers. 
Statistical entity-topic models. In Proceedings of KDD, 2006. 
[17] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The 
author-topic model for authors and documents. In Proc. UAI, 
2004. 

[18] I. Simon, N. Snavely, and S. M. Seitz. Scene summarization 

for online image collections. In Proc. ICCV, 2007. 

[19] I.  Titov  and  R.  McDonald.  Modeling  online  reviews  with 

multi-grain topic models. In Proc. WWW, 2008. 

[20] C.  Wang,  J.  Wang,  X.  Xie,  W.-Y.  Ma.  Mining  geographic 
knowledge  using  location  aware  topic  model.  In  Proc.  GIR, 
2007. 

[21] X. Wu, J. Li, Y. Zhang, S. Tang, and S.-Y. Neo. Personalized 
multimedia  web  summarizer  for  tourist.  In  Proc.  WWW, 
2008. 

[22] Y.-T.  Zheng,  M. Zhao,  Y.  Song,  H.  Adam,  U.  Buddemeier, 
A. Bissacco, F. Brucher, T.-S. Chua, and H. Neven. Tour the 
world: building a web-scale landmark recognition engine. In 
Proc. CVPR, 2009. 

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA410