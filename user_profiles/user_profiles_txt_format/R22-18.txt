Compiler Parallelization of C Programs for Multi-Core DSPs

with Multiple Address Spaces

Institute for Computing Systems Architecture

Institute for Computing Systems Architecture

Bj ¨orn Franke

(ICSA)

School of Informatics
University of Edinburgh

B.Franke@sms.ed.ac.uk

M.F.P. O’Boyle

(ICSA)

School of Informatics
University of Edinburgh
mob@inf.ed.ac.uk

ABSTRACT
This paper develops a new approach to compiling C pro-
grams for multiple address space, multi-processor DSPs. It
integrates a novel data transformation technique that ex-
poses the processor location of partitioned data into a par-
allelization strategy. When this is combined with a new ad-
dress resolution mechanism, it generates eﬃcient programs
that run on multiple address spaces without using message
passing. This approach is applied to the UTDSP bench-
mark suite and evaluated on a four processor TigerSHARC
board, where it is shown to outperform existing approaches
and gives an average speedup of 3.25 on the parallel bench-
marks.

Categories and Subject Descriptors
D.3.4 [Programming Languages]: Processors—Compil-
ers, Optimization

General Terms
Algorithms, Experimentation, Measurement, Performance

Keywords
DSPs, Data Partitioning, Address Resolution, Multiple Ad-
dress Space Compilation

1.

INTRODUCTION.

Although multi-core DSPs oﬀer a cost-eﬀective method of
achieving high performance, porting existing uni-processor
applications to such parallel architectures is currently com-
plex and time-consuming. There are no commercially avail-
able compilers that will take existing sequential DSP pro-
grams and map them automatically onto a multi-processor
machine [17]. Instead, users are typically required to rewrite
their code as a process network or a set of communicating

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CODES+ISSS’03, October 1–3, 2003, Newport Beach, California, USA.
Copyright 2003 ACM 1-58113-742-7/03/0010 ...$5.00.

sequential processes [12]. Such an approach is well known to
be highly non-trivial and error-prone, possibly introducing
deadlock.

Rewriting an application in a parallel manner is a highly
specialized skill. What is needed is a tool that takes existing
programs and maps them automatically onto the new multi-
processor architecture eﬃciently. Although there has been
over 20 years of research into auto-parallelizing compilation
in scientiﬁc computing [5, 6, 7, 2], this has not taken place
in the embedded domain. This is due to two main reasons:
(i) DSP programs are written in C rather than Fortran [7]
and make extensive use of pointer arithmetic and (ii) the
multiple address spaces of multi-processor DSPs is diﬃcult
to compile for. We overcome the ﬁrst problem by using an
array recovery technique developed in our earlier work [3].
The second problem is solved by combining a novel data
transformation scheme, that exposes the processor ID of re-
mote data with a new address resolution technique that uses
local pointer arrays to track remote data. This low overhead
technique greatly simpliﬁes code generation producing code
similar to that produced for single-address space paralleliza-
tion [5] without introducing complex (and potentially dead-
locking) message passing code. By embedding these two
techniques into an overall parallelization strategy, we have
developed an auto-parallelizing C compiler for DSP appli-
cations that outperforms existing approaches on multiple
address space embedded processors.

The paper is structured as follows. Section 2 provides a
motivating example and is followed in section 3 by a detailed
description of our auto-parallelization scheme. Section 4
empirically evaluates our approach and is followed by a short
review of related work and some concluding remarks.

2. MOTIVATION & EXAMPLE

Our approach exploits the fact that although multi-processor

DSP machines typically have multiple address spaces, part
of each processor’s memory space is visible from other pro-
cessors, unlike pure message-passing machines. However,
unlike single address space machines, such as the SGI Ori-
gin 3000, a processor must know both the identity of the
remote processor and the location in memory of the re-
quired data value. For example, ﬁgure 1 (from [1]) shows the
global memory map of a multi-processor system comprising
the Analog Devices TigerSHARC processor. Each proces-
sor typically has its internal address space for accesses to
local data. These accesses are purely local and not reﬂected

on the external bus. In addition, the processors’ memories
form a global address space where each processor is assigned
a certain range of addresses. This global address space is
used for bus-based accesses to remote data where the global
address (or equivalently the remote processor’s identity and
the data’s local address) must be known.

We have developed a novel technique which combines single-

address space parallelization approaches with a novel ad-
dress resolution mechanism, based on a simple descriptor
data structure, which, at runtime, determines the processor
and memory location of all items.

2.1 Example

Figure 1: Global Memory Map [1]

The example in ﬁgures 2 and 3 illustrates the main points
of this paper. The code in ﬁgure 2, box (1) is typical of
C programs written for DSP processors; it is hybrid of two
programs n real updates and convolution from the DSP-
stone benchmark suite. The use of post-increment pointer
traversal is a well know idiom [19]. This form, however, will
prevent many optimizing compilers from performing aggres-
sive optimization and will prevent attempts at paralleliza-
tion. The second box (2) in ﬁgure 2 shows the program after
array recovery [3].

The pointers are replaced with array references based on
the loop iterator. SPMD owner-computes parallelization by
partitioning the data and computation across the processor
nodes is straightforward here. There is just one array di-
mension and one loop, both of which are partitioned by the
number of processors. In this example we assume there are
four processors. Standard partitioning [5] would generate
the program in box (3) and, for single address space ma-
chines, this would be suﬃcient. To access remote data in
multi-address space machines, however, the processor loca-

tion of partitioned data needs to be explicitly available. Our
scheme, achieves this by data strip-mining [14] each array to
form a two dimensional array whose inner index corresponds
to the four processors. This allows the processor location of
partitioned data to be explicitly available. For instance, the
array D is now partitioned such that D[0][0...7] resides
on processor 0 and D[1][0...7] resides on processor 1 etc.
Similarly for arrays C,A and B. The iterator is similarly par-
titioned to iterate over the work allocated to it, 0 . . . 7. The
resulting code is shown in ﬁgure 2, box (4). Only the decla-
rations of D and B are shown to simplify presentation of the
later transformation stages.

We now need to generate a separate program for each pro-
cessor. The partitioned code for processor 0 (as speciﬁed by
z) is shown in ﬁgure 2, box (5). The code for processors
1,2,3 are identical except for #define z 1,2 or 3. Multiple
address space machines require remote, globally-accessible
data to have a distinct name to local data1. Thus, each
of the globally-accessible sub-arrays are renamed as follows:
D[0][0...7] becomes D0[0...7] and D[1][0...7] becomes
D1[0...7] etc. Similarly for arrays C, A and B. On proces-
sor 0 D0 is declared as a variable residing on that processor
while D1, D2,D3 are declared extern (see the second and
third lines of box (6) ﬁgure 2) . For processor 1, D1 is de-
clared local and the remainder are declared extern etc.

To access both local and remote data, a local pointer ar-
ray is set up on each processor. This simple descriptor data
structure is an array containing four pointer elements, which
are assigned to the start address of the local arrays on the
four processors. We use the original name of the array D[][]
as the pointer array *D[] and then initialize the pointer
array to point to the four distributed arrays int *D[4] =
{D0,D1,D2,D3} (see box (6), ﬁgure 2) . This is shown dia-
grammatically in ﬁgure 3 for arrays B and D. The bold lines
represent the data accessed by processor 0. Thus, there is
local access to array D0 but remote access to array B3 by
processor 0. Using the original name means that we have
exactly the same array access form in all uses of the arrays
D,C,A,B as in box(4). This has been achieved by using the
fact that multi-dimensional arrays in C are arrays of arrays
and that higher dimensions arrays are deﬁned as containing
an array of pointers to sub-arrays2. From a code genera-
tion point of view this greatly simpliﬁes implementation and
avoids complex and diﬃcult to automate message passing.

3. OVERALL PARALLELIZATION ALGO-

RITHM

Our overall parallelization algorithm is shown in ﬁgure 4.
Pointer to array conversion is ﬁrst applied to enable data
dependence analysis (for further details see [3]). Once the
program is in a pointer free form, standard data dependence
analysis is applied to determine if the program is parallel
and if so, we check to see if the amount available justiﬁes
parallelization3.

3.1 Partitioning and Mapping

1Otherwise they are assumed to be private copies
2As deﬁned in section 6.5.2.1 of the ANSI C standard para-
graphs 3 and 4
3Currently we multiply the parallelized loop trip count by
the number of operations and check it is above a certain
threshold before continuing

int *p_d,p_c,p_a,p_b;

Original code (1)

for (i = 0 ; i <=31 ; i++)

*p_d++

= *p_c++ + *p_a++ * *p_b-- ;

int D[32], C[32], A[32], B[32];

Array Recovery (2)

for (i = 0 ; i <=31 ; i++)

D[i]

= C[i] + A[i] * B[32-i] ;

Standard Partitioning (3)

int D[32], C[32], A[32], B[32];

lo = (myid *32/numprocs);
hi = ((myid + 1) *32/numprocs) -1;

for (i = lo ; i <=hi ; i++)

D[i]

= C[i] + A[i] * B[32-i] ;

Data transformation exposing processor ID (4)

int D[4][8], B[4][8];

for (z = 0; i<=3; i++)

for (i = 0; i<=7; i++)

D[z][i]

= C[z][i] + A[z][i] * B[4-z][7-i];

One program per processor (5)

ysis

#define z 0

int D[4][8], B[4][8];

for (i = 0; i<=7; i++)

D[z][i]

= C[z][i] + A[z][i] * B[4-z][7-i];

Address resolution: local pointer arrays(6)

#define z 0

int D0[8];
*/
extern int D1[8], D2[8], D3[8]; /* remote */

/* local

int B0[8];
*/
extern int B1[8], B2[8], B3[8]; /* remote */

/* local

int *D[4] ={D0,D1,D2,D3}; /* pointer array */
int *B[4] ={B0,B1,B2,B3}; /* pointer array */

for (i = 0; i<=7; i++)

D[z][i]

= C[z][i] + A[z][i] * B[4-z][7-i];

Figure 2: Example showing data transformation and
address resolution scheme.

D

D

D

D

0

7

0

7

D0

B0

D1

B1

D2

B2

D3

B3

B

B

B

B

proc 0

proc 1

proc 2

proc 3

Figure 3: Data layout of arrays B, D for ﬁgure 2
box (5). Each processor has a local pointer array.
The contents of the pointer arrays on processor 0
are illustrated. The bold lines represent the data
accessed by the program in ﬁgure 2 box (6)

1. Convert pointers to arrays

2. IF pointer free THEN perform data dependence anal-

(a) IF parallelizable and worthwhile

i. Determine data partition
ii. Partition + transform data and code
iii. Perform address resolution

Figure 4: Overall parallelization algorithm

We exploit data parallelism within DSP programs by par-
titioning the data and computation across the processors.
Choosing the best data partition has been studied for many
years and is NP-complete [5]. In this paper we use a sim-
ple method based on exploiting parallelism and reducing
communication [16]; later work will use more sophisticated
analysis. The key point is that our partitioning approach
using data strip-mining makes the processor identiﬁer ex-
plicit, which is exactly what is needed to statically determine
whether data is local or remote.

3.1.1 Notation

The loop iterators can be represented by a column vector
J = [j1, j2, . . . , jM ]T where M is the number of enclosing
loops. Note the loops do not need to be perfectly nested
and occur arbitrarily in the program. The loop ranges are
described by a system of inequalities deﬁning the polyhedron
or iteration space BJ ≤ b. The data storage of an array can
also be viewed as a polyhedron. We introduce array indices
I = [i1, i2, . . . , iN ]T , to describe the array index space. This

space is given by the polyhedron AI ≤ a. We assume that
the subscripts in a reference to an array can be written as
UJ +u, where U is an integer matrix and u is a vector. Thus
in ﬁgure 2, box(2) the array declaration D[32] is represented
by

−1
1

[i1] ≤

0
31

(cid:20)

(cid:21)

(cid:20)

(cid:21)

(1)

i.e. the index i1 ranges over 0 ≤ i1 ≤ 31 The loop bounds
are represented in a similar manner and the subscript of
D[i] is simply [1][j1] + [0].

3.1.2 Partitioning

Determining those index dimensions that may be evalu-
ated in parallel, in general, gives a number of options and
therefore we use a simple technique to reduce communica-
tion based on data alignment. If two array references have
every element in a certain dimension corresponding to the
same index space points then we say they are aligned.
i.e.
a[i][j] and b[i][k] are aligned on the ﬁrst index but not on the
second. If two arrays are aligned with respect to a particular
index, then no matter how those individual array elements
are partitioned, any reference with respect to this index will
always be local. Partitioning based on alignment tries to
maximize the rows that are equal in a subscript matrix.

Let δ(x, y) be deﬁned as follows:

δ(x, y) =

1 x = y ∧ x (cid:8)= 0
0 otherwise

(cid:26)

This function determines whether two subscripts are non-

zero and equal. The function H(i) deﬁned as:

H(i) =

δ(U 1

i , U t
i )

t

X

which measures how well a particular index i of an array
read in assignment, U t, is aligned with the assigned array,
U 1. For each index the value of H is calculated, the index
with the highest value being the one to partition along.

This technique is applied across all statements and in gen-
eral there will be conﬂicting partition requirements. Cur-
rently, we consider only those statements in the deepest
nested loops as they dominate execution time and calcu-
late the value of H across all these statements for diﬀerent
parallel indices i. The highest value, M axH, determines the
index to partition along. We construct a partition matrix P
deﬁned:

Pi =

eT
i
0

(cid:26)

i = M axH
otherwize

(4)

where eT
i

is the ith row of the identity matrix. We also
construct a sequential matrix S containing those indices not
partitioned such that P + S = I. In the example in ﬁgure 2
there is only one index to partition along therefore

P =

1

S =

0

(5)

3.1.3 Mapping

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(2)

(3)

The data strip-mining matrix S is deﬁned:

S =

(.)%p
(.)/p

(cid:20)

(cid:21)

where p is the number of processors. In our example in ﬁgure
2, p = 4 For further details of the transformation framework
see [14]. Here, the transformation is used to expose the
processor ID of all array references and is critical to our
scheme. To show this, let T be the mapping transformation

T = PS + S

thus the partitioned indices are strip-mined and the sequen-
tial indices left alone. In our example
T = [1]S + [0] = S

(8)

and the new array indices, I(cid:1), are given by

I(cid:1)

= T I

The new array bounds, A(cid:1)I(cid:1) ≤ a(cid:1) are then found using

T O
O T

AT −1I(cid:1) ≤

T O
O T

a

(cid:20)

(cid:21)

(cid:20)

(cid:21)

which transforms the bounds in equation (1) to:

−1

0
0 −1
1
0
1
0

i1
i2

≤ 2

(cid:20)

(cid:21)

0
0
3
7

6

7

6

7

6

7

6

7

2

3

3

4

5

i.e int D[4][8] The new array subscripts are also found:
U (cid:1) = T U. In general, without any further loop transforma-
tions, this will introduce mods and divs into the array ac-
cesses. In our example in ﬁgure 2 we would have D[i%4,i/4]
= C[i%4,i/4] + A[i%4,i/4] * B[(32-i)%4,(32-i)/4]

5

4

However, this framework [14] always generates a suitable
recovery loop transformation, in this case the same trans-
formation T . The original loop bounds, BJ ≤ b in ﬁgure 2,
box(2) are given by

i.e. the iterator i ranges over 0 ≤ i ≤ 31. Applying T to
the enclosing loop iterators gives the new iterators

−1
1

[j1] ≤

0
31

(cid:20)

(cid:21)

(cid:20)

(cid:21)

J (cid:1)

= T J

The new iterator bounds, B(cid:1)J (cid:1) ≤ b(cid:1) are then found using

T O
O T

BT −1J (cid:1) ≤

T O
O T

b

(cid:20)

(cid:21)

(cid:20)

(cid:21)

which transforms the bounds in equation (12) to:

−1

0
0 −1
0
1
0
1

j1
j2

≤ 2

(cid:20)

(cid:21)

0
0
3
7

6

7

6

7

6

7

6

7

2

3

3

i.e 0 ≤ z ≤ 3, 0 ≤ i ≤ 7 as shown in box (4) of ﬁgure 2.
Updating the access matrices we have, for array D:

5

5

4

4

U (cid:1)(cid:1)

= T UT −1

=

1 0
0 1

j1
j2

(cid:20)

(cid:21) (cid:20)

(cid:21)

(6)

(7)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

Once the array indices to partition have been decided, we
data strip-mine the indices I based on the partition matrix
P and strip-mine matrix S to give the new array indices I(cid:1).

i.e.D[z][i]. The resulting code is shown in ﬁgure 2 box(4)
where we have exposed the processor ID of each reference
without any expensive subscript expressions.

1. For each program qi ∈ 1, . . . p

(a) For each arrayName
i. For j ∈ 1, . . . , p

A. IF (j (cid:2)= i) THEN insert (extern)
B. insert(TYPE arrayNamej[N/p];)

ii. insert (TYPE *arrayName[p] = ()
iii. For j ∈ 1, . . . , p − 1

A. insert (arrayNamej ,)

iv. insert (arrayNamep ,);)

Figure 5: Address Resolution

3.2 Address Resolution

Once the array is partitioned across several processors,
each local partition has to be given a new name as there is
no single address space supported. Furthermore, the names
must be distinct as two variables having the same name
on distinct processors are assumed to be private variables.
We therefore introduce a new name equal to the old name
followed by the processor identify number. Thus, in a four
processor system, X will be replaced by four local arrays
X0, X1, X2, X3. We wish one of these arrays to reside and
be allocated on the local processor and the remainder be
declared extern allowing remote references, the address of
which will be resolved by the linker.

In order to minimize the impact on code generation, a
pointer array of size p is introduced which points to the
start address of each of the p sub-arrays. Unlike the sub-
arrays, this pointer array is replicated across the p processors
and is initialized by an array initialization statement at the
beginning of the program. The complete algorithm is given
in ﬁgure 5 where the function insert, inserts the declarations.
Figure 2, box (6), shows the declarations inserted for two
of the arrays, D and B. The declarations for the remaining
arrays are omitted due to lack of space.

The only further change is the type declaration whenever
the arrays are passed into function. The type declaration is
changed from int [] [] to *int[] and this must be prop-
agated interprocedurely. Once this has been applied, no
further transformation or code modiﬁcation is required.

3.2.1 Synchronization

It is beyond the scope of this paper to describe synchro-
nization placement, though this is essential for correct ex-
ecution. We mark all cross-processor dependences and use
a graph based algorithm to insert the minimal number of
barrier synchronizations [15].

4. EXPERIMENTAL FRAMEWORK

The best publicly available, auto-parallelizing, C com-
piler SUIF produces shared-memory data parallel C codes
from sequential C input ﬁles and required the porting of
the SUIF parallel runtime library to our target architecture.
We prototyped our algorithm in the SUIF 1.3 compiler and
evaluated it against the well-known DSP benchmark suite,
UTDSP [11]. UTDSP contains both DSP kernels and ap-
plications. The programs were executed on a Transtech TS-
P36N board with a cluster of four cross-connected 250MHz
TigerSHARC TS-101 DSPs, all sharing the same external
bus and 128MB of external SDRAM. The programs were
compiled with the Analog Devices VisualDSP++ Compiler

(version 6.1.18) with full optimization; all timings are cy-
cle accurate.
In addition to the partitioning and address
resolution scheme described in the paper, further locality
optimisations to exploit DMA transfers were also applied.
This is beyond the scope of this paper. For further details
see [4].

4.1 Performance

4.1.1 Parallelism and Scalability

While 19 of the 30 UTDSP programs are parallel, only
9 of the 19 are determined proﬁtably parallelizable by our
technique. The others are either sequential or exhibit too
little work to parallelize and are rejected at stage 2(a) in
our overall algorithm (see ﬁgure 4). Figure 6 shows how our
approach scales with the number of processors for three se-
lected programs, the Compress, FIR 256/64 and Mult 10 10
benchmarks. Mult 10 10 only contains a 10 by 10 array
yet after distributing the data across the processors, some
speedup is available.
In the other cases, with larger data
sizes, the full potential of parallel execution can be exploited
giving linear speedup with the number of processors.

Figure 6: Speedup for diﬀerent numbers of proces-
sors

4.1.2 Speedup due to Parallelization

An overview of the speedup of the selected UTDSP pro-
grams over their sequential versions is shown in ﬁgure 7. All
programs beneﬁt from our approach and show a substantial
speedup in the range of 1.54 to 5.69. Linear or close to linear
speedup is achieved for four programs (FIR256/64, IIR4/64,
Compress, Edge Detect), whereas one program (JPEG ﬁlter)
experiences super-linear speedup due to better sequential
code generation after parallelization. The remaining three
programs do not scale that well with the number of pro-
cessors, still the speedup due to parallelization is at least
1.5. On average, we have a speedup of 3.25 across the four
processors.

5. RELATED WORK

A good overview of existing parallelization techniques is
given by Gupta et al.
[6]. Cache-coherent multiprocessors
with distributed shared memory are the subject of Chan-
dra et al.
[5]. Although compilers for such machines must

7. REFERENCES

[1] Analog Devices, Inc. TigerSHARC DSP Hardware

Speciﬁcation Norwood, Mass., 2001.

[2] J. Anderson, S. Amarasinghe and M. Lam, Data and

computation transformations for multiprocessors
ACM PPoPP, 1995.

[3] B. Franke , M. F.P. O’Boyle, Array recovery and
high-level transformations for DSP applications,
ACM Transactions on Embedded Computing Systems
(TECS) 2 (2),2003.

[4] B. Franke , M. F.P. O’Boyle, Combining Program

Recovery, Auto-parallelisation and Locality Analysis
for C programs on Multi-processor Embedded
Systems (to appear) IEEE PACT 2003.

[5] R. Chandra, D.-K. Chen, R. Cox, D.E. Maydan,

N. Nedeljkovic, J.M. Anderson, Data Distribution
Support on Distributed Shared Memory
Multiprocessors. ACM PLDI, Las Vegas, 1997.

[6] R. Gupta, S. Pande, K. Psarris, V. Sakar,

Compilation Techniques for Parallel Systems.
Parallel Computing, 25(13), 1999.

[7] S. Hiranandani, K. Kennedy, C.-W-. Tseng,

Compiling Fortran D for MIMD Distributed-Memory
Machines. CACM,35,(8), 1992.

[8] A. Kalavade, J. Othmer, B. Ackland, K.J. Singh,
Software Environment for a Multiprocessor DSP.
ACM/IEEE Design Automation Conference, 1999.

[9] I. Karkowski, H. Corporaal, Exploiting Fine- and
Coarse-grain Parallelism in Embedded Programs.
IEEE PACT, , 1998.

[10] J.R. Larus. Compiling for Shared-Memory and

Message-Passing Computers. ACM LOPLAS 2, (1-4),
1993.

[11] C.G. Lee, UTDSP Benchmark Suite.

http://www.eecg.toronto.edu/ corinna/DSP

[12] E.A. Lee, Dataﬂow process networks. Proceedings of

the IEEE, 83(5), May 1995.

[13] D.M. Lorts, Combining Parallelization Techniques to

Increase Adaptability and Eﬃciency of
Multiprocessing DSP Systems. DSP-2000, Hunt,
Texas, 2000.

[14] O’Boyle M.F.P and Knijnenberg P.M.W., Integrating

Loop and Data Transformations for Global
Optimisation, JPDC, 62, April 2002.

[15] O’Boyle M.F.P. and Stohr E.A., Compile Time

Barrier Synchronisation Minimisation, IEEE TPDS,
13(6), June 2002.

[16] O’Boyle M.F.P. and Hedayat G.A., Data Alignment:

Transformation to Reduce Communication on
Distributed Memory Architectures, IEEE Scalable
High Performance Computing Conference, 1992.

[17] E. Rypkema, E.F. Deprettere, B. Kienhuis,

Compilation from Matlab to Process Networks.
CASES, 1999.

[18] J. Teich, L. Thiele, Uniform Design of Parallel

Programs for DSP. IEEE Int. Symp. Circuits and
Systems, Singapore, June 1991.

[19] V. Zivojnovic, J.M. Velarde, C. Schlager, H. Meyr,

DSPstone: A DSP-Oriented Benchmarking
Methodology. Proc. Signal Processing Applications &
Technology, Dallas, 1994.

Figure 7: Speedup: UTDSP benchmarks on 4 pro-
cessors

incorporate data distribution and data locality increasing
techniques [2], they are not faced with the problem of mul-
tiple, but globally-addressable address spaces. Compiler-
Implemented Shared Memory (CISM) as described by Larus
[10] and Hiranandani et al.
[7] is a method to establish
shared memory on message-passing computers. However,
these approaches assume separate distributed address spaces
and require complex run-time bookkeeping.
In [2], data
tiling is used to improve spatial locality but the represen-
tation does not allow easy integration with other loop and
data transformations.

An early paper [18] described how DSP programs may
be parallelized but gave no details or experimental results.
Similarly, in [8] an interesting overall parallelization frame-
work is described but no mechanism or details of how paral-
lelization might take place is provided. In [13] the impact of
diﬀerent parallelization techniques is considered, however,
this was user-directed and no automatic approach provided.
In [9] a semi-automatic parallelization method to enable
design-space exploration of diﬀerent multi-processor con-
ﬁgurations based on the MOVE architecture is presented.
However, no integrated data partitioning strategy was avail-
able and a single address space was assumed in the example
codes. Furthermore, in the experiments, communication was
modelled in their simulator and thus the issue of mapping
parallelism combined with distributed address spaces was
not addressed.

6. CONCLUSION

In this paper we have developed a new compiler paral-
lelization approach that maps C programs onto multiple ad-
dress space multi-DSPs. By using a novel data transforma-
tion and address resolution mechanism, we have achieved an
average 3.25 speedup on four processors for those UTDSP
program’s containing suﬃcient parallelism.

The code generated is easy to read and amenable to fur-
ther sequential optimization. Future work will consider the
integration of multi-processor and uni-processor optimiza-
tions and consider the integration of task and pipeline par-
allelism.

