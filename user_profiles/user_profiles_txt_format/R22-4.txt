Workload Characterization Supporting the Development of
Domain-Speciﬁc Compiler Optimizations Using Decision

Trees for Data Mining

Damon Fenacci

d.fenacci@sms.ed.ac.uk

John Thomson

john.thomson@ed.ac.uk

Björn Franke

bfranke@inf.ed.ac.uk

Institute for Computing

Systems Architecture (ICSA)

School of Informatics
University of Edinburgh

United Kingdom

ABSTRACT
Embedded systems have successfully entered a broad vari-
ety of application domains such as automotive and industrial
control, telecommunications, networking, digital media, con-
sumer equipment, oﬃce automation and many more. In this
paper we investigate if there exist any fundamental diﬀer-
ences between application domains that justify the devel-
opment and tuning of domain-speciﬁc compilers. We de-
velop an automated approach that is capable of identifying
domain-speciﬁc workload characterizations and presenting
them in a readily interpretable format based on decision
trees. The generated workload proﬁles summarize key re-
source utilization issues and enable compiler engineers to
address the highlighted bottlenecks. We have evaluated our
methodology against the industrial Eembc benchmark suite
and three popular embedded processors and have found that
workload proﬁles diﬀer signiﬁcantly between application do-
mains. We demonstrate that these characteristics can be
exploited for the development of domain-speciﬁc compiler
optimizations. In a case study we show average performance
improvements of up to 44% for a class of networking appli-
cations.

Categories and Subject Descriptors
C.4 [Computer Systems Organization]: Performance of
Systems—Performance Attributes

General Terms
Experimentation, Measurement, Performance

Keywords
Workload Characterization, Data Mining, Decision Trees

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SCOPES ’10, June 28-29, 2010, St. Goar, Germany.
Copyright 2010 ACM 978-1-4503-0084-1/10/06 ...$10.00.

1.

INTRODUCTION

Embedded processor are used in a vast range of diﬀerent
application domains. This is reﬂected, for example, in the
provision of domain-speciﬁc benchmarks oﬀered by the Em-
bedded Microprocessor Benchmark Consortium (EEMBC)
consortium covering embedded applications domains such
as automotive, consumer, digital entertainment, Java, net-
working, oﬃce automation and telecoms. For some of these
domain, hardware vendors have developed specialized em-
bedded processors such as digital signal processors, multime-
dia processors and network processors. They contribute to
more eﬃcient solutions at a lower price due to their domain
specialization. However, the same is not true for the com-
piler technology targeting embedded applications. In fact,
compiler research in this area is driven by the development
of new hardware and has largely focused on targeting spe-
ciﬁc architectural features such as dual memory banks [28],
zero-overhead loop buﬀers [33] and irregular data paths [17].
More recently, adaptive techniques [7] have found their way
into optimizing compilers, but their application is restricted
to tuning of heuristics [22, 30] and selecting the order of
code transformation sequences [1]. So far, little advantage
is taken of domain-speciﬁc workload characteristics to fur-
ther specialize existing compilers to a particular embedded
domain. We believe this is due to two reasons: (a) the lack
of work in characterizing diﬀerent embedded application do-
mains, and (b) the lack of knowledge transfer between the
workload characterization and compiler development com-
munities.

In this paper we investigate a data mining approach using
human-interpretable decision trees to explore the character-
istics of a broad range of embedded applications. We seek to
identify and characterize the speciﬁc traits of diﬀerent em-
bedded domains in such a way that they can be exploited for
the development of domain-speciﬁc optimizing compilers.

1.1 Motivation

An earlier workload characterization of embedded applica-
tions [25] from diﬀerent domains has looked into the resource
utilization of processor components such as the arithmetic-
logic unit (ALU), load/store unit (LSU), multiplier and di-
visors (MULTDIV), shifters (SHIFT) and the branch unit
(BRANCH). Unfortunately, the results of this study have

been averaged over a number of platforms and no further
information is provided. We have therefore repeated the
experiments with greater level of detail and show results
for three embedded architectures (details in Table 3) and
ﬁve application domains (Automotive, Consumer, Network-
ing, Oﬃce, Telecom) using Kiviat diagrams similar to those
in [25] in Figure 1.

A comparison across architectures reveals signiﬁcant dif-
ferences in the utilization of functional resources. For ex-
ample, telecom applications show a relatively high propor-
tion of SHIFT operations on the Intel StrongARM platform,
however, no such eﬀect can be observed on the Freescale
MPC7410 and ARC 750D processors. At the same time
the BRANCH unit on the ARC processor shows a higher
utilization than on the Intel and Freescale platforms. Inter-
estingly, consumer applications exhibit a higher BRANCH
utilization on the ARC processor, but this not matched on
the other two processors. There is no obvious correlation be-
tween the BRANCH frequency across architectures. On the
other hand, visual inspection of the Kiviat diagrams reveals
a number of similarities. The utilization of the load/store
unit (LSU) is signiﬁcantly higher for networking applica-
tions across all three platforms. Furthermore, automotive
and oﬃce applications on the StrongARM result in similar
patterns, and so do automotive and consumer applications
for the Freescale processor and automotive, consumer and
telecom applications for the ARC core, respectively. The
diagrams suggest that certain application domains such as
automotive and oﬃce show a similar resource utilization for
some processors, whereas other domains such as networking
exhibit unique resource utilization patterns. Furthermore,
the data suggest that some of the characteristics diﬀer across
platforms (e.g. SHIFT for automotive), but other features
are correlated (LSU for networking). However, it remains
unclear what the underlying causes are for these diﬀerent
workload patterns. More important, it is not obvious if and
how speciﬁc workload characteristics could be exploited for
the tuning of domain-speciﬁc compilers.

1.2 Approach and Contributions

We put forward the hypothesis that if we can successfully
construct a classiﬁer to distinguish applications from diﬀer-
ent embedded domains then the classiﬁer itself is a charac-
terization of the corresponding application domains. The
possible problem with this approach is that many classi-
ﬁers, e.g. artiﬁcial neural networks, are not open to human
interpretation. Thus, we propose the use of decision trees
for data mining that are simple to understand and interpret
and have been shown to work well even with small data sets.
We test the above hypothesis and induce decision trees for
the industrial EEMBC benchmark suite with its diﬀerent ap-
plication domains based on dynamic program features such
as the instruction mix and cache utilization for three pop-
ular embedded processors. In a cross-validation experiment
we demonstrate that the induced decision trees are capable
of accurately predicting the application domain and, hence,
can be used to derive a meaningful domain characterization.
Among the contributions of this paper are:

1. The investigation of decision trees for characterizing

embedded applications from diﬀerent domains,

2. the comparison with a clustering based workload char-

acterization methodology, and

Features

Processor Components
Memory Write/Read
Operation Type

ALU, LSU, Branch, Mult./Div., Shift, FPU
Memory Read/Write, Cache Reads/Writes
Arith., Logic, Move, Sign Ext., Bit Manip.

Table 2: Overview of the aggregated feature sets.

3. the demonstration that the obtained workload charac-
terization can be successfully used to pinpoint perfor-
mance bottlenecks and to eventually identify domain-
speciﬁc compiler transformations.

The remainder of this paper is structured as follows. We
introduce our workload characterization methodology in sec-
tion 2 before presenting the setup and details of the empir-
ical evaluation in section 3. This is followed by a discussion
of our results in section 4 and a small-scale case study in
section 5. In section 6 we establish the context of our work
in the existing body of related work. Finally, we summarize
and conclude in section 7.

2. WORKLOAD CHARACTERIZATION

METHODOLOGY

Classiﬁcation tries to predict an attribute (“label”) of a
previously unseen data item based on other available at-
tributes (“features”) of this item. A popular classiﬁcation
algorithm that is used extensively in data mining and ma-
chine learning is the decision tree. The decision tree algo-
rithm is easy to use and implement, has low computational
complexity and, most importantly, its result is readily avail-
able for interpretation by non-statisticians despite its infor-
mation theoretical foundations.

A decision tree classiﬁer is induced from a training data
set where both the features and the labels are present. After
this initial training period the decision tree can be used as
a predictive model, i.e. to predict the label of a new data
item where only the features, but not the associated label,
are given. However, in this paper we use decision trees dif-
ferently. Rather than using them for prediction we use the
induced decision tree directly for workload characterization.
In our case, the features characterizing a program comprise
the instruction mix and other micro-architectural counters
(see section 3.2) and the label corresponds to one of the ap-
plication domains (e.g. “automotive” or “networking”). The
decision tree then establishes a relationship between the ob-
served counters and the membership to a particular domain.
Each internal node of a decision tree corresponds to a
conditional expression relating the value of a feature to a
constant threshold. Such nodes are usually labeled with the
name of the feature and the particular threshold value. Each
branch departing from the node corresponds to possible in-
tervals of the tested value. Leaf nodes, on the other hand,
specify the label to be returned if that leaf is reached.

Following the path from the root of the decision tree to
one of its leaves then produces a sequence of conditions such
as “data cache miss rate greater than 10%” that characterize
the items described by the leaf category (e.g. “automotive”).
For categories that contain examples that diﬀer signiﬁcantly
from each other, more than one leaf node may be generated
(e.g. “automotive 1” and “automotive 2”).

In this paper we use the standard C4.5 decision tree algo-
rithm introduced by Quinlan [26]. During the construction
of the tree the algorithm chooses for each node exactly that

Intel StrongARM

ARC 750D

Freescale MPC7410

Figure 1: Kiviat diagrams showing the resource utilization for diﬀerent EEMBC benchmarks and embedded
platforms.

!"#$%&’()*!+,%-#"(.&/"$#01201301401501601701!89:;:9<=>*:?@26A!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001CDD<E>!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001(>F>E:;*:?@273!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001+>9?:GH<IJ*:?@270!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001,:IK8;>G!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002@AA=B?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?C?B;<!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:D;EF=GH*;DI366!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;GJ9<?E!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?*;@A37B!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002CDD=E?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?F?E;<*;@A383!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:@;GH=IJ!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;IK9<?G!"#$%&’()*!+,%-#"(.&/"$#01201301401501601701!89:;:9<=>*:?@26A!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001CDD<E>!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001(>F>E:;*:?@273!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001+>9?:GH<IJ*:?@270!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001,:IK8;>G!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002@AA=B?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?C?B;<!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:D;EF=GH*;DI366!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;GJ9<?E!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?*;@A37B!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002CDD=E?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?F?E;<*;@A383!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:@;GH=IJ!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;IK9<?G!"#$%&’()*!+,%-#"(.&/"$#01201301401501601701!89:;:9<=>*:?@26A!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001CDD<E>!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001(>F>E:;*:?@273!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001+>9?:GH<IJ*:?@270!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001,:IK8;>G!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002@AA=B?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?C?B;<!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:D;EF=GH*;DI366!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;GJ9<?E!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?*;@A37B!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002CDD=E?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?F?E;<*;@A383!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:@;GH=IJ!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;IK9<?G!"#$%&’()*!+,%-#"(.&/"$#01201301401501601701!89:;:9<=>*:?@26A!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001CDD<E>!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001(>F>E:;*:?@273!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001+>9?:GH<IJ*:?@270!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001,:IK8;>G!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002@AA=B?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?C?B;<!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:D;EF=GH*;DI366!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;GJ9<?E!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?*;@A37B!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002CDD=E?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?F?E;<*;@A383!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:@;GH=IJ!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;IK9<?G!"#$%&’()*!+,%-#"(.&/"$#01201301401501601701!89:;:9<=>*:?@26A!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001CDD<E>!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001(>F>E:;*:?@273!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001+>9?:GH<IJ*:?@270!"#$%&’()*!+,%-#"(.&/"$#0B00120B00130B00140B00150B00160B00170B001,:IK8;>G!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002@AA=B?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?C?B;<!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:D;EF=GH*;DI366!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;GJ9<?E!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002!9:;<;:=>?*;@A37B!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002CDD=E?!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002(?F?E;<*;@A383!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002+?:@;GH=IJ!"#$%&’()*!+,%-#"(.&/"$#01002301002401002501002601002701002801002,;IK9<?GIntel StrongARM

ARC 750D

Freescale MPC7410

Figure 2: Decision trees induced for the EEMBC benchmarks (max. depth 5). Leaves represent application
domains and branches represent conjunctions of features that lead to those domains. The highlighted paths
refers to the networking domain that contains the IP reassembly benchmark discussed in section 5.
Labels: DCACHERM: “Data Cache Read Misses”, MEMWRITE: “Memory Writes”, CONV: ”Sign Extensions”, MULTDIV:
”Multiplications/Divisions”, DCACHEL1RM: “Level 1 Data Cache Read Misses”, ITLBM: “Instruction TLB Misses”
The thickness of colored bars in leaves indicate how many benchmarks fall in that leave and colors indicate how many of each
group do.

Platform

StrongARM SA-1110

ARC 750D

Freescale MPC7410

all instructions
-

all instructions
Branch Pred. Hits/Misses

all instructions
Branches Taken/Pred./Always Taken

Instruction Set
Branches
Memory
• Instr. Cache

• Data Cache

• Instr. TLB
• Data TLB
• Other Mem.

Other

Reads/Read Misses

Instr. Cache Hits/Misses

Reads/Read Misses
Writes/Write Misses
Reads/Read Misses
Reads/Read Misses
Pages

Hits/Misses
Dirty Misses

Memory Reads/Writes

Instr. Count
Cycles

Instr. Count
Cycles

L1 Hits/Misses
L2 Hits/Misses
L1 Read/Write Hits/Misses
L2 Read/Write Hits/Misses
Hits/Misses
Read/Write Hits/Misses
SDRAM Read/Write Accesses
SDRAM Page Hits/Misses
Instr. Count
Cycles

Table 1: Overview of the program features gathered by the simulators.

attribute that splits the data most eﬀectively by maximizing
the information gain.

3. EXPERIMENTAL SETUP AND EVALU-

ATION METHODOLOGY

In this section we provide an outlook of our experimental

setup and evaluation methodology.

3.1 Benchmarks

We use the EEMBC benchmark suite [31] for the evalua-
tion of our proposed workload characterization methodology.
Each of the 49 benchmarks belongs to one of following ﬁve
application domains: automotive, consumer, networking, of-
ﬁce and telecom.

3.2 Program Features

Programs are characterized by sets of numerical features
collected from simulation runs using instruction set simula-
tors of the three processor platforms considered in this pa-
per. Essentially, the features correspond to counters main-
taining instruction and micro-architectural event frequen-
cies. The speciﬁc features are listed in table 1. We have
grouped the features by functionality (e.g. ALU, branches
etc.) similar to the approach taken in [25]. These grouped
features are summarized in table 2. Contrary to [25] and [11]
the features considered in this paper are machine-dependent
as we primarily focus on the identiﬁcation of platform-and
domain-speciﬁc workload proﬁles.

3.3 Platforms

We have evaluated our workload characterization method-
ology against three popular embedded RISC architecture
families:
Intel StrongARM SA-1100, Freescale MPC7410
and ARC 750D. The platform details (including software
development tool chains) are summarized in table 3.

3.4 Evaluation Methodology

We have evaluated the accuracy of the decision tree char-
acterization approach using k-fold cross validation. This
technique partitions the data set into k subsets and, in turn,
uses k − 1 partitions to induce the decision tree that is sub-
sequently evaluated against the kth data partition. Cross
validation is essential to avoid over-ﬁtting and at the same
time provides a metric of how well the induced decision tree
describes the observed applications and their domain mem-
bership. In the experiments we conducted we decided to use
the special case of k-fold cross validation called leave-one-
out cross validation in which k is set to the total number of
observations.

Platform

StrongARM

ARC 750D

MPC7410

Processor Core
• Pipeline

5-stage

• Execution Order

In-Order

7-stage
(interlocked)
In-Order

Yes
Yes

Sep. int., FP
& vector pipelines
Out-Of-Order
8 FUs, dual issue
Yes
Yes

8k(I)/8k(D)
-/-
Yes
Simple RAM
native

32k(I)/32k(D) (8)
1024k
Yes
SDRAM/MPC106
60x bus

No (SW)
No

16k(I)/8k(D)
-/-
Yes
Simple RAM
native

SimIt-ARM ARC simulator
Default
Emulated

FP library
Emulated

Freescale SimG4
Default
Native

GCC 3.3.2
-O3

GCC 4.2.1
-O3

GCC 4.0.1
-O3

• FP Support
• Branch Pred.
Memory System
• Level 1 Cache
• Level 2 Cache
• MMU
• Main Memory
• Bus
Simulation
Simulator
• Options
• I/O
SW Dev. Tools
• Compiler
• Compiler Opt.

Table 3: Details of the StrongARM SA-1110, ARC
750D, and Freescale MPC7410 platforms.

Following normalization we use greedy, forward feature
selection to automatically eliminate redundant and useless
program features. The advantage of feature selection over
principal components analysis (PCA), which has been used
in e.g. [11], is that features are selected based on their sig-
niﬁcance in describing the observed data rather than just
their variation. The selected features are also more amend-
able to human interpretation than the projected principal
components resulting from PCA.

4. RESULTS AND INTERPRETATION

In this section we present our results and compare the
workload characterizations obtained from data mining with
decision trees to those obtained from statistical clustering.

4.1 Characterization by Decision Trees

All features
All with feature selection
Grouped features
Grouped w. feat. select.

StrongARM ARC 750D

SA-1100
82.88%
90.62%
85.62%
89.11%

Freescale
MPC7410
64.12%
89.17%
77.86%
85.00%

73.10%
85.09%
70.34%
85.09%

Table 4: Cross-validation results for decision tree
characterization.

The decision trees induced from the EEMBC benchmarks
and features grouped by function are shown in Figure 2.
In addition, table 4 summarizes the cross-validation results
for decision trees built with feature sets (“all instructions”,
“instruction groups”, with and without feature selection).

The most striking result is that the cross-validation accu-
racy of the decision trees is generally high, especially after
feature selection. The accuracy is in the range of 85-90%,
indicating that the program features can be used to success-
fully predict the application domain and, hence, the induced

decision trees can be trusted as reliable domain characteri-
zations.

The decision trees shown in Figure 2 reveal a number of
interesting facts. First, across the three platforms the sin-
gle most important feature discriminating applications do-
mains is the rate of level 1 data cache misses. Networking
applications, in particular, are characterized by their poor
D-cache performance and most networking benchmarks can
be distinguished from all other application domains based
on just this single feature.
In our case study in section 5
we will demonstrate how this information can be used to
guide the compiler engineer towards a cache-aware code and
data transformation speciﬁcally targeted at the networking
domain and that has little signiﬁcance for a compiler target-
ing e.g. general-purpose applications. Second, automotive
applications are spread over several leaf nodes characterized
by broadly diﬀerent feature sets. This indicates that the
applications of automotive domain do not exhibit homoge-
neous characteristics, but in fact comprise sub-domains with
rather heterogeneous characteristics. For example, on the
Intel StrongARM automotive applications either show low
data cache miss rates, but high ALU and FPU activity, or
a higher rate of D-cache misses and memory write activity,
but lower ALU utilization. This indicates that automotive
applications are either compute-intensive kernels with good
spatial and temporal locality or less compute-intensive ap-
plications with non-local memory access patterns. Third,
misclassiﬁcations occur most frequently between automo-
tive and consumer applications, indicating that these two
domains share some characteristics that make them hard to
distinguish. This can be seen in the colored boxes attached
to the leaves of the decision trees that represent the actual
domains (as opposed to the predicted ones that are shown
as labels of the leaves). Finally, telecom applications show
low data cache read miss rates across platforms and at the
same time perform only a modest number of ALU and very
few, if any, FPU operations.

4.2 Comparison with Statistical Clustering

Statistical clustering [34] approaches to workload charac-
terization are popular (e.g.
[27, 23]) and have been used
more recently to gain insight to benchmark variation [16]
and to exploit similarity between programs for performance
prediction [14]. We compare our decision tree based ap-
proach to a recent clustering methodology [32] and use this,
in particular, to study workload similarities across applica-
tion domains and hope to gain additional insights to those
domains such as automotive and consumer where decision
trees have a below average classiﬁcation success rate. Unlike
classiﬁcation statistical clustering does not rely on known
labels, but operates solely on the attributes describing the
data items (“unsupervised learning”). Clustering seeks struc-
ture in the input data, ﬁnding clusters of input points which
broadly share similar features. From this it may be possible
to classify the input data into sets, according to their prox-
imity to each cluster in the feature space. Juxtaposing the
generated clusters with the known application domains (see
Figure 3) enables us to investigate both the spread of typ-
ical characteristics within a single application domain and
the similarity of characteristics across domains. This infor-
mation may be used in combination with the decision trees
to identify sub-domains and domains sharing similar behav-
ior despite their diﬀerent application contexts. In this paper

Figure 4: Trace graph for memory reads for IP re-
assembly.

we use k-means clustering for workload characterization [2].
This technique based on Lloyd’s algorithm [20]. It starts by
partitioning the input space into k initial sets and calculates
the centroid of each set. It then constructs a new partition
by associating each point with the closest centroid, after
which the centroids are recalculated and the algorithm re-
peated until convergence. Input to the k-means clustering
are the number k of clusters and the initial set. We have
chosen k to be the number of leaf nodes generated by the
decision tree characterization and used a random set initial-
ization.

The known application domains and the generated clus-
ters are combined and contrasted in Figure 3. The y-axis
of the scatter graphs represents clusters, whereas the x-
axis corresponds to the application domains as deﬁned by
EEMBC benchmark suite. Each dot in the scatter graph
represents one particular benchmark program. For any par-
ticular column in the diagram we can see how applications
from a single domain spread over clusters with diﬀerent char-
acteristics. Each row, however, corresponds to a single clus-
ter and shows how programs with similar behavior can be
found in diﬀerent application domain. The bar plots next
to the y-axis represent the coordinates of the corresponding
cluster centroid.

Figure 3 reveals applications domains and clusters rarely
coincide. For some domains such as consumer and automo-
tive we see a spread over several clusters, whereas the ma-
jority of oﬃce, telecom and networking applications can be
found in a small number of clusters with just a few outliers.
Consumer applications typically span across four to six clus-
ters with vastly diﬀerent characteristics. This is because the
88 benchmark runs contained in the consumer domain deal
with diﬀerent kinds of data (e.g.
image, video processing,
audio coding) and perform diﬀerent types of algorithms (e.g.
conversion, compression etc.). The “automotive” group with
its 14 applications spans fewer clusters and these correspond
roughly to the leaf nodes identiﬁed by the decision tree ap-
proach. Most of the applications from the networking and
telecom domains fall in two to three distinct clusters that
again have been previously discovered and characterized us-
ing decision trees. If we look at the same graphs horizon-
tally, we notice that some clusters correspond to only one
or two benchmark groups whereas others span (almost) all
of them. For instance, clusters labeled “1”, which show high
data cache miss rates, always correspond to networking ap-

Intel StrongARM

ARC 750D

Freescale MPC7410

Figure 3: This scatter graph shows the relation between clusters obtained with k-means clustering and the
EEMBC application domains. Some domains comprise several clusters indicating heterogeneous workloads in
those domains. Similarly, some clusters span across domains indicating that similar workload characteristics
can be found in diﬀerent domains. The bar charts represent the coordinates of the cluster centroids. Numbers
on the left of the bar plots on the y-axis are used as reference in section 4.2, which explains the graphs in
detail.

	

				 !"#$%#"&# '(	
	




	
 !

""#$%%"
%	




	

 !
"#$%$&'(()$
(
)
plications. Clusters labeled “2”, which show lower data cache
miss rates and medium ALU/sign extension instruction us-
age, correspond partly to consumer and partly to telecom.
For example, the Viterbi decoder benchmark program (“tele-
com”) is present in this cluster for all architectures. Across
architectures the clusters are not necessarily homogeneous,
e.g. cluster “2” includes MPEG-2 decoding (“consumer”) on
the Intel StrongARM, and bit allocation (“telecom”) on the
ARC 750D, however, on the Freescale MPC7410 it contains
both of them. A subset of networking (in particular IP
packet check, packet ﬂow and TCP ) and consumer bench-
marks (mainly RGB conversion) are included in cluster “3”.
Less correspondence can be found for other clusters. For ex-
ample, clusters labeled “4” spread over (almost) all groups
for all architectures and contain some common benchmark
programs but, except for the fact that all architectures show
medium-low values for all features taken into account, no
stronger statement can be made.

In summary, both decision trees and statistical clustering
are capable of extracting deﬁning characteristics of a set of
programs. For example, the distinctive D-cache behavior
of networking applications has been picked up by both ap-
proaches. Similarly, the existence of sub-domains in the con-
sumer application domain has been detected by both meth-
ods. Decision trees are a powerful instrument to identify
and represent the diﬀerences between application domains.
For the three platforms under consideration clear character-
izations of the ﬁve embedded application domains deﬁned
by the EEMBC benchmark suite have been derived. Deci-
sions trees appear more capable of representing diﬀerences
across domains. The clusters generated by the statistical
clustering technique instead are more abstract as applica-
tion domain membership is not handled explicitly, but the
clusters are represented by their (virtual) centroid and the
speciﬁc data items contained in the cluster. Clustering also
appears to be somewhat sensitive to the choice of param-
eters [32] and requires some amount of tuning before the
strongest results are produced. While clustering is useful
to identify benchmark similarity across domains it appears
not so well suited to represent the distinct characteristics
of individual application domains. The combined considera-
tion of decision trees and clustering approaches is promising,
especially if clustering is used to further investigate the ini-
tial ﬁndings produced by decision tree data mining. We feel
the possible “recipe”-like representation of the decision tree
diagrams make them more favorable to compiler engineers
who seek to identify performance bottlenecks and guidance
in how to tune their compilers. Given that most compiler
developers are typically not trained in statistics the graph-
ical notation and rule-based interpretation of decision trees
make them more accessible to this group.

5. CASE STUDY

In this section we present a case study targeting the indus-
try standard EEMBC networking benchmarks. Inspection
of the induced decision trees depicted in Figure 2 shows that
the networking benchmarks exhibit high cache miss rates
across all three platforms. In fact, the decision trees list the
data cache miss rate as the most important criteria to distin-
guish networking applications from the remaining domains,
indicating we should focus our attention to the causes of this
poor cache behavior if we seek to improve the compiler per-
formance for this domain. Other benchmark domains, e.g.

struct mbuf {

struct m_hdr m_hdr; /* header */
...
union {

struct m_ext MH_ext; /* link to external payload */
char MH_databuf[MHLEN]; /* internal payload */

} M_dat;

...
}

Figure 6: C struct for packets that include payloads.

automotive, exhibit much smaller cache miss rates and are
more accurately described as compute-bound (StrongARM,
ARC) or suﬀering from poor I-TLB behavior (MPC7410).

The decision tree analysis in paragraph 4.1 has clearly re-
vealed that networking applications suﬀer from a data cache
bottleneck. Hence, we now perform a more detailed memory
access analysis to identify the causes of this bottleneck. This
is also the way we expect users of our proposed workload
characterization to identify their compiler tuning opportu-
nities: (a) Use the decision tree characterization to pinpoint
the critical resource, and then (b) use a specialized tool to
perform a detailed analysis of that particular resource.

Lack of temporal and spatial locality are the known con-
tributors to low cache utilization, so we employ a ﬁne-grained
memory proﬁling analysis to obtain a more detailed picture
of the memory accesses. Figure 4 shows an excerpt of the
memory access trace of the IP reassembly benchmark gen-
erated by Pin [21].

An inspection of the memory trace shown in Figure 4
reveals that memory is frequently accessed at discrete ad-
dresses with large unused gaps in-between. This leads to a
characteristic regular access pattern with discrete horizon-
tal stripes. The gaps between repeatedly accessed addresses
are just 256 bytes wide and never subsequently accessed
but nevertheless pollute the data caches. With this addi-
tional information available we have inspected the applica-
tion source code and found that IP packets are stored in a
single C structure containing separate elements for the IP
header and the payload. The structure is deﬁned as shown
in Figure 6. Complete packets (i.e. header and payload) are
stored in successive order in memory, however, IP reassem-
bly only ever reads the IP packet headers and does not touch
the actual payload. Hence, we ﬁnd the useful header infor-
mation interleaved with unused payloads that introduce the
previously observed gaps between accessed data items. Af-
ter identifying the cause of the low data cache utilization
of the IP reassembly benchmark (and EEMBC networking
applications in general) we manually applied the structure
splitting transformation [37] to this benchmark. Structure
splitting splits a C structure into its components and prop-
agates this information to their use sites. The eﬀect is that
the aforementioned interleaving is removed and data items
the IP packet headers) are stored
of the same type (i.e.
together and separate from the the payload.
In fact, the
transformed code shows an improved data cache behavior,
with cache hit rate increasing from 69% to 78% for the Stron-
gARM SA1100 platform and from 92% to 94% for the ARC
750D, which resulted in a total speedup of 1.77 and 1.78 re-
spectively. We have subsequently applied structure splitting
to the complete set of EEMBC benchmarks and found that
this transformation is applicable to the remaining network-
ing codes in the same node of the decision tree as IP reassem-
bly. Surprisingly, structure splitting is genuinely networking

Figure 5: Speedups for EEMBC networking after structure splitting on the StrongARM and ARC platforms.

speciﬁc and ineﬀective for any other application contained in
the EEMBC suite. The full set of results for the networking
applications is shown in Figure 5 where an average speedup
of 1.27 for StrongARM SA1100 and 1.44 for ARC 750D re-
sulting from structure splitting can be observed. Varying
speedups between architectures are mainly due to the dif-
ferent cache organizations of the two targets. Variations in
overall performance improvements for a single target orig-
inate from diﬀerent buﬀer sizes and how well these match
the cache line size.

6. RELATED WORK

In this section we discuss the related work in the areas of
workload characterization and machine-learning based adap-
tive compilation.

There exists a large body of work on workload characteri-
zation [9, 8, 35], much of which focuses on desktop [18], high-
performance [29] or database workloads [36]. In this paper
we are more concerned about embedded applications origi-
nating from diﬀerent domains of embedded systems. Most
relevant to our own research is the existing workload char-
acterization [25] provided by the EEMBC consortium. This
characterization, however, has been carried out to enable
designers to select the most relevant benchmarks from the
EEMBC suite in terms of program similarity and to infer
expected performance ﬁgures for their own applications or
platforms.
In contrast, we are interested how embedded
application domains diﬀer from each other and what fea-
tures can possibly be exploited for future domain-speciﬁc
compiler development.
In [12] a characterization method-
ology for general-purpose and domain-speciﬁc benchmark
suites is presented. We build on this work and follow a
similar clustering approach, however, there exists a num-
ber of important diﬀerences. First, we explicitly include
platform-speciﬁc features (as opposed to micro-architecture-
independent characteristics) in our characterization as we
are interested in speciﬁc compiler/architecture interactions.
Second, [12] focuses on phase-level characterization of com-
plex applications with a distinct phase behavior, whereas
we have chosen smaller, but domain-speciﬁc and compute-
intensive benchmarks representative for deeply embedded
systems running a single, ﬁxed application. Finally, we de-
velop a human interpretable characterization supporting the
compiler engineer whereas [12] seeks to identify the unique
and diverse behaviors of diﬀerent benchmark suites. Sim-
ilarly, [11] and [16] aim at deriving and measuring micro-
architecture-independent program features for clustering.
Eventually, each cluster is expressed by a single “most repre-

sentative” application. Both papers use principal component
analysis (PCA) for dimension reduction.
[11] uses Kiviat
diagrams for the representation of the results, and [16] in-
troduces K-means for clustering. In [14] program similarity
is exploited to facilitate performance prediction. Code cov-
erage and input variability for a number of embedded ap-
plications is studied in [15]. This work aims at supporting
compiler and architecture research by identifying weaknesses
such as superﬂuous functions and insuﬃcient code coverage
of existing benchmarks.

Classiﬁcation using decision trees has been employed for
the exploration of cache hierarchies for commercial appli-
cation workloads [10], generation of compiler optimization
heuristics [22], branch prediction in [3], dynamic power man-
agement [5], and packet classiﬁcation in network proces-
sors [19]. K-means clustering is discussed in [24] and [16]
for benchmark categorization.

Adaptive compilation [6] largely focuses on machine learn-
ing for improved phase ordering in an optimizing compiler [1]
or tuning of optimization heuristics [22]. or compiler set-
tings [4][13]. While this improves the performance of exist-
ing compilers it does not help identify the code generation
and optimization deﬁcits that demand genuinely new trans-
formations rather than tweaking of existing ones.
In con-
trast, in this paper we apply data mining techniques to ex-
tract human interpretable information supporting the com-
piler engineer in developing new, domain-speciﬁc compiler
optimizations.

7. SUMMARY AND CONCLUSIONS

In this paper we have presented a methodology for an
automated workload characterization which may lead to the
identiﬁcation of tuning opportunities for domain-speciﬁc com-
pilers. Our proposed method uses decision trees and pro-
duces human-interpretable results. Therefore, it can be used
by compiler developers seeking to identify domain-speciﬁc
performance bottlenecks and guidance on how to address
these. We have demonstrated how decision trees can be
used to detect the speciﬁc characteristics of ﬁve embed-
ded applications domains. We demonstrate how the de-
cision tree characterization can be exploited to identify a
domain-speciﬁc compiler optimization missing.
In a case
study we demonstrate how our methodology directs the com-
piler engineer to a speciﬁc cache bottleneck, which can be
resolved by applying a non-standard data transformation.
For the EEMBC networking applications this leads to aver-
age speedups of 1.27 and 1.44 for ARC 750D and for Strong-
ARM, respectively.

	















 !"#"""$"""%"&"'"("# )* )+

8. REFERENCES
[1] F. Agakov, E. Bonilla, J.Cavazos, B.Franke, G. Fursin,

M. O’Boyle, J. Thomson, M. Toussaint, and C. Williams.
Using machine learning to focus iterative optimization. In
Proceedings of the 2006 International Symposium on Code
Generation and Optimization (CGO), 2006.

[2] C. Bishop. Neural Networks for Pattern Recognition.

Oxford University Press, USA, 1995.

[3] B. Calder, D. Grunwald, M. Jones, D. Lindsay, J. Martin,

M. Mozer, and B. Zorn. Evidence-based static branch
prediction using machine learning. ACM Transactions on
Programming Languages and Systems (TOPLAS),
19(1):188–222, 1997.

[4] J. Cavazos, G. Fursin, F. Agakov, E. Bonilla, M. F. P.

O’Boyle, and O. Temam. Rapidly selecting good compiler
optimizations using performance counters. In Proceedings
of the 2007 International Symposium on Code Generation
and Optimization (CGO), pages 185–197, Washington, DC,
USA, 2007. IEEE Computer Society.

[5] E. Chung, L. Benini, and G. De Micheli. Dynamic power
management using adaptive learning tree. In ICCAD ’99:
Proceedings of the 1999 IEEE/ACM international
conference on Computer-aided design.

[6] K. D. Cooper, A. Grosul, T. J. Harvey, S. Reeves,

D. Subramanian, L. Torczon, and T. Waterman. ACME:
adaptive compilation made eﬃcient, 2005.

[7] K. D. Cooper and T. Waterman. Investigating adaptive

compilation using the MIPSPro compiler. In In Proc. of the
Symp. of the Los Alamos Computer Science Institute, 2003.

[8] Y. Ding and K. Newman. Automatic workload

characterization. In Proceedings of CMG ’00, 2000.

[9] A. B. Downey and D. G. Feitelson. The elusive goal of

workload characterization. ACM SIGMETRICS
Performance Evaluation Review, 26(4):14–29, 1999.
[10] P. Elakkumanan, L. Liu, V. Kumar Vankadara, and

R. Sridhar. CHIDDAM: A data mining based technique for
overcoming the memory bottleneck problem in commercial
applications.

[11] K. Hoste and L. Eeckhout. Microarchitecture-independent
workload characterization. The 40th Annual IEEE/ACM
International Symposium on Microarchitecture (MICRO),
27(3):63–72, 2007.

[12] K. Hoste and L. Eeckhout. Characterizing the unique and
diverse behaviors in existing and emerging general-purpose
and domain-speciﬁc benchmark suites. 2008 IEEE
International Symposium on Performance Analysis of
Systems and software (ISPASS), pages 157–168, April 2008.

[13] K. Hoste and L. Eekhout. COLE: Compiler Optimization

Level Exploration. In CGO ’08: Proceedings of the
International Symposium on Code Generation and
Optimization, New York, NY, USA, 2008.

[14] K. Hoste, A. Phansalkar, L. Eeckhout, A. Georges, L. K.
John, and K. D. Bosschere. Performance prediction based
on inherent program similarity. In Proceedings of the
International Conference on Parallel Architectures and
Compilation Techniques (PACT), 2006.

[15] H. C. Hunter and W.-M. W. Hwu. Code coverage and
input variability: eﬀects on architecture and compiler
research. In Proceedings of the 2002 International
Conference on Compilers, Architecture, and Synthesis for
Embedded Systems (CASES), 2002.

[16] A. Joshi, A. Phansalkar, L. Eeckhout, and L. John.

Measuring benchmark similarity using inherent program
characteristics. IEEE Transactions on Computers,
55(6):769–782, June 2006.

[17] Y.-H. Lee and C. Chen. An eﬃcient code generation

algorithm for non-orthogonal DSP architecture. Journal of
VLSI Signal Processing Systems, 47(3):281–296, 2007.

[18] J. M. Lin, Y. Chen, W. Li, Z. Tang, and A. Jaleel. Memory

characterization of SPEC CPU2006 benchmark suite. In
Proceedings of the 2008 Workshop for Computer
Architecture Evaluation of Commerical Workloads

(CAECW), 2008.

[19] D. Liu, B. Hua, X. Hu, and X. Tang. High-performance

packet classiﬁcation algorithm for many-core and
multithreaded network processor. In Proceedings of the
2006 International Conference on Compilers, Architecture
and Synthesis for Embedded Systems (CASES).

[20] S. Lloyd. Least squares quantization in PCM. IEEE

Transactions on Information Theory, 28(2):129–137, 1982.

[21] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser,

G. Lowney, S. Wallace, V. J. Reddi, and K. Hazelwood.
Pin: building customized program analysis tools with
dynamic instrumentation. In Proceedings of the 2005 ACM
SIGPLAN Conference on Programming Language Design
and Implementation (PLDI), 2005.

[22] A. Monsifrot, F. Bodin, and R. Quiniou. A machine

learning approach to automatic production of compiler
heuristics. In Artiﬁcial Intelligence: Methodology, Systems,
Applications, pages 389–409. Springer Verlag, 2002.

[23] O. I. Pentakalos, D. A. Menascz, and Y. Yesha. Automated

clustering-based workload characterization. In In
Proceedings of the 5th NASA Goddard Mass Storage
Systems and Technologies Conference, 1996.

[24] A. Phansalkar, A. Joshi, L. Eeckhout, and L. John.

Measuring program similarity: Experiments with SPEC
CPU benchmark suites. In IEEE International Symposium
on Performance Analysis of Systems and Software
(ISPASS), pages 10–20, 2005.

[25] J. Poovey. Characterization of the EEMBC benchmark

suite. http://www.eembc.org, 2007.

[26] J. Quinlan. C4.5: programs for machine learning. Morgan

Kaufmann, 1993.

[27] K. E. E. Raatikainen. Cluster analysis and workload

classiﬁcation. SIGMETRICS Perform. Eval. Rev.,
20(4):24–30, 1993.

[28] V. Sipkova. Eﬃcient variable allocation to dual memory

banks of DSPs. In Proceedings of the 7th International
Workshop on Software and Compilers for Embedded
Systems (SCOPES 2003), 2003.

[29] D. Skinner and W. Kramer. Understanding the causes of
performance variability in HPC workloads. Proceedings of
the 2005 IEEE International Symposium on Workload
Characterization.

[30] M. Stephenson, S. Amarasinghe, and M. Martin. Meta

optimization: Improving compiler heuristics with machine
learning. In Proceedings of the 2003 ACM SIGPLAN
Conference on Programming Language Design and
Implementation (PLDI), 2003.

[31] The Embedded Microprocessor Benchmark Consortium.

EEMBC benchmarks. http://www.eembc.org, 2008.
[32] J. Thomson. Using Machine Learning to Automate

Compiler Optimisation. PhD thesis, School of Informatics,
University of Edinburgh, 2008.

[33] G.-R. Uh, Y. Wang, D. Whalley, S. Jinturkar, C. Burns,

and V. Cao. Eﬀective exploitation of a zero overhead loop
buﬀer. 1999.

[34] U. von Luxburg and B. S. David. Towards a statistical

theory of clustering. In PASCAL Workshop on Statistics
and Optimization of Clustering, 2005.

[35] K. Yan. Characterization and classiﬁcation of modern

micro-processor benchmarks. Master’s thesis, New Mexico
State University, 2004.

[36] P. S. Yu, M. syan Chen, H.-U. Heiss, and S. Lee. On

workload characterization of relational database
environments. IEEE Transactions on Software
Engineering, 18:347–355, 1992.

[37] Y. Zhong, M. Orlovich, X. Shen, and C. Ding. Array

regrouping and structure splitting using whole-program
reference aﬃnity. Proceedings of the 2004 ACM SIGPLAN
Conference on Programming Language Design and
Implementation (PLDI), 39(6):255–266, 2004.

