An Efﬁcient and Reliable Scientiﬁc Workﬂow System(cid:3)

Tulio Tavares(cid:3), George Teodoro(cid:3), Tahsin Kurcy, Renato Ferreira(cid:3),

Dorgival Guedes(cid:3), Wagner Meira Jr(cid:3), Umit Catalyureky,

Shannon Hastingsy, Scott Ostery, Steve Langellay, and Joel Saltzy

(cid:3)Department of Computer

Science

Universidade Federal de Minas

Gerais, Brazil

yDepartment of Biomedical

Informatics

The Ohio State University, USA

Abstract

This paper presents a fault tolerance frame-
work for applications that process data using a
distributed network of user-deﬁned operations in
a pipelined fashion. The framework saves inter-
mediate results and messages exchanged among
application components in a distributed data man-
agement system to facilitate quick recovery from
failures. The experimental results show that the
framework scales well and our approach intro-
duces very little overhead to application execution.

1 Introduction

Scientiﬁc workﬂow systems [6, 5, 9] provide
scientists with a suite of tools and infrastructure
to build data analysis applications from reusable
components and execute them. The challenges
to implementing workﬂow middleware support for
scientiﬁc applications are many. Analysis often-
times requires processing of large volumes of data
through a series of simple and complex opera-
tions. In biomedical domain, with advanced imag-
ing sensors and scanners, it is possible to acquire
very high resolution microscopic images of tis-
sues and organs quickly. Each image can be up to
Gigabytes in size and hundreds (even thousands)
of images can be obtained from a single sample.
These images are analyzed through a series of data
processing operations, including signal extraction,
registration, segmentation, and feature classiﬁca-

(cid:3)This research was supported in part by the National
Science Foundation under Grants #ACI-0203846, #ACI-
0130437, #ANI-0330612, #ACI-9982087, #CCF-0342615,
#CNS-0406386, CNS #0403342, #CNS-0426241, NIH NIBIB
BISTI #P20EB000591, Ohio Board of Regents BRTTC
#BRTT02-0003.

tion. To support such types of data processing
efﬁciently, a workﬂow middleware system should
leverage distributed computing power and stor-
age space (both disk and memory space) and im-
plement optimizations for large data retrieval and
scheduling of I/O and computation components.
Another challenging issue is to enable fault toler-
ance in the middleware fabric. An analysis work-
ﬂow with complex operations on large data can
take long time to execute. The probability of a
failure during execution should be considered [8].
Efﬁcient mechanisms are needed to support recov-
ery from a failure without having to redo much of
the computation already done.

In this paper, we propose and evaluate a fault
tolerance framework for applications that process
data using a pipelined network of user-deﬁned op-
erations in a distributed environment. In an ear-
lier work [11], we described a middleware sys-
tem to create workﬂows from compiled programs
and shared libraries and manage their execution.
We extend this framework to provide support for
fault tolerance in scientiﬁc data analysis work-
ﬂows. The extensions provide functionality and
protocols to efﬁciently manage input, intermedi-
ate, and output data and associated metadata and to
recover from certain types of faults that may occur
in the system. In our approach, intermediate re-
sults and messages exchanged among application
components are maintained in a distributed data
management infrastructure along with additional
metadata. The infrastructure consists of a per-
sistent storage manager that stores check-pointed
information in a distributed database and a dis-
tributed cache that reduces the overhead of check-
pointing. We have developed a protocol among
the various components of the system to manage

Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)0-7695-2833-3/07 $20.00  © 2007check-points and related information efﬁciently.
The experimental results show that our approach
provides an asynchronous data storage mechanism
that minimizes overhead to the execution of the
workﬂow.

2 Related Work

Bowers et. al. [2] describe a framework to sup-
port the structured embedding of generic control-
ﬂow components within dataﬂow process net-
works. The framework has some mechanisms that
can be used to develop workﬂows via reusable
control-intensive subtasks. The authors present a
mechanism for data retransmission, but they do
not examine the impact of this mechanism on
fault tolerance. Kola [6] proposes a strategy of
fault-isolation that decouples data placement and
computation in order to reduce the need for re-
computation. Kola’s work also differentiates be-
tween persistent and transient failures and auto-
matically recovers from transient faults. However,
our work differs in that his approach requires in-
put from the user and does not provide transpar-
ent mechanisms. The Phoenix [7] system provides
some tools that allow fault tolerance for data in-
tensive applications. These tools can detect fail-
ures and classify them into transient or persistent
failures. The tools can also handle each tran-
sient failure appropriately. However, the recov-
ery from such failures is carried out according the
speciﬁcations provided by the user before execu-
tion. Tatebe presents the architecture of the Grid
Datafarm [10]. This ﬁle system aims to man-
age Petabyte-scale distributed datasets. The Grid
Datafarm system employs replication for fault tol-
erance. Our work, on the other hand, aims to sup-
port fault tolerance by maintaining the state of the
system in a distributed environment.

3 Middleware Framework

In this section, we provide an overview of the
core framework employed in our work. The frame-
work is designed to support development and exe-
cution of data analysis as a network of interacting
components in a distributed environment.

3.1 Anthill and Mobius

The framework and its components are imple-
mented using the Anthill [3] and Mobius [4] mid-
dleware systems. Anthill supports execution of
component-based applications in distributed het-
erogeneous environments. Its programming model

is based on the ﬁlter-stream model implemented in
a system called Datacutter [1].

In the ﬁlter-stream paradigm, an application
is implemented as a network of data processing
components, referred to as ﬁlters. A ﬁlter ex-
change data in application-speciﬁc chunks through
streams, and has a main processing function that
carries out application-speciﬁc computation on re-
ceived data chunks. In many applications, process-
ing of data can be done in a pipelined fashion, in
which ﬁlters at different stages of the workﬂow ex-
ecute concurrently and process. A ﬁlter in the sys-
tem can receive data from multiple upstream ﬁlters
and send data to multiple downstream ﬁlters. Each
chunk can be associated with application-deﬁned
metadata. The ﬁlter-stream paradigm also allows
for combined use of task-parallelism (via multiple
ﬁlters executing concurrently on different hosts)
and data-parallelism (through multiple copies of
any of the application ﬁlters).

Anthill extends the original ﬁlter-stream model
implemented in Datacutter [1] in several aspects.
In particular, it allows applications to be modeled
by general digraphs. Another speciﬁc feature it in-
corporates is the mechanism called labeled stream
which allows the selection, based upon the data
being transfered, of one speciﬁc ﬁlter copy when
several are instantiated by the application. This
feature is very important when the ﬁlter it refers to
has a state that is partitioned across several hosts
and the data is associated with speciﬁc portions of
that state.

framework.

Mobius [4] is a distributed metadata and data
management
It provides a vir-
tualization of backend data sources as XML
databases and enables on-demand creation of
XML databases in a distributed environment such
as the Grid. The Mobius Global Model Exchange
(GME) service provides support for creating, man-
aging, referencing, and versioning XML schemas
under name spaces. Each document stored, in-
dexed, and managed in the Mobius environment
is required to conform to a schema registered
in GME. The Mobius Mako service enables on-
demand creation and management of databases
across multiple host machines. Documents con-
forming to schemas can be stored, queried, and re-
motely retrieved from Mako instances. The data
stored in a Mako is indexed so it can be queried
efﬁciently using XPath.

In our implementation, Anthill provides the
runtime support for distributed execution. The
components to support fault tolerance are imple-
mented as Anthill ﬁlters. We use the Mobius mid-

Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)0-7695-2833-3/07 $20.00  © 2007dleware as our persistent data storage manager.

3.2 Architecture of Framework

The framework is composed of three main com-
ponents (see Figure 1): the shared library reposi-
tory, the program maker, and the run-time envi-
ronment. The ﬁrst two parts allows users to store
and share components and provides a toolkit for
generating workﬂows based on shared components
from the repository. The run-time environment is
divided into a data-intensive workﬂow execution
support system, a distributed workﬂow manage-
ment subsystem, and an XML data storage mid-
dleware (also referred to here as the persistent stor-
age manager). The data-intensive workﬂow execu-
tion (DIWE) component is implemented on top of
Anthill. The shared library repository, the program
maker and the DIWE components are described in
more detail in another paper [11]. In this work we
introduce the workﬂow management system and
the persistent storage system to support fault toler-
ance.

3.2.1 Workﬂow Management System (WMS)

This component is divided into the Workﬂow
Meta-Data Manager (WMDM) subsystem and the
In-Memory Data Storage (IMDS) subsystem. The
WMDM is responsible for managing the entire
workﬂow execution. The IMDS works as a in-
termediary between application ﬁlters and the per-
sistent storage manager (PSM). We have extended
the DIWE component to provide transparent com-
munication with the Workﬂow Management Sys-
tem (WMS). These extensions facilitate exchange
of information between application ﬁlters and the
WMS. This information includes which ﬁlters are
available for data processing, which chunks have
been processed, and so on.

Workﬂow Meta-data Manager (WMDM) works
as a data manager for the entire workﬂow execu-
tion. This component was developed as an Anthill
ﬁlter. The WMDM has information about each
dataset read or written by the application on the
ﬂy. It is also responsible for deciding on demand
which portions of the input data is processed by
which ﬁlter.
It monitors and manages the data
chunks (messages) received and created on the ﬂy
by the workﬂow.
It stores all the meta-data re-
lated to these messages. For example, it creates a
unique identiﬁer for each message and also knows
where it is stored, which ﬁlter instance has pro-
cessed or created the message, etc. It provides a

Shared Library Repository

Get Libraries

Program Maker

Filter Maker

Program Descriptor

Application Filters

    Runtime Environment 

Data-Intensive Workflow Execution Support (DIWE)

In Memory Data Storage (IMDS)

Workflow Meta-data Manager (WMDM)

Workflow Management System (WMS)

XML Data Storage MiddleWare (PSM)

Figure 1. Framework Components

protocol for accessing, creating and updating the
necessary metadata during the execution. When
the workﬂow starts running, the WMDM decides
which data chunks will be sent to which ﬁlter in-
stances. This data partition is done on demand as
each time a ﬁlter reads input, a request is received
by the WMDM. The goal is to always assign a lo-
cal data chunk to the ﬁlter.
In-Memory Data Storage (IMDS) works as a
cache between the user application and the PSM.
This component was also developed as an Anthill
ﬁlter. It implements a distributed cache and mech-
anisms for reading and writing information (e.g.,
data chunks) from/to the PSM. Based on the meta-
data provided by the WMDM, it reads the neces-
sary data from the PSM and stores the outputs of
each component. Multiple instances of the IMDS
can be instantiated in the environment. Each of
these instances has its own memory space to cache
data chunks during workﬂow execution. When
the user application is instantiated, the instances
of the application ﬁlters are assigned to the IMDS
instances. This assignment is done such that one
IMDS instance is responsible for a subset of the
application ﬁlter instances. For example, an IDMS
instance can be responsible for application ﬁlter
instances that are running on the same node as the
IDMS instance.

During workﬂow execution, when a ﬁlter re-
quests for a data chunk to process, the request is
sent to the IMDS instance to which the ﬁlter is
assigned. To decide which data chunk should be
retrieved, the WMDM is consulted – there may
be multiple WMDM instances; a WMDM instance
is responsible for managing a subset of IMDS in-

Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)0-7695-2833-3/07 $20.00  © 2007stances. Based on the meta-data provided by the
WMDM, the IDMS instance reads the necessary
data chunk from the PSM, stores it in memory, and
sends it to the application ﬁlter. This component
also stores the intermediate data sent through the
streams between application ﬁlters, and the out-
put data. When there is no more memory space to
store data chunks in an IMDS instance, the IMDS
instance sends a subset of data chunks in its cache
to the PSM.

3.2.2 Persistent Storage Manager (PSM)
The persistent storage manager is built on top of
Mobius [4] which is used to instantiate data stores
in the distributed environment. The PSM is used
to store the input and output datasets of the appli-
cation workﬂows. These datasets are deﬁned by
XML schemas and are stored as XML databases.
When the application outputs a document, this out-
put is ﬁrst sent to the IMDS that stores this data in
the PSM. To facilitate fault tolerance in the sys-
tem, the PSM is also used to store data chunks ex-
changed through application ﬁlter streams. Again
such data chunks are ﬁrst sent to the IMDS that
interacts with the PSM to efﬁciently store them.

4 Support for Fault Tolerance

In this section, we present the approach em-
ployed in our framework for fault tolerance and
describe the protocol used by the different com-
ponents of the system to enable fault detection and
fault recovery.

chunk. This information is utilized by the recovery
protocol described in Section 4.4. In our frame-
work, we identify four types of dependencies be-
tween output data chunks and input data chunks:
(1) 1 to 1: this is the simplest type. When a ﬁl-
ter receives one message (i.e., one data chunk), it
can process the data in this message and send the
(2) n to
result to the next ﬁlter in the pipeline.
1: a ﬁlter has to receive n messages, from one or
more streams, before generating one output. The
n may vary according to the application and also
may assume different values during the application
execution. (3) n to m: this is similar to the n to 1,
but in this type, m output messages may be gener-
ated by one ﬁlter, in one or more streams. (4) 1 to
n: this is characterized by one ﬁlter receiving one
message, and generating many outputs using one
or more output stream.

Data chunks can assume one of the follow-
ing three different processing states: (1) Not Pro-
cessed: Every data chunk in the initial dataset is
considered “not processed” in the beginning of the
workﬂow execution. This means they are available
to be processed. (2) Being Processed: The data
chunk is in the “being processed” status when a
ﬁlter in the workﬂow gets it to process. All the
data chunks created, on the ﬂy, by the workﬂow ﬁl-
ters are considered they are being processed, once
they were created and sent to other ﬁlter to pro-
cess. (3) Processed: One data chunk is considered
“processed” only when the data chunks that are
dependent to it are created and stored in the PSM.

4.1 Message Logging

4.2 Creating Message Logs

A number of mechanisms and protocols have
been developed to add fault tolerance and achieve
high availability in centralized and distributed sys-
tems [12]. The most common mechanisms are
based on checkpointing, message logging, and roll-
back recovery. We employ the message logging
approach.
In message logging, all messages re-
ceived by a process are saved in a message log, so
the application can be restored from its most re-
cent checkpoint by replaying the messages using
the same order they were received. In our system,
data chunks that are consumed and produced by
application ﬁlters are check-pointed in the system
as message logs.

In order for the system to correctly create mes-
sage logs and recover an application from faults,
the system needs to keep track of dependencies
among all data chunks, which are being produced
and consumed, and the processing status of each

The creation and management of message logs
are coordinated by the WMDM. In our current im-
plementation, the fault tolerance support can re-
cover applications with ﬁlters that present data de-
pendency types of 1 to 1 or n to 1. Log cre-
ation is done as follows. The IMDS is responsi-
ble for storing the data chunks exchanged through
streams and for keeping the state of each data
chunk. For improved fault tolerance, the state of
a data chunk is also replicated in the WMDM. If
either the IMDS or the WMDM fails, the other can
be used to reconstruct the global state. When the
IMDS receives a data chunk, it stores it in its mem-
ory cache. When the available local memory space
is exceeded, a subset of data chunks are staged to
the PSM. The set and number of data chunks to be
staged is determined based on the amount of mem-
ory needed to store the new data chunk in memory.
When an application ﬁlter instance wants to

Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)0-7695-2833-3/07 $20.00  © 2007Application Filters

A

B

6. Process document

1. READ_DOC

5. AVAILABEL_DOC

In-Memory Data Storage
 (transparent copies)

IMDS 1

IMDS 2

3. FREE_DOC_ID

2. RET_FREE_DOC

4. Retrive doc from Mobius

WMDM 1

WMDM 2

Workflow Meta-data Manger
 (transparent copies)

Persistent Storage

Mobius

Mobius

(a)

Application Filters

 A

1. GET_DEPS

2. RET_DOC_DEPS

B

3. WRITE_DOC

In-Memory Data Storage
 (transparent copies)

Workflow Meta-data Manager
 (transparent copies)

Persistent Storage

IMDS 1

IMDS 2

5. NEW_DOC_ID

8. DOC_PROCESSED

7. END_NEW_DOC_IN_CACHE_ACK

9. DOC_PROCESSED_ACK

4. BEGIN_NEW_DOC_IN_CACHE

WMDM 1

6. END_NEW_DOC_IN_CACHE

WMDM 2

If necessary, 
write some documents 
to Mobius

Mobius

Mobius

Figure 2. Communication between components. (a) Filter A reads data and (b) Filter
A writes data to Filter B.

(b)

read a data chunk from storage, a READ DOC
message (shown in Figure 2(a)) is generated and
sent to the IMDS. The IMDS asks (via mes-
sage RET FREE DOC) the WMDM for one data
chunk, related to the ﬁlter A input stream. The
WMDM searches its database for a data chunk
that is not processed.
It then returns the meta-
data of this data chunk to the IMDS via message
FREE DOC ID. Using the chunk metadata, the
IMDS is able to retrieve the data chunk either from
its local memory or from the PSM (by sending the
AVAILABLE DOC message).

After ﬁlter A instance has generated an out-
put data chunk, it can send the output data chunk
to upstream ﬁlter B. With the output data chunk,
its dependencies are also transferred. The depen-
dency information will be needed later to update
the status of the data chunks to processed. Thus,
messages (GET DEPS) and (RET DOC DEPS) in
Figure 2(b) are used to retrieve data chunk depen-
dency information from the IMDS.

When ﬁlter B in Figure 2(b) receives a data
chunk through its input stream, the data chunk
is also stored in the IMDS and the dependencies

of this chunk are updated to the processed sta-
tus. The chunk and its dependency information
are transferred to the IMDS with the WRITE DOC
message. The IMDS checks the available mem-
ory to cache this chunk. If no space is available,
some of the cached data chunks are staged to the
PSM. The IMDS then notiﬁes the WMDM that
one data chunk was created by one of the appli-
cation ﬁlters and that the chunk needs an iden-
tiﬁer to be stored. For identiﬁer creation,a Two
Phase Commit Protocol was used. This proto-
col is carried out as follows. With message BE-
GIN NEW DOC IN CACHE, the IMDS notiﬁes
the WMDM of the new data chunk. An identiﬁer is
created for the chunk and is returned to the IMDS
in message NEW DOC ID. To complete the pro-
tocol, messages 6 and 7 are also transmitted. At
the end of message exchanges, both the IMDS
and the WMDM know that the data chunk has
been stored in the IMDS with the correct identi-
ﬁer. The next step is to update the dependencies of
the data chunk to processed in the WMDM. This
is done via message DOC PROCESSED. Mes-
sage DOC PROCESSED ACK is sent back to the

Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)0-7695-2833-3/07 $20.00  © 2007IMDS to make sure the WMDM has received and
updated the status.

4.3 Detecting Faults

Our fault detection mechanism is built on the
mechanisms included in the Anthill infrastructure,
which are based on the Parallel Virtual Machine
(PVM) fault detection package. In our fault model,
we deﬁne two types of failures: process faults
and node faults. All other faults are mapped to
one of these two types. A process fault occurs,
when a process exits unexpectedly or is killed.
When a machine becomes unreachable (due to net-
work failure or machine crash), it is treated as a
node fault. Node faults are detected through time-
outs. Thus, unlike process faults, a node fault may
not be detected immediately. In our framework,
we assume any of the following components can
fail: any application ﬁlter instance, the IMDS, the
WMDM, and the PSM. The failure of an applica-
tion ﬁlter instance, or the IMDS, or the WMDM
is detected via the Anthill fault detection mecha-
nism, since these components are implemented as
Anthill ﬁlters. The PSM is considered in failed
state, when the IMDS or the WMDM tries to com-
municate with it N times without success1.

4.4 Recovering from Faults

In this section we describe how the system re-
covers from the three different points of failure:
user application ﬁlter, the IMDS and the WMDM.
We assume that these components can fail at any
time of the execution. Once one fault has occurred,
another fault cannot happen until the the system is
recovered. In our current implementation, the sys-
tem cannot recover using a saved state, if the PSM
fails. That is, the execution is stopped and initial-
ized from the beginning.

4.4.1 Application Filter Faults
When a fault occurs in an application ﬁlter, all the
application ﬁlter instances are killed by the sys-
tem and the workﬂow is restarted. This approach
is chosen because the state of the workﬂow can
be reconstructed from the dependency informa-
tion and the data chunk state information saved in
the IMDS and/or the PSM. In the recovery phase,
the system has to determine which chunks will
have to be re-processed and re-generated and when
they will be re-processed. Re-processing of data
chunks, which were generated by a ﬁlter but not

1In our implementation ,the default value of N is 10.

yet processed by the next ﬁlter, can be carried out
using one of three different approaches. The ﬁrst
is to re-process the data chunks at the end of exe-
cution of the workﬂow. Another approach is to re-
process these chunks ﬁrst, and then start process-
ing the remaining data chunks. The last approach
is to start processing new chunks and re-processing
data chunks that have been saved at the same time.
Our current implementation employs the last strat-
egy.

4.4.2 In-Memory Data Storage Faults
When an instance of the IMDS fails, the Anthill
fault mechanism restarts the failed IMDS instance.
It then sends a notiﬁcation message to all other
instances of the IMDS. The restarted IMDS in-
stance and the other IMDS instances all execute
the recovery step. This is because, during the ex-
ecution of the workﬂow, it is not guaranteed that
the metadata associated with data chunks, i.e., the
processing status of a data chunk (not processed,
being processed, processed), is stored in the same
IMDS instance, where the data chunks themselves
are stored. The IMDS instances ask the WMDM
(i.e., all of its instances) for the data chunk states
stored in the WMDM. This is done to guarantee
that in the beginning of an execution, after a fail-
ure, all data chunks stored in the IMDS instances
will have the correct status. The failed IMDS ﬁl-
ter also needs to notify the WMDM that the data
chunks that it locally maintained were lost and that
it will have to do a rollback based on data chunk
dependencies. For a data chunks that is lost, the
status of its dependencies must be updated to not
processed, so those data chunks are retrieved and
re-processed in the workﬂow.

4.4.3 Workﬂow Meta-data Manager Faults
When an instance of the WMDM fails, the Anthill
fault mechanism restarts the failed WMDM in-
stance and notiﬁes the other instances. In a fault,
all of the state of the WMDM instance is lost. The
restarted instance initializes the data chunk state
information based on the initial client query, which
deﬁned the input set of data chunks. Then, it asks
all the IMDS instances, which are assigned to this
WMDM instance, for their data chunk states. Af-
terward, it asks all the IMDS instances for the data
chunks that are processed. After this last step, the
internal state of the WMDM instance has been re-
constructed.

The next steps are executed by all the WMDM
instances. The ﬁrst step is to verify if there is any
pending transactions that need to be executed. The

Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)0-7695-2833-3/07 $20.00  © 2007Figure 3. Application workﬂow. Four different analysis operations are applied to
digitized microscopy images.

WMDM waits for the IMDS notiﬁcation saying, if
there are any transactions to be executed. The sec-
ond step is to update all the data chunks with state
“being processed” to “not processed”. These data
chunks are the data chunks that were being pro-
cessed just before the fault occurred, or that had
just been read by the IMDS, or that have been pro-
cessed but have not had their state updated to “pro-
cessed”.

5 Experimental Results

We have evaluated the performance of our
framework and the cost of recovering from faults
using a biomedical image analysis application.
The workﬂow of the application is given in Fig-
ure 3. We used a dataset consisting of 866 images,
acquired from tissue specimens from a mouse pla-
centa using a high-resolution digital microscopy
scanner. The size of the dataset is 23.94 Giga-
bytes. In our experiments, the dataset was stored
in the Persistent Storage Manager across multiple
nodes in the system. The experiments were run
on a cluster of 20 PCs, which are connected us-
ing a Fast Ethernet Switch. Each node has a AMD
Athlon(tm) Processor 3200+ and 2 GB main mem-
ory and runs Linux 2.6 as the operating system. In
the experiments one IMDS instance was instanti-
ated on each cluster node and one WMDM on one
of the nodes.

The ﬁrst set of experiments evaluated the sys-
tem’s performance, when partial results from a
stage are saved in the system, as the number of
nodes is scaled. In Figure 4, the execution time
of the last stage of the workﬂow is shown, when
the partial results form the “Color Classiﬁcation”
ﬁlter are saved or not saved in the system. As is
seen form the ﬁgure, the execution time decreases
as more nodes are added for application execution.
In addition, the overhead is very small and about
6.7% of the total execution time on average.

To evaluate the cost of fault detection and re-
covery, the following types of faults were intro-

Execution Time − Color Classification and Tissue Segmentation (866 images)
 60000

Saving intermediate state
Not saving intermediate state

i

)
c
e
s
(
 
e
m
T
 
n
o
i
t
u
c
e
x
E

 50000

 40000

 30000

 20000

 10000

 0

 2

 4

 8

 12

 16

 20

Number of Nodes

Figure 4. Performance results for the
color classiﬁcation and tissue seg 
mentation stages.

duced during execution: (1) one instance of the ap-
plication’s ﬁlters is killed around halfway through
the experiment. (2) one ﬁlter instance is killed in
the ﬁrst part of the experiment and other in the
second part of the experiment.
(3) The experi-
ment is divided in three parts and one ﬁlter in-
stance is killed in each of them. (4) The IMDS or
the WMDM is killed around the halfway through
the experiment. Figure 5 shows the increase in the
execution time when each of the fault types are in-
troduced in the application.
In the charts, “Test
Time” denotes the total execution time and “Pro-
cessing Function” is the time spent in processing
data in the process function of the ﬁlter. The pro-
cessing function time increases when the system
recovers from faults, as some of the data has to be
reprocessed. Another factor contributing to the in-
crease in the total execution time is the fact that all
the application ﬁlters have to be restarted and the
data lost during fault should be read again. How-
ever, as the charts show, the cost of recovery is low
when compared to the overall execution time.

6 Conclusions

This paper proposed a framework to incorpo-
rate fault detection and recovery in workﬂows that
process large volumes of data in a pipelined fash-

Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)0-7695-2833-3/07 $20.00  © 2007Fault Type vs. Execution Time (8 nodes - FGBG)

Test Time
deviation
Processing Function
deviation

no

1 app

IMDS

WMDM

2 appl

3 appl

Fault Type

(a)

Fault Type vs. Execution Time (8 nodes - Hist. Norm.)

Test Time
deviation
Processing Function
deviation

)
s
d
n
o
c
e
s
(
 

i

e
m
T
n
o

 

i
t

u
c
e
x
E

)
s
d
n
o
c
e
s
(
 

i

e
m
T
n
o

 

i
t

u
c
e
x
E

 1800

 1600

 1400

 1200

 1000

 800

 600

 400

 200

 0

 3000

 2500

 2000

 1500

 1000

 500

 0

no

1 app

IMDS

WMDM

2 appl

3 appl

Fault Type

(b)

Figure 5. Performance of the system
when faults are introduced.

ion. The framework employs a “message logging”
approach and implements a distributed mechanism
to maintain intermediate results and messages ex-
changed among application components. The ex-
perimental evaluation shows that it introduces lit-
tle overhead, scales well, and can enable fast re-
covery from certain types of faults. This makes
the framework a viable platform for long running,
data-intensive workﬂows.

References

[1] M. D. Beynon, T. Kurc, U. Catalyurek, C. Chang,
A. Sussman, and J. Saltz. Distributed processing of
very large datasets with datacutter. Parallel Com-
put., 27(11):1457–1478, 2001.

[2] S. Bowers, B. Ludascher, A. H. Ngu, and
T. Critchlow. Enabling scientiﬁc workﬂow reuse
through structured composition of dataﬂow and
In Proceedings of IEEE Workshop
control-ﬂow.
on Workﬂow and Data Flow for Scientiﬁc Appli-
cations (SciFlow 2006), Atlanta, GA, April 2006.

[3] R. A. Ferreira, W. M. Jr., D. Guedes, L. M. A.
Drummond, B. Coutinho, G. Teodoro, T. Tavares,
R. Arajo, and G. T. Ferreira. Anthill: A scal-
able run-time environment for data mining appli-
cations. In 17th International Symposium on Com-
puter Architecture and High Peformance Comput-
ing, Rio de Janeiro, RJ, 2005.

[4] S. Hastings, S. Langella, S. Oster, and J. Saltz.
Distributed data management and integration: The
mobius project. In Global Grid Forum Semantic
Grid Applications Wkshp, Jun. 2004.

[5] S. Hastings, M. Ribeiro, S. Langella, S. Oster,
U. Catalyurek, T. Pan, K. Huang, R. Ferreira,
J. Saltz, and T. Kurc. Xml database support for
distributed execution of data-intensive scientiﬁc
workﬂows. SIGMOD Record, 34, 2005.

[6] G. Kola, T. Kosar, J. Frey, M. Livny, R. J. Brunner,
and M. Remijan. Disc: A system for distributed
data intensive scientiﬁc computing. In Proceeding
of the First Workshop on Real, Large Distributed
Systems (WORLDS’04), San Francisco, CA, De-
cember 2004.

[7] G. Kola, T. Kosar, and M. Livny. Phoenix: Mak-
ing data-intensive grid applications fault-tolerant.
In Proceedings of 5th IEEE/ACM International
Workshop on Grid Computing, 2004.

[8] G. Kola, T. Kosar, and M. Livny. Faults in large
distributed systems and what we can do about
In Proceedings of 11th European Confer-
them.
ence on Parallel Processing (Euro-Par 2005), Lis-
bon, Portugal, August 2005.

[9] B. Lud¨ascher, I. Altintas, C. Berkley, D. Higgins,
E. Jaeger-Frank, M. Jones, E. Lee, J. Tao, and
Y. Zhao. Scientiﬁc workﬂow management and the
Kepler system. Concurrency and Computation:
Practice & Experience, Special Issue on Scientiﬁc
Workﬂows, 18(10):1039–1065, 2005.

[10] O. Tatebe, Y. Morita, S. Matsuoka, N. Soda, and
S. Sekiguchi. Grid datafarm architecture for petas-
cale data intensive computing. In 2nd IEEE/ACM
International Symposium on Cluster Computing
and the Grid (CCGrid), 2002.

[11] G. Teodoro, T. Tavares, R. Ferreira, T. Kurc,
W. Meira, D. Guedes, T. Pan, and J. Saltz. Run-
time support for efﬁcient execution of scientiﬁc
In In-
workﬂows on distributed environmments.
ternational Symposium on Computer Architecture
and High Performance Computing, Ouro Preto,
Brazil, October 2006.

[12] M. Treaster. A survey of fault-tolerance and fault-
recovery techniques in parallel systems. Dec.
2005.

Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)0-7695-2833-3/07 $20.00  © 2007