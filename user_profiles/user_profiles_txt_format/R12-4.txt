IEEE International Conference on Data Engineering
IEEE International Conference on Data Engineering

Processing Group Nearest Group Query

Ke Deng 1, Hu Xu 2, Shazia Sadiq 3,Yansheng Lu 4,Gabriel Pui Cheong Fung 5,Heng Tao Shen 6

The University of Queensland, Australia

{dengke1,shazia3,pcfung5,htshen6}@itee.uq.edu.au

Huazhong University of Science and Technology, China
hxu2@smail.hust.edu.cn, lys4@mail.hust.edu.cn

Abstract— Given a data point set D, a query point set Q
and an integer k, the Group Nearest Group (GNG) query
ﬁnds a subset of points from D, ω (|ω| ≤ k), such that the
total distance from all points in Q to the nearest point in
ω is no greater than any other subset of points in D, ω(cid:2)
(|ω(cid:2)| ≤ k). GNG query can be found in many real applications.
In this paper, Exhaustive Hierarchical Combination algorithm
(EHC) and Subset Hierarchial Reﬁnement algorithm (SHR) are
developed for GNG query processing. The superiority of SHR in
terms of efﬁciency and quality compared to existing algorithms
developed originally for data clustering is demonstrated.

I. INTRODUCTION

(cid:2)

This paper studies a new query type, named Group Nearest
Group (GNG) query, which can be regarded as a generic
version of the Group Nearest Neighbor (GNN) query [1].
While a GNN query retrieves a single point in D, a GNG
query aims to ﬁnd a subset ω from D such that |ω| ≤ k
q∈Q d(q, ω) is minimized where d(q, ω) is the shortest
and
distance from q to ω (i.e., the Euclidean distance from q to
the nearest point in ω). GNG query has many applications in
business analysis and decision support systems. For example,
when planning a meeting for a group of people, a venue can
be found from available locations by issuing a GNN query.
With the maturity of teleconference techniques, people can
aggregate to several convenient locations in order to further
reduce the traveling cost. Due to the consideration such
as the limit of budget or adminstration cost, the maximum
number of venues k is usually limited. Under this constraint,
a GNG query can be used to ﬁnd a set of venues such that
the total traveling distance for people to their nearest venues
is minimal. When k = 1, a GNG query is reduced to a GNN
query.

a data point in D (a location available to be a conference venue)
a query point in Q (the location of a person)

p4

p1

p2

p3

(a)

p1

p3

(b)

p2

Fig. 1. Two examples of GNG query when k = 3.

(cid:2)

(cid:2)

Given a set of data points D, a query point set Q and an
integer value k (1 ≤ k ≤ min{|D|, |Q|}), a GNG query ﬁnds
a subset of points ω from D (|ω| ≤ k), such that
q∈Q d(q, ω)
q∈Q d(q, ω(cid:3)) for every ω(cid:3) ∈ P(S) (|ω(cid:3)| ≤ k), where
≤
P(S) is the power set of D. A GNG query may return k
points as shown in Figure 1(a). In some cases, a GNG query
may return a solution with less than k points. For example
in Figure 1(b), p1 is the nearest location for all persons. No
matter what the setting of k is, the minimal total traveling
distance is the distance from all persons to p1. If we are
aware of the cardinality of the solution, say k(cid:3), processing
GNG query is to ﬁnd the subset of k(cid:3) points that produces
the minimum total traveling distance. Unfortunately, k(cid:3) is not
known in advance when processing GNG query. To avoid
evaluating all the subsets of 1, .., k points, we observe that
the subset of k points producing the minimum total traveling
distance must contain the solution. For example in Figure 1(b),
{p1, p2, p3} produces the minimum total traveling distance
but {p4, p2, p3} not. The solution may be contained in many
subsets of k points. Processing GNG query just needs to ﬁnd
one of them. In the obtained subset, the ﬁnal solution of GNG
query can be easily found by dropping those points to which
no query point is grouped. For example, p1 is the ﬁnal solution
by dropping p2, p3 from the subset {p1, p2, p3} in Figure 1(b).
Thus, processing GNG query is to ﬁnd one subset of k points
in D that produces the minimum total distance. This is the
focus of this paper.

In this paper, Exhaustive Hierarchical Combination algo-
rithm (EHC) and Subset Hierarchical Replacement algorithm
(SHR) are proposed for GNG query processing. In order to
optimize the performance, the data points are hierarchically
represented by blocks, e.g., using R-tree. We process GNG
query by treating the blocks as data points to ﬁnd an inter-
mediate solution in higher hierarchical level ﬁrst. To reﬁne
the solution, the search space in lower hierarchical level is
minimized by following the guided search direction. In EHC,
every set of k blocks is evaluated in high hierarchical level
and the set with the current best value (i.e., the minimum
total distance) are reﬁned by visiting their children in next
level. Different from EHC, SHR approaches the solution as
optimal as possible by iteratively replacing elements in an
initial subset. In higher hierarchical level, each block is treated
as a data point by SHR to replace every element in the subset
and the resultant subset with the current best value is reﬁned

1084-4627/09 $25.00 © 2009 IEEE
1084-4627/09 $25.00 © 2009 IEEE
DOI 10.1109/ICDE.2009.186
DOI 10.1109/ICDE.2009.186

1144
1144

by visiting the children of the block. EHC is capable to provide
the optimal solution. However, the performance of EHC drops
down dramatically when k is greater than 2. In contrast, SHR
is linear with k. SHR is inspired by k-medoids clustering
algorithms PAM [2] and CLARANS [3]. Both PAM and
CLARANS can be used to process GNG queries. Compared
to PAM, SHR provides solution of same quality while the
performance is better by 2-3 orders of magnitude. Compared
to CLARANS, solution using SHR is the best in terms of
quality, and SHR outperforms CLARANS by several times
in most cases. While two dimensional space is discussed, the
proposed algorithms can be applied in multiple dimensional
space with support of any hierarchical data structure.

II. EXHAUSTIVE HIERARCHICAL COMBINATION

ALGORITHM (EHC)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

The notations used in this paper are summarized in Table I.
In EHC, a GNG query is processed by traversing the R-tree
of D. At the root, every set of k entries are combined to
form a set of subsets {ω1, .., ωn}. For a query point q ∈ Q
to an entry E in a subset ω, the lower bound distance is
mindist(q, E) and upper bound distance is maxdist(q, E).
The points in Q can be grouped according to lower bound
distances and upper bound distances separately. As a con-
q∈Q mindist(q, ω)
sequence, ω gets two distance sums,
q∈Q maxdist(q, ω). Clearly, any subset of k child leaf
and
nodes of ω (i.e., the leaf nodes under the entries in ω) must
q∈Q mindist(q, ω) and
have a distance sum in between
(cid:2)
q∈Q mindist(q, ω)
ub.
lb associated
as
with ω is denoted as
ub. After all subsets
{ω1, .., ωn} compute
ub, they are inserted into a
ub in H is assigned to a threshold
heap H. The minimum
lb ≥ τ , it
τ . If a subset ω in H has lower bound distance
can be safely pruned from H. That is, the child leaf nodes of
ω cannot compose a subset of size k with a distance sum less
than τ . For the remaining subsets in H, following the best-ﬁrst
traverse fashion, the ω with the minimum
lb is selected and
the direct child nodes of ω (i.e., the nodes directly referred by
entries in ω) are visited to compose a set of new subsets. The
(cid:2)
ub are computed for each of these new subsets.
After inserting them into H, the current minimum
ub in
H is used to update the threshold τ . Any subset in H with

q∈Q maxdist(q, ω). Thus, we denote
(cid:2)
q∈Q maxdist(q, ω) as
lb and so does
lb and
(cid:2)

(cid:2)
(cid:2)
(cid:2)ω

(cid:2)ω
(cid:2)

lb and

lb and

(cid:2)ω

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

TABLE I

SUMMARY OF NOTATIONS

Notation

Explanation

D
Q
τ
ω

d(q, ω)

mindist(q, E)

maxdist(q, E)

a data point set (e.g., conference venues)
a query point set (e.g., people)
a threshold
a subset of k points in D
the minimum Euclidean distance from
a point q to ω
the minimum Euclidean distance from
a point q to an entry E in a node of R-tree
the maximum Euclidean distance from
a point q to an entry E in a node of R-tree

(cid:2)

(cid:2)

lb ≥ τ can be safely pruned. These operations are repeated
lb in H is associated with a subset of
until the minimum
ub). This subset is returned
leaf nodes (in this case
by EHC. For description clarity, the subsets are inserted into
heap and then pruned. But the pruning can also take place
before inserting subsets into H.

lb =

(cid:2)

(cid:2)

E1

0
3
21

14
17
37

E2

13
15
4

25
27
15

E3

29
27
25

33
31
29

E4

16
7
28

19
14
31

q1
q2
q3

p5
N2

q3

p4

p6

N0 (root)
E1 E2 E3 E4

p1

N1

p3

p2

q1

q2

N1
p1 p2 p3
N2
p4 p5 p6

N4

p9 p10

N3
p7 p8

p9

N4

p10

p7

N3

p8

N0

Query points

Data points

Heap

{7,46,E1,E2} {24,60,E1,E3} {24,59,E1,E4} {24,48,E2,E4}

{32,67,E2,E3} {48,62,E3,E4}

lower and upper bound of distance sum associated with {E1, E2}

T

46

Fig. 2. An example of GNG query processing using EHC (k=2).

Figure 2 shows an example of EHC. The entries in the root
are combined into subsets of size k = 2 and these subsets are
ub in H is 46. Using
inserted into a heap. The minimum
τ = 46 as the threshold, the subset {E3, E4} can be pruned
from the heap. Then {E1, E2} are replaced by all possible
subsets of size 2 composed by direct child nodes of E1, E2.

(cid:2)

III. SUBSET HIERARCHIAL REFINEMENT ALGORITHM

(SHR)

EHC is workable only for small k even though the
optimization is applied. In this section, we describe the Subset
Hierarchial Reﬁnement algorithm (SHR). It can provide a
high quality solution while the performance is linear with k,
D and Q. The time complexity is O(k(|D| − k)|Q|). SHR is
inspired by PAM [2] and CLARANS [3] clustering algorithm.
SHR works basically as follows: ﬁrst, an initial subset ω is
identiﬁed; then, SHR tries to reﬁne ω by tentatively replacing
elements in ω by points in D − ω until the total distance
cannot be reduced further. The reﬁnement
is an iterative
process. Before going to the details of SHR, we ﬁrst propose
two pruning rules which explore the proximity of the query
points to prune the irrelevant subsets.

Initial Candidate Pruning (ICP): Given D, Q and a subset
ω ⊂ D, we tentatively use points in D − ω to replace the
points in ω in order to obtain an updated ω which produces a
smaller total distance. During this process, a point p(cid:3) ∈ D − ω
can be pruned if d(q, p(cid:3)) ≥ d(q, ω) for ∀q ∈ Q.
Further Candidate Pruning (FCP): Given D, Q and a subset
ω ⊂ D, we tentatively use points in D − ω to replace a point
p ∈ ω in order to obtain an updated ω which produces a
smaller total distance. During this process, p(cid:3) ∈ D − ω can be
pruned if the resultant total distance cannot be reduced. (cid:2)

1145
1145

Note that the rule ICP and FCP are applicable if p(cid:3) is an
entry of an R-tree node (where the distance to an entry is the
minimum distance to this entry). The query processing starts
by selecting an initial subset ωini. We can arbitrarily select k
points from D as ωini. However, if ωini produces a smaller
total distance, fewer replacements are required to reach the
ﬁnal solution and thus a better overall performance. In this
work, we employ an approach based on k-means algorithm
which considers the distribution of both Q and D. First, Q
is clustered using k-means algorithm. Then, for each obtained
center (i.e., mean), the nearest neighbors from D are retrieved
as members of ωini until there are k distinct points in ωini.

(cid:2)

Let the current ω (ωcur) be ωini. For the initial ωcur, SHR
q∈Q(q, ωcur). Then we try to
computes a threshold τ =
ﬁnd a better solution by replacing points in ωcur by points in
D − ωcur with support of R-tree. This is an iterative process.
At the root N for the ﬁrst iteration, we tentatively replace each
p ∈ ωcur by every entry E ∈ N and compute the resultant
total distance sum. For an entry E and a point p ∈ ωcur,
only if the resultant sum < τ , we record {sum, p, E} in a
heap H (empty initially); otherwise, this replacement can be
safely pruned according to the pruning rules. After processing
tentative replacements between all entries in N and all points
in ωcur, if H is empty, SHR terminates by returning ωcur since
no replacement can further reduce the total distance; else if
H is not empty, the element h ∈ H with the minimum sum
is removed from H for further processing. For the h with
h.E referring to a non-leaf node, the entries in this node are
processed in the similar way as we process the root. The only
difference is that these entries only tentatively replace with h.p
rather than all p ∈ ωcur. For h with h.E referring to a leaf
node, say p(cid:3), the ﬁrst iteration is done by replacing h.p with
p(cid:3) in ωcur and resetting τ = h.sum. In the next iteration, the
R-tree is browsed again with the updated ωcur and τ . SHR
terminates until no further reduction of the total distance is
possible as discussed above.

As a further step to cut off the tentative replacements, τ
can be updated during the R-tree traverse within one iteration.
It is similar to the method used in EHC. When tentatively
replacing a point p ∈ ωcur with an entry E, we can also
group query points based on their maximum distances to E.
The consequential total distance is the upper bound of the total
distance of the subset which is obtained by replacing p with
any data point under E in R-tree. It is denoted as
ub. If
(cid:2)
ub. For an element h ∈ H, it can

ub < τ , τ is updated to

(cid:2)

(cid:2)

be pruned safely pruned if h.sum ≥ τ .

A. Analysis

1) SHR Versus PAM and CLARANS: k-medoids algorithms
PAM and CLARANS can be used to process GNG queries.
Using PAM, k centers are randomly selected in the ﬁrst step to
form the initial set of medoids, say ω. In each iteration of the
second step, PAM performs following operations. Tentatively
replacing each current medoid p ∈ ω by every point p(cid:3) =
D − ω, the cluster quality improvement is computed, e.g.,
the reduction of the distance sum. Among all such tentative

the one with the maximum improvement

is
replacements,
carried out. In next iteration, PAM repeats this operation until
the cluster quality cannot be improved any further. The total
complexity of PAM in one iteration is O(k(|D| − k)2). For a
large D, such computation becomes costly. If the initial subset
is the same, it is clear SHR and PAM returns the identical
solution. However, compared to PAM, SHR is much more
efﬁcient by avoiding unnecessary tentative replacements with
support of R-tree and pruning rules.

In CLARANS, one randomly selected point in ωcur is tenta-
tively replaced by one randomly selected point in D − ωcur. If
the total distance can be reduced, ωcur is updated by carrying
out this replacement immediately. For the updated ωcur, if no
better solution is found after maxneighbor attempts, a local
optimal solution is assumed to be reached. From numlocal
local optimal solutions, the global solution is returned. By
setting a higher value of maxneighbor, the quality of solution
can be improved to approach the quality level of PAM and
SHR. In order to compare SHR with CLARANS in terms of
performance, suppose SHR and CLARANS have a common
initial subset ω. In the best case of CLARANS, ω is the ﬁnal
solution such that no better point in D − ω can be found
by conducting maxneighbor tentative replacements. Then,
CLARANS returns ω. As suggested in [3], maxneighbor is
1.25% of the number of the possible replacements between
all points in D and all points in ω, and thus the subsets
evaluated by CLARANS is 1.25%*k(|D|−k) in the best case.
In SHR, each node in high level of R-tree tentatively replaces
all p ∈ ω and, following the best-ﬁrst traverse fashion, we visit
the children of the node by replacing which the total distance
can be reduced the most. The total subsets evaluated by SHR
is m ∗ n where m is the number of nodes visited and n is
the fan-out of node. n is a constant in a given R-tree and the
value is 50 in our experiments. In the best case of SHR, ω is
the ﬁnal solution such that m is logn |D|. For a medium size
D (|D| = 20k in the P P data set used in our experiments),
m∗n in SHR is smaller than 1.25%*k(|D|−k) in CLARANS
by several times in the best case. The larger is D, the better
is the relative performance of SHR compared to CLARANS.
In general cases, SHR is deliberately designed to always
reﬁne the currently most promising subset when traversing the
index structure while CLARANS randomly select a subset to
process. Thus, we expect the performance of SHR is usually
better than CLARANS.

2) Q Region and Pruning rules: The distribution region of
Q impacts the pruning rules. In general, if Q is distributed in
a relatively small region compared to the distribution region
of D,
the pruning rule ICP works very well. As shown
in the example in Figure 3(a), the MBR (Minimum Bound
Rectangle) of Q is around 10% of MBR of D. The data points
outside the grey area are pruned by ICP, such as p3, p4, p5. In
Figure 3(b), the MBR of Q is around 45% of MBR of D. The
grey area increases signiﬁcantly and the points can be pruned
by ICP is none in this example.

For the points cannot be pruned by ICP, the pruning rule
FCP is applied. Given the current subset ω, when tentatively

1146
1146

)

%

(
 
y
c
a
r
u
c
c
a

1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
UN2

 

s
e
s
s
e
c
c
a
 
e
g
a
p
 
e
g
a
r
e
v
a

105

104

103

102

101

100

 

{p 1, p 2} is the current subset
p 4

M B R of D

p 3

p 1

q 1
q 3

q 2

p 2

q 4

M B R  of Q

p 5

M BR  of D

M B R  of Q

q 1

p 3

p 1

q 3

p 4
q 2

p 2

q 4

p 5

(a )
Fig. 3. Q region and pruning rules (k=2)

(b)

replacing a point p ∈ ω by p(cid:3) ∈ D − ω, p(cid:3) is pruned according
to FCP if the resultant total distance cannot be reduced. In the
case of large MBR of Q, the point p(cid:3) ∈ D − ω not pruned
by ICP may be pruned by FCP when tentatively replacing
a point p ∈ ω. For example, the point p5 pruned by ICP
in Figure 3(a) can be pruned by FCP in Figure 3(b) when
tentatively replacing p1 by p5. That is, even though ICP loses
pruning ability in case of large MBR of Q, FCP pruning rule
still works effectively.

IV. EXPERIMENTS

the North America,

The experiments are conducted using a PC (Intel Core2
6600 CPU 2.4GHz, 2GB memory). Four data sets are tested,
(i) a real data set P P with 24493 points is obtained
from (www.rtreepotral.org) which consists of the populated
(ii) a real data set N E
places of
(www.rtreepotral.org), containing 123593 postal addresses
(points) (New York, Philadelphia and Boston), (iii) a synthetic
data set U N 1 with 6 × 105 uniformly distributed points, and
(iv) a synthetic data set U N 2 with 500 uniformly distributed
points. These data sets are uniformed into a unit region. Q
is distributed in an area whose MBR is a percentage of the
whole data space, denoted as M . The disk page is set to
1024 Bytes and a 1M Bytes buffer is used. All the data sets
are indexed by R-trees for EHC and SHR. For PAM and
CLARANS, these data sets are stored on a number of disk
pages. We implement the PAM and CLARANS algorithms
and the k-means algorithm [4], [5]. To make a fair comparison
for SHR, PAM and CLARANS, a common initial subset is
computed using the k-means approach as discussed in III.
For CLARANS, the ratio of maxneighbor is set to 1.25%
as suggested in [3]. The experimental results reported are the
average of 100 queries. The default settings of experiments
are Q = 64, k = 6, M = 10%, and D = P P .

In this experiment, the quality of solution returned by EHC,
SHR, PAM, CLARANS and the k-means approach (that is,
the initial subset) are examined by comparing the average
total distances. GNG query aims to ﬁnd a subset of points
in D which produces the minimum total distance. For GNG
query processing algorithms, the solution producing a smaller
average total distance has a higher quality. Since EHC is only
workable for small k, we use PAM not EHC as the baseline to
normalize the quality, i.e., the total distance returned by any
algorithm is divided by the total distance returned by PAM.
Figure 4(a) demonstrates the experimental results by setting

k = 2 on four data sets, and Figure 4(b) demonstrates the
experimental results by setting k from 2 to 10 on the data set
P P . The quality of SHR demonstrated is same as that of PAM
and is quite close to the optimal solution returned by EHC.

EHC

KMEAN
 

CLARANS

SHR

PP

UN1

NE

2

4

8

10

|D|

(a) accuracy vs. |D|

6
k

(b) accuracy vs. k

Fig. 4. Effect of |D| and k (|Q|=64, M =10%)

The number of disk page accesses and the CPU cost are
compared between SHR, PAM, EHC and CLARANS while
k increases. When k is greater than 2, EHC costs too much
and thus is omitted. SHR demonstrates a consistently better
performance in all settings.

EHC

CLARANS

PAM

SHR
 

)

%

(
 
y
c
a
r
u
c
c
a

1.4
1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

)
c
e
s
(
 
t
s
o
c
 
U
P
C

104

103

102

101

100

10−1

2

10

20

30

40

50

2

10

20

30

40

50

k

k

(a) PA vs. k (PP)
Fig. 5. Effect of k (|Q| = 64, M = 10%)

(b) CPU vs. k (PP)

V. CONCLUSION

Two GNG query processing algorithms, Exhaustive
Hierarchical Combination algorithm (EHC) and Subset
Hierarchial Reﬁnement (SHR), are proposed. EHC is capable
to provide the optimal solution even though it is workable
when k is small. The other important application of GNG
query is data quality management such as similarity join where
the maximum matching records in one date set is constrained.

ACKNOWLEDGMENT

The work reported in this paper is supported by grants
DP0663272 and DP0773122 from the Australian Research
Council.

REFERENCES

[1] D. Papadias, Q. Shen, Y. Tao, and K. Mouratids, “Group nearest neighbor

queries,” ICDE, 2004.

[2] L. Kaufman and P. Rousseeuw, “Finging group in data: an introduction

to cluster analysis,” John Wiley & Sons, 1990.

[3] R. Ng and J. Han, “Efﬁcient and effective clustering method for spatial

data mining,” VLDB, 1994.

[4] I. Dhillon and D. Modha, “A data-clustering algorithm on distributed
memory multiprocessors,” Large-Scale Parallel Data Mining, pp. 245–
260, 1999.

[5] J. Macqueen, “Some methods for classiﬁcation and analysis of multi-
variate observations,” Fifth Berkeley Symposium on Math Statistics and
Probability, pp. 281–297, 1967.

1147
1147

