Security investment (failures) in ﬁve economic environments:
A comparison of homogeneous and heterogeneous user agents ∗

Jens Grossklagsa

Nicolas Christinb

John Chuanga

aSchool of Information

University of California, Berkeley

Berkeley, CA 94720

{jensg,chuang}@ischool.berkeley.edu

bCyLab Japan and Information Networking Institute

Carnegie Mellon University

1-3-3-17 Higashikawasaki-cho, Chuo-ku

Kobe, Hyogo 650-0044, Japan

nicolasc@cmu.edu

Working draft: Id: paper.tex 175 2008-03-04 02:30:53Z jensg

Abstract

Security interactions in networked systems, and the associated user choices, due to their complex-
ity, are notoriously difﬁcult to predict, and sometimes even harder to rationalize. We argue that users
often underestimate the strong mutual dependence between their security strategies and the economic
environment (e.g., threat model) in which these choices are made and evaluated. This misunderstanding
weakens the effectiveness of users’ security investments.

We study how economic agents invest into security in ﬁve different economic environments, that
are characteristic of different threat models. We consider generalized models of traditional public
goods games (e.g., total effort and weakest link) and two recently proposed games (e.g., weakest target
game). Agents may split their contributions between a public good (protection) and a private good (self-
insurance). We can examine how incentives may shift between investment in a public good (protection)
and a private good (insurance), subject to factors such as network size, type of attack, loss probability,
loss magnitude, and cost of technology. We compare Nash equilibrium predictions for homogeneous
and heterogeneous user populations. We also provide results for the social optima for different classes
of attacks and defenses in the case of homogeneous agents.

∗This is a working paper in preparation for archival journal submissions and represents a subset of results from an ongoing
project on security economics. Some of the preliminary results on the homogeneous case presented here have been published in
[21]. Results on the case of heterogeneous agents are available also in [22]. We hope to further improve this draft with the feedback
of the workshop participants.

1

Introduction

The Internet has opened new and attractive channels to publicize and market products, to communicate
with friends and colleagues, and to access information from spatially distributed resources. Though it has
grown signiﬁcantly, the network’s architecture still reﬂects the cooperative spirit of its original designers
[37]. Unfortunately, today’s network users are no longer held together by that same sense of camaraderie
and common purpose. For instance, concrete evidence of the tragedy of the commons [23] occurring in peer-
to-peer ﬁlesharing networks has been documented for a long time [2]. Accordingly, studies of networking
protocols and user interaction have been assuming users to be selﬁsh and to act strategically [43].

Selﬁsh users are one thing, but the expansion of the Internet has also attracted individuals and groups
with often destructive motivations; these “attackers” intend to improve on their perceived utility by ex-
ploiting or creating security weaknesses and harming or inconveniencing other network users [46]. Some
malicious entities are motivated by peer recognition, or curiosity, and are often undecided regarding the ethi-
cal legitimacy of their behavior [19, 20]. Others have clearly demonstrated ﬁnancial goals [15]. Problematic
behaviors and threats include attacks on the network as a whole, attacks on selected end-points, undesir-
able forms of interactions such as spam e-mail, and annoyances such as Web pages that are unavailable or
defaced. As a result, users cannot rely and trust other network participants [12].

When asked in surveys, network users say they are interested in preventing attacks and mitigating the
damages from computer and information security breaches [1]. Researchers and industry have responded
by developing numerous security technologies to alleviate many of the aforementioned problems [4, 41],
which is expected to help improving individual security practices.

Nevertheless, security breaches are common, widespread and highly damaging. The “I Love You” virus
[30], Code Red [32] and Slammer worms [31], to cite the most famous cases, have infected hundreds of
thousands of machines and caused, all together, billions of dollars in damages. This high ﬁnancial impact is
explained by recent surveys [6, 8], which shows strong evidence that comprehensive security precautions,
be they patching, spyware-removal tools, or even sound backup strategies, are completely amiss from a vast
majority of systems surveyed.

In other words, despite a self-professed interest in security, most individuals do not implement any
security on their systems, even though security technology is (by and large) readily available. We propose
to investigate the root causes of the disconnect between users’ actions and their intentions.

In practice, there is a large variety of situations in which users face security threats, and an equally
large number of possible responses to threats. However, we postulate in this paper that one can model most
security interactions through a handful of “security games,” and with a small number of decision parameters
upon which each user can act.

More precisely, building upon public goods literature [26, 49], we consider the classical best shot, total
effort, and weakest link games, and will analyze them in a security context. We complement these three
games with a novel model, called the “weakest target” game, which allows us to describe a whole class of
attacks ranging from insider threats to very aggressive worms. Furthermore, while most research on the

2

economics of security focuses on security investments as a problem with a single variable (e.g., amount of
money spent on security), our analysis is the ﬁrst security study to decouple protection investments (e.g.,
setting up a ﬁrewall) from self-insurance coverage (e.g., archiving data as back up). This decoupling allows
us to explain a number of inefﬁciencies in the observed user behaviors.

This paper is only a ﬁrst step toward a more comprehensive modeling of user attitudes toward security
issues. Indeed, the present study relies on game theory, mostly using Nash equilibrium and social optima
concepts. As such, we primarily view this study as a theoretical basis for follow up experimental work using
laboratory experiments with human participants. We nevertheless show that the models and results derived
here allow to gain considerable insights.

The rest of this paper is organized as follows. We elaborate in Section 2 the relationship of our work
with related research, and discuss some important security consequences that results from homogeneous and
heterogeneous user agent populations. In Section 3 we introduce our game-theoretic models. We present
an analysis of the Nash equilibria in the homogeneous user case in (Section 4) and for a heterogeneous user
base in (Section 5). We discuss differences between the individually rational solution and the social optima
determined by a social planner in (Section 6) for all of these games, however, we defer the analysis of the
heterogenous case for a revised version of this paper. We discuss our ﬁndings in Section 7. We conclude in
Section 8.

2 Related Work

The economics of information security is a growing research area with a diverse set of participating re-
searchers from various disciplines. Important common anchors are the observations that misaligned incen-
tives and positive and negative externalities play signiﬁcant roles in the strategies used by each party in the
battle between attackers and potential victims [4, 5].

Economics as a tool for security analysis has gained in importance since the economy of attackers has
become increasingly rational (e.g., motivated by greed), over the last years [15]. This increasingly rational
behavior stands in contrast to that exhibited by the hacker communities of the 1980s and 1990s, who valued
reputation, intellectual achievement, and even entertainment above ﬁnancial incentives [19, 20].

Most of the initial results obtained in security economics research concern the analysis of optimal se-
curity investments. For example, Gordon and Loeb [18] as well as Hausken [25] focus on the impact of
different security breach functions and degrees of vulnerability on an entity’s investment strategy. More
specialized models have been proposed to analyze in more depth a subset of important security management
problems. For instance, August and Tunca [7] scrutinize optimal system update strategies when patching a
system against security vulnerabilities is costly. Rescorla investigates the impact of code quality control on
vulnerability of software [36].

From a policy standpoint, Bull et al. [9] observe the state of heterogeneous networks and argue that no
single security policy will be applicable to all circumstances. They argue that, for a system to be viable
from a security standpoint, individuals need to be empowered to control their own resources and to make

3

customized security trade-offs. This stands in contrast to the traditional centralized structure where all se-
curity decision are made by a central planner (e.g., the IT department). Nevertheless, as Anderson suggests,
organizational and structural dependencies have to be considered in individual security decision making [3].
While many models prescribe behavior in individual choice situations, the focus of our work is to model
and study strategic interaction with respect to security decisions in networked systems, in an effort to under-
stand the impact of individual choices on a larger group. Such interaction usually involves common as well
as conﬂicting interests. (Pure conﬂict, in which the interests of the two antagonists are completely opposed,
is a special case.) This mutual dependence as well as opposition guarantees for a much richer scenario for
analysis [40].

In the context of mutual dependence, Varian [49] introduces the analysis of system reliability within a
public good framework. He discusses the best effort, weakest link and total effort games, as originally ana-
lyzed by Hirshleifer [26]. The main difference from classical public goods theory is that within the frame-
work of computer reliability “considerations of costs, beneﬁts, and probability of failure become paramount,
with income effects being a secondary concern.”[49] Varian focuses on two-player games with heteroge-
neous effort costs and beneﬁts from reliability.1 He also adds an inquiry into the role of taxes and ﬁnes, and
differences between simultaneous and sequential moves.

Our work generalizes [49] in several aspects. First, instead of considering security decisions to be
determined by a single “security” variable, we identify two key components of a security strategy: self-
protection (e.g., patching system vulnerabilities) and self-insurance (e.g., having good backups). More
precisely, we allow agents to self-protect and/or self-insure their resources in N -player games. We also
contrast the three canonical games discussed by Varian with two more complex “weakest target” games
that represent a more complicated incentive structure, which we believe applies to a whole class of security
issues.

Outside the information security context, the dual role of self-protection and self-insurance was ﬁrst
recognized by [14]. To provide a more precise deﬁnition, self-protection stands for the ability to reduce the
probability of a loss – for example, by installing a ﬁrewall application which limits the amount of trafﬁc
allowed to communicate with one’s network. Self-insurance, on the other hand, denotes a reduction in the
magnitude of a loss, e.g., by performing regular backups on existing data. Some technologies and practices
such as disconnecting a computer from a network do both. Ehrlich and Becker [14] focus in their analysis on
the comparison of self-protection and self-insurance to market insurance. They ﬁnd that, for rare loss events,
there is less incentive to self-insure losses than to use market insurance. This is due to their assumption,
that the price of self-insurance is independent of the probability of the loss. An additional result is that the
demand for self-insurance grows with the base loss of a security threat. As an outcome of their work, they
characterize self-insurance and market insurance as substitutes, and self-protection and market insurance as
complements. Our analysis complements the work in [14] by extending the concepts of self-protection and
self-insurance to the public goods and security context.

1A distinction between reliability and security, in terms of consequences, may exist [27]. In this study, we do not follow this

distinction and consider reliability as a key component of security.

4

2.1 Homogeneity versus heterogeneity in system security

Both the homogeneous and heterogeneous cases are relevant to security analysis. Homogeneous agents
are characteristic of large populations following the same practices and choices by end-users, for instance,
when most security decisions (e.g., patching) are automated, and all users run similar software. The lack
of diversity, in particular in the market for operating systems, lends credibility to such scenarios [17], and
is cited as a strong motivator for developers of malicious code to exploit the resulting correlated risks or to
cheaply repeat attacks.

However, there are strong reasons to compare the homogeneous case with a model that intrpoduces

heterogeneous agents into the analysis of security decision making.

Security through diversity.
Recent technical proposals aim to achieve higher resilience to attacks by
introducing diversity in network and protocol design. For example, Zhuang et al. report of a set of formal
analysis tools that introduce heterogeneity in multi-person communication protocols [51]. O’Donnell and
Sethu develop and test distributed algorithms optimizing the distribution of distinct software modules to
different nodes in a network [33]. Lv et al. study potential improvements in the scalability of P2P networks
by exploiting heterogeneity in the user population [29]. Danezis and Anderson ﬁnd in the presence of het-
erogeneous preferences that censorship-resistance of a system will be weakened if materials are distributed
randomly across all nodes rather than according to users’ preferences [13]. Research in IT economics has
evaluated the decision making of a ﬁrm when faced with the option of increased diversity in its software
base. In Chen et al. the decision for increased heterogeneity depends largely on the assumed risk attitudes
of the organization [10]. Investments into heterogeneity will change the expectation of losses and attack
probabilities, but they also impact the cost of protection and self-insurance.

Chameleonic threats.
Increased diversity is not a sufﬁciently strong protection against correlated se-
curity threats anymore. Already in 1995 the ﬁrst macro viruses started targeting MS ofﬁce on all com-
patible systems.2 Modern cross-platform malware is capable of targeting also different operating systems.
For instance, Linux-Bi-A/Win-Bi-A is written in assembler and able to compromise Windows and Linux
platforms. Malicious code is also capable of crossing the boundary between desktop and mobile devices.
Potentially even more disruptive is malware carrying multiple exploit codes at once. For example, Provos et
al. report that Web-based malware often includes exploits that are used ’in tandem’ to download, store and
then execute a malware binary [35]. These trends render users vulnerable to propagated threats if owners of
different IT systems perceive protection as too costly or ineffective.

Heterogeneous investments patterns. Different organizations follow distinct patterns of IT investment.
Parts of organizations often depend on legacy systems including weakly protected systems, or “boat anchors”

2The macro virus (Winword-Concept) targeted Microsoft Word on Apple and Microsoft systems. For more details see: http:

//web.textfiles.com/virus/macro003.txt.

5

with limited value to an organization [50]. Such legacy systems can allow skilled attackers to intrude a
network. More generally, organizations and end users justify security investments with different assumptions
about potential losses and probabilities of being attacked. This often depends on different knowledge about
threats and means of protection and insurance [1]. This diversity is reﬂected in users’ choices and security
practices [6, 8]. Similarly, security decisions can follow different security paradigms often reﬂected in
different organizational structures, for instance remote replication vs. offsite tape storage.

In this paper, we formally explore both assumptions about the composition of agent populations, by
studying individuals’ incentives in non-cooperative games. In particular, we focus on the impact of homo-
geneous and heterogeneous agents on system security in different network structures.

3 Five canonical security games

A security game is a game-theoretic model that captures essential characteristics of decision making to
protect and self-insure resources within a network of agents. In this section, we summarize the security
games we analyze. We provide the equations for the heterogeneous user case and omit presenting the
simpliﬁed formulas for the homogeneous case that are available in [21].

As discussed earlier, we model security as a hybrid between public and private goods. On the one
hand, as was previously observed by Varian [49], the success of security (or reliability) decision making
frequently depends on a joint protection level determined by all participants of a network. The computation
of the protection level will often take the form of a public goods contribution function. Because network
protection is a public good, it may allow, for certain types of contribution functions, individuals to free-ride
on others’ efforts. At the same time, some individuals may also suffer from inadequate protection efforts by
other members if those have a decisive impact on the overall protection level.

In addition to self-protection, network participants can decide to self-insure themselves from harm. The
success of insurance decisions is completely independent of protection choices made by the individual and
others. Consequently, the games we consider share qualities of private (on the insurance side) and public
(on the protection side) goods.

All security games we introduce share the following key assumptions: (i) all entities in the network
share a single purely public protection output, and (ii) a single individual decides on protection efforts for
each entity – we do not assume a second layer of organizational decision making.

Compared to the homogeneous case [21], protection costs per unit are not necessarily identical for each
entity, and, while in the formal analysis that follows we make the assumption that all decisions are made
simultaneously, we later discuss the impact of relaxing the synchronization assumption.

We develop security games from a basic model with the following payoff structure. Each of N ∈ N
players receives an individual endowment Mi. If she is attacked and compromised successfully she faces
a loss Li. Attacks arrive with a probability of pi (0 ≤ pi ≤ 1), which albeit exogenous, is also dependent
on the player under consideration; pi remains constant over time. Players have two security actions at their
disposition. Player i chooses an insurance level 0 ≤ si ≤ 1 and a protection level 0 ≤ ei ≤ 1. Finally,

6

bi ≥ 0 and ci ≥ 0 denote the unit cost of protection and insurance, respectively. The generic utility function
of Player i is deﬁned as:

Ui = Mi − piLi(1 − si)(1 − H(ei, e−i)) − biei − cisi ,

(1)

where, following common game-theoretic notation, e−i denotes the set of protection levels chosen by players
other than i. H is a contribution function of ei, which is required to be deﬁned for all values over (0, 1)N .
However, we do not place, for now, any further restrictions on the contribution function (e.g., continuity).

From Eqn. (1), the magnitude of a loss depends on three factors: i) whether an attack takes place (pi), ii)
whether the individual invested in self-insurance (1 − si), and iii) the magnitude of the joint protection level
(1 − H(ei, e−i)). Self-insurance always lowers the loss that an individual incurs when compromised by an
attack. Protection probabilistically determines whether an attack is successful. Eqn. (1) therefore yields an
expected utility.

We rely on ﬁve games in the following discussion. In selecting and modeling these games we paid atten-
tion to comparability of our security games to prior research (e.g., [26, 38, 49]). The ﬁrst three speciﬁcations
for H represent important baseline cases recognized in the public goods literature. To allow us to cover most
security dilemmas, we add two games, which we originally introduced only in the context of homogeneous
agents [21].

Total effort security game: The global protection level of the network depends on the sum of contribu-
tions normalized over the number of all participants. That is, we deﬁne H(ei, e−i) = 1
i ei, so that
N
Eqn. (1) becomes

P

Ui = Mi − piLi(1 − si)(1 −

ek) − biei − cisi .

(2)

1
N

X

k

Economists identiﬁed the sum of efforts (or total effort) contribution function long before the remaining
cases included in this paper [26]. We consider a slight variation of this game to normalize it to the desired
parameter range.

As a practical example of a total effort game in practice, consider parallelized ﬁle transfers, as in the
BitTorrent peer-to-peer service. It may be the case that an attacker wants to slow down transfer of a given
piece of information; but the transfer speed itself is a function of the aggregate effort of the machines
participating in the transfer. Note that, the attacker in that case is merely trying to slow down a transfer, and
is not concerned with completely removing the piece of information from the network: censorship actually
results in a different, “best shot” game, as we discuss later.

Weakest-link security game: The overall protection level depends on the minimum contribution offered
over all entities. That is, we have H(ei, e−i) = min(ei, e−i), and Eqn. (1) takes the form:

Ui = Mi − piLi(1 − si)(1 − min(ei, e−i)) − biei − cisi .

(3)

7

The weakest-link game is the most often recognized public goods problem in computer security. Once the
perimeter of an organization is breached it is often possible for attackers to leverage this advantage. This
initial compromise can be the result of a weak password, an inconsistent security policy, or some malicious
code inﬁltrating a single client computer. Another example is that of a two-way communication (e.g., TCP
ﬂow), where the security of the communication is determined by the least secure of the communication
parties. For instance, a TCP ﬂow between a host with a perfectly secure TCP/IP stack and a host with an
insecure TCP/IP stack can be easily compromised.

Best shot security game:
offered over all entities. Hence, we have H(ei, e−i) = max(ei, e−i), so that Eqn. (1) becomes

In this game, the overall protection level depends on the maximum contribution

Ui = Mi − piLi(1 − si)(1 − max(ei, e−i)) − bei − csi .

(4)

Among information systems, networks with built-in redundancy, such as peer-to-peer, sensor networks, or
even Internet backbone routes, share resilience qualities with the best shot security game; for instance, to
completely take down communications between two (presumably highly connected and highly secure) back-
bone nodes on the Internet, one has to shut down all possible routes between these two nodes. Censorship-
resistant networks are another example of best shot games. A piece of information will remain available to
the public domain as long as a single node serving that piece of information can remain unharmed [13].

Weakest target security game (without mitigation): Here, an attacker will always be able to compro-
mise the entity (or entities) with the lowest protection level, but will leave other entities unharmed. This
game derives from the security game presented in [11]. Formally, we can describe the game as follows:

(

H(ei, e−i) =

0 if ei = min(ei, e−i),
1 otherwise,

which leads to

(

Ui =

Mi − piLi(1 − si) − biei − cisi
Mi − biei − cisi

if ei = min(ei, e−i),
otherwise.

The weakest target game differs from the weakest link. There is still a decisive security level that sets
the benchmark for all individuals. It is determined by the individual(s) with the lowest chosen effort level.
However, in this game all entities with a protection effort strictly larger than the minimum will remain
unharmed.

In information security, this game captures the situation in which an attacker is interested in securing
access to an arbitrary set of entities with the lowest possible effort. Accordingly, she will select the machines
with the lowest security level. An attacker might be interested in such a strategy if the return on attack effort
is relatively low, for example, if the attacker uses a compromised machine to distribute spam. Such a strategy
is also relevant to an attacker with limited skills, a case getting more and more frequent with the availability
of automated attack toolboxes [47]; or, when the attacker’s goal is to commandeer the largest number of

(5)

(6)

8

machines using the smallest investment possible [15]. Likewise, this game can be useful in modeling insider
attacks – a disgruntled employee may for instance very easily determine how to maximize the amount of
damage to her corporate network while minimizing her effort.

Weakest target security game (with mitigation): This game is a variation on the above weakest target
game. The difference is that, the probability that the attack on the weakest protected player(s) is successful
is now dependent on the security level min ei chosen. That is,

(

H(ei, e−i) =

1 − ei
1

if ei = min(ei, e−i),
otherwise,

so that

(

Ui =

M − piLi(1 − si)(1 − ei) − biei − cisi
M − biei − cisi

if ei = min(ei, e−i),
otherwise.

This game represents a nuanced version of the weakest target game. Here, an an attacker is not necessarily
assured of success. In fact, if all individuals invest in full protection, not a single machine will be com-
promised. This variation allows us to capture scenarios where, for instance, an attacker targets a speciﬁc
vulnerability, for which an easily deployable countermeasure exists.

Limitations: As suggested by Hirshleifer [26], practical scenarios may involve social composition func-
tions combining two or more of these ﬁve games. Revisiting our earlier examples, protecting a communica-
tion ﬂow between two hosts may be a “weakest-link” game, until a certain level of host security is reached
at both hosts. At that point, the attacker may start to target the routes between the hosts rather than the hosts
themselves, and it becomes a “best-shot” game. Other realistic environments may be better characterized by
slight variations on a given game (e.g., “the total of the three best shots”). We nevertheless believe that the
ﬁve games described above may capture a large number of practical cases, as our argument made in earlier
work [21] is actually strengthened by being able to compare the representative user case with the scenario
with diverging user preferences.

(7)

(8)

4 Nash equilibrium analysis with homogeneous agents

We next determine the equilibrium outcomes where each individual chooses protection effort and self-
insurance investments unilaterally, in an effort to maximize her own utility. We ﬁrst consider the case
of homogeneous agents. In (Section 5) we will identify differences to the heterogeneous case. In Section 6,
we then compare results Nash equilibria to the protection efforts and self-insurance levels chosen if coordi-
nated by a social planner. However, currently we have only completed the social optimality analysis for the
representative user case.

We assume homogeneous users to share the same values for cost of protection and self-insurance. Indi-

viduals also face the same threats with identical consequences if compromised.

9

4.1 Total effort

Let us focus on player i, and consider ek for k 6= i as exogenous. Then, Ui is a function of two variables, ei
and si. From Eqn. (2), Ui is twice differentiable in ei and si, with ∂2Ui/∂s2
i = 0. Hence,
according to the second derivative test, only (ei, si) ∈ {(0, 0), (0, 1), (1, 0), (1, 1)} can be an extremum –
that is, possible Nash equilibria are limited to these four values (or to strategies yielding a payoff constant
regardless of ei and/or si). As long as at least one of b or c is strictly positive, (ei, si) = (1, 1) is always
dominated by either (ei, si) = (1, 0) or (ei, si) = (0, 1) and cannot deﬁne a Nash equilibrium. Let us
analyze the three other cases:

i = 0 and ∂2Ui/∂e2

• (ei, si) = (0, 0). Replacing in Eqn. (2), we get

Ui = M − pL

1 −





ek

 .

1
N

X

k6=i

• (ei, si) = (0, 1). Replacing in Eqn. (2), we get

Ui = M − c .

• (ei, si) = (1, 0). Replacing in Eqn. (2), we get

Ui = M − pL

1 −

−

ek

 − b .





1
N

1
N

X

k6=i

(9)

(10)

(11)

Result 1: After investigating Eqs. (9–11) we can identify three Nash equilibrium strategies.

• Full protection eq.: If pL > bN and c > b + pL N −1

N , meaning that protection is cheap, potential
losses are high, and insurance is extremely overpriced, then the (only) Nash equilibrium is deﬁned by
everybody protecting but not insuring, that is, (ei, si) = (1, 0).

• Full self-insurance eq.: In the other cases where pL > bN , (ei, si) = (0, 1) is a Nash equilibrium.
Also, if c < pL < bN (expected losses above insurance costs), then (ei, si) = (0, 1), is a Nash
equilibrium.

• Passivity eq.: If pL < bN and pL < c, then the expected losses are small enough so that complete

passivity, deﬁned by (ei, si) = (0, 0) for all players, is a Nash equilibrium.

Proof. First, consider the case when pL > bN . This means that Ui as given by Eqn. (11) is higher
than that given by Eqn. (9), which means that only (ei, si) = (0, 1) and (ei, si) = (1, 0) are potential
utility-maximizing strategies. If, for a player i, the initial protection levels picked by other players satisfy
P
pL , then Ui given by Eqn. (11) dominates that given by Eqn. (10), and player i

k6=i ek > N − 1 − N c−b

10

accordingly chooses (ei, si) = (1, 0). (Note this is only possible if b < c.) All players have the same
rationale, and we end up with everybody picking (ei, si) = (1, 0), which is a Nash equilibrium.

On the other hand, if P

k6=i ek < N − 1 − N c−b

pL , then player i chooses (ei, si) = (0, 1). All players
adopt the same strategy, so that we end up with an insurance-only conﬁguration, where everybody gets a
utility M − c. If pL N −1
N −1 , this is a Nash equilibrium with ∀i(ei, si) = (0, 1).
(This is always the case when b > c.) On the other hand, with pL < (c−b)N
N −1 , people have an incentive to
switch to the strategy (ei, si) = (1, 0), and to remain in that strategy.

N + b > c, that is, pL > (c−b)N

Next, consider the case when pL < bN . The utility given by Eqn. (9) is greater than that given by
Eqn. (11). Then, for each player i, if the initial protection levels picked by other players satisfy P
k6=i ek >
N (pL − c), then Eqn. (9) gives a higher utility than Eqn. (10).3 Hence, player i picks (ei, si) = (0, 0) as
a strategy. If the game is synchronized, all players have the same rationale and we end up in a situation
where everybody selects (ei, si) = (0, 0). If pL < c then all players are content staying at (ei, si) = (0, 0),
otherwise, they switch to (ei, si) = (0, 1), and stay in that strategy.

Increasing number of players N: As the number of players increases, protection equilibria become more
and more unlikely to occur. Indeed, in a total effort scenario, “revenues” yielded by a player’s investment in
security have to be shared with all of the other participants, making it an increasingly uninteresting strategy
for the player as the network grows.

4.2 Weakest-link

Let e0 = mini(ei). From Eqn. (3), we have Ui = M − pL(1 − si)(1 − e0) − bei − csi, so that for all i,

Ui ≤ M − pL(1 − si)(1 − e0) − be0 − csi ,

which is reached for ei = e0. So, in a Nash equilibrium, everybody picks the same ei = e0. It follows that
Nash equilibria are of the form (e0, 0) or (0, 1). For strategy (ei, si) = (e0, 0), we have

while for strategy (ei, si) = (0, 1), we have

Ui = M − pL + (pL − b)e0 ,

Ui = M − c .

(12)

(13)

Which strategy is a Nash equilibrium depends therefore on the relative values of pL, b and c, and the sign of
(pL − c) − (pL − b)e0.

3The border case P

k6=i ek = N (pL − c) yields a situation where the initial constellation of parameters form an unstable Nash

equilibrium, where no one has any incentive to change their strategy.

11

Result 2:
However, there exist multiple pure protection equilibria.

In the weakest link security game, we can identify three types of Nash equilibrium strategies.

Denote by ˆe0 the minimum of the protection levels initially chosen by all players. We have

• Multiple protection equilibria: If pL > b and either 1) pL < c or 2) pL ≥ c and ˆe0 > (pL − c)/(pL −
b) then (ei, si) = (ˆe0, 0) for all i is a Nash equilibrium. Everybody picks the same minimal security
level, but no one has any incentive to lower it further down. This equilibrium can only exist for b ≤ c,
and may be inefﬁcient, as it could be in the best interest of all parties to converge to ei = 1, as we
discuss later in Section 6.

• Full self-insurance eq.: If pL > c and either 1) pL < b or 2) pL ≥ b and ˆe0 < (pL − c)/(pL − b),
then (ei, si) = (0, 1) for all i is a Nash equilibrium: essentially, if the system is not initially secured
well enough (by having all parties above a ﬁxed level), players prefer to self-insure.

• Passivity eq.: If pL < b and pL < c, then (ei, si) = (0, 0) is the only Nash equilibrium – both

insurance and protection are too expensive.

Notice that if ˆe0 = (pL − c)/(pL − b), then both full self-insurance ((ei, si) = (0, 1) for all i) and
protection ((ei, si) = (ˆe0, 0) for all i) form a Nash equilibrium. In particular, if b = c (pL > b and pL > c),
and ˆe0 = 1, full protection (ei, si) = (1, 0) and full self-insurance (ei, si) = (0, 1) are Nash strategies.

Increasing number of players N: The weakest link security game, much like the tacit coordination game
of [48] has highly volatile protection equilibria when the number of players increase. In fact, any protection
equilibrium has to contend with the strategic certainty of a self-insurance equilibrium. To view this, consider
the cumulative distribution function F (ei) over the protection strategies ei of a given player i. From what
precedes, with pure strategies, in the Pareto-optimum, F (1) = 1 and F (ei) = 0 for ei < 1. Assuming all N
players use the same c.d.f. F , then the c.d.f. of e0 = mini{ei} is given by Fmin(e0) = 1 − (1 − F (e0))N
[48]. So, Fmin(1) = 1 and Fmin(e0) = 0 for e0 < 1 as well. Now, assume there is an arbitrarily small
probability ε > 0 that one player will defect, that is F (0) = ε. Then, Fmin(0) converges quickly to 1 as
N grows large. That is, it only takes the slightest rumor that one player may defect for the whole game to
collapse to the (ei, si) = (0, 1) equilibrium.

4.3 Best shot

Let e∗ = maxi(ei). Eqn. (4) gives

Ui = M − pL(1 − si)(1 − e∗) − bei − csi .

Clearly, (ei, si) = (1, 1) is suboptimal, so that three strategies may yield the highest payoff to user i.

• Selecting (ei, si) = (0, 0) yields Ui = M − pL(1 − e∗).

12

• Selecting (ei, si) = (1, 0) yields Ui = M − b.

• Selecting (ei, si) = (0, 1) yields Ui = M − c.

Result 3: From the above relationships, we can identify the following pure Nash equilibrium strategies.

• Full self-insurance eq.: If b < c we ﬁnd that the self-insurance equilibrium (∀i, (ei, si) = (0, 1)) is

the only possible Nash equilibrium.

• Passivity eq.: If pL < b and pL < c agents prefer to abstain from security actions (∀i, (ei, si) =

(0, 0)).

In particular, there is no protection equilibrium in this game. For one protection equilibrium to exist, we
would need b < c and pL > b. But even assuming that this is the case, as long as the game is synchronized,
players endlessly oscillate between securing as much as possible (ei = 1) and free-riding (ei = 0). This is
due to the fact that as soon as one player secures, all others have an incentive to free-ride. Conversely, if
everybody free-rides, all players have an incentive to deviate and secure as much as possible.

Increasing number of players N:
In the absence of coordination between players, the outcome of this
game is globally independent of the number of players N , as there is no protection equilibrium, and the
insurance equilibrium is independent of the number of players. However, the game may be stabilized by
using player coordination (e.g., side payments) for low values of N , something harder to do as N grows.

4.4 Weakest-target (without mitigation)

Fix the strategy point and let ε < pL
2b . Let e0 be the minimum effort level of any player. Then no player
selects a higher effort than e0 + ε because it dominates all higher effort levels. However, any player at e0
would prefer to switch to e0 + 2ε. Then the change in her payoff is greater than pL − 2 pL
2b b = 0. Because
this deviation is proﬁtable this strategy point is not an equilibrium.

Result 4:
for non trivial values of b, p, L and c do not exist.

In the weakest-target game with an attacker of inﬁnite strength we ﬁnd that pure Nash equilibria

Mixed strategy equilibria. While no pure Nash equilibria exist, let us explore the existence of a mixed
strategy equilibrium. We use the shorthand notation ei = e, si = s here, and consider mixed strategies for
choosing e. There are two cases to consider.

Case c > pL:
any self-insurance.

If c > pL then dominance arguments immediately lead to s = 0 meaning that nobody buys

An equilibrium strategy may be parameterized by e. For a given player, the utility function U becomes
a function of a single variable e. Let f (e) be the probability distribution function of effort in the weakest-
target game and let F (e) be the cumulative distribution function of effort. Assuming only one player is at the

13

minimum protection level, shall an attack occur, the probability of being the victim is then (1 − F (e))N −1.
(All N players choose protection levels greater than e.)

Then the utility is given by

U = M − pL(1 − F (e))N −1 − be .

(14)

In a Nash equilibrium, the ﬁrst-order condition dU /de = 0 must hold, so that:

(N − 1)pLf (e)[1 − F (e)]N −2 − b = 0

If we substitute G = (1 − F (e)) and g = −f we can write GN −2dG/de = −b/p(N − 1)L, which, by
integration yields

that is

With G(0) = 1,

Differentiating, we get

and, replacing g = −f we ﬁnd,

Z G(0)

G(e)

GN −2dG =

Z 0

−b

p(N − 1)L

e

dˆe ,

GN −1(cid:12)
(cid:12)

G(0)
G(e) =

−b
pL

e .

(cid:18)

G(e) =

1 −

(cid:19) 1
e

N −1

.

b
pL

g(e) = −

1

N − 1

(cid:18)

1 −

b
pL

b
pL

(cid:19)− N −2
e

N −1

,

f (e) =

1

N − 1

(cid:18)

1 −

b
pL

b
pL

(cid:19)− N −2
e

N −1

,

as the probability distribution function of self-protection in a mixed Nash equilibrium.

Case c ≤ pL: Now let us consider a game with insurance under the more reasonable assumption c ≤ pL;
that is, insurance is not overpriced compared to expected losses. Dominance arguments indicate that a Nash
strategy must be of the form (e, s) ∈ {(e, 0), e ≥ 0} ∪ {(0, 1)}.

Let q be the probability that a player chooses strategy (e, s) = (0, 1). That is, F (0) = q. Because

insurance is independent of protection, we can reuse Eqn. (15) with the new boundary G(0) = 1 − q:

However, since we are now including self-insurance, a second condition must hold. The payoff for strategy
(e, s) = (0, 1) must equal the payoff for all other strategies.

(cid:18)

G(e) =

(1 − q)N −1 −

(cid:19) 1
e

N −1

b
pL

14

(15)

(16)

(17)

Speciﬁcally, we may compare payoffs for strategies (e, s) = (ε, 0) and (e, s) = (0, 1) which gives, by

continuity as ε → 0,

Together Eqs. (17) and (18) yield:

pL(1 − q)N −1 = c .

which, differentiating, gives

This allows us to compute how often strategy (e, s) = (0, 1) is played:

F (e) = 1 − G(e) = 1 −

(cid:18) c − be

(cid:19) 1

N −1

,

pL

f (e) =

1

N − 1

b
pL

pL

(cid:18) c − be

(cid:19) 1

N −1 −1

.

q = F (0) = 1 −

(cid:19) 1

N −1

.

(cid:18) c
pL

(18)

(19)

(20)

Result 5:
egy exists. The individual’s strategy is given by Eqs. (19) and (20).

In the weakest-target game with an attacker of inﬁnite strength, a mixed Nash equilibrium strat-

Increasing number of players N: From Eqn. (20), we can directly infer that an increase in the number of
participating players decreases the probability that a full self-insurance strategy is chosen. When N grows
large, q tends to zero, which means that players increasingly prefer to gamble in order to ﬁnd a protection
level that leaves them unharmed.

4.5 Weakest target (with mitigation)

Let us assume that there exists a Nash equilibrium where 0 < K < N players who satisfy ei = e0 =
min(ei, e−i), while (N − K > 0) players satisfy ei > e0. We can show that such an equilibrium does
not exist and that players rather congregate at the highest protection level if certain conditions are met. By
computing the partial derivatives ∂Ui/∂si and ∂Ui/∂ei, and discriminating among values for ei and si, we
get the following results. (We refer the reader to Appendix A for the complete proof.)

Result 6:
exist.

In contrast to the inﬁnite strength weakest-target game we ﬁnd that a pure Nash equilibrium may

• Full protection eq.: If b ≤ c we ﬁnd that the full protection equilibrium (∀i, (ei, si) = (1, 0)) is the

only possible pure Nash equilibrium.

• For b > c we can show that no pure Nash equilibrium exists.

• There are no pure self-insurance equilibria.

15

Mixed strategy equilibrium To complement this analysis we also present the mixed strategy equilibrium.
The derivation is similar to the one given by Eqs. (14–20), however, with an additional substitution step. This
gives the resulting distribution,

F (e) = 1 −

(cid:18) c − be

(cid:19) 1

N −1

pL(1 − e)

,

so that

f (e) =

1

N − 1

(cid:18) (b − c)pL
pL2(1 − e)2

(cid:19) (cid:18) c − be

(cid:19)− N −2

N −1

.

pL(1 − e)

Interestingly, the probability of playing (e, s) = (0, 1) remains

q = F (0) = 1 −

(cid:19) 1

N −1

(cid:18) c
pL

(21)

(22)

Note that if c < b there is a zero probability that e = 1 will be chosen by any player. The upper bound for
protection effort is given by emax = c/b.

Result 7:
rium strategy exists. The relevant equations are given in Eqs. (21–22).

In the weakest-target game with an attacker of ﬁnite strength we ﬁnd that a mixed Nash equilib-

5 Nash equilibrium analysis with heterogeneous agents

In this section, our focus is to understand how the inclusion of heterogeneous actors inﬂuences predictions
compared to a model with representative agents. In Section 2, we have discussed arguments for and against
homogeneity in security models. In the modeling of economic phenomena, added complexity (e.g., adding
agents with more diverse tastes) does not always change strategic predictions substantially. On the other
hand, we expect that heterogeneity impacts the actions of agents in security games in different ways, for
example by: 1) Negotiating the trade-off between protection and insurance, 2) Highlighting certain strategies
and focal points due to the inherent differences in the agent population, 3) (De-)stabilizing equilibrium
predictions derived in the homogeneous case. We expect several conclusions from the homogeneous case to
remain relevant. But as Hartley [24] argued “representative agents models conceal heterogeneity whether it
is important or not.” This analysis aims at pinpointing key differences and discuss their implications.

5.1 Total effort

The total effort game yields considerably different results depending on the number of players involved.

Two-player game Let us ﬁrst start the discussion for the simple case N = 2. From the game description
given by Eqn. (2), we get U1(e1, s1) = M1 − p1L1(1 − s1)(1 − (e1 + e2)/2) − b1e1 − c1s1 for Player 1. The
second partial derivative test indicates that there is no local extremum, so that the only possible maxima of U1

16

Figure 1: Reaction functions for a two-player total effort game. Bold lines and dots indicate potential
Nash equilibria.

are given by U1(0, 0) = M1−p1L1(1−e2/2), U1(1, 0) = M1−p1L1(1/2−e2/2)−b1, U1(0, 1) = M1−c1,
or U1(1, 1) = M1 − b1 − c1. With b1 > 0, we immediately see that U1(0, 1) > U1(1, 1), which tells us that
fully insuring and protecting at the same time is a strictly dominated strategy for Player 1. The passivity
strategy (ei, si) = (0, 0) dominates the “protect-only” strategy (ei, si) = (1, 0) when b1 > p1L1/2.

Assuming b1 ≤ p1L1/2, the “protect-only” (1, 0) strategy dominates the “insure-only” (0, 1) strategy

for Player 1 if and only if (all quantities being assumed to be deﬁned):

e2 > 1 − 2

c1 − b1
p1L1

.

(23)

A similar rationale yields the corresponding conditions for Player 2, leading to the reaction functions e1(e2)
and e2(e1) plotted in Figure 1. By deﬁnition, Nash equilibria are characterized by ﬁxed points e1(e2) =
e2(e1). From the above analysis summarized in Figure 1, this occurs for two values: when both agents fully
protect and when both agents abstain from investing in protection. We note that both ﬁxed points are stable,
meaning that, if they are reached, minimal deviations in the strategy of one player are unlikely to perturb the
actions of the other player.

Result 8: The two-player total effort security game with heterogeneous agents presents the following equi-
libria:

17

21e2e2e  (   )1e1e  (   )211011−2(c −b )/p L2   c  −b 11−2122p  L1e• Full protection eq.: If b1 ≤ p1L1/2, b2 ≤ p2L2/2 (protection costs are modest for both players),
and the initial values e1(0) and e2(0) satisfy either e1(0) > 1 − 2(c2 − b2)/(p2L2) or e2(0) >
1 − 2(c1 − b1)/(p1L1) (at least one player is initially fairly secure, or at least one player faces very
high insurance costs) then the (only) Nash equilibrium is deﬁned by both players protecting but not
insuring, that is, (ei, si) = (1, 0).

• Multiple eq. without protection: If the conditions above do not hold, then we have an insecure equi-
libria. Both players converge to e1 = 0 and e2 = 0. Their respective investments in insurance depend
on whether their insurance premium is smaller than their potential losses: a player will fully insure if
and only if ci < piLi, and will be passive otherwise.

A particularly interesting feature of the two-player version of the game is that expensive insurance or
protection costs at either of the players directly condition which equilibrium can be reached. For instance,
if one of the players has to pay a very high insurance premium in front of its protection costs, she will elect
to protect, likely leading the other player to protect as well. Conversely, if either of the players faces a
high protection premium (bi > piLi/2), the game will likely converge to an equilibrium without protection
efforts. As we discuss later, this property can be used by some form of intervention to have the game
converge to a desirable equilibrium.

More generally, in this game, each of the two players generally tracks what the other is doing. When
moves are made perfectly simultaneously, this may result in oscillations between insecure and secure con-
ﬁgurations. The only exception to this tracking behavior occurs when one player faces high security costs
and a low insurance premium, while the other faces the opposite situation (low security costs, very high
insurance premium). In such a case, the game converges to the ﬁrst player insuring, and the second player
protecting. In short, extreme parameter values allow to remove network effects in this game.

In the more general case N ≥ 2, we ﬁrst notice that, for a security strategy to
N -player game (N large)
be meaningful, we need to have bi < piLi/N . This means that, as the number of player increases, individual
protection costs have to become very small, or expected losses have to considerably increase. Failing that,
insurance or passivity is always a better option.

Second, from Eqn. (2), we obtain that Eqn. (23) is generalized to

1

N − 1

X

j6=i

ej > 1 −

N

N − 1

ci − bi
piLi

,

(24)

as a condition for player i to select a protection-only strategy as opposed to an insurance-only strategy.
Eqn. (24) tells us that, for large values of N , changes in a single player’s protection strategy are unlikely to
have much of an effect on the other players’ strategies. Indeed, each player reacts to changes in the average
protection level over the (N − 1) other players.

This observation brings the question of exactly how robust the N -player game is to a change in the
strategy played by a given individual. Are “domino effects” possible, where changes in a single player’s

18

strategy, albeit with a minimal effect on all other players, lead another player to switch strategies, and
eventually to large groups changing their plays?

To help us answer this question, let us consider N > 2, and K ≤ N arbitrary players that are initially
(at time 0) unprotected. For instance, assume without loss of generality that Players 1, . . . , K are initially
unprotected, and that

c2 − b2
p2L2

≥

c3 − b3
p3L3

≥ . . . ≥

cK − bK
pKLK

.

Further assume that at a later time t > 0, Player 1 switches her strategy to full protection, that is, e1(t) = 1.
Assuming all players may have an incentive to protect (i.e., for all i, bi < piLi/N ), Player 2 would also
switch to full protection only if

that is, only if

which reduces to

Player 2’s switch causes Player 3 to switch too only if

1

N − 1

X

j6=2

ej(t) > 1 −

1

N − 1

c2 − b2
p2L2

N − 1

ej(0) +

> 1 −

N − 1

N − 1

1

1

X

j6=2

X

j>K

1

1

N − 1

ej(0) +

> 1 −

N − 1

N − 1

1

1

c2 − b2
p2L2

,

c2 − b2
p2L2

.

1

N − 1

X

j6=3

ej(t) > 1 −

1

N − 1

c3 − b3
p3L3

,

that is,

From Eqs. (25) and (26) we get

1

N − 1

X

j>K

ej(0) +

> 1 −

2

N − 1

1

N − 1

c3 − b3
p3L3

.

c2 − b2
p2L2

−

c3 − b3
p3L3

< 1 .

Iterating over the K players that are initially not protecting, we get:

max
2≤i≤K

ci − bi
piLi

− min
2≤i≤K

ci − bi
piLi

< K − 1 .

We can follow an identical derivation for the case where the K players switch from a protection strategy to
a non-protection strategy. We then obtain the following necessary condition for “domino effects” to occur
over K players, that is a switch in Player 1’s strategy causing a switch in the strategy of K players:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
2≤i≤K

ci − bi
piLi

− min
2≤i≤K

ci − bi
piLi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

< K − 1 .

19

(25)

(26)

(27)

Figure 2: Reaction functions for a two-player weakest-link game. Bold lines and dots indicate potential
Nash equilibria.

Result 9: We have derived a stability measure of the heterogeneity of a total effort security game with
N agents (Eqn. (27)). The more heterogeneous the players are, the more unlikely Eqn. (27) is to hold for
large values of K. In other words, the more heterogeneous a system is, the more likely it is to be resilient to
perturbations due to a single individual changing strategies.

5.2 Weakest-link

Here again, we start by considering a two-player game. Computing partial derivatives in ei and si from
Eqn. (3), we observe that each player chooses either (ei, si) = (0, 1) (insurance strategy) or (ei, si) =
(minj6=i ej, 0) (protection strategy, where in the two-player version of the game minj6=i ej is naturally equal
to the protection value chosen by the other player) in order to maximize their utility function.

Looking at the payoffs that can be obtained in both cases leads us to the reaction functions of both
players, which we plot in Figure 2. In the ﬁgure, we see that a ﬁxed-point is attained when e1 = e2 = 0
(insurance-only equilibria) and when both e1 and e2 are greater than max{(p1L1 −c1)/(p1L1 −b1), (p2L2 −
c2)/(p2L2 − b2)}.

Result 10: Generalizing to N players, we obtain the following distinction for the weakest link security
game:

20

11e2e2e  (   )1e1e  (   )2110(p L −b )222(p L −c )22211(p L −c )/(p L −b )111e• Full protection eq.: If, for all i, piLi > bi, and either 1) piLi < ci, or 2) piLi ≥ ci and ˆe(0), the

minimum of the security levels initially chosen by all players, satisﬁes

ˆe(0) > max
1≤i≤N

{(piLi − ci)/(piLi − bi)} ,

then we have a Nash equilibrium where everyone picks (ˆe(0), 0).

• Multiple eq. without protection: All players select ei = 0 if the conditions above do not hold. The
value of insurance they select depends on their respective valuations. Players for whom insurance
is too expensive (piLi < ci) do not insure, with si = 0, while others choose full insurance, that is
si = 1.

The likelihood of reaching a full protection equilibrium is conditioned by the player which has the largest
difference between protection and insurance costs relative to its expected losses. In particular, it only takes
one player with an insurance premium smaller than its protection cost (bi > ci) to make the full protection
equilibrium unreachable. Hence, when N grows large, we expect protection equilibria to become more and
more infrequently observed.

5.3 Best shot

Looking at the variations of the payload function Ui given in Eqn. (4) as a function of ei and si tells us there
are three possibilities for maximizing Ui: a passivity strategy (0, 0), a secure-only strategy (1, 0) and an
insure-only strategy (0, 1).

We get Ui(0, 0) = Mi − piLi(1 − max{e−i}), Ui(1, 0) = Mi − bi, and Ui(0, 1) = Mi − ci. We
immediately notice that bi > ci leads Player i to never invest in protection: either the player is passive, or
she insures. If, on the other hand bi ≤ ci, then player i chooses a protection strategy over a passivity strategy
if and only if (bi assumed greater than 0) we have max{e−i} < 1 − bi/piLi. We plot the reaction functions,
in a two-player case, in Figure 3.

Result 11: For the two-player best shot security game we can identify the following equilibria:

• Protection eq.: In contrast to the homogeneous case a protection equilibrium does exist. The Nash

equilibrium is a free-riding equilibrium where one player protects, and the other does not.

• Multiple eq. without protection: If bi > ci for all player i individuals will choose to self-insure or

remain passive.

In the homogeneous version of the game, we note that these Nash equilibria are not reached in a syn-
chronized game with N players, as players would constantly oscillate between free-riding and protecting.
With heterogeneous players, however, it is possible to reach a Nash equilibrium. Indeed, if the initial pro-
tection levels chosen satisfy max{e−i(0)} > 1 − bi/piLi for all players but one, this last player will be the
only one to secure, while everybody else will defect. Note that there should be only one player choosing

21

Figure 3: Reaction functions for a two-player best shot game. Bold dots indicate potential Nash equilib-
ria. Protection costs are assumed here to be smaller than insurance costs for both players.

to secure for a Nash equilibrium to be reached – as soon as at least two players decide to protect, each will
defect in the next round hoping to free-ride on the other protecting players. In other words, if there exists a
unique i for which the initial constellation of protection levels satisﬁes

max{e−i(0)} < 1 − bi/piLi ,

(28)

then a Nash equilibrium where all players free-ride on player i is reached as long as bi < ci. This situation
could happen when only one player faces disproportionate losses compared to other players, or her security
costs are very small.

Result 12: When protection levels are initially randomly set, protection equilibria in the best shot game
are increasingly unlikely to happen as the number of players N grows.

Assume that the initial protection levels, ei(0) for 1 ≤ i ≤ N are set independently and at random, that
is, that they can be expressed as a random variable with cumulative distribution function F . Then for any
Player k, the probability that ek(0) < 1 − bi/piLi is simply F (1 − bi/piLi). It follows that Eqn. (28) is
satisﬁed for Player i with probability F (1 − bi/piLi)N −1.

Next, we want Eqn. (28) to be violated for all players other than i. Eqn. (28) is defeated for a given
Player k with probability 1 − F (1 − bk/pkLk)N −1. Consequently, it is defeated for all Players j 6= i with
probability Q

j6=i(1 − F (1 − bj/pjLj)N −1).

22

1−b /p L1e2e2e  (   )1e1e  (   )21p  L1b11101−222eIt follows that the probability ρi that Eqn. (28) is satisﬁed only for Player i is given by

ρi = F

(cid:16)

1 − bi
piLi

(cid:17)N −1 Q

1 − F

j6=i

(cid:18)

(cid:16)

1 − bj
pj Lj

(cid:17)N −1(cid:19)

.

Then, the probability that a protection equilibrium can be reached is given by P
terize mutually exclusive events. To simplify notations, let xi = F

(cid:17)

(cid:16)

1 − bi
piLi

i ρi, since the ρi’s charac-

. Rearranging terms gives

Let k = arg maxi

nQ

1 − xN −1

j

j6=i

. Then we have

X

ρi =

X

Y

(cid:16)

(cid:17)

1 − xN −1

j

− N

Y

(cid:16)

1 − xN −1

j

(cid:17)

.

i

(cid:16)

X

i

i

j6=i

(cid:17)o

j6=k

j

j

ρi ≤ N

Y

(cid:16)

(cid:17)

1 − xN −1

j

− N

Y

(cid:16)

1 − xN −1

j

(cid:17)

,

which gives us, after rearranging

X

i

ρi ≤ N xN −1

k

Y

(cid:16)

1 − xN −1

j

(cid:17)

,

j6=k

which tends to zero as N increases, as soon as xk = F (1 − bk/pkLk) < 1.

This is notably the case if we assume a function F strictly monotonous increasing on [0, 1], and positive

security costs (bi > 0) for all players.

5.4 Weakest target

As in the homogeneous case, Nash equilibria for the weakest target game are quite different depending on
whether or not we are considering that mitigation is possible.

Without mitigation.
In the weakest target game without mitigation, we ﬁnd that, in the homogeneous
case where bi = b, ci = c, pi = p and Li = L, there are no pure strategy Nash equilibrium. The proof can
be extended to the heterogeneous case, as we discuss next.

Let us assume that the minimum protection level over all players is set to ˆe < 1. Then, we can group
players in two categories: those who play ei = ˆe, and those who set ei > ˆe. By straightforward dominance
arguments coming from the description of the payoffs in Eqn. (6), players who select ei > ˆe select ei = ˆe+ε,
where ε > 0 is inﬁnitesimally small, and si = 0 . Let
(cid:26) piLi
2bi

(1 − si) +

cisi
2bi

ε < min

(cid:27)

.

i

Players who play ei = ˆe would actually prefer to switch to ˆe + 2ε. Indeed, the switch in strategies allows a
payoff gain of

Ui(ˆe + 2ε, 0) − Ui(ˆe, si) = −2biε + piLi(1 − si) + cisi > 0 .

23

Hence, this strategy point is not a Nash equilibrium. It follows that the only possible equilibrium point
would have to satisfy ei = 1 for all ei. However, in that case, all players are attacked, which ruins their
security investments. All players therefore have an incentive to instead select ei = ˆe = 0, which, per the
above discussion, cannot characterize a Nash equilibrium.

Result 13:
values of bi, pi, Li and ci do not exist.

In the weakest-target game without mitigation we ﬁnd that pure Nash equilibria for non trivial

With mitigation.
In the weakest target game with mitigation, we ﬁnd that, with homogeneous agents,
a full protection Nash equilibrium exists as long as protection costs are smaller than insurance costs. An
exactly identical proof can be conducted in the heterogeneous case to show that a full protection equilibrium
is reached if bi < ci for all i.

On the other hand, it only takes one of the players to face high security costs to make this equilibrium
Indeed, if there exists k such that bk > ck, then Player k will always prefer a full-insurance
collapse.
strategy ((ek, sk) = (0, 1)) over a full-protection strategy ((ek, sk) = (1, 0)). This will immediately lead
other players to try to save on security costs by picking ei = ε > 0 as small as possible. We then observe
an escalation as in the unmitigated version discussed above. Hence, heterogeneity actually threatens the
(precarious) stability of the only possible Nash equilibrium.

Result 14:
may exist.

In contrast to the weakest-target game without mitigation we ﬁnd that a pure Nash equilibrium

• Full protection eq.: If bi ≤ ci for all agents we ﬁnd that the full protection equilibrium (∀i, (ei, si) =

(1, 0)) is the only possible pure Nash equilibrium.

• If bi > ci for any agent we can show that no pure Nash equilibrium exists.

• There are no pure self-insurance equilibria.

6

Identiﬁcation of social optima with homogeneous agents

Organizations and public policy actors frequently attempt to identify policies that provide the highest utility
for the largest number of people. This idea has been operationalized with the social optimum analysis. It
states that a system has reached the optimum when the sum of all players’ utilities is maximized. That is,
the social optimum is deﬁned by the set of strategies that maximize P
i Ui. Consider N players, and denote
by Φ(e1, s1, . . . , eN , sN ) the aggregate utility, Φ(e1, s1, . . . , eN , sN ) = P
i Ui(ei, si). The social optimum
maximizes Φ(si, ei) over all possible (si, ei) ∈ [0, 1]2N . Because enforcing a social optimum may at times
be conﬂicting with the optimal strategy for a given (set of) individual(s), to enforce a social optimum in
practice, we may need to assume the existence of a “social planner” who essentially decides, unopposed,
the strategy each player has to implement.

24

Below we discuss key-differences of the optimal strategy derived by a social planner with the individually-
optimal strategy for the case of homogeneous agents. We are currently working on the comparison of the
heterogeneous case for potential presentation at the workshop.

6.1 Total effort game

Summing the utility given by Eqn. (2) over i, and performing the substitution E = P
we get

i ei and S = P

i si,

Φ = N (M − pL) + (pL − b)E + (pL − c)S −

ES .

pL
N

Φ is continuous and twice differentiable in E and S, with

( ∂Φ

∂E = pL − b − pL
∂S = pL − c − pL

N S ,
N E .

∂Φ

Furthermore, ∂2Φ/∂E∂S = −pL/N < 0, while ∂2Φ/∂E2 = ∂2Φ/∂S2 = 0. The second derivative
test tells us that the only possible extrema of Φ are reached for the boundary values of E and S, that is
(E, S) ∈ {0, N }2. In other words, the only possible social optima are 1) passivity (for all i, (ei, si) =
(0, 0)), 2) full protection (for all i, (ei, si) = (1, 0)), or 3) full insurance (for all i, (ei, si) = (0, 1)). As
long as one of b or c is strictly positive, a social planner will never advise agents to invest into protection
and self-insurance at the same time.

By comparing the values of Φ in all three cases, we ﬁnd that if b < pL and b < c then all agents are
required to exercise maximum protection effort (ei, si) = (1, 0). With c < pL and c < b all agents will
self-insure at the maximum possible (ei, si) = (0, 1). A social planner will not encourage players to invest
in security measures if they are too expensive (c > pL and b > pL).

In the total effort security game we observe that in the Nash equilibrium there is almost always
Result 15:
too little protection effort exerted compared to the social optimum. In fact, for a wide range of parameter
settings no protection equilibria exist while the social optimum prescribes protection at a very low threshold.

• Protection: Except for very unbalanced parameter settings (i.e., pL > bN and c > b + pL N −1
N )

agents refrained from full protection. Now full protection by all agents is a viable alternative.

• Self-insurance: Full self-insurance now has to compete with full protection effort under a wider range

of parameters.

• Passivity: Agents remain passive if self-insurance is too expensive (c > pL). However, we ﬁnd a sub-
stantial difference with respect to protection behavior. Agents would selﬁshly refrain from protection
efforts if pL < bN since they would only be guaranteed the N -th part of their investments as returns.
Now the social planner can ensure that all agents protect equally so that it is beneﬁcial to protect up
until b < pL.

25

6.2 Weakest link game

In the weakest link game agents are required to protect at a common effort level to be socially efﬁcient. We
compute Φ by summing Eqn. (3) over i, and can express Φ as a function of ei, si and e0 = mini(ei):

Φ = N M − N pL(1 − e0) − b

ei + (pL(1 − e0) − c)

X

i

X

si .

i

In particular, for all i, we obtain

∂Φ/∂si = pL(1 − e0) − c .

Studying the sign of ∂Φ/∂si as a function of e0 allows us to determine the social optimum.
> 0. This requires pL > c, and is equivalent to e0 < 1 − c

First assume, that ∂Φ
∂si

pL . si = 1 gives
i ei − cN (independent of e0). We then need to pick

the extremum, which is of the form Φ = N M − b P
ei = e0 = 0 for all i, to get

Φ = N (M − c) ,

(29)

as a possible social optimum.

Now, assume that ∂Φ
∂si

one should pick si = 0, which leads to Φ = N M − pL(1 − e0)N − b P
for all i, leading to Φ = N (M − pL + (pL − b)e0). There are two cases to distinguish:

≤ 0, which is equivalent to having e0 ≥ 1 − c/pL. Then, to get the extremum,
i ei. Φ is maximized for ei = e0

• If pL > b, then one should choose e0 = 1, and get

Φ = N (M − b) .

Φ = N (M − pL) ,

• If, on the other hand, pL ≤ b then one should choose e0 = max(1 − c

pL , 0). That is, if pL ≤ c, then

we get e0 = 0, and

but if pL > c, then the social optimum could only be given by e0 = 1 − c
condition, and be of the form

pL , to satisfy our initial

(cid:18)

Φ = N

M − b − c +

(cid:19)

.

bc
pL

However it is clear that as long as the coefﬁcients are non-trivial, that last equation does not characterize a
social optimum.

In summing, if b < c and b < pL the social planner requires all agents to protect with maximum effort
(ei, si) = (1, 0). If c < b and c < pL the social planner requires all agents to self-insure (ei, si) = (0, 1).
Finally, the Nash equilibrium and social optimum coincide when security costs are high. Agents do not
invest in protection or self-insurance if b > pL or c > pL.

(30)

(31)

(32)

26

Result 16: The availability of self-insurance lowers the risk of below-optimal security in the Nash equi-
librium since agents have an alternative to the unstable Pareto-optimal protection equilibrium. From the
analysis of the weakest link game with many agents we know that deviation from the Pareto-optimal highest
protection level is very likely. A social planner can overcome these coordination problems.

• Protection: The Pareto-optimal Nash equilibrium coincides with socially optimal protection. How-

ever, the protection level would likely be lower in the Nash case due to coordination problems.

• Self-insurance: The self-insurance equilibria are equivalent for the Nash and social optimum analysis.

• Passivity: A social planner cannot expand the range of parameter values at which it would be socially

beneﬁcial to protect or self-insure while passivity would be prescribed in the Nash equilibrium.

6.3 Best shot game

We compute the social optimum by summing Ui given in Eqn. (4) over i, yielding that Φ can be expressed
as a function of ei, si, and e∗ = maxi(ei), as

Φ = N M − N pL(1 − e∗) − b

ei + (pL(1 − e∗) − c)

X

i

It is immediate that, to maximize Φ, one should pick ei = 0 for all i, except for one participant j, where
ej = e∗ ≥ 0. We then get

Φ = N M − N pL(1 − e∗) − be∗ + (pL(1 − e∗) − c)

so that ∂Φ/∂si = pL(1 − e∗) − c, which tells us under which conditions on e∗ (and consequently on b, c,
and pL) self-insurance is desirable.

We distinguish between two cases:

• If e∗ < 1 − c

pL , then ∂Φ
∂si

> 0, and so, one should pick si = 1 for all i. Then, we get

X

si .

i

X

si ,

i

Φ = N M − N c − be∗ ,

which is maximized for e∗ = 0 (satisfying our condition e∗ < 1 − c

pL ) and

Φ = N (M − c) .

(33)

• If e∗ ≥ 1 − c

pL , then ∂Φ
∂si

≤ 0, and so, one should pick si = 0 for all i.4 Then, we get

4In the equality case, the value of Φ is independent of that of si so that we can pick si = 0.

Φ = N M − N pL + (N pL − b)e∗ .

27

If b < N pL, then Φ increases in e∗, so and is maximized for e∗ = 1, with

If b < N pL, then one should pick e∗ as small as possible, that is, given our initial condition, e∗ =
1 − c

pL , and we get

Φ = N M − b .

Φ = N (M − c) −

bc
pL

.

(34)

(35)

With b > 0, c > 0, and pL > 0, it is clear that Eqn. (35) is not a social optimum, since it is dominated by
Eqn. (33).

To summarize, we ﬁnd that if b/c < N (i.e., protection is not at a prohibitive cost compared to insurance
and/or there is a reasonably large number of players), the social optimum is to have one player protect
as much as possible, the others not protect at all, and no one insures.
In practice, this may describe a
situation where all participants are safely protected behind an extremely secure ﬁrewall. If, on the other
hand b/c > N , which means there are either few players, insurance is very cheap compared to protection,
then the best strategy is to simply insure all players as much as possible.

In the best shot security Nash outcome there is almost always too little effort exerted compared
Result 17:
to the social optimum. Exceptions are few points in which full self-insurance remains desirable for the social
planner and all agents remain passive.

• Protection: Surprisingly, while protection is not even a Nash strategy we ﬁnd that a social planner

would elect an individual to exercise full protection effort.

• Self-insurance: Full self-insurance by every player is only desirable if protection costs are large.
Therefore, for most cases the strategy of a social planner will not coincide with the only Nash equi-
librium strategy.

• Passivity: In the Nash equilibrium agents are also too inactive. Passivity is highly undesirable from
a social planner’s perspective. Only if N pL < b no agent will be selected to exercise maximum
protection effort (while self-insurance might remain an option).

It is important to note that the social optimum variation that requires full protection by one individual
results in the whole population being unharmed, since one highly secure individual is enough to thwart all
attacks. Therefore, it is easy to see that protection is extremely desirable from a planners perspective. Out
of the three classical public goods games with homogeneous agents the best shot game can beneﬁt the most
from a guiding hand.

6.4 Weakest target security game (without mitigation)

We compute the social optimum by using Eqn. (8), assuming that 1 ≤ K ≤ N players pick e0 = mini(ei).
Further assume, without loss of generality, that these players’ indices are ranked from 1 to K. From Eqn. (6),

28

we get

For all i, we have

Φ = N M − b

ei − c

si − pL

(1 − si) .

X

i

X

i

K
X

i=1

∂Φ
∂ei

= −b ,

so, with b > 0, the social minimum is reached for a set of ei’s as small as possible. Further, consider the
function Φins where players 1, . . . , K all pick an insurance level s0 = 1 (all other values are chosen the
same as in Φ). Then we get

Φins − Φ = −c

(1 − si) − pL

−(1 − si) ,

K
X

i=1

K
X

i=1

that is, rearranging

With K ≥ 1 and a non-prohibitive insurance cost c < pL, we get

Φins − Φ = (pL − c)

(1 − si) .

K
X

i=1

Φins ≥ Φ ,

for any other constellation of si. From what precedes, the K players picking the lowest security level e0
should choose e0 = 0, si = 1. The N − K players above e0 should pick as low as possible a positive
security level, say ε > 0. Replacing in Φ, we get

Φ = N M − b(N − K)ε − cK − c

X

i>K

si ,

which implies that for i > K, the social optimum satisﬁes si = 0, so that,

Φ = N M − bN + K(bε − c) ,

from which we conclude that K = 1 maximizes Φ.

a single player to exacerbate no protection effort.

To summarize, we ﬁnd that in the weakest target game without mitigation a social planner would direct

Essentially, this player serves as a direct target for a potential attacker. However, as long as c < pL
the player would be directed to maximize self-insurance (ei, si) = (0, 1). If insurance is too expensive
(c > pL) then the social planner would prefer to leave the player uninsured (ei, si) = (0, 0). This strategy
is independent of the cost of protection. The remaining N − 1 players have to select their protection effort
as ei = ε > 0 (as small as possible). These players will not be attacked, and therefore will set their self-
insurance to the possible minimum (ε, 0). Passivity by all players is never an option in the social optimum.

29

Result 18: A social planner can easily devise a strategy to overcome the coordination problems observed
in the Nash analysis for the weakest target game with mitigation. We found that no pure Nash strategy exists
and, therefore, had to rely on the increased rationality requirement for entities to play a mixed strategy.5 The
average payoff for each player in the social optimum is considerably higher compared to the mixed Nash
equilibrium.

Understandably, without side-payments the node with the lowest protection effort is worse off compared
to his peers. However, the social planner could choose to devise a so-called “honeypot” system with the sole
goal of attracting the attacker while only suffering a marginal loss. A honeypot is a computer system (or
another device) that is explicitly designed to attract and to be compromised by attackers. It serves usually
a double purpose. First, it will detract attention from more valuable targets on the same network. Second,
if carefully monitored it allows gathering of information about attacker strategies and behaviors, e.g., early
warnings about new attack and exploitation trends [34].

An interesting aspect of the social optimum solution is the question how the individual is selected (if a
honeypot system cannot be devised). Obviously, a social planner might be able to direct an individual to
serve as a target (in particular, if c < pL). However, if insurance costs are large being a target requires
an almost certain sacriﬁce (dependent on the value of p). In anthropology and economics there are several
theories that relate to an individuals willingness to serve as a sacriﬁcial lamb. Most prominently, altruism and
heroism come to mind. Simon also introduced the concept of docility. This theory refers to an individual’s
willingness to be taught or to defer to the superior knowledge of others [45].

6.5 Weakest target security game (with mitigation)

We adopt the same strategy for ﬁnding Φ’s maximum as in the unmitigated case – that is, summing Eqn. (6)
over i, and then studying the variations of Φ over K, si and e0.

We have here:

Φ = N M − b

ei − c

si − pL(1 − e0)

(1 − si) .

X

i

X

i

K
X

i=1

Clearly the social optimum will satisfy si = 0 for i > K. Call Φ0 the total utility obtained when si = s0
for i ≤ K, and compute the difference between Φ0 and Φ:

Φ0 − Φ =

(s0 − si)(pL(1 − e0) − c) .

K
X

i=1

If e0 < 1 − c
same derivation as in the inﬁnite strength case yields: e0 = 0, s0 = 1, K = 1 and

pL , then Φ0 ≥ Φ if s0 ≥ si for all i. In other word, an extremum is reached for s0 = 1. The

5Economists are generally cautious regarding the assumption that individuals can detect and adequately respond to mixed

strategy play by opponents [42].

Φ = N M − c − b(N − 1)ε ,

30

for an arbitrarily small ε > 0. Conversely if we choose e0 > 1 − c
s0 = 0, and an extremum is reached for e0 = 1, K = N . We get

pL then we have to choose s0 ≤ si, so that

Φ = N M − N b .

Which of these two extrema is the larger one? Given that ε can be arbitrarily small, it basically depends
on whether N b < c or not, that is, the social optimum depends on the relative cost of insurance versus
protection.

From this derivation, the ﬁrst observation is that the social planner might prescribe the same strategy
as in the case of the weakest target game without mitigation. However, now the planner has a second
alternative. Since an attacker will not be able to compromise players if they are fully protected we ﬁnd that
(ei, si) = (1, 0) for all N players is a feasible strategy. The tipping point between the two strategies is at
N b < c. If this condition holds the social planner would elect to protect all machines in favor of offering
one node as honeypot and investing in its self-insurance. Note that again we ﬁnd that if protection and
self-insurance are extremely costly the planner will elect to sacriﬁce one entity without insurance. Passivity
is not a preferable option.

Result 19: Compared to the weakest target game without mitigation the social planner is better off if pro-
tection is cheap. Otherwise the planner has to sacriﬁce a node with or without self-insurance. Interestingly,
while compared to the pure Nash equilibrium outcome the social planner can increase the overall utility in
the network we ﬁnd that security expenditures are lowered. In the Nash equilibrium agents were willing to
fully protect against threats as long as (b ≤ c).

The last observation also holds for the mixed strategy case in both weakest target games (with or without
mitigation). That is, agents exert more effort in the Nash equilibrium (except when N b < c for the game
with mitigation).

7 Discussion of results

The results we obtained, and notably the disconnect between social optima and Nash equilibria we observed,
lead to a number of remarks that may prove relevant to organizational strategy. However, we want to preface
this discussion by pointing out that our analysis is a ﬁrst comparison of different security games with two
security options under common, but restrictive assumptions.

Most notably, we assume agents to be risk-neutral providers of the public protection good.

In our
game formulation we also simpliﬁed cost of protection (and insurance) to be linear. Including different risk
preferences, as well as uncertainty and limited information about important parameters of the game would
be important steps towards a sensitivity analysis of our results. Shogren found, for example, that risk-averse
agents will increase their contributions if information about other agents actions is suppressed [44]. Others,
e.g., [39], have obtained more nuanced results. We defer a more extensive analysis of such phenomena

31

to future work, but believe that the main trends and differentiating features between security games we
observed remain largely unchanged.

Security scenario identiﬁcation: We ﬁnd that security predictions vary widely between the ﬁve different
games. Similarly, policies set by a social planner do not only yield different contribution levels but may also
switch the recommended security action from protection to self-insurance and vice versa. Chief Security
Ofﬁcers’ tasks involve a careful assessment of threat models the company is faced with.

We want to emphasize that an integral part of the threat model should be an assessment of the organiza-
tional structure including system resources and employees. Similarly important is a detailed consideration
whether resources are protected independently or by an overarching system policy. For example, replica-
tion, redundancy and failover systems (that automatically switch to a standby database, server or network
if the primary system fails or is temporarily shut down for servicing) should most likely not be treated as
independent resources.

Managers should consider how the organizational structure of resources matches potentially existing
policies. For example, we can see that a policy that requires full protection by every individual is sub-optimal
if the most likely threat and organizational structure ﬁts the description of a best shot game. Contributions
resources are squandered and are likely to deteriorate. Not to mention that employees may simply ignore the
policy over time. See, for example, recent survey results that highlight that 35% of white-collar employees
admit to violations of security policies [28].

Security scenario selection: A security professional might be faced with a unidentiﬁable organization
and system-policy structure. However, we want to highlight that our research allows a more careful choice
between security options if managers can redesign organizations and policies. For example, the choice
between a system-wide ﬁrewall and intrusion detection system versus an individual alternative has important
implications on how incentives drive security-relevant behavior over time. Individual systems will better
preserve incentives, however, might have negative cost implications. The same choice applies between the
availability of backup tools and protective measures.

Leveraging strategic uncertainty: The example of the weakest-target game shows the importance of
the degree of dependency between agents. We show that in larger organizations a much lower average
level of self-insurance investments will be achieved because the strategic dependence between actors is
reduced. However, in turn more agents will elect to protect their resources (ei > 0 for more players). In
contrast, agents in small groups will respond to the increasing strategic uncertainty caused by the increased
interdependency by self-insuring their resources more often.

Introducing a social planner into the weakest-target game completely removes strategic uncertainty and
leads to both reduced self-insurance and protection investments. This apparent paradox emphasizes that
higher security investments do not necessarily translate in higher security – but instead that how the invest-
ments are made are crucial to the returns.

32

8 Conclusions

We model security decision-making by homogeneous and heterogeneous agents in a selection of ﬁve games.
Some of these games have historical foundations in public good theory (weakest-link, best-shot, and total
effort) whereas others were proposed recently (weakest target, with or without mitigation). Agents have two
security actions at their disposal. They can contribute to a network-side protection pool or invest in a private
good to limit losses.

At ﬁrst we are considering homogeneous populations of users, where all participants have the same
utility function. In practice, the homogeneity assumption is reasonable in a number of important cases,
particularly when dealing with very large systems where a large majority of the population have the same
aspirations. For instance, most Internet home users are expected to have vastly similar expectations and
identical technological resources at their disposal; likewise, modern distributed systems, e.g., peer-to-peer
or sensor networks generally treat their larger user base as equals.

However, the fact that the Internet is increasingly used as a common vector between different busi-
nesses, and even as a bridge between completely different user bases – for instance, acting as a bridge
between mobile phone networks, home users, and e-commerce retailers, emphasizes the need for consider-
ing heterogeneous agents, even though the games considered may become far less tractable.

We ﬁnd several key differences between the case of representative and diverse agents. For example,
we found that in the total effort game stability increases with more pronounced heterogeneity in the agent
population. The existence of a protection equilibrium in the weakest link game is threatened if only one
agent prefers to self-insure or to remain passive. In the best shot game heterogeneous agents can overcome
coordination problems more easily, so that a protection equilibrium is now possible, even though reaching
this equilibrium grows increasingly unlikely with a larger number of agents participating in the network.
Surprisingly, predictions for pure Nash equilibria of the weakest target games remain unchanged. However,
mixed strategies do now have to take consideration of the heterogeneity of agents. We defer the computation
of mixed strategies to future work.

When comparing the Nash and social optima for homogeneous agents we ﬁnd that the effects of central
planning compared to laissez-faire considerably differ according to the game considered. While in a number
of traditional cases borrowed from the public good literature, we observe that a central planner may increase
the average protection level of the network, we also note that strategic decisions are highly impacted by the
level of inter-dependency between the actions of different players.

In particular, we found that the common wisdom that having a central planner who decides upon security
implementation always yields higher protection contributions by individual players does not hold. Indeed,
it may at times be much more advantageous from an economic standpoint to invest in self-insurance instead
of protecting systems, or to select a few, unprotected, sacriﬁcial lambs in order to divert the attention of
potential attackers. This is particularly the case in situations which exhibit a “strategic uncertainty” due to
a very strong correlation between the actions of different agents, for instance, in our weakest target game
where the least secure player is always the one attacked.

33

8.1 Future research directions

First, we wish to extend our analysis to more formally explain the impact of limited information on agents
strategies. In particular, in computer security and distributed networks the assumption of full information is
useful as a ﬁrst approximation but requires further validation. Similarly, we plan to analyze the robustness of
our model by studying the inﬂuence of other simplifying assumption (e.g., linear cost parameters). Further-
more, we intend to evaluate strategy changes if moves are conducted sequentially rather than simultaneously.
In the context of decision making on the Internet Friedman et al. also distinguish between synchronous and
asynchronous moves [16].

Second, we are currently developing a set of laboratory experiments to conduct user studies and attempt
to measure the differences between perfectly rational behavior and actual strategies played. Our preliminary
investigations in the ﬁeld notably evidence that players often experiment with different strategies to try to
gain a better understanding of the game they are playing.

We are determined to incorporate our ﬁndings in updated models of system security. In prior work we
challenged the assumption that all players are perfectly rational. In [11] we assumed agents to also accept
strategies that are near rational and studied how system convergence prediction change.

Our research agenda of formal analysis combined with laboratory experiments is aimed to increase the
understanding of individual and organizational security decision making. However, we are also interested
in the design of meaningful security policies and aim at developing actionable guidelines for IT managers
and other practitioners.

9 Acknowledgments

Paul Laskowski greatly improved this manuscript with his tremendously helpful feedback. We further appre-
ciate the detailed evaluations of the anonymous reviewers at WWW08, EC’08, and WEIS 2008. This work
is supported in part by the National Science Foundation under ITR award ANI-0331659. Jens Grossklags’
research is also partially funded by TRUST (Team for Research in Ubiquitous Secure Technology), under
support from the NSF (award CCF-0424422) and the following organizations: BT, Cisco, ESCHER, HP,
IBM, iCAST, Intel, Microsoft, ORNL, Pirelli, Qualcomm, Sun, Symantec, Telecom Italia, United Tech-
nologies, and AFOSR (#FA 9550-06-1-0244).

References

1993.

[1] A. Acquisti and J. Grossklags. Privacy and rationality in individual decision making. IEEE Security & Privacy,

3(1):26–33, January–February 2005.

[2] E. Adar and B. Huberman. Free riding on Gnutella. First Monday, 5(10), October 2000.

[3] R. Anderson. Why cryptosystems fail. In Proceedings of ACM CCS’93, pages 215–227, Fairfax, VA, November

34

[4] R. Anderson. Why information security is hard - an economic perspective. In 17th Applications Security confer-

ence (ACSAC), New Orleans, LA, December 2001.

[5] R. Anderson and T. Moore. The economics of information security. Science, 314(5799):610–613, October 1998.

[6] AOL/NSCA. Online safety study, October 2004. Available at: http://www.security.iia.net.au/

downloads/safety_study_v04.pdf.

[7] T. August and T. Tunca. Network software security and user incentives. Management Science, 52(11):1703–

1720, November 2006.

[8] Bruskin Research. Nearly one in four computer users have lost content to blackouts, viruses and hackers ac-
cording to new national survey, 2001. Condensed results available at: http://www.corporate-ir.net/
ireye/ir_site.zhtml?ticker=iom&script=410&layout=-6&item_id=163653.

[9] J.A. Bull, L. Gong, and K. Sollins. Towards security in an open systems federation.

In Proceedings of the
Second European Symposium on Research in Computer Security (ESORICS), Springer LNCS No. 648, pages
3–20, Toulouse, France, November 1992.

[10] P. Chen, G. Kataria, and R. Krishnan. On software diversiﬁcation, correlated failures and risk management, April

2006. Available at SSRN: http://ssrn.com/abstract=906481.

[11] N. Christin, J. Grossklags, and J. Chuang. Near rationality and competitive equilibria in networked systems.
In Proceedings of ACM SIGCOMM’04 Workshop on Practice and Theory of Incentives in Networked Systems
(PINS), pages 213–219, Portland, OR, August 2004.

[12] D. Clark, J. Wroclawski, K. Sollins, and R. Braden. Tussle in cyberspace: deﬁning tomorrow’s Internet. In

Proceedings of ACM SIGCOMM’02, pages 347–356, Pittsburgh, PA, August 2002.

[13] G. Danezis and R. Anderson. The economics of resisting censorship. IEEE Security & Privacy, 3(1):45–50,

[14] I. Ehrlich and G.S. Becker. Market insurance, self-insurance, and self-protection. Journal of Political Economy,

January–February 2005.

80(4):623–648, July 1972.

[15] J. Franklin, V. Paxson, A. Perrig, and S. Savage. An inquiry into the nature and causes of the wealth of internet
miscreants. In Proceedings of 14th ACM CCS, Alexandria, VA, October/November 2007. Available from CMU
at http://www.cs.cmu.edu/˜jfrankli/acmccs07/ccs07_franklin_eCrime.pdf.

[16] E. Friedman, M. Shor, S. Shenker, and B. Sopher. An experiment on learning with limited information: non-
convergence, experimentation cascades, and the advantage of being slow. Games and Economic Behavior,
47(2):325–352, May 2004.

[17] D. Geer, C. Pﬂeeger, B. Schneier, J. Quarterman, P. Metzger, R. Bace, and P. Gutmann. Cyberinsecurity:
The cost of monopoly. how the dominance of microsoft’s products poses a risk to society, 2003. Avail-
able from Computer & Communications Industry Association at http://www.ccianet.org/papers/
cyberinsecurity.pdf.

[18] L.A. Gordon and M. Loeb. The economics of information security investment. ACM Transactions on Information

and System Security, 5(4):438–4572, November 2002.

35

[19] S. Gordon. The generic virus writer. In Proceedings of the International Virus Bulletin Conference, pages 121 –

138, Jersey, Channel Islands, 1994.

[20] S. Gordon. Virus writers - the end of the innocence?

In 10th Annual Virus Bulletin Conference (VB2000),
Orlando, FL, September 2000. Available from IBM Research at http://www.research.ibm.com/
antivirus/SciPapers/VB2000SG.htm.

[21] J. Grossklags, N. Christin, and J. Chuang. Secure or insure? A game-theoretic analysis of information security
games. In Proceedings of the 2008 World Wide Web Conference (WWW08), pages 209–218, Beijing, China,
April 2008.

[22] J. Grossklags, N. Christin, and J. Chuang. Security and insurance management in networks with heterogeneous
agents. In Proceedings of the Ninth ACM Conference on Electronic Commerce (EC’08), Chicago, IL, July 2008.

[23] G. Hardin. The tragedy of the commons. Science, 162(3859):1243–1248, December 1968.

[24] J. Hartley. Retrospectives: The origins of the representative agent. The Journal of Economic Perspectives,,

10(2):169–177, Spring 1996.

[25] K. Hausken. Returns to information security investment: The effect of alternative information security breach
functions on optimal investment and sensitivity to vulnerability. Information Systems Frontiers, 8(5):338–349,
December 2006.

[26] J. Hirshleifer. From weakest-link to best-shot:

the voluntary provision of public goods. Public Choice,

41(3):371–386, January 1983.

[27] P. Honeyman, G.A. Schwartz, and A. van Assche. Interdependence of reliability and security. In Workshop on

Information Systems and Economics (WISE 2007), Pittsburgh, PA, June 2007.

[28] Information Systems Audit and Control Association. Telephone survey conducted by MARC Research, October

2007. Find information at http://biz.yahoo.com/bw/071031/20071031005079.html?.v=1.

[29] Q. Lv, S. Ratnasamy, and S. Shenker. Can heterogeneity make gnutella scalable? In Proceedings of the First

International Workshop on Peer-to-Peer Systems (IPTPS), Cambridge, MA, March 2002.

[30] S. Malphrus. The “I Love You” computer virus and the ﬁnancial services industry, May 2000. Testimony before
the Subcommittee on Financial Institutions of the Committee on Banking, Housing, and Urban Affairs, U.S.
Senate. http://www.federalreserve.gov/BoardDocs/testimony/2000/20000518.htm.

[31] D. Moore, V. Paxson, S. Savage, C. Shannon, S. Staniford, and N. Weaver. Inside the Slammer worm. IEEE

Security and Privacy, 1(4):33–39, July 2003.

[32] D. Moore, C. Shannon, and J. Brown. Code-Red: a case study on the spread and victims of an internet worm. In
Proceedings of 2nd ACM/USENIX Internet Measurement Worskop, pages 273–284, Marseille, France, November
2002.

[33] A. O’Donnell and H. Sethu. On achieving software diversity for improved network security using distributed
coloring algorithms. In Proceedings of the 11th ACM Conference on Computer and Communications Security
(CCS), pages 121–131, Washington DC, USA, October 2004.

[34] N. Provos. A virtual honeypot framework. In Proceedings of the 13th USENIX Security Symposium, pages 1–14,

San Diego, CA, August 2004.

36

[35] N. Provos, D. McNamee, P. Mavrommatis, K. Wang, and N. Modadugu. The ghost in the browser: Analysis
of web-based malware. In Proceedings of the First USENIX Workshop on Hot Topics in Understanding Botnets
(HotBots’07), cambridge, MA, April 2007.

[36] E. Rescorla. Security holes... who cares? In Proceedings of the 12th USENIX Security Symposium, pages 75–90,

[37] J. Saltzer, D. Reed, and D. Clark. End-to-end arguments in system design. ACM Transactions on Computer

Washington, DC, August 2003.

Systems, 2(4):277–288, November 1984.

[38] T. Sandler and K. Hartley. Economics of alliances: The lessons for collective action. Journal of Economic

Literature, XXXIX(3):869–896, September 2001.

[39] T. Sandler, F. Sterbenz, and J. Posnett. Free riding and uncertainty. Economic Review, 31(8):1605–1617, De-

cember 1987.

[40] T.C. Schelling. The Strategy of Conﬂict. Oxford University Press, Oxford, UK, 1965.

[41] B. Schneier. Applied Cryptography: Protocols, Algorithms, and Source Code in C (2nd edition). Wiley Computer

Publishing, New York, NY, 2 edition, 1995.

[42] J. Shachat and J.T. Swarthout. Do we detect and exploit mixed strategy play by opponents? Mathematical

Methods of Operations Research, 59(3):359–373, July 2004.

[43] S. Shenker. Making greed work in networks: A game-theoretic analysis of switch service disciplines. IEEE/ACM

Transactions on Networking, 3(6):819–831, December 1995.

[44] J.F. Shogren. On increased risk and the voluntary provision of public goods. Social Choice and Welfare,

7(3):221–229, September 1990.

[45] H. Simon. Altruism and economics. American Economic Review, 83(2):156–161, May 1993.

[46] E. Skoudis. Malware: Fighting malicious code. Prentice Hall, Upper Saddle River, NJ, 2004.

[47] The Honeynet Project. Know your enemy: the tools and methodologies of the script-kiddie, July 2000. Available

online at http://project.honeynet.org/papers/enemy/.

[48] J.B. Van Huyck, R.C. Battallio, and R.O. Beil. Tacit coordination games, strategic uncertainty, and coordination

failure. American Economic Review, 80(1):234–248, 1990.

[49] H.R. Varian. System reliability and free riding. In L.J. Camp and S. Lewis, editors, Economics of Information
Security (Advances in Information Security, Volume 12), pages 1–15. Kluwer Academic Publishers, Dordrecht,
The Netherlands, 2004.

[50] N. Weaver and V. Paxson. A worst-case worm.

In Proceedings (online) of the Third Annual Workshop on
Economics and Information Security (WEIS’04). Available at http://www.dtc.umn.edu/weis2004/
weaver.pdf.

[51] L. Zhuang, J. D. Tygar, and R. Dhamija. Injecting heterogeneity through protocol randomization. International

Journal of Network Security, 4(1):45–58, 2007.

37

A Nash equilbrium in the weakest target game with mitigation

Assume the existence of a Nash equilibrium where 0 < K < N “type I” players satisfy ei = min(ei, e−i) =
e0, while (N − K > 0) players satisfy ei > e0 (“type II” players). Type-II players converge to si = 0 and
ei = e0 + ε with ε > 0 inﬁnitesimally small, since they are not going to be attacked, and therefore satisfy

Type-I players, for their part, satisfy

Ui = M − be0 − bε .

Ui = M − pL(1 − si)(1 − e0) − be0 − csi .

Assume for now that potential losses L are “large enough” compared to insurance and security prices,
namely pL > c, and pL > b. We have

and

We have three possibilities:

∂Ui
∂e0

∂Ui
∂si

= pL(1 − si) − b ,

= pL(1 − e0) − c .

Ui = M − b − c +

bc
pL

.

• si = 1 − b/pL. Then, regardless of the value chosen for e0, we have Ui constant and equal to

• si < 1 − b/pL. Then, Ui increases in e0, which leads to picking e0 = 1. Likewise, Ui decreases in

si, which leads to picking si = 0. We get

• si > 1 − b/pL. Then, Ui decreases in e0, which leads to picking e0 = 1. Likewise, Ui decreases in

si, which leads to picking si = 0. We obtain

(cid:16) 1

(cid:17)

pL = bc

Because c− bc
pLb > 0, we know the best type-I players can do is either Ui = M −c,
or Ui = M − b, depending on the respective values of b and c. Again, we have to distinguish between three
cases:

b − 1

= bc pL−b

pL

• b < c (insurance is more expensive than protection). Then, type-I players pick e0 = 1, si = 0, and
Ui = M − b. This is a stable Nash equilibrium, and because e0 = 1, all players are type-I players
(i.e., everybody gets attacked, but doesn’t really care because the attacker is not powerful enough to
knock down the security protections).

Ui = M − b .

Ui = M − c .

38

Figure 4: Utility functions and Nash equilibria in the weakest target game. The existence of Nash
equilibria depends on the relative values of b and c.

• b > c (protection is more expensive than insurance). Then, type-I players pick e0 = 0, si = 1, and
Ui = M − c. Type-II players are at Ui = M − bh for any h > 0. This means that type-I players have
an incentive to become type-II players by slightly increasing their protection, and discarding their
insurance. Then, everybody switches back to being attacked but now with e0 = h and si = 0, which,
since ∂Ui
∂si

> 0, immediately draws people to revert to si = 1. The utility Ui is then

(cid:12)
(cid:12)
(cid:12)e0=h

Ui = M − bh − c ,

which leads all players to again increase their security level by h, and shutting down insurance by
setting si = 0. (The associated gain in utility is c − bh, which is much greater than the bh saved by
reverting back to e0 = 0 while keeping insurance.)
This escalation continues until everybody reaches e0 = 1 − c

pL . Then, because ∂Ui
∂si

= 0,

(cid:12)
(cid:12)
(cid:12)e0=1− c

pL

people do not have an incentive to increase si = 0 at all. The utility of everybody, at that stage is

Ui = M − c − b +

bc
pL

.

Increasing e0 inﬁnitesimally leads to not be attacked (a saving of c), so people do that. Eventually,
people keep upping their security levels until approaching e0 = c/b, si = 1. (See Figure 4.) At
that stage, all type-I (attacked) players have two equivalent choices: either set e0 = c/b, si = 0
(not attacked), or decreasing e0 to zero, with si = 1. The utility is indeed M − c in both cases. If
everybody adopts the same protection strategy, however, then everybody becomes a type-I player, and
the utility goes down to M − 2c. Then, everybody has an incentive to have e0 = 0, si = 1, but once

39

safe players0M1e0Ui1−c/pLM−bM−b−c+bc /pLM−cM−2c0M−bMM−cM−b−c+bc /pL11−c/pLattacked playerssafe playerse0Uiattacked playersthis is done, everybody has an incentive to defect, and set again e0 > 0, si = 0. In short, there is a
perpetual cycle - no Nash equilibrium in that conﬁguration.

• b = c. Here again e0 = 1, si = 0 is the only stable Nash equilibrium for the same reasons as stated

above.

In summing, when the attacker is not omnipotent, people should have a strong incentive to pick high
levels of protection and to not insure. The situation is markedly different from the previous case, where the
power of the attacker essentially drove people to only insure, without protecting.

If protection is expensive, however, there is no stable Nash equilibrium.

40

