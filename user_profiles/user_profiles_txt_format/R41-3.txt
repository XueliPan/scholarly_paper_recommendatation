When Information Improves Information Security 

Jens Grossklags   Benjamin Johnson   Nicolas Christin 

March 17, 2009 

CMU-CyLab-09-004 

CyLab 

Carnegie Mellon University 

Pittsburgh, PA 15213 

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 

 
 
 
 
 

 

 

When Information Improves Information Security∗

(Extended version)

Jens Grossklagsa

Benjamin Johnsonb

Nicolas Christinc

aSchool of Information

University of California, Berkeley

Berkeley, CA 94720

jensg@ischool.berkeley.edu

bCyLab

Carnegie Mellon University

5000 Forbes Avenue
Pittsburgh, PA 15213

johnsonb@andrew.cmu.edu

cInformation Networking Institute and CyLab

Carnegie Mellon University

5000 Forbes Avenue
Pittsburgh, PA 15213

nicolasc@andrew.cmu.edu

Carnegie Mellon CyLab Technical Report, March 2009

Abstract

We investigate a mixed economy of an individual rational expert and several na¨ıve near-sighted
agents in the context of security decision making. Agents select between three canonical security ac-
tions to navigate the complex security risks of weakest-link, best shot and total effort interdependencies.
We further study the impact of two information conditions on agents’ choices. We provide a detailed
overview of a methodology to effectively determine and compare strategies and payoffs between the
different regimes. To analyze the impact of the different information conditions we propose a new
formalization. We deﬁne the price of uncertainty as the ratio of the expected payoff in the complete
information environment over the payoff in the incomplete information environment.

∗This work is supported in part by the National Science Foundation under ITR award ANI-0331659 (100x100), and the Army
Research Ofﬁce through contract number DAAD19-02-1-0389. Jens Grossklags’ work is also funded through a University of
California MICRO project grant in collaboration with DoCoMo USA Labs.

1 Introduction

Users frequently fail to deploy, or upgrade security technologies, or to carefully preserve and backup their
valuable data [20, 27], which leads to considerable monetary losses to both individuals and corporations ev-
ery year. This state of affairs can be partly attributed to economic considerations. End users may undertake
a cost-beneﬁt analysis and decide for or against certain security actions [13, 31]. However, this risk man-
agement explanation overemphasizes the rationality of the involved consumers [19]. In practice, consumers
face the task to “prevent security breaches within systems that sometimes exceed their level of understand-
ing” [5]. In other words, the amount of information users may be able to acquire and/or to process, is much
more limited than is required for a fully rational choice.

We focus on decision-making in different security scenarios that pose signiﬁcant challenges for average
users to determine optimal security strategies, due to interdependencies between users [16, 23]. Interdepen-
dencies occur when the actions of a given user have an effect on the rest of the network, in part or as a whole
(externalities), or when the status of a given user impacts that of other users. For example, consumers who
open and respond to unsolicited advertisements increase the load of spam for all participants in the network.
Similarly, choosing a weak password for a corporate VPN system can facilitate compromise of many user
accounts.

We anticipate the vast majority of users to be non-expert, and to apply approximate decision-rules that
fail to accurately appreciate the impact of their decisions on others [2]. In particular, in this paper, we assume
non-expert users to conduct a simple self-centered cost-beneﬁt analysis, and to neglect interdependencies.
Such users would secure their system only if the vulnerabilities being exploited can cause a direct annoyance
to them (e.g., their machine becomes completely unusable), but would not act when they cannot perceive
or understand the effects of their insecure behavior (e.g., when their machine is used as a relay to send
moderate amounts of spam to third parties).

In contrast, an advanced, or expert user fully comprehends to which extent their and others’ security
choices affect the network as a whole, and responds rationally. The ﬁrst contribution of this paper is to study
the strategic optimization behavior of such an expert user in an economy of inexperienced end users, using
three canonical security games that account for network effects [16].

The second contribution of this paper is to address how the security choices by users are mediated by
the information available on the severity of the threats the network faces. We assume that each individual
faces a randomly drawn probability of being subject to a direct attack. We study how the decisions of the
expert user differ if all draws are common knowledge, compared to a scenario where this information is
only privately known. With this approach we provide two important baseline cases for the impact of the
expert agent. We further propose a metric to quantify the differences in the total expected payoff between
the two information conditions, which we term the “price of uncertainty,” per analogy with Koutsoupias and
Papadimitriou’s “price of anarchy” [22].

By evaluating the price of uncertainty for a range of parameters in different security scenarios, we can
determine which conﬁgurations can accommodate limited information environments (i.e., when being less

2

informed does not signiﬁcantly jeopardize an expert user’s payoff), as opposed to conﬁgurations where
expert users and non-expert users achieve similar outcomes due to a lack of available information.

We ﬁrst discuss selected work related to our analytic model (Section 2). In Section 3, we summarize the
security games framework we developed in prior work, and detail our assumptions about agent behaviors
and information conditions. We present our methodology and formal analysis in Section 4. We discuss the
results and their implications in Section 5. Finally, we close with concluding remarks in Section 6.

2 Related work

In our prior work we have reviewed the research area of the economics of security in depth [16]. In this
paper we conduct a decision-theoretic analysis for a sophisticated (expert) agent who interacts with a group
of users that follow a simple but reasonable rule-of-thumb strategy. Our work signiﬁcantly differs from
prior decision-theoretic approaches. Gordon and Loeb present a model that highlights the trade-off between
perfect and cost-effective security [14]. They consider the protection of an information set that has an
associated loss if compromised, probability of attack, and probability that attack is successful. They show
that an optimizing ﬁrm will not always defend highly vulnerable data, and only invest a fraction of the
expected loss. Cavusoglu et al. [7] consider the decision-making problem of a ﬁrm when attack probabilities
are externally given compared to a scenario when the attacker is explicitly modeled as a strategic player in
a game-theoretic framework. Their model shows that if the ﬁrm assumes that the attacker strategically
responds then in most considered cases its proﬁt will increase. Schechter and Smith [30] consider the
decision-theoretic analysis from the perspective of the potential intruder. They highlight several modeling
alternatives for attacker behavior and their payoff consequences. The analytic work on security investments
and level of penalties for offenses is complemented by empirical research [29, 33].

We structure the remainder of the review of related literature and background information into three

selected areas in which we are making a research contribution.

2.1 Bounded rationality

Acquisti and Grossklags summarize work in the area of behavioral economics and psychology that is of
relevance for privacy and security decision-making [2]. Users’ decisions are not only limited by cognitive
and computational restrictions (i.e., bounded rationality), but are also inﬂuenced by systematic psychological
deviations from rationality.

Recent research has investigated agents that overemphasize earlier costs and beneﬁts at the expense of
their future well-being [1, 3]. Christin et al. suggest that agents respond near-rationally to the complexity
of networked systems [8]. In their model individuals are satisﬁed with a payoff within a small margin of the
optimal outcome.

Different from the above work that considers all players to act the same, the current paper studies a
mixed economy, with expert and non-expert users co-existing. While expert users are as rational as possible,

3

non-expert users deviate from rationality by adopting approximate (rules-of-thumb) decision strategies. In
practice, users frequently have to rely on rules-of-thumb when a ”quantitative method to measure security
levels” is not available [26]. Economic analysis including rule-of-thumb choices have been discussed outside
of the security context, e.g., [10, 11, 24].

2.2 Limited information

In the context of the value of security information, research has been mostly concerned with incentives for
sharing and disclosure. Several models investigate under which conditions organizations are willing to con-
tribute to an information pool about security breaches and investments when competitive effects may result
from this cooperation [12, 15]. Empirical papers explore the impact of mandated disclosures [6] or publi-
cation of software vulnerabilities [34] on the ﬁnancial market value of corporations. Other contributions to
the security ﬁeld include computation of Bayesian Nash outcomes for an intrusion detection game [25], and
security patrol versus robber avoidance scenarios [28].

We conduct a comparative analysis of strategies and payoffs for a sophisticated agent in a security model
when the likelihood of a directed attack is either common or private knowledge. In particular, we evaluate
the inﬂuence of the lack of information given different organizational dependencies [35].

2.3 Heterogeneous agents

In our previous work we have analyzed both the case of homogeneous [16] and heterogeneous agents [17].
When considering heterogeneous agents, however, we have focused on differences in the costs agents may
face. We assumed that users differ in the price they have to pay for protection and self-insurance, and that
they have different perceived or actual losses associated with successful (uninsured) security compromises.
In this paper we analyze the case of agents facing different attack probabilities. Indeed in practice, different
targets, even if they are part of a same network, are not all equally attractive to an attacker: a computer
containing payroll information is for instance, considerably more valuable than an old “boat anchor” sitting
under an intern’s desk.

Given certain differences in the attractiveness of a particular target the question remains how a defender
is able to determine a reasonable estimate of the attack probability. Such a problem far exceeds the scope
of this paper, whose main goal is to study the impact of information (or lack thereof) on security strategies,
and we refer the reader to the threat modeling literature. (See [4] for an introduction and references.)

3 Decision Theoretic Model

We next summarize the security games we analyze, and extend models we previously proposed [16] to the
case of an economy consisting of an expert user and several unsophisticated users.

4

3.1 Basic model

Self-protection and self-insurance.
In practice, the arsenal of a defender may include several actions to
prevent successful compromises and to limit losses that result from a breach. In Grossklags et al. [16]
we provide a model that allows a decoupling of investments in the context of computer security. On the
one hand, the perimeter can be strengthened with a higher self-protection investment (e.g., implementing or
updating a ﬁrewall). On the other hand, the amount of losses can be reduced by introducing self-insurance
technologies and practices (e.g., backup provisions). Formally, player i chooses an insurance level 0 ≤ si ≤
1 and a protection level 0 ≤ ei ≤ 1. b ≥ 0 and c ≥ 0 denote the unit cost of protection and insurance,
respectively, which are homogeneous for the agent population. So, player i pays bei for self-protection and
csi for insurance.

Interdependency. We focus in this work on tightly coupled networks [35].1 In a tightly coupled network
all defenders will face a loss if the condition of a security breach is fulﬁlled whereas in a loosely coupled
network consequences may differ for network participants. We denote H as a “contribution” function that
characterizes the effect of ei on agent’s utility Ui, subject to the protection levels chosen (contributed) by
all other players. We require that H be deﬁned for all values over (0, 1)N . We distinguish three canonical
cases that we discussed in-depth in prior work [16]:

• Weakest-link: H = min(ei, e−i).

• Best shot: H = max(ei, e−i).

• Total effort: H = 1
N

P

k ek.

where, following common notation, e−i denotes the set of protection levels chosen by players other than i.

Attack probabilities, network size and endowment. Each of N ∈ N agents receives an endowment
M .
If she is attacked and compromised successfully she faces a loss L. We assume that each agent i
draws an individual attack probability pi (0 ≤ pi ≤ 1) from a uniform random distribution. This models
the heterogeneous preferences that attackers have for different targets, due to their economic, political, or
reputational agenda. The choice of a uniform distribution ensures the analysis remains tractable, while
already providing numerous insights. We conjecture that different distributions (e.g., power law) may also
be appropriate in practice.

3.2 Player behavior

At the core of our analysis is the observation that expert and non-expert users differ in their understanding of
the complexity of networked systems. Indeed, consumers’ knowledge about risks and means of protection

1There is an ongoing debate whether researchers should assume full connectivity of a network graph given modern computer

security threats such as worms and viruses. (Personal communication with Nicholas Weaver, ICSI.)

5

with respect to privacy and security can be quite varied [2], and ﬁeld surveys separate between high and low
expertise users [32].

Na¨ıve (non-expert) user. Average users underappreciate the interdependency of network security goals
and threats [2, 32]. We model the perceived utility of each na¨ıve agent to only depend on the direct security
threat and the individual investment in self-protection and self-insurance. The investment levels of other
players are not considered in the na¨ıve user’s decision making, despite the existence of interdependencies.
We deﬁne the perceived utility for a speciﬁc na¨ıve agent j as:

P Uj = M − pjL(1 − sj)(1 − ej) − bej − csj .

Clearly, perceived and realized utility actually differ: by failing to incorporate the interdependencies of
all agents’ investment levels in their analysis, na¨ıve users may achieve sub-optimal payoffs that actually are
far below their own expectations. This paper does not aim to resolve this conﬂict, and, in fact, there is little
evidence that users will learn the complexity of network security over time [32]. We argue that non-expert
users would repeatedly act in an inconsistent fashion. This hypothesis is supported by ﬁndings in behavioral
economics that consumers repeatedly deviate from rationality, however, in the same predictable ways [21].

Sophisticated (expert) user. Advanced users can rely on their superior technical and structural under-
standing of computer security threats and defense mechanisms, to analyze and respond to changes in the
environment [9]. In the present context, expert users, for example, have less difﬁculty to conclude that
the goal to avoid censorship points at a best shot scenario, whereas the protection of a corporate network
frequently suggests a weakest-link optimization problem [16]. Accordingly, a sophisticated user correctly
understands her utility to be dependent on the interdependencies that exist in the network:

Ui = M − piL(1 − si)(1 − H(ei, e−i)) − bei − csi .

Binary strategies. We further restrict the actions available to each player (of either type) to make the
Instead of picking a continuous protection level 0 ≤ ei ≤ 1, players only have the
analysis tractable.
choice between ei = 0 (“do not protect”) or ei = 1 (“protect”) only. Likewise, the parameter space
for si is restricted to a binary choice si ∈ {0, 1}. While this may seem a very strong restriction, prior
analysis [16, 17] showed that, for all models we look at, Nash equilibria are all of the form (ei, si) ∈
{(0, 0), (0, 1), (1, 0)} (respectively, “passivity,” “full insurance,” and “full protection”) for all players i,
even when players can choose ei and si from a continuous spectrum of values. As such, binary choices for
ei and si are not necessarily a poor approximation of actual behavior.

3.3

Information conditions

Our analysis is focused on the decision making of the expert user subject to the bounded rational behaviors
of the na¨ıve network participants. That is, more precisely, the expert agent maximizes their expected utility

6

subject to the available information about other agents’ drawn threat probabilities and their resulting actions.
Two different information conditions may be available to the expert agent:

Complete information: Actual draws of attack probabilities pj for all j 6= i, and her own drawn proba-
bility of being attacked pi.

Incomplete information: Known probability distribution of the unsophisticated users’ attack threat, and
her own drawn probability of being attacked pi.

Therefore, the expert agent can accurately infer what each agent’s investment levels are in the complete
information scenario. Under incomplete information the sophisticated user has to develop an expectation
about the actions of the na¨ıve users.

4 Analysis methodology

In the remainder of this discussion, we will always use the index i to denote the expert player, and j 6= i to
denote the na¨ıve players. For each of the three games, weakest-link, best shot, and total effort, our analysis
proceeds via the following ﬁve-step procedure.

1. Determine player i’s payoff within the game for selected strategies of passivity, full insurance, and
full protection. As shown in [16, 17] through a relatively simple analysis, player i can maximize their
utility only by relying on (one or more of) these three strategies.

2. Determine the conditions on the game’s parameters (b, c, L, N , pi, and if applicable, pj for j 6= i)

under which player i should select each strategy.

3. Determine additional conditions on the game’s parameters such that the probability (relative to pi) of

each case, as well as the expected value of pi within each case can be easily computed.

4. Determine player i’s total expected payoff relative to the distribution on pi and all other known pa-

rameters.

5. In the case of complete information, eliminate dependence on pj for j 6= i by taking, within each

parameter case, an appropriate expected value.

Diligent application of this method generates a table recording the total expected payoffs for player i,
given any valid assignment to the parameters b, c, L, N . In the process it also generates tables of selection
conditions, probabilities, and expected payoffs for each possible strategy; and in the complete information
case, gives results for total expected payoffs conditioned on the exact draws of pj by the other players. The
results are presented in Tables 1–15.

In the remainder of this section we illustrate this method by considering, for each step listed above, one

game and one parameter case for which we have applied the appropriate step.

7

Step 1 example: Passivity payoff computation. Let us consider the challenge of determining payoffs
for player i’s passivity in the best shot game, under the conditions of limited information and param-
eter constraints b ≤ c. The general payoff function for the best shot game is obtained by substituting
H(ei, e−i) = max(ei, e−i) into the general utility function for all games, i.e. U (i) = M − piL(1 − si)(1 −
H(ei, e−i)) − bei − csi. Doing this, we obtain U (i) = M − piL(1 − si)(1 − max(ei, e−i) − bei − csi. To
get the payoff for player i’s passivity we plug in ei = si = 0 to obtain




Ui =

M − piL, if maxj6=i ej = 0

.



M, if maxj6=i ej = 1

Now in the incomplete information case, we do not know any of the pj for j 6= i, so we do not know all
the parameters to compute the required payoff. However, since we assume that the pj are independently and
uniformly distributed in [0, 1], we can compute an expected value for this payoff as follows. The probability
(over pj) that none of the other players protect (i.e. that maxj6=i pj < b/L) is exactly (b/L)N −1, and in this
case the payoff would be M − piL. The probability (over pi) that at least one of the other players protect
(i.e. that b/L ≤ maxj6=i pj) is exactly 1 − (b/L)N −1, and in this case the payoff would be M . Thus the total
expected payoff for selecting the passivity strategy is (b/L)N −1(M − piL) + (1 − (b/L)N −1)M , which
simpliﬁes to M − piL(b/L)N −1. We record this as the payoff result for passivity in the incomplete game,
with b ≤ c, as can be seen in Table 6.

Step 2 example: Strategy selection. Let us next consider the challenge of determining parameter condi-
tions under which we should select player i’s strategy in the weakest link game. Since this is a second step,
consider the game payoffs in Table 1 as given. We are interested in determining player i’s most strategic
play for any given parameter case. Select for consideration the case b ≤ c with incomplete information.
(Note: this is the most difﬁcult case for this game).

To determine the optimal strategy for player i, we must select the maximum of the quantities Passivity:
M −piL, Insurance: M −c, and Protection: M −b−piL(1−(1−b/L)N −1). We should choose passivity if it
is better than insurance or protection, i.e. M −piL > M −c and M −piL > M −b−piL(1−(1−b/L)N −1).
We should choose insurance if it is better than passivity or protection, i.e. M − c ≥ M − piL and M − c >
M − b − piL(1 − (1 − b/L)N −1). We should choose protection if it is better than passivity or insurance,
i.e. M − b − piL(1 − (1 − b/L)N −1) ≥ M − piL and M − b − piL(1 − (1 − b/L)N −1) ≥ M − c.

Re-writing the above inequalities as linear constraints on pi , we choose passivity if pi ≤ c/L and
L(1−(1−b/L)N −1) ; and we choose

L(1−(1−b/L)N −1) ; we choose insurance if pi > c/L and pi >

c−b

b

pi ≤
protection if

c−b

L(1−(1−b/L)N −1) ≤ pi ≤

L(1−(1−b/L)N −1) .

b

For simplicity of computation, we would like to have our decision mechanism involve only a single
inequality constraint on pi. To obtain this it is necessary and sufﬁcient to determine the ordering of the three
terms: c
L ,

L(1−(1−b/L)N −1) , and

L(1−(1−b/L)N −1) .

c−b

b

It turns out that there are only two possible orderings for these three terms. The single inequality c <
L(1−(1−b/L)N −1) ; while the reverse inequality

(1−b/L)N −1 determines the ordering: c

L(1−(1−b/L)N −1) <

L <

c−b

b

b

8

b

(1−b/L)N −1 ≤ c determines the reverse ordering on all three terms. This observation suggests we should
add sub-cases under b ≤ c depending on which of these two inequalities holds. See Table 2.

represented by a single linear inequality on pi. If c ≤
(because the new case conditions also guarantee pi <

Within each new sub-case the criterion for selecting the strategy that gives the highest payoff can now be
(1−b/L)N −1 , then passivity wins so long as pi < c/L;
L(1−b/L)N −1 ). Similarly insurance wins if pi ≥ c/L.
when

Protection never wins in this case because we cannot have

c−b

b

b

b

≤ pi ≤

L(1−(1−b/L)N −1)

. The computations for the case

L(1−(1−b/L)N −1)
b

(1−b/L)N −1 < c are similar;

we also have

b

(1−b/L)N −1 <

c−b

L(1−(1−b/L)N −1)

the results are recorded in Table 2.

Step 3 example: Case determination. Now, consider the challenge of determining additional constraints
on parameters in the total effort game, so that in any given case, the total payoffs can be represented by
simple closed form functions of the game’s parameters. Since this is a third step, we assume the second step
has been diligently carried out and consider the strategy conditions given in Table 12 as given. For brevity,
we consider only the incomplete information case under the assumption b ≤ c.

To illustrate the problem we are about to face, consider the condition for selecting passivity in the
incomplete game and case: b + b2(N − 1)/L < c. The condition here is that pi < bN/L. This condition
is possible if and only if bN < L. The case conditions determined thus far do not specify which of these is
the case; so for subsequent computations, we will need to know which it is, and therefore must consider the
two cases separately.

Going beyond this particular example, there are several other values in this table where a similar phe-
nomenon occurs. In particular, we need new cases to determine whether each of the following relations
b−b/N ≤ 1. (See Table 12). To combine these with previous cases
holds: bN/L ≤ 1,
in a way that avoids redundancy, we rewrite the conditions involving c as linear inequalities on c; obtaining
c ≤ b + (L − b)/N and c ≤ 2b − b/N .

b+(L−b)/N ≤ 1, and c−b

c

We are thus left to reconcile these additional cases with the current cases b ≤ c ≤ b + b2
L (N − 1) and
b + b2
L (N − 1) < c. To do this efﬁciently we must know the order of the terms b + L−b
N , and
b + b2
L (N − 1). Fortunately, it turns out that there are only two possible orderings on these terms; and
furthermore, which of the two orderings it is depends on the relation bN < L which we already needed to
specify as part of our case distinctions. If bN ≤ L, then b + b2
N and if bL > N ,
then the reverse relations hold.

L (N − 1) ≤ 2b − b

N ≤ b + L−b

N , 2b − b

Assuming limited information, b ≤ c, and dividing all cases according to bN ≤ L, it requires a total of
5 cases to determine all important relationships among important parameters for this game. We may have
bN ≤ L and b ≤ c ≤ b + b2
N ≤ c;
bN > L and c ≤ b + L−b

L (N − 1); bN ≤ L and b + b2

L (N − 1) < c < 2b − b

N ; bN ≤ L and2b − b

N < c. For reference, see table 15.

N ; and bN > L and b + L−b

Step 4 example: Total payoff computation. Let us determine the total expected payoff for the expert
player with incomplete information in the best shot game with b ≤ c. As intermediate steps we must
compute the probability that each strategy is played, along with the expected payoff for each strategy. The

9

total payoff is then given by (Probability of passivity · Expected payoff for passivity) + (Probability of
insurance · Expected payoff for insurance) + (Probability of protection · Expected payoff for protection).

The expected probability of passivity in this case is 1, with a payoff of M − piL(b/L)N −1. To get an
expected payoff, we compute the expected value of pi within this case. Since there is no constraint on pi
and it is drawn from a uniform distribution its expected value is 1/2. Thus the expected payoff for this case
is M − (L/2)(b/L)N −1. The total expected payoff is thus M − (L/2)(b/L)N −1.

Step 5 example: Eliminating dependencies on other players. Consider the challenge of examining the
total expected payoff for player i, who has complete information, and rewriting this payoff in a way that
is still meaningful as an expected payoff, but does not depend on any pj for j 6= i. The reason we want
to do this last step is so we can compare complete information payoff results with incomplete information
payoff results. We can only do this if the direct dependence on privileged information is removed from
the complete information case payoff. Our method of information removal involves taking an appropriate
expected value.

For this example we consider the best shot game with complete information in the case b ≤ c. Since
this is a ﬁfth step, we should assume that the fourth step – computing the expected payoff for player i as a
function of parameters that may include pj for j 6= i – has been accomplished.

Indeed, by following steps 1–4, the total expected payoffs for player i (conditioned on other players) in
the case b ≤ c can be derived, subject to two additional sub-cases. If maxj6=i pj ≤ b/L, then the expected
payoff is M − c + c2/L; while if b/L < maxj6=i pj, then the expected payoff is M − b + b2/L.

To generate an appropriate “a posteriori” expected payoff over all choices of pj, we compute the proba-
bility (over choice of pj) that we are in case maxj6=i pj ≤ b/L times the payoff for that case, plus the proba-
bility (over pj) that we are in the case b/L < maxj6=i pj times the payoff for that case. We obtain (b/L)N −1 ·
(cid:2)M − c + c2/L(cid:3) +
· (cid:2)M − b + b2/L(cid:3). The end result is M − b (1 − b/2L) (b/L)N −1. See
Table 10.

1 − (b/L)N −1i

h

Table 1: Weakest link security game: Payoffs for different strategies under different information conditions

Case

c < b

b ≤ c

b ≤ c and minj6=i pj < b
L
b ≤ c and b
L ≤ minj6=i pj
c < b

Information

Type

Payoff
Passivity
Complete M − piL
Complete M − piL
Complete M − piL

Incomplete M − piL
Incomplete M − piL

Self-Insurance

Payoff

M − c
M − c
M − c

M − c

M − c

Payoff

Protection

M − b − piL
M − b − piL

M − b

M − b − piL

M − b − piL

(cid:16)

1 − (cid:0)1 − b

L

(cid:1)N −1(cid:17)

10

Table 2: Weakest link security game: Conditions to select protection, self-insurance or passivity strategies

Case

c < b

b ≤ c and minj6=i pj < b
L
b ≤ c and b
L ≤ minj6=i pj
c < b

b ≤ c ≤

b
L )N −1
(1− b
L )N −1 < c

b
(1− b

Information

Type

Complete
Complete
Complete

Incomplete
Incomplete

Conditions
Passivity
pi < c
L
pi < c
L
pi < b
L
pi < c
L
pi < c
L

Conditions

Self-Insurance

pi ≥ c
L
pi ≥ c
L
NEVER!
pi > c
L
pi ≥ c
L

Incomplete

pi <

b
L )N −1
L(1− b

pi >

c−b
“
1−(1− b

L

L )N −1”

Conditions
Protection
NEVER!
NEVER!
pi ≥ b
L
NEVER!
NEVER!

b
L(1− b

L )N −1 ≤ pi,

pi ≤

c−b
L(1−(1− b

L )N −1)

Table 3: Weakest link security game: Probabilities to select protection, self-insurance or passivity strategies

Probability
Passivity

Probability

Self-Insurance

Probability
Protection

Information

Type

Complete
Complete
Complete

Incomplete
Incomplete

Incomplete

c
L
c
L
b
L
c
L
c
L

Case

c < b

WC1
WC2a
WC2b

WI1
WI2

WI3

WI4

b ≤ c and minj6=i pj < b
L
b ≤ c and b
L ≤ minj6=i pj
c < b

b ≤ c ≤

b
L )N −1
(1− b
b
L )N −1 < c and
(1− b
(cid:16)
1 − (cid:0)1 − b

L

c < b + L

(cid:1)N −1(cid:17)

b
(1− b
(cid:16)
1 − (cid:0)1 − b

L )N −1 < c and
(cid:1)N −1(cid:17)

L

≤ c

b + L

1 − c
L
1 − c
L

0

1 − c
L
1 − c
L

1 − b
L

0
0

0
0

c−b
L(1−(1− b

L )N −1)
b
L )N −1
L(1− b
b
L )N −1
L(1− b

−

1 −

b
L )N −1
L(1− b

1 −

c−b
“
1−(1− b

L

L )N −1”

Incomplete

b
L )N −1
L(1− b

0

11

Table 4: Weakest link security game: Total expected game payoffs, conditioned on other players

Information

Total Expected Payoff for player i

(conditioned on other players)

Type

Complete
Complete
Complete

Incomplete
Incomplete

Case

c < b

WC1
WC2a
WC2b

WI1
WI2

WI3

WI4

b ≤ c and minj6=i pj < b
L
b ≤ c and b
L ≤ minj6=i pj
c < b

c < b + L

(cid:1)N −1(cid:17)

b ≤ c ≤

b
(1− b

(cid:16)

b
L )N −1
(1− b
L )N −1 <
1 − (cid:0)1 − b
L )N −1 <

L

b
(1− b
1 − (cid:0)1 − b

L

(cid:16)

b + L

(cid:1)N −1(cid:17)

≤ c

M − c + c2
2L
M − c + c2
2L
M − b + b2
2L
M − c + c2
2L
M − c + c2
2L

Incomplete

M − c +

b2
2L(1− b

L )N −1 +

(c−b)2
“
1−(1− b

L )N −1”

2L

Incomplete M − b − L
2

(cid:16)

1 − (cid:0)1 − b

L

(cid:1)N −1(cid:17)

+

b2
L )N −1
2L(1− b

Table 5: Weakest link security game: Total expected game payoffs, not conditioned on other players

b ≤ c ≤

b
(1− b
b
(1− b

L )N −1 < c < b + L
L )N −1 < b + L

(cid:16)

b
L )N −1
(1− b
(cid:16)
1 − (cid:0)1 − b

L

1 − (cid:0)1 − b

L

(cid:1)N −1(cid:17)

(cid:1)N −1(cid:17)

Incomplete

M − c +

≤ c

Incomplete M − b − L
2

Information

Type

Complete

Complete

Incomplete
Incomplete

Total Expected Payoff for player i
(not conditioned on other players)

M − c + c2

2L + (c − b) (cid:0)1 − c+b

2L

(cid:1) (cid:0)1 − b

L

(cid:1)N −1

M − c + c2
2L

M − c + c2
2L
M − c + c2
2L

L )N −1 +

b2
2L(1− b
(cid:16)
1 − (cid:0)1 − b

(c−b)2
“
1−(1− b
2L
(cid:1)N −1(cid:17)

+

L

L )N −1”

b2
L )N −1
2L(1− b

M − c + c2
2
(cid:16)
(cid:17) (cid:16)
1 − b2
L2

1 − (cid:0)1 − b

L

(cid:1)N −1(cid:17)

Naive

Naive

M − b + b2

2L − L

2

Case

c < b

b ≤ c

c < b

c < b

b ≤ c

WC1

WC2

WI1
WI2

WI3

WI4

WN1

WN2

12

Table 6: Best shot security game: Payoffs for different strategies under different information conditions

Case

c < b

b ≤ c

b ≤ c and maxj6=i pj < b
L
b ≤ c and b
L ≤ maxj6=i pj
c < b

Information

Type

Complete
Complete
Complete

Payoff
Passivity
M − piL
M − piL

M

M − piL
Incomplete
Incomplete M − piL (cid:0) b

L

(cid:1)N −1

Payoff

Payoff

Self-Insurance

Protection

M − c
M − c
M − c

M − c

M − c

M − b
M − b
M − b

M − b

M − b

Table 7: Best shot security game: Conditions to select protection, self-insurance or passivity strategies

b ≤ c and maxj6=i pj < b/L
b ≤ c and b/L ≤ maxj6=i pj

Case

c < b

c < b
b ≤ c

Type

Information Conditions
Passivity
pi < c/L
pi < b/L
ALWAYS!

Complete
Complete
Complete

pi < c/L
Incomplete
Incomplete ALWAYS!

Conditions

Self-Insurance

pi ≥ c/L
NEVER!
NEVER!

pi ≥ c/L
NEVER!

Conditions
Protection
NEVER!
pi ≥ b/L
NEVER!

NEVER!
NEVER!

Table 8: Best shot security game: Probabilities to select protection, self-insurance or passivity strategies

Case

c < b

BC1
BC2a
BC2b

BI1
BI2

b ≤ c and maxj6=i pj < b
L
b ≤ c and b
L ≤ maxj6=i pj
c < b
b ≤ c

Information

Type

Complete
Complete
Complete

Incomplete
Incomplete

c
L
b
L
1

c
L
1

Probability
Passivity

Probability

Self-Insurance

Probability
Protection

1 − c
L

0
0

0

1 − c
L

1 − b
L

0

0

0
0

13

Table 9: Best shot security game: Total expected game payoffs, conditioned on other players

Case

c < b

b ≤ c

b ≤ c and maxj6=i pj < b
L
b ≤ c and b
L ≤ maxj6=i pj
c < b

Information

Total Expected Payoff

Type

Complete
Complete
Complete

Incomplete

Incomplete

M − c + c2
2L
M − b + b2
2L

M

M − c + c2
2L
(cid:1)N −1

M − L
2

(cid:0) b
L

BC1
BC2a
BC2b

BI1

BI2

Table 10: Best shot security game: Total expected game payoffs, not conditioned on other players

Case

Information

Total Expected Payoff

Complete
Complete M − b (cid:0)1 − b

(cid:1)N −1

BC1

BC2

BI1

BI2

BN1
BN2

c < b

b ≤ c

c < b

b ≤ c

c < b
b ≤ c

Type

Incomplete

Incomplete

Naive
Naive

M − c + c2
2L
(cid:1) (cid:0) b
L
M − c + c2
2L
(cid:1)N −1

2L

M − L
2

(cid:0) b
L

M − c + c2
2
M − b + b2
2L

Table 11: Total effort security game: Payoffs for different strategies under different information conditions

Payoff
Passivity
M − piL

Case

Information

Type

Complete
Complete

c < b
b ≤ c

c < b
b ≤ c

M − piL (1 − K/N )

Incomplete
Incomplete M − pi (b + (L − b)/N )

M − piL

Self-Insurance

Payoff

M − c
M − c

M − c
M − c

Payoff

Protection

M − b − piL (1 − 1/N )

M − b − piL (1 − (K + 1)/N )

M − b − piL (1 − 1/N )
M − b − pi (b − b/N )

14

Table 12: Total effort security game: Conditions to select protection, self-insurance or passivity strategies

Case

c < b

b ≤ c ≤ b(N − K)

b(N − K) < c

c < b
b ≤ c ≤ b + b2

L (N − 1)

b + b2

L (N − 1) < c

Information

Type

Complete
Complete

Complete

Incomplete
Incomplete

Incomplete

Conditions
Passivity
pi < c
L
c

pi <

N )

L(1− K
pi < bN
L

pi < c
L
pi < c
pi < bN
L

b+ L−b

N

Conditions

Self-Insurance

pi ≥ c
L
c

pi ≥

pi >

N )

L(1− K
c−b

L(1− K+1
N )
pi ≥ c
L
pi ≥ c
b+ L−b
pi > c−b
b− b
N

N

Conditions
Protection
NEVER!
NEVER!

bN
L ≤ pi ≤

c−b

L(1− K+1
N )

NEVER!
NEVER!

bN

L ≤ pi ≤ c−b
b− b
N

Table 13: Total effort security game: Probabilities to select protection, self-insurance or passivity strategies

Probability
Passivity

Probability

Self-Insurance

Probability
Protection

1 − c
L
c

1 −

L(1− K

N )

L(1− K

N )

1 −

c−b

N )
L(1− K+1

N )
L(1− K+1

c−b

− bN
L

1 − bN
L

Complete

L(1− K

N )

1 −

c

L(1− K

N )

1 − c
L
1 − c

b+ L−b

N

b+ L−b

N

b(N − K) < c < b + L (cid:0)1 − K+1

(cid:1)

N

TC1
TC2

TC3

TC4

TC5

TC6

TI1
TI2

TI3

TI4

TI5

TI6

Case

c < b

bN ≤ L and

b ≤ c ≤ b(N − K)

bN ≤ L and

bN ≤ L and
b + L (cid:0)1 − K+1
L < bN and

N

(cid:1) ≤ c

b ≤ c < L (cid:0)1 − K

(cid:1)

N

L < bN
and L (cid:0)1 − K+1

(cid:1) < c

N
c < b

bN ≤ L and

b ≤ c ≤ b + b2

L (N − 1)

bN ≤ L and
2b − b
N ≤ c
L < bN and

b ≤ c < b + L−b
N
L < bN and
b + L−b
N ≤ c

c
L
c

bN
L

bN
L

c

1

c
L
c

bN
L

bN
L

c

1

Information

Type

Complete
Complete

Complete

Complete

Complete

Incomplete
Incomplete

Incomplete

15

bN ≤ L and

Incomplete

b + b2

L (N − 1) < c < 2b − b

N

Incomplete

1 − c−b
b− b
N

c−b
b− b
N

− bN
L

1 − bN
L

Incomplete

b+ L−b

N

1 − c

b+ L−b

N

0
0

0

0

0
0

0

0

0

0

0

0

Table 14: Total Effort security game: Total expected game payoffs, conditioned on other players

Case

Information

Total Expected Payoff

TC1
TC2

bN ≤ L and

TC3

bN ≤ L and

c < b
b ≤ c ≤ b(N − K)
b(N − K) < c < b + L (cid:0)1 − K+1
b + L (cid:0)1 − K+1
b ≤ c ≤ L (cid:0)1 − K

(cid:1) ≤ c

(cid:1)

N

N

N

(cid:1)

bN ≤ L and
TC4
TC5 L < bN and
TC6 L < bN and L (cid:0)1 − K

(cid:1) < c

N

TI1
TI2

TI3

TI4
TI5

TI6

bN ≤ L and

bN ≤ L and

b + b2

bN ≤ L and
L < bN and

c < b
b ≤ c ≤ b + b2

L (N − 1)

L (N − 1) < c < 2b − b
N ≤ c

N

2b − b
b ≤ c < b + L−b
N

L < bN and

b + L−b

N ≤ c

Complete M − b − L
2
Complete

M − c +

Type

Complete
Complete

Complete

Complete

Incomplete
Incomplete

Incomplete

Incomplete
Incomplete

Incomplete

M − c + c2
2L
c2

M − c +

M − c + b2N

2L(1− K

N )
2L + (c−b)2
2L(1− k+1
N )
(cid:1) + b2N
(cid:0)1 − K+1
N
c2

2L

M − L
2

2L(1− K
N )
(cid:1)
(cid:0)1 − K
N
M − c + c2
L
c2

M − c +

M − c + b2N

M − b − 1
2

2(b+ L−b
N )
2L + (c−b)2
2(b− b
N )
(cid:1) + b2N
(cid:0)b − b
N
c2

2L

M − c +

M − 1
2

N )
2(b+ L−b
(cid:0)b + L−b
(cid:1)

N

5 Results

5.1 Strategies and payoffs

Our results provide us with insights into security decision-making in networked systems. We can recognize
several situations that immediately relate to practical risk choices. We start with basic observations that are
relevant for all three games, before discussing the different games and information conditions in more detail.

General observations applicable to all three security games. Every scenario involves simple cost-
beneﬁt analyses for both sophisticated and na¨ıve agents [13]. Agents remain passive when the cost of
self-protection and self-insurance exceeds the expected loss. Further, they differentiate between the two
types of security actions based on their relative cost. This behavior describes what we would usually con-
sider as basic risk-taking that is part of everyday life: It is not always worth protecting against known risks.
One important feature of our model is the availability of self-insurance. If c < b the decision scenario
signiﬁcantly simpliﬁes for all games and both information conditions. This is because once insurance is
applied, the risk and interdependency among the players is removed. The interesting cases for all three
games arise when b ≤ c and protection is a potentially cost-effective option. Within this realm insurance
has a more subtle effect on the payoffs.

There are important differences between the two agent types. The expert agent considers the strate-
gic interdependencies of all agents’ choices. Therefore, given the same draw of an attack probability she

16

Table 15: Total effort security game: Total expected game payoffs, not conditioned on other players

Case

c < b

TC1

TC2-4

bN ≤ L and b ≤ c

Information

Type

Complete

Complete

TC5-6

L < bN and b ≤ c

Complete

TI1
TI2

TI3

TI4

TI5

TI6

TN1
TN2

c < b

bN ≤ L and

Incomplete
Incomplete

b ≤ c ≤ b + b2

L (N − 1)

bN ≤ L and

Incomplete

b + b2

L (N − 1) < c < 2b − b

N

bN ≤ L and
2b − b
N ≤ c

Incomplete

L < bN and b ≤ c < b + L−b
N
N ≤ c

L < bN and b + L−b

Incomplete

Incomplete

c < b
b ≤ c

Naive
Naive

Total Expected Payoff

*PbN − c
b c

k=0

P r[k] ·

M − c +

M − c + c2
2L

(cid:18)

(cid:18)

+ PbN −1− N
k=bN − c

+ PN −1

k=bN − N

(cid:16)

P r[k] ·

M − c + b2N

L (c−b)c
b +1c
L (c−b)c P r[k] ·
P r[k] ·
L +1c P r[k] · (cid:0)M − L

M − b − L
2
(cid:18)

M − c +

k=bN − cN

*PbN − cN
L c
k=0
+ PN −1

(cid:19)

c2

N )

2L(1− k
2L + (c−b)2
N )
2L(1− k+1
(cid:0)1 − k+1
(cid:1) + b2N
(cid:19)

2L

N

(cid:19)

(cid:17)

c2

N )

2L(1− k
2N (N − k)(cid:1)

M − c + c2
L
c2

M − c +

N )
2(b+ L−b

M − c + b2N

2L + (c−b)2
N )
2(b− b

M − b − 1
2

(cid:0)b − b

N

(cid:1) + b2N

2L

M − c +

M − 1
2

c2

2(b+ L−b
N )
(cid:0)b + L−b
(cid:1)

N
M − c + c2
2
(cid:1) + b2
(cid:0)b − b

N

L

M − b − 1
2

(cid:0)1 − 1

2N

(cid:1)

*P r[k] = (cid:0)N −1

(cid:1) (cid:0)1 − b

(cid:1)k (cid:0) b
L

L

k

(cid:1)N −1−k

is the probability that exactly k players other than i choose protection.

17

sometimes prefers to self-insure, or remain passive when na¨ıve agents would always protect without further
consideration. However, the expert agent never protects when the na¨ıve would not (given the same pi).

The na¨ıve agents face a payoff reduction as a result of their limited understanding of correlated threats,
but even the sophisticated agent can experience a similar payoff reduction due to limited information. On
the one hand, she might invest in self-protection or self-insurance when it is not necessary because the na¨ıve
agents collectively or individually secured the network. On the other end, she may fail to take a security
action when a (typically low probability) breach actually occurs. It is important to mention that she acted
rationally in both situations, but these additional risks remain.

Basic payoffs for different security actions: We can immediately observe that the additional risk due to
limited information results from different mechanisms for each security scenario. In the weakest-link game
(Table 1) we ﬁnd that self-protection carries a risk for the expert agent with limited information that at least
one na¨ıve agent chooses not to protect. This would result in a break-down of system security and a waste of
self-protection expenditure. In contrast, in the best shot game (Table 6) the investment in preventive action
always secures the network but with limited information this may be a duplicative effort. In the total effort
game these risks are more balanced (Table 11). The expert can add or withhold her N -th part of the total
feasible security contribution. Depending on the cost of security she has to estimate the expected number of
na¨ıve contributors K in order to respond adequately.

Conditions for choice between different security actions:
In the weakest-link game and complete infor-
mation, the expert agent can utilize the lowest attack probability that any na¨ıve agent has drawn. If this value
is below the required threshold for protection, (i.e. if minj6=i pj < b/L), then the sophisticated agent will
never protect. Otherwise, depending on her own draw she will make or break a successful defense. Under
incomplete information she has to consider the likelihood (1 − b/L)N −1 that all na¨ıve agents protect. In all
cases there is now a residual likelihood that she might self-insure. See Table 2.

In the best shot game the fully informed expert can simply determine the highest likelihood of being
attacked for any naive agent to decide whether she should contribute to system protection. With full or
limited information, it is obvious that she will only have to contribute very rarely, and can mostly rely on
others’ efforts. Nevertheless, it is surprising to ﬁnd that in the incomplete information scenario the expected
payoff from passivity always dominates the expected payoff for protection, even when the expected loss
is near total (pi ∼ 1). The sophisticated user with limited information will never protect. Under neither
information condition is it optimal to self-insure if b ≤ c. See Table 7 for details.

Next consider the the total effort game (Table 12). Under full information with b ≤ c, all decision con-
ditions depend non-trivially on K, the number of contributors to protection. Under incomplete information
the expert must compute the expected value of K, which is (1 − b/L)(N − 1). The case differences between
complete and incomplete conditions reﬂect the replacement of K with E[K], and subsequent simpliﬁcation.
In all cases, the critical factor for the decision to protect is whether the potential loss is N times greater than
the cost of protection (i.e. piL ≥ bN ).

18

Case boundaries for choice between different security actions:
In Figure 3, we plot the cases used to
record total expected payoffs for the expert agent in Tables 5, 10, and 15. The associated tables for the prob-
abilities of self-protection, self-insurance and passivity (within each case) are available in the companion
technical report [18].

In the weakest-link game only cases 3 and 4 allow for investments in self-protection. We ﬁnd that
increasing the number of agents, N , results in a shrinkage of both cases 3 and 4 to the beneﬁt of case 2.
In contrast, the determination of case boundaries in the best shot game is independent of the size of the
network. Finally, in the total effort game only cases 3 and 4 allow for rational self-protection investments.
Again an increase in the network size reduces the prevalence of these cases (since bN ≤ L is a necessary
condition).

Payoffs: Tables 5, 10, and 15 contain the total expected payoff for decisions made by the sophisticated
agent, but also for the na¨ıve agents.

We have already highlighted that for c < b all agents follow the same simple decision rule to decide
between passivity and self-insurance. Therefore, payoffs in this region are identical for all agent types
in the case of homogeneous security costs. But, there are payoff differences among all three information
conditions for some parts of the parameter range when b ≤ c.

Consider the graphs in Figure 1. We plot the payoff functions for sophisticated agents types under the
different information conditions, as well as the payoff output for the non-expert agent. It is intuitive that the
na¨ıve agents suffer in the weakest-link game since they do not appreciate the difﬁculty to achieve system-
wide protection. Similarly, in the best shot game too many unsophisticated agents will invest in protection
lowering the average payoff. In the total effort game, sophisticated agents realize that their contribution is
only valued in relation to the network size. In comparison, na¨ıve agents invest more often. Further, the
payoff proﬁle of the unsophisticated agents remains ﬂat for b < c. This reﬂects the fact that the na¨ıve agent
ignores the insurance option whenever protection is cheaper.

We can observe that the sophisticated agents will suffer from their misallocation of resources in the
weakest-link game when information is incomplete. In the best shot game this impact is limited, but there is
a residual risk that no na¨ıve agent willingly protects due to an unlikely set of draws. In such cases the fully
informed expert could have chosen to take it upon herself to secure the network. In the total effort game we
observe a limited payoff discrepancy for expert users as a result of limited information.

5.2 Price of uncertainty

From a system design perspective it is important to select parameter settings (e.g., making available speciﬁc
security technologies) that maximize user utility and are robust to changes in the environment. The security
games we analyze in this paper are a signiﬁcant challenge in both aspects. In particular, from Figure 1 we
can infer that the penalty for the lack of complete information about attack threats can be highly variable
depending on the system parameters. We argue that the reduction of this disparity should be an important

19

(a) Weakest link

(b) Best shot

(c) Total effort

Figure 1: Total expected payoffs for the strategic player under different information conditions, compared
with that of the na¨ıve agents, expressed as a function of the self-protection cost c. L = M = 1, N = 4, and
b is ﬁxed to b = 0.20 in this set of examples.

design goal. To further this goal we propose a mathematical formulation that we call the price of uncertainty.
We then apply this measure to the analysis of security games.

Deﬁnition: We are interested in a mathematical measure that allows us to quantify the payoff loss due
to incomplete information for sophisticated agents, that can be applied to a variety of decision-theoretic
scenarios.
It is nontrivial to arrive at a deﬁnitive answer for this problem statement; but as a ﬁrst step
towards this goal we are drawing motivation from the work by Koutsoupias and Papadimitriou on worst-case
equilibria where they deﬁne a quantity called the price of anarchy [22]. We deﬁne the price of uncertainty
as the ratio:

Expected payoff in the complete information environment
Expected payoff in the incomplete information environment

Observations: Consider Figure 4 which gives, for all three security games, a heat plot for the price of
limited information over all choices of b and c with L, M, N ﬁxed at L = M = 1 and N = 4. The most
remarkable feature of these graphs are the different hotspot regions. In the weakest-link game we ﬁnd that
higher price of uncertainty ratios are to be found within the boundaries of cases 3 and 4. Both cases allow
for self-protection in the presence of incomplete information and therefore balance the various risks more
directly than the remaining cases. (Case 1 and 2 associate zero probability with self-protection.)

In the best shot scenario the peak region is located trivially within the boundaries of case 2. We know
that the expert player will never protect under incomplete information but is subject to the residual risk of
a system-wide security failure. For N = 4 the likelihood of such a breakdown is already very small, and
decreases with N . Still this outcome is feasible and most pronounced for protection costs that are about a
half to two-thirds of the loss, L. For higher b the disincentive of buying self-protection and the potential loss
are relatively balanced resulting in a lower price of uncertainty.

In the total effort game we observe multiple hotspot regions. Cases 4 and 6 are unaffected by an increased
price of uncertainty. They are characterized by the absence of self-insurance as a feasible strategy. This eases
the decision-making problem of the expert, and reduces the likelihood of a misspent security investment.

20

Weakest link (L=M=1, b=0.20, N=4) 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1Total expected payoffSelf−insurance cost (c)IncompleteCompleteNaive 0.5Naive 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1Total expected payoffSelf−insurance cost (c)Best shot (L=M=1, b=0.20, N=4)IncompleteComplete 0.5Naive 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1Total expected payoffSelf−insurance cost (c)Total effort (L=M=1, b=0.20, N=4)IncompleteComplete 0.5(a) Weakest link

(b) Best shot

(c) Total effort

Figure 2: Total expected payoffs for the strategic player under different information conditions, compared
with that of the na¨ıve agents, expressed as a function of the self-protection cost b. L = M = 1, N = 4, and
c is ﬁxed to c = 0.20 in this set of examples.

(a) Weakest link

(b) Best shot

(c) Total effort

Figure 3: Strategy boundaries in the incomplete information scenario for the sophisticated player. The
different cases refer to Tables 5, 10 and 15. L = M = 1 and N = 4 in this set of examples.

(a) Weakest link

(b) Best shot

(c) Total effort

Figure 4: Price of uncertainty for the various games. L = M = 1, N = 4 in this set of examples.

21

Naive 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1Total expected payoffSelf−protection cost (b)Weakest link (L=M=1, c=0.20, N=4)IncompleteComplete 0.5Naive 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1Total expected payoffSelf−protection cost (b)Best shot (L=M=1, c=0.20, N=4)IncompleteComplete 0.5Naive 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1Total expected payoffSelf−protection cost (b)Total effort (L=M=1, c=0.20, N=4)IncompleteComplete 0.5Case 4 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Self−insurance cost (c)Self−protection cost (b)Case 1Case 2Case 3 0Case 2 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Self−insurance cost (c)Self−protection cost (b)Case 1 0 0 0.2 0.4 0.6 1 0.8Self−insurance cost (c)Self−protection cost (b)Case 1Case 4Case 2Case 3Case 6Case 5 0 0.2 0.4 0.6 0.8 1 1.18 0.2 0.4 0.6 0.8 1Self−protection cost (b) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Self−insurance cost (c) 1 1.02 1.04 1.06 1.08 1.1 1.12 1.14 1.16 0 1.02 0.2 0.4 0.6 0.8 1Self−protection cost (b) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Self−insurance cost (c) 1 1.005 1.01 1.015 0 0.2 0.4 0.6 0.8 1Self−protection cost (b) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Self−insurance cost (c) 1 1.005 1.01 1.015 1.02 1.025 1.03 06 Conclusions

In our work we emphasize that security decision-making is shaped by the structure of the task environment
as well as the knowledge and computational capabilities of the agents. To that effect, we study security
investment choices in three canonical scenarios [16, 35]. Decisions are made from three distinct security
actions (self-protection, self-insurance or passivity) to navigate the complex security risks of weakest-link,
best shot and total effort interdependencies. In these environments, we investigate the co-habitation of a
single fully rational expert and N − 1 na¨ıve agents. The na¨ıve agents are near-sighted and fail to account
for the decisions of other agents, and instead follow a simple but reasonable self-centered rule-of-thumb.
We further study the impact of limited information on rational agents’ choices. To guide the reader through
our analysis, we provide a detailed overview and examples of our methodology to compare strategies and
payoffs.

In the kingdom of the blind, is the one-eyed man king? We ﬁnd that in general, the na¨ıve agents match
the payoff of the expert when self-insurance is cheap, but not otherwise. Even with limited information, the
sophisticated agent can generally translate her better structural understanding into decisions that minimize
wasted protection investments, or an earlier retreat to the self-insurance strategy when system-wide security
is (likely) failing. A notable exception is the weakest link game with incomplete information, where the
payoff of the sophisticated agent degrades to that of the na¨ıve agent as insurance gets more expensive.

Tragically, her impact on the improvement of system-wide security is never positive (in comparison to
her replacement by an unsophisticated agent). While our expert agent is rational, she is not benevolent.
Acting selﬁshly, the set of scenarios for which protection is her best option is always a subset of the set of
scenarios for which the na¨ıve agent chooses protection.

Monarchs or netizens? To complement our study we are interest in studying properties of a network
with varying fractions of expert to na¨ıve users. Further, we want to address the desire of some computer
experts to sacriﬁce individual resources to improve system resilience to attacks, by introducing benevolent
agents.

Our work further evidences the limits of independently organized security in fully connected systems.
David Clark summarized his vision about Internet governance: “We reject: kings, presidents, and voting.
We believe in: rough consensus and running code.” While this tenet has been a driver of the sudden growth
of large-scale networks, current approaches to overcome the coordination problems need to be organized on
the intermediary level. For example, OpenDNS independently released a tool to track the spread of major
worms. Other approaches we want to study are vigilante behavior, and peer-to-peer support.

What is the cost of the emperor’s lack of information? To analyze the impact of the different infor-
mation conditions we have proposed a new mathematical formalization guided by prior work on worst-case
equilibria [22]. We deﬁne the price of uncertainty as the ratio of the payoff in the complete information
environment to the payoff in the incomplete information environment. Our analysis of Figure 4 is a ﬁrst step
in that direction, however, a more formal analysis is subject to future work.

Finally, a system designer is not only interested in the payoffs of the network participants given different

22

information realities (e.g., due to frequent changes in attack trends). He is also concerned with how well-
fortiﬁed the organization is against attacks. To that effect we plan to include a more thorough presentation
of the parameter conditions that cause attacks to fail due to system-wide protection, and when they succeed
(due to coordination failures, passivity, and self-insurance).

References

[1] A. Acquisti. Privacy in electronic commerce and the economics of immediate gratiﬁcation. In Proceedings of

the 5th ACM Conference on Electronic Commerce (EC’04), pages 21–29, New York, NY, May 2004.

[2] A. Acquisti and J. Grossklags. Privacy and rationality in individual decision making. IEEE Security & Privacy,

3(1):26–33, January–February 2005.

[3] A. Acquisti and H. Varian. Conditioning prices on purchase history. Marketing Science, 24(3):367–381, Summer

[4] R. Anderson. Security Engineering: A Guide to Building Dependable Distributed Systems. Wiley Computer

Publishing, New York, NY, 2 edition, 2001.

[5] D. Besnard and B. Arief. Computer security impaired by legitimate users. Computers & Security, 23(3):253–264,

[6] K. Campbell, L. Gordon, M. Loeb, and L. Zhou. The economic cost of publicly announced information security

breaches: Empirical evidence from the stock market. Journal of Computer Security, 11(3):431–448, 2003.

[7] H. Cavusoglu, S. Raghunathan, and W. Yue. Decision-theoretic and game-theoretic approaches to IT security

investment. Journal of Management Information Systems, 25(2):281–304, Fall 2008.

[8] N. Christin, J. Grossklags, and J. Chuang. Near rationality and competitive equilibria in networked systems.
In Proceedings of ACM SIGCOMM’04 Workshop on Practice and Theory of Incentives in Networked Systems
(PINS), pages 213–219, Portland, OR, August 2004.

[9] D. D¨orner. The Logic Of Failure: Recognizing And Avoiding Error In Complex Situations. Metropolitan Books,

2005.

May 2004.

1996.

[10] A. Etzioni. On thoughtless rationality (rules-of-thumb). Kyklos, 40(4):496–514, November 1987.

[11] R. Frank. Shrewdly irrational. Sociological Forum, 2(1):21–41, December 1987.

[12] E. Gal-Or and A. Ghose. The economic incentives for sharing security information. Information Systems Re-

[13] L. Gordon and M. Loeb. Managing Cyber-Security Resources: A Cost-Beneﬁt Analysis. McGraw-Hill, New

search, 16(2):186–208, June 2005.

York, NY, 2006.

[14] L.A. Gordon and M. Loeb. The economics of information security investment. ACM Transactions on Information

and System Security, 5(4):438–457, November 2002.

[15] L.A. Gordon, M. Loeb, and W. Lucyshyn. Sharing information on computer systems security: An economic

analysis. Journal of Accounting and Public Policy, 22(6):461–485, November 2003.

23

[16] J. Grossklags, N. Christin, and J. Chuang. Secure or insure? A game-theoretic analysis of information security
games. In Proceedings of the 2008 World Wide Web Conference (WWW’08), pages 209–218, Beijing, China,
April 2008.

[17] J. Grossklags, N. Christin, and J. Chuang. Security and insurance management in networks with heterogeneous
agents. In Proceedings of the 9th ACM Conference on Electronic Commerce (EC’08), pages 160–169, Chicago,
IL, July 2008.

[18] J. Grossklags, B. Johnson, and N. Christin. When information improves information security. Technical re-
port, UC Berkeley & Carnegie Mellon University, CyLab, February 2009. Available at http://people.
ischool.berkeley.edu/˜jensg/research/paper/EC09-report.pdf.

[19] C. Jaeger, O. Renn, E. Rosa, and T. Webler. Risk, uncertainty, and rational action. Earthscan Publications,

London, UK, 2001.

[20] Kabooza. Global backup survey: About backup habits, risk factors, worries and data loss of home PCs, January

2009. Available at: http://www.kabooza.com/globalsurvey.html.

[21] D. Kahneman and A. Tversky. Choices, values and frames. Cambridge University Press, Cambridge, UK, 2000.

[22] E. Koutsoupias and C. Papadimitriou. Worst-case equilibria. In Proceedings of the 16th Annual Symposium on

Theoretical Aspects of Computer Science, pages 404–413, 1999.

[23] H. Kunreuther and G. Heal. Interdependent security. Journal of Risk and Uncertainty, 26(2–3):231–249, March

[24] M. Lettau and H. Uhlig. Rules of thumb versus dynamic programming. American Economic Review, 89(1):148–

2003.

174, March 1999.

[25] Y. Liu, C. Comaniciu, and H. Man. A bayesian game approach for intrusion detection in wireless ad hoc net-
works. In Proceedings of the Workshop on Game Theory for Communications and Networks, page Article No.
4, 2006.

[26] J. McCalley, V. Vittal, and N. Abi-Samra. Overview of risk based security assessment. In Proceedings of the

1999 IEEE PES Summer Meeting, pages 173–178, July 1999.

[27] NCSA/Symantec. Home user study, October 2008. Available at: http://staysafeonline.org/.

[28] P. Paruchuri, J. Pearce, J. Marecki, M. Tambe, F. Ordonez, and S. Kraus. Playing games for security: An efﬁcient
exact algorithm for solving bayesian stackelberg games. In Proceedings of the 7th Int. Conf. on Autonomous
Agents and Multiagent Systems (AAMAS 2008), pages 895–902, Estoril, Portugal, May 2008.

[29] I. Png, C. Wang, and Q. Wang. The deterrent and displacement effects of information security enforcement:

International evidence. Journal of Management Information Systems, 25(2):125–144, Fall 2008.

[30] S. Schechter and M. Smith. How much security is enough to stop a thief? In Proceedings of the Seventh Inter-

national Financial Cryptography Conference (FC 2003), pages 122–137, Gosier, Guadeloupe, January 2003.

[31] B. Schneier. Beyond Fear. Springer Verlag, New York, NY, 2006.

[32] J. Stanton, K. Stam, P. Mastrangelo, and J. Jolton. Analysis of end user security behaviors. Computers &

Security, 2(24):124–133, March 2005.

24

[33] D. Straub. Effective IS Security: An Empirical Study. Information Systems Research, 3(1):255–276, September

1990.

[34] R. Telang and S. Wattal. An empirical analysis of the impact of software vulnerability announcements on ﬁrm

stock price. IEEE Transactions on Software Engineering, 33(8):544–557, 2007.

[35] H.R. Varian. System reliability and free riding. In L.J. Camp and S. Lewis, editors, Economics of Information
Security (Advances in Information Security, Volume 12), pages 1–15. Kluwer Academic Publishers, Dordrecht,
The Netherlands, 2004.

25

A Technical appendix

A.1 Derivations for weakest link game

Weakest link security game. Derivations for total expected game payoffs, conditioned on other play-
ers: The following derivations refer to Table 4.

Cases (WC1) and (WC2a) and (WI1):

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

= [M − E[pi] · L] ·
i
· L

M −

=

(cid:17)

h

(cid:16) c
2L

i

h c
L
h c
L

·

h

+ [M − c] ·
i

1 −
h

+ [M − c] ·

1 −

i

c
L

c
L

+ [M − b − E[pi] · L] · [0]
i

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

= [M − E[pi] · L] ·

+ [M − c] · [0] + [M − b] ·

1 −

(cid:21)

(cid:20) b
L
(cid:20) b
L

(cid:21)

·

(cid:21)

+ [M − b] ·

1 −

(cid:20)

(cid:21)

b
L

(cid:20)

(cid:21)

b
L

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

= [M − E[pi] · L] ·

+ [M − c] ·

1 −

+

M − b − E[pi] · L

1 −

1 −

· [0]

 

(cid:18)

(cid:19)N −1!#

b
L

i

h c
L
h c
L

·

i

h

h

"

i

c
L

i

c
L

+ [M − c] ·

1 −

26

= M −

− c +

c2
2L

c2
L

= M − c +

c2
2L

Case (WC2b):

(cid:20)

=

M −

(cid:19)

(cid:18) b
2L

· L

= M −

− b +

b2
2L

b2
L

= M − b +

b2
2L

Case (WI2):

h

=

M −

(cid:17)

i
· L

(cid:16) c
2L

= M −

− c +

c2
2L

c2
L

= M − c +

c2
2L

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

Case (WI3):

= [M − E[pi] · L] ·

"

b
L (cid:0)1 − b
L
 
(cid:18)

(cid:1)N −1

b
L

#

"

+ [M − c] ·

1 −

#

c − b
L (cid:0)1 − (1 − b

L )N −1(cid:1)

(cid:19)N −1!#

"

·

c − b
L (cid:0)1 − (1 − b

L )N −1(cid:1) −

#

"

+ [M − c] ·

1 −

+

M − b − E[pi] · L

1 −

1 −

 

=

M −

b
2L (cid:0)1 − b
L

(cid:1)N −1

!

#

"

· L

·

b
L (cid:0)1 − b
L

(cid:1)N −1

"

"

"

 

1
2

c − b
L (cid:0)1 − (1 − b

L )N −1(cid:1) +

b
L (cid:0)1 − b
L
#

!

 

(cid:18)

· L

1 −

1 −

b
L

(cid:1)N −1

#

(cid:1)N −1

b
L (cid:0)1 − b
L
c − b
L (cid:0)1 − (1 − b
(cid:19)N −1!#

#

L )N −1(cid:1)

b
L (cid:0)1 − b
L

(cid:1)N −1

+

M − b −

"

·

c − b
L (cid:0)1 − (1 − b

= M −

L )N −1(cid:1) −
b2
2L (cid:0)1 − b
L

(cid:18)

−

L
2

1 − (1 −

)N −1

b
L

(cid:1)2N −2 − c +




(cid:19)

·





L

= M −

−

(cid:16)

2L

(cid:1)2N −2 − c +

b2
2L (cid:0)1 − b
L
(c − b)2
1 − (cid:0)1 − b
L
c2 − 2bc + b2

(cid:1)N −1(cid:17) +

= M − c +

L (cid:0)1 − (1 − b

L )N −1(cid:1) +

b2
2L (cid:0)1 − b
L

(cid:1)2N −2

b2
L (cid:0)1 − b
L

= M − c +

b2
2L (cid:0)1 − b
L

(cid:1)N −1 +

(cid:16)

(c − b)2
1 − (cid:0)1 − b
L

2L

(cid:1)N −1(cid:17)

#

b
L (cid:0)1 − b
L

(cid:1)N −1

c2 − bc
L (cid:0)1 − (1 − b

L )N −1(cid:1) − b ·

"

c − b
L (cid:0)1 − (1 − b

(cid:16)

c − b
1 − (cid:0)1 − b
L
c2 − bc
L (cid:0)1 − (1 − b



2

 

(cid:1)N −1(cid:17)



−

L )N −1(cid:1) −

(cid:16)

L

b
L (cid:0)1 − b
L
bc − b2
1 − (cid:0)1 − b
L

L )N −1(cid:1) −
!2


(cid:1)N −1

(cid:1)N −1(cid:17) +

b2
L (cid:0)1 − b
L

(cid:1)N −1

(cid:1)N −1 −

(cid:16)

(c − b)2
1 − (cid:0)1 − b
L

2L

(cid:1)N −1(cid:17)

27

Case (WI4):

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

#

(cid:1)N −1

"

b
L (cid:0)1 − b
L
 
(cid:18)

= [M − E[pi] · L] ·

+ [M − c] · [0]

(cid:19)N −1!#

"

b
L

#

(cid:1)N −1

b
L (cid:0)1 − b
L
#

"

"

"

+

M − b − E[pi] · L

1 −

1 −

·

1 −

 

=

M −

b
2L (cid:0)1 − b
L

(cid:1)N −1

!

#

"

· L

·

+

M − b −

1 +

 

1
2

b
L (cid:0)1 − b
L
!

 

(cid:1)N −1

(cid:18)

· L

1 −

1 −

b
L (cid:0)1 − b
L

(cid:1)N −1

"

(cid:1)2N −2 − b ·

1 −

(cid:1)2N −2 − b +

b
L (cid:0)1 − b
L

(cid:1)N −1

#

−

L
2

b2
L (cid:0)1 − b
L

(cid:1)N −1 −

!

 

L
2

(cid:18)

(cid:18)

1 −

1 −

(cid:19)N −1!#

"

·

1 −

b
L

#

b
L (cid:0)1 − b
L


(cid:19)

(cid:1)N −1

 

1 − (1 −

)N −1

·

1 −

b
L

(cid:19)N −1!

!2


b
L (cid:0)1 − b
L

(cid:1)N −1

= M −

= M −

(cid:18)

+

L
2

= M −

b2
2L (cid:0)1 − b
L
b2
2L (cid:0)1 − b
L
b
L

b2
2L (cid:0)1 − b
L
 

L
2

1 − (1 −

)N −1

(cid:19)  

b2
L2 (cid:0)1 − b
L

(cid:1)2N −2

b2
L (cid:0)1 − b
L

(cid:1)N −1 −

L
2

(cid:1)2N −2 − b +

(cid:18)

(cid:19)N −1!

b
L

+

b2
2L (cid:0)1 − b
L

(cid:1)N −1

= M − b −

1 −

1 −

b
L

b
L

 

(cid:18)

1 −

1 −

(cid:19)N −1!

+

b2
2L (cid:0)1 − b
L

(cid:1)2N −2 −

b2
2L (cid:0)1 − b
L

(cid:1)N −1

Weakest link security game. Derivations for total expected game payoffs, not conditioned on other
players: The following derivation refers to Table 5. To remove dependence on pj for j 6= i in case WC2,
we simply take a weighted sum of the total payoffs for cases WC2a and WC2b, where the weight is deter-
mined by the probability of minj6=i pj < b
L assuming that each pj is drawn from the uniform distribution
over [0, 1] (and assuming b ≤ c).

We have:

28

Case (WC2):

Probability[Case (WC2a)] · ExPayoff[Case (WC2a)] + Probability[Case (WC2b)] · ExPayoff[Case (WC2b)]

 

(cid:18)

=

1 −

1 −

b
L

(cid:19)N −1!

(cid:20)

·

M − c +

+

1 −

·

M − b +

"(cid:18)

(cid:19)N −1#

(cid:20)

(cid:21)

b2
2L

= M − c +

+

c −

1 −

−

b −

(cid:19) (cid:18)

(cid:19)N −1

(cid:18)

(cid:19) (cid:18)

(cid:19)N −1

b
L

b2
2L

1 −

b
L

(cid:21)

c2
2L

b
L
(cid:19) (cid:18)

c2
2L

(cid:18)

(cid:18)

= M − c +

+

c − b −

c2 − b2

2L

1 −

b
L

(cid:19)N −1

(cid:18)

(cid:19) (cid:18)

(cid:19)N −1

c + b
2L

1 −

b
L

= M − c +

+ (c − b)

1 −

A.2 Derivations for best shot game

c2
2L
c2
2L
c2
2L

Best shot security game. Derivations for total expected game payoffs, conditioned on other players:
The following derivations refer to Table 9.

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

Cases (BC1) and (BI1):

= [M − E[pi] · L] ·
i
· L

M −

=

(cid:17)

h

(cid:16) c
2L

i

h c
L
h c
L

·

h

+ [M − c] ·
i

1 −
h

+ [M − c] ·

1 −

i

c
L

c
L

+ [M − b] · [0]
i

= M −

− c +

c2
2L

c2
L

= M − c +

c2
2L

Case (BC2a):

(cid:20)

=

M −

(cid:19)

(cid:18) b
2L

· L

= M −

− b +

b2
2L

b2
L

= M − b +

b2
2L

29

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

= [M − E[pi] · L] ·

+ [M − c] · [0] + [M − b] ·

1 −

(cid:21)

(cid:20) b
L
(cid:20) b
L

(cid:21)

·

(cid:21)

+ [M − b] ·

1 −

(cid:20)

(cid:21)

b
L

(cid:20)

(cid:21)

b
L

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

= [M ] · [1] + [M − c] · [0] + [M − b] · [0]

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

=

M − E[pi] · L

· [1] + [M − c] · [0] + [M − b] · [0]

Case (BC2b):

= M

Case (BI2):

"

(cid:19)N −1#

(cid:18) b
L

(cid:19)N −1

= M −

= M −

· L

(cid:18) b
L
(cid:19)N −1

(cid:19)

(cid:18) 1
2
(cid:18) b
L

L
2

Best shot security game. Derivations for total expected game payoffs, not conditioned on other play-
ers: The following derivation refers to Table 10. To remove dependence on pj for j 6= i in case BC2, we
simply take a weighted sum of the total payoffs for cases BC2a and BC2b, where the weight is determined
by the probability of minj6=i pj < b
L assuming that each pj is drawn from the uniform distribution over
[0, 1].

Probability[Case (BC2a)] · ExPayoff[Case (BC2a)] + Probability[Case (BC2b)] · ExPayoff[Case (BC2b)]

We have:

Case (BC2):

·

M − b +

+

1 −

· [M ]

"

(cid:19)N −1#

(cid:18) b
L

(cid:19)N −1

(cid:19)N −1

(cid:20)

=

(cid:18) b
L

= M − b

= M − b

1 −

(cid:18) b
L

(cid:18)

(cid:19)N −1

+

b2
2L
(cid:19) (cid:18) b
L

b
2L

(cid:21)

b2
2L
(cid:18) b
L
(cid:19)N −1

A.3 Derivations for total effort game

Total Effort security game. Derivations for total expected game payoffs, conditioned on other players:
The following derivations refer to Table 14.

30

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

(cid:20)
M − b − E[pi] · L

(cid:18)

1 −

(cid:19)(cid:21)

1
N

· [0]

Case (TC1): c < b

= [M − E[pi] · L] ·

+ [M − c] ·

1 −

i

h c
L
h c
L

·

i

h

h

i

c
L

+

i

c
L

+ [M − c] ·

1 −

h

=

M −

(cid:17)

i
· L

(cid:16) c
2L

= M −

− c +

c2
L

c2
2L

c2
L

= M − c +

Cases (TC2) and (TC5):

(cid:20)
M − E[pi] · L

(cid:18)

=

1 −

(cid:20)
M − b − E[pi] · L

(cid:18)

+

1 −

K + 1

(cid:19)(cid:21)

"

·

K
N

#

c

(cid:1)

L (cid:0)1 − K
N
(cid:19)(cid:21)

· [0]

N
(cid:18)

· L

1 −

"

 

=

M −

!

c

(cid:1)

2L (cid:0)1 − K
N
c2

= M −

2L (cid:0)1 − K
N
c2

= M − c +

2L (cid:0)1 − K
N

(cid:1)

(cid:1) − c +

c2

L (cid:0)1 − K
N

(cid:1)

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

+ [M − c] ·

1 −

"

#

c

L (cid:0)1 − K
N

(cid:1)

(cid:19)#

"

·

K
N

#

c

L (cid:0)1 − K
N

(cid:1)

+ [M − c] ·

1 −

"

#

c

L (cid:0)1 − K
N

(cid:1)

31

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

+ [M − c] ·

1 −

"

#

c − b

L (cid:0)1 − K+1
N

(cid:1)

#

(cid:1) −

bN
L

c − b

L (cid:0)1 − K+1
N
"

·

(cid:21)

+ [M − c] ·

1 −

#

c − b

L (cid:0)1 − K+1
N

(cid:1)

c − b

L (cid:0)1 − K+1
N

(cid:1) +

bN
L

#

(cid:18)

· L

1 −

(cid:19)#

"

·

K + 1

N

#

c − b

L (cid:0)1 − K+1
N

(cid:1) −

bN
L

Case (TC3):

(cid:20)
M − E[pi] · L

(cid:18)

=

1 −

K
N

(cid:19)(cid:21)

(cid:21)

(cid:20) bN
L

·

(cid:20)
M − b − E[pi] · L

(cid:18)

+

1 −

K + 1

(cid:19)(cid:21)

"

N

(cid:19)(cid:21)

=

M −

(cid:21)

(cid:20) bN
2L

(cid:18)

· L

1 −

K
N

(cid:20) bN
L

·

(cid:20)

"

= M −

1 −

− c +

+

M − b −

"

1
2
(cid:18)

b2N 2
2L

 

− b

c − b

L (cid:0)1 − K+1
N

(cid:1) −

= M −

b2N 2
2L

(cid:18)

1 −

K
N

(cid:19)

K
N

bN
L

(cid:19)

= M − c +

= M − c +

(c − b)2
L (cid:0)1 − K+1
N
b2N
2L

+

(cid:1) +

b2N
L
(c − b)2

2L (cid:0)1 − K+1
N

(cid:1)

c2 − bc
L (cid:0)1 − K+1
N

(cid:1)

!

(cid:18)

1 −

−

L
2

K + 1

N



 

(cid:19)



c − b

L (cid:0)1 − K+1
N

(cid:1)

!2

−

(cid:18) bN
L

(cid:19)2





 

!

c − b

L (cid:0)1 − K+1
N

(cid:1)

+

b2N
L

−

(c − b)2

2L (cid:0)1 − K+1
N

(cid:1) +

b2N 2
2L

(cid:18)

1 −

K + 1

(cid:19)

N

− c +

c2 − bc
L (cid:0)1 − K+1
N
(c − b)2

(cid:1) − b

−

2L (cid:0)1 − K+1
N

(cid:1) −

b2N
2L

32

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

+ [M − c] · [0] +

(cid:20)
M − b − E[pi] · L

(cid:18)

1 −

K + 1

(cid:19)(cid:21)

(cid:20)

N

·

(cid:21)

1 −

bN
L
(cid:19)(cid:21)
K + 1

(cid:19)(cid:21)

(cid:18)

· L

1 −

bN
L

N

(cid:20)

·

1 −

(cid:21)

bN
L

+ [M − c] · [0] +

M − b −

1 +

(cid:19)

−

L
2

(cid:18)

1 −

K + 1

(cid:19) (cid:18)

1 −

(cid:18)

(cid:20) 1
2

(cid:19)

b2N 2
L2

(cid:20)

N
(cid:19)

K + 1

N

+

b2N 2
2L

(cid:18)

1 −

K + 1

(cid:19)

N

Case (TC4):

(cid:20)
M − E[pi] · L

(cid:18)

=

1 −

(cid:20)

=

M −

(cid:18)

· L

1 −

(cid:19)(cid:21)

·

(cid:19)(cid:21)

K
N
K
N

(cid:18)

(cid:21)

(cid:21)

(cid:20) bN
L
(cid:20) bN
L
bN
L

·

(cid:19)

(cid:19)

K
N
K
N
L
2
K + 1

(cid:18)

1 −

N

b2N
L
K + 1

(cid:19)

N

(cid:19)

+

b2N
2L

(cid:18)

L
2
b2N
2L

−

(cid:21)

(cid:18)

(cid:18)

(cid:20) bN
2L
b2N 2
2L
b2N 2
2L

b2N
L
(cid:18)

L
2

= M − b +

−

= M − b −

1 −

= M −

1 −

− b

1 −

= M −

1 −

− b +

−

1 −

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

· [1] + [M − c] · [0] +

(cid:20)
M − b − E[pi] · L

(cid:18)

1 −

K + 1

(cid:19)(cid:21)

· [0]

N

Case (TC6):

(cid:20)
M − E[pi] · L

(cid:18)

=

1 −

(cid:19)(cid:21)

K
N

= M −

1 −

(cid:18)

L
2

(cid:19)

K
N

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

(cid:20)
M − b − E[pi] · L

(cid:18)

1 −

(cid:19)(cid:21)

1
N

· [0]

Case (TI1): c < b

= [M − E[pi] · L] ·

+ [M − c] ·

1 −

i

h c
L
h c
L

·

i

h

h

i

c
L

+

i

c
L

+ [M − c] ·

1 −

h

=

M −

(cid:17)

i
· L

(cid:16) c
2L

= M −

− c +

c2
L

c2
2L

c2
L

= M − c +

33

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

N
!

"

·

#

c

b + L−b
N
(cid:19)#

L − b

(cid:18)

·

b +

N

"

+ [M − c] ·

1 −

"

·

#

c

b + L−b
N

#

c

b + L−b
N
"

+ [M − c] ·

1 −

#

c

b + L−b
N

(cid:20)
M − b − E[pi]

(cid:18)

+

b −

(cid:19)(cid:21)

b
N

· [0]

Cases (TI2) and (TI5):

(cid:20)
M − E[pi]

(cid:18)

=

b +

L − b

(cid:19)(cid:21)

"

 

=

M −

c

(cid:1)

2 (cid:0)b + L−b
N
c2

= M −

2 (cid:0)b + L−b
N
c2

= M − c +

2 (cid:0)b + L−b
N

(cid:1)

(cid:1) − c +

c2

b + L−b
N

Case (TI3)

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

"

#

c − b
b − b
N

"

#

(cid:20)
M − b − E[pi]

(cid:18)

+

b −

(cid:19)(cid:21)

"

·

b
N

c − b
b − b
N

−

bN
L

#

c − b
b − b
N
#

(cid:20)

"

+

M − b −

(cid:20)
M − E[pi]

(cid:18)

=

b +

L − b

(cid:19)(cid:21)

N

(cid:21)

(cid:20) bN
L

·

+ [M − c] ·

1 −

=

M −

(cid:18) bN
2L

(cid:19)

(cid:18)

·

b +

N

L − b

(cid:19)(cid:21)

(cid:21)

(cid:20) bN
L

·

+ [M − c] ·

1 −

 

1
2

bN
L

+

c − b
b − b
N

L − b

(cid:19)

(cid:19)#

"

!

(cid:18)

·

b −

b
N

c − b
b − b
N

−

bN
L

·

!

= M −

 

− b

(cid:18)

b +

b2N 2
2L2

c − b
b − b
N

−

bN
L

!

N

−

1
2

− c + c

(cid:19)  

(cid:18)

b −

b
N

 

c − b
b − b
N

= M −

b2N 2
2L2

(cid:18)

b −

(cid:19)

−

b
N

b2N
2L

− c + c

(cid:1)2 −
!

(c − b)2
(cid:0)b − b
N
c − b
b − b
N

 

!

b2N 2
L2

− b

 

!

c − b
b − b
N

+

b2N
L

= M − c + c

− b

(c − b)2
2 (cid:0)b − b
N

 

(cid:1) +
!

−

!

b2N 2
2L2

(cid:18)

b −

(cid:19)

b
N

c − b
b − b
N

+

b2N
2L

−

(c − b)2
2 (cid:0)b − b
N

(cid:1)

 

c − b
b − b
N
(c − b)2
b − b
N
b2N
2L

+

= M − c +

= M − c +

+

b2N
2L

−

(c − b)2
2 (cid:0)b − b
N

(cid:1)

(c − b)2
2 (cid:0)b − b
N

(cid:1)

34

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

(cid:18)

b +

(cid:19) (cid:18)

L − b

(cid:19)(cid:21)

·

(cid:19)(cid:21)

N
L − b

b +

(cid:21)

(cid:20) bN
L
(cid:20) bN
L
(cid:18)

·

(cid:21)

(cid:20)

+ [M − c] · [0] +

(cid:20)
M − b − E[pi]

(cid:18)

+

M − b −

+

b −

·

1 −

(cid:19) (cid:18)

(cid:19) (cid:18)

1 −

b
N
bN
L

(cid:18) bN
2L
(cid:18)

bN
2L
(cid:18)

b −

b −

(cid:19)

b
N

1
2
b
N

+

− b

1 −

(cid:19)

−

bN
L

− b +

b2N
L

−

bN
2L

b2N 2
2L2

(cid:18)

b −

b
N

−

1
2

b −

b
N

(cid:19)(cid:21)

(cid:20)

b
N
(cid:20)

b −

(cid:19)(cid:21)

(cid:21)

bN
L

·

1 −

(cid:21)

bN
L

b
N

b −

(cid:18)

−

1
2
(cid:19)

(cid:19)

(cid:18)

(cid:19) (cid:18)

1 −

(cid:19)

+

(cid:19)

(cid:18)

bN
L
bN
2L

b −

(cid:19)

b
N

Case (TI4)

=

=

(cid:20)

M −

(cid:20)
M − E[pi]
(cid:18) bN
2L
b2N 2
2L2
b2N 2
2L2
b2N
2L

= M −

= M −

(cid:18)

(cid:18)

b −

b −

= M +

− b −

b −

= M − b −

b −

(cid:18)

1
2

(cid:19)

(cid:19)

N

−

−

(cid:18)

(cid:19)

+

b
N
b
N
1
2
b
N

b2N
2L
b2N
2L
(cid:19)
b
N
b2N
2L

Case (TI6):

(cid:20)
M − E[pi]

(cid:18)

=

b +

L − b

(cid:19)(cid:21)

= M −

b +

(cid:18)

1
2

N
(cid:19)

L − b

N

Payoff[passivity] · P r[passivity] + Payoff[insurance] · P r[insurance] + Payoff[protection] · P r[protection]

· [1] + [M − c] · [0] +

(cid:20)
M − b − E[pi]

(cid:18)

b −

(cid:19)(cid:21)

b
N

· [0]

Total effort security game. Derivations for total expected game payoffs, not conditioned on other
players: The following derivation refers to Table 15. For the total effort game, the dependence on other
players is noted in terms of the integer K, the number of players other than player i who choose protection.
To remove dependence on this K we must compute an appropriate expected value. To begin we rewrite each
of the case expressions as a linear constraint on K. After doing this it becomes clear that cases TC2 through
TC4 are mutually exclusive and exhaustive in terms of K, and similarly for cases TC5 and TC6. We deﬁne
case TC2-4 to be the union of cases TC2, TC3, and TC4. similarly, we deﬁne case TC5-6 to be the union of
cases TC5 and TC6. Now to compute an expected payoff for case TC2-4, we take the sum, over all possible
values k for K, of the probability that exactly k players protect, times the payoff for this k (considering the
case TC2, TC3, or TC4, that such a choice of K = k determines). We proceed similarly to compute the
expected payoff for case TC5-6.

To obtain the expected payoff for TC2-4 we compute:

35

Case (TC2-4):

N −1
X

k=0

=

+

+

P r[k] · Payoff assuming TC2-4 and that K = k

bN − c
b c
X

k=0

 

P r[k] ·

M − c +

!

c2

2L (cid:0)1 − k
N

(cid:1)

bN −1− N
X

L (c−b)c

k=bN − c

b +1c

N −1
X

k=bN − N

L (c−b)c

 

(cid:18)

P r[k] ·

M − c +

+

P r[k] ·

M − b −

1 −

b2N
2L

(cid:18)

L
2

!

(c − b)2
2L (cid:0)1 − k+1
N

(cid:1)

k + 1

(cid:19)

N

(cid:19)

+

b2N
2L

and to obtain the expected payoff for TC5-6 we compute:

P r[k] · Payoff assuming TC5-6 and that K = k

Case (TC5-6):

N −1
X

k=0

bN − cN
X

L c

k=0

N −1
X

=

+

k=bN − cN

L +1c

 

P r[k] ·

M − c +

c2

2L (cid:0)1 − k
N

(cid:1)

!

(cid:19)

P r[k] ·

M −

(N − k)

(cid:18)

L
2N

where as before, P r[k] = (cid:0)N −1
k

(cid:1) (cid:0)1 − b
L

(cid:1)k (cid:0) b
L

(cid:1)N −1−k

is the probability that exactly k players other

than player i choose protection.

36

