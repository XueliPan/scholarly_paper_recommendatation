Optimal Outlier Removal in High-Dimensional Spaces

John Dunagan∗

Santosh Vempala∗

Abstract

We study the problem of ﬁnding an outlier-free subset of a set of points (or a
probability distribution) in n-dimensional Euclidean space. As in [BFKV 99], a point x
is deﬁned to be a β-outlier if there exists some direction w in which its squared distance
from the mean along w is greater than β times the average squared distance from the
mean along w. Our main theorem is that for any ǫ > 0, there exists a (1
ǫ) fraction of
the original distribution that has no O( n
ǫ ))-outliers, improving on the previous
bound of O(n7b/ǫ). This is asymptotically the best possible, as shown by a matching
lower bound. The theorem is constructive, and results in a 1
1−ǫ approximation to the
following optimization problem: given a distribution µ (i.e. the ability to sample from
it), and a parameter ǫ > 0, ﬁnd the minimum β for which there exists a subset of
probability at least (1

ǫ) with no β-outliers.

ǫ (b + log n

−

−

1 Introduction

The term “outlier” is a familiar one in many contexts. Statisticians have several notions
of outliers[BG 97, DG 93]. Typically they quantify how far the outlier is from the rest of
the data, e.g. the diﬀerence between the outlier and the mean or the diﬀerence between
the outlier and the closest point in the rest of the data. In addition, this diﬀerence might
be normalized by some measure of the “scatter” of the set, e.g. the range or the standard
deviation. Data points that are outside some threshold are labelled outliers.

Identifying outliers is a fundamental and ubiquitous problem. The outliers in a data set
might represent experimental error, in which case it would be desirable to remove them.
They could aﬀect the performance of a computer program, by slowing down or even mislead-
ing an algorithm; machine learning is an area where outliers in the training data could cause
an algorithm to ﬁnd a wayward hypothesis. Even from a purely theoretical standpoint, re-
moving outliers could lead to simpler mathematical models, or the outliers themselves might
constitute the phenomenon of interest.

How does one ﬁnd outliers? To address this question we have to ﬁrst answer another: what
precisely is an outlier? In this paper we will assume that the data consists of points (or a
distribution) in n-dimensional Euclidean space. In the one-dimensional case, one could use
one of the deﬁnitions alluded to above, viz. a point is an outlier if its distance from the
mean is greater than some factor times the standard deviation. In ﬁgure 1, the top data
set depicts this deﬁnition: the data points are the solid circles, and the mean, along with

∗

Department of Mathematics, MIT, Cambridge MA, 02139. Email: {jdunagan, vempala}@math.mit.edu

Supported in part by NSF Career award CCR-9875024.

1

the mean plus or minus one standard deviation, are the hash marks. The leftmost point is
1.86 standard deviations away from the mean.

The following generalization to higher dimensions was used in [BFKV 99]. Let P be a set
n. A point x in P is called a β-outlier if there exists a vector w such that the
of points in
squared length of x along w is more than β times the average squared length of P along w,
i.e. if

R

(wT x)2 > βEx

P [(wT x)2]

∈

Note that (wT x)2 is the squared distance along w from the origin. In ﬁgure 1, the bottom
two pictures show how diﬀerent points may be the furthest outliers for diﬀerent choices of
w. In each graph, the solid circles are the data points, the line is the direction w, and the
hash marks along the line are the projections of the data points onto the line.

Figure 1: Deﬁning Outliers

R

n has a long history in statistics and machine learning. An
This deﬁnition of an outlier in
equivalent deﬁnition using terminology from the ﬁeld of machine learning is “a point is a
γ2-outlier if it has Mahalanobis distance greater than γ.” A statistician might say “after
normalizing by the covariance of the data, the point is more than γ away from the origin.”
The constructive procedure for identifying outliers in section 2 shows the equivalence of our
deﬁnition to these two other deﬁnitions.

The ﬁrst problem we address is the following: does there exist a small subset of (a point
set) P whose removal ensures that the remaining set has no outliers? More precisely, what
is the smallest β such that on removing a subset consisting of at most an ǫ fraction of P ,
the remaining set has no β-outliers (with respect to the remaining set)?

A natural approach is to ﬁnd all β-outliers in the set and remove them. This can be done
by ﬁrst applying a linear transformation (described in section 2) that results in the average
squared length of the distribution being 1 along every unit vector (the so-called isotropic
position). Isotropic position has been used to speed up random walks in [LKS 95]. Bringing
a distribution into isotropic position allows us to identify outliers easily. Now a point that is

2

a β-outlier simply has squared length more than β. The main diﬃculty is that the remaining
set might still have outliers — it is possible that points that were previously not outliers
now become outliers. Can this happen repeatedly and force us to throw out most of the
set?

Our main result is that the answer to this question is “no” for a surprisingly small value of β.
n
We present it below in a more general framework. Let
b denote the set of n-dimensional
In place of the point set P in the discussion above we have
b-bit integers,
n
b . For a probability distribution µ, let µ(S) denote the
any probability distribution µ on
probability of a subset of space S.

1, . . . , 2b
{

n.
}

Z

Z

Theorem 1 (Outlier Removal over Integer Support) 1 Let µ be a probability distri-
bution on

n
b . Then for every ǫ > 0, there exists S and

Z

such that
(i) µ(S)
(ii) max

1

ǫ

−

≥
(wT x)2 : x
{

S

∈

} ≤

β = O

(b + log

n
ǫ

)

(cid:17)

n
ǫ

(cid:16)

∈

βE[(wT x)2 : x

S] for all w

n
∈ R

The proof of theorem 1 (section 4) is constructive. Before proving theorem 1, we will
prove a similar theorem about distributions with arbitrary support (theorem 2, section 3).
Although the hypothesis on the support of the distribution in theorem 2 is much weaker,
we need an additional assumption. The proofs of theorems 1 and 2 make use of the same
principal idea.

In section 2, we describe (two variants of) an algorithm for outlier removal. The theorems
can be proven using either variant. Although the theorems are not obvious, the algorithm
is extremely simple. To convince the reader of this, we include a matlab implementation of
the algorithm in section 11.

For a point set with m points (m > n) the algorithm runs in O(m2n) time. In section 5
we show that the algorithm can also be used on an unknown distribution if it is allowed
to draw random samples from the distribution. The number of samples required is ˜O( n2b
ǫ )
and the running time is ˜O( b2n5

ǫδ4 ) for accuracy (1 + δ).

One variant of our algorithm is identical to the algorithm of [BFKV 99], the immediate
inspiration for our work. The bound on β in theorems 1 and 2 improves on the previous
best bound of O( n7b
ǫ ) given in [BFKV 99]. There it was used as a crucial component in the
ﬁrst polytime algorithm for learning linear threshold functions in the presence of random
noise. Due to the high value of β, the bound on the running time of the learning algorithm,
although polynomial, is a somewhat prohibitive ˜O(n28). In contrast, our theorem implies
an improved bound of ˜O(n5) for learning linear thresholds from arbitrary distributions in
the presence of random noise. Further, our bound on β is asymptotically the best possible.
This is shown in section 6 by an example where for any ǫ < 1
2 , a bound on β better than
Ω( n

log 1

ǫ )) is not possible.

ǫ (b

−

Our main theorem gives an extremal bound on β. A natural follow-up question is the com-
plexity of achieving the best possible β for any particular distribution. Given a distribution
1An early version of this work[DV 01] claimed a slightly diﬀerent version of theorem 1 with an insuﬃciently

strong hypothesis.

3

µ and a parameter ǫ, we want to ﬁnd a subset of probability at most ǫ whose removal leaves
an outlier-free set with the smallest possible β. We show this question to be NP-hard even
in the one-dimensional case by a reduction to subset-sum. In section 7, we prove that our
algorithm achieves a ( 1
1
−

ǫ )-approximation to the best possible β for any given ǫ.

In some cases, it may be desirable to translate the data set so that the origin coincides with
the mean, rather than having a ﬁxed origin. We prove the following corollary for standard
deviations from the mean in section 8. Let µ be a probability distribution on Zn
b . Then for
ǫ) fraction of the distribution such that along every direction,
any ǫ > 0, there exists a (1
ǫ )) standard deviations in that
no point is further away from the mean than O(
direction. We also give a ( 1
ǫ
3ǫ )-approximation algorithm for the corresponding optimization
−
1
−
problem.

ǫ (b + log n

p

−

n

In section 9, we prove a theorem describing a connection between outlier removal and robust
statistics. In section 10, we conclude by proving some technical properties of matrices that
are used elsewhere in the paper.

2 Algorithms for Outlier Removal

The ﬁrst question we address is that of detecting outliers. Since our deﬁnition of a γ2-outlier
involves all directions, it might not be obvious that this can be done in ﬁnite time.

In order to detect outliers, we use a linear transformation. Let M = E[xxT ] where x is a
sample drawn according to the probability distribution µ. If M is positive deﬁnite, there
1x. This
exists a matrix A such that M = A2. Consider the transformed space z = A−
transformation preserves outliers: if z is a β-outlier in direction w in the transformed space,
1w in the untransformed space,
the corresponding x = Az is a β-outlier in direction w′ = A−
and vice versa. The transformed distribution is in isotropic position [LKS 95], and we will
refer to the transformation as rounding. Such transformations have previously been used
in the design of algorithms to make geometric random walks more eﬃcient [LKS 97]. If M
does not have full rank, it is still positive semi-deﬁnite, and we instead round µ in the span
of M . For those familiar with the deﬁnitions of Mahalanobis distance or normalizing by the
covariance of the data set, this transformation shows the equivalence between our deﬁnition
of an outlier and these two other deﬁnitions.

For an isotropic distribution, any point x that is an outlier for some direction w is also
an outlier in the direction x. This follows from the fact that an isotropic distribution has
E[(wT x)2] = 1 for every w such that
= 1, and that the projection of the point x on to
w
|
|
x
a direction w is greatest when w = x/
. Thus, outlier identiﬁcation is easy for isotropic
|
|
distributions.

The ﬁrst algorithm has the following simple form: while there are β-outliers, remove them;
stop when there are no outliers. In the description below, µ is the given distribution and
β = γ2, where the exact value of β is speciﬁed in the proofs of theorems 1 and 2.

Algorithm 1 (Restriction to Ellipsoids):

1. Round µ. If there exists x such that

> γ, let S =

x
|
|

x :
{

x
|

| ≤

γ

. Retain only points
}

in S.

2. Repeat until the condition is not met.

4

Algorithm 1 is identical to the outlier removal algorithm of [BFKV 99]. The following
variant of the above algorithm will be signiﬁcantly easier to analyze. Whereas in the previous
algorithm we removed outliers in every direction in one step, in Algorithm 2 we only remove
outliers in one direction per step.

Algorithm 2 (Restriction to Slabs):

(wT x)2
1. Round µ. If there exists a unit vector w such that max
{

}

> γ2, let S =

x :
{

(wT x)2

γ2

. Retain only points in S.
}

≤

2. Repeat until the condition is not met.

3 Outlier Removal over Arbitrary Support

We will prove the following theorem about outlier removal over a distribution with arbitrary
support before proceeding to prove theorem 1. We refer to conditions (I, II) in the hypothesis
of theorem 2 as the full-dimensional condition. In theorem 1 we will remove this condition,
replacing it only by a condition on the support of the distribution.

Theorem 2 (Outlier Removal over Arbitrary Support) Let µ be a probability distri-
bution on

n satisfying

R

∀

∀

(I)

unit vector ˆw,

( ˆwT x)2dµ

(II)

unit vector ˆw,

S : µ(S)

∀

1

¯ǫ,

≥

−

S( ˆwT x)2dµ
R

Then for every ǫ such that 0 < ǫ

¯ǫ, there exists S and

R2

r2

≤

≥

≤

β = O

ln

R
r

(cid:19)

R

n
ǫ

(cid:18)

∈

βE[(wT x)2 : x

S] for all w

n
∈ R

such that
(i) µ(S)
(ii) max

1

ǫ

−

≥
(wT x)2 : x
{

S

∈

} ≤

To prove the theorem, we analyze the set S returned by either algorithm. This set S is
clearly β-outlier free. It remains to show that we do not discard too much of the distribution.
The main idea of the proof is to show that in every step the volume of an associated dual
ellipsoid increases. By bounding the total growth of the dual ellipsoid volume over the
course of the algorithm, we will deduce that no more than a certain fraction of the original
probability mass is thrown out before the algorithm terminates.

Towards this end, we will need some deﬁnitions. For a matrix M such that M = A2, deﬁne
the ellipsoids E(M ) and W (M ) as

E(M ) =

x :
{

A−
|

1x

1
}

| ≤

and W (M ) =

x :
{

Ax
|

| ≤

1
.
}

We will refer to E(M ) and W (M ) as the primal inertial ellipsoid and the dual ellipsoid
respectively. For any subset S of

n, we denote by MS the matrix given by

R
µ(x)xxT = E[xxT : x

S] Pr[x

S]

∈

∈

MS =

S
Xx
∈

5

In other words, MS is the M obtained after restricting µ to S (zeroing out points outside
of S, not renormalizing the distribution). We denote this restricted probability distribution
directly by µS. Throughout this chapter, µS will denote a restriction of µ to the subset of
space S, never a new and unrelated distribution. The useful property attained by rounding
with respect to µS (the restriction of the original distribution to S) is that

E[(wT x)2 : x

S] Pr[x

S] = 1

∈

∈

for every unit vector w, where the expectation and probability are with respect to x drawn
from µ. We will actually prove theorem 2 with E[(wT x)2 : x
S] in place of
E[(wT x)2]. Note that this is a stronger statement than the original theorem. Let x
µS
denote x

S : µ(x) > 0, and let span(µS) denote the span of

S] Pr[x

∈

∈

∈

x
{

∈

.
µS}

∈

We will also need the following elementary facts about ellipsoids: the volume of a full-
dimensional ellipsoid is given by the product of the axis lengths times the volume of the
has axes given by the
unit ball, which we will denote by f (n). The ellipsoid
singular vectors of A. The axis lengths of W (M ) and E(M ) are given by the singular values
1 and A, and so they are reciprocals. It follows that V ol(W (M ))V ol(E(M )) = (f (n))2,
of A−
a function solely of the dimension.

A−
|

x :
{

1
}

| ≤

1x

Lemma 1 relates the dual volume growth to the loss of probability mass, and lemma 2 upper
bounds the total dual volume growth.

Lemma 1 (Restriction to a Slab) Let γ be ﬁxed, and let µ be a full-dimensional
isotropic distribution. Suppose

= 1 such that

w,

w
|

|

∃

max

(wT x)2
{

}

> γ2E[(wT x)2]

Let S =

x : (wT x)2
{

≤

γ2

and p = Pr[x /
∈

}

S]. Then

V ol(W (MS))

≥

epγ2/2V ol(W (M ))

Proof: Let a2 = E[(wT x)2 : x

S] Pr[x

S]. Starting from the identity

E[(wT x)2] = Ex

∈

∈

∈
S[(wT x)2] Pr[x

S] + Ex /
∈

S[(wT x)2] Pr[x /
∈

∈

S]

and using that (wT x)2

γ2 for all x not in S, we get that 1

a2 + γ2p, which implies

≥

≥

a2

1

≤

−

γ2p

≤

γ2p

e−

We now construct a vector w′ of length 1/a belonging to the dual ellipsoid of µS. Letting
w′ = w/a suﬃces since w is a unit vector by assumption and

We also show that every v

W (M ) also belongs to W (MS). We have

a2 = E[(wT x)2 : x

S] Pr[x

S] = wT MSw

∈
T MSw′

1 = w′

∈

w′

W (MS)

⇒

∈

⇒

∈

MS = M

µ(x)xxT .

−

S
Xx /
∈

6

Hence,

vT MSv = vT M v

µ(x)vT xxT v

−

S
Xx /
∈
µ(x)(vT x)2

vT M v

1

≤

≤

= vT M v

−

S
Xx /
∈

∈

W (MS) (the last step is from the assumption that v

implying that v
W (M )). The
length of a point on the boundary of an ellipsoid lower bounds the length of the longest
axis. Since at least one axis of the dual ellipsoid has length 1/a, and all the other axes have
length at least 1, V ol(W (MS))
(1/a)f (n) while V ol(W (M )) = f (n), implying the dual
volume grows by at least a factor of eγ2p/2. This concludes the proof of lemma 1.

≥

∈

Note that if we desire to apply the lemma to analyze the result of a later iteration of
Algorithm 2, where µT goes to µT

S, we simply replace the starting identity by

Ex

T [(wT x)2] Pr[x

T ] = Ex

∈

T

∈

∩

∈

T

∈

∩

S] + Ex

T

∈

\

S[(wT x)2] Pr[x

T

S]

∈

\

∩
S[(wT x)2] Pr[x

The analysis and conclusion remain the same.

Lemma 2 (Dual Volume Growth) Let µ be a distribution satisfying

(I)

unit vector ˆw,

( ˆwT x)2dµ

(II)

unit vector ˆw,

S : µ(S)

∀

1

¯ǫ,

≥

−

S( ˆwT x)2dµ
R

∀

∀

For any S∗, let µS∗ be the restriction of µ to S∗. Assume µ(S∗)

1

¯ǫ. Then

R2

r2

≤

≥

≥

−

V ol(W (M ))

V ol(W (MS∗))

f (n)
Rn

≥

f (n)
rn

≤

Proof: First we lower bound the initial dual volume, V ol(W (M )). Consider any vector v
of length at most 1/R. We have

vT M v = E[(vT x)2] =

(vT x)2dµ

(v2R2)

≤

1

≤

Z

so v belongs to the dual ellipsoid. Thus the dual ellipsoid initially has volume at least
f (n)/Rn.

Next we upper bound V ol(W (MS∗)). Consider any vector v of length more than 1/r. Then

vT MS∗v =

(vT x)2dµ

(v2r2) > 1

ZS∗

≥

Thus v is not in W (MS∗), and thus the ultimate volume of the dual ellipsoid is no more
than the volume of the sphere of radius 1/r, yielding the claimed upper bound.

R

7

In the proof of theorem 2 below, µS∗ will be the ﬁnal distribution resulting from application
of either algorithm. Using lemmas 1 and 2, we prove that Algorithm 2 terminates with
S = S∗ satisfying theorem 2.
Proof of Theorem 2: Let β = 4 n
r + 1). Suppose that the algorithm terminates with
subset S∗ after having thrown out no more than ǫ′ of the original probability mass. Then
we have that for every w,

ǫ (ln R

max

(wT x)2 : x
{

∈

S∗

} ≤

γ2E[(wT x)2 : x

S∗] Pr[x

S∗]

∈

∈

We remind the reader again that normalizing µS∗ so that it is a probability distribution
on points from µ, rather than with points outside of S∗ replaced by zeros, increases the
right-hand side of this inequality by the factor 1/µ(S∗), but does not increase the left-hand
side. Thus the inequality will still be true even if we normalize µS∗. We thus achieve a
β-outlier free subset with

β = γ2 = 4

(ln

+ 1)

n
ǫ

R
r

It now remains to show that ǫ′ ≤
ǫ, i.e. that we do not throw out more of the probability
mass than claimed. Suppose that during the ith iteration of the algorithm through step 1, a
pi fraction of the original points are thrown out. Then the total amount thrown out is
pi.
γ2
2 P pi. Comparing

By lemma 1, the total amount of dual volume increase is
this to our bound on the total increase in the dual volume from lemma 2 yields

i epiγ2/2 = e

P

Q

γ2
2 P pi

e

(

R
r

≤

)n = en ln R

r

1
2

⇒

γ2ǫ′ =

(4

ln

)ǫ′

n ln

R
r

≤

1
2

n
ǫ
ǫ′

⇒

≤

R
r
ǫ/2

¯ǫ, since we relied on this in applying lemma 2
The one remaining catch is showing that ǫ′ ≤
above. By slight overloading of notation, we let ǫ′ denote the cumulative probability mass
that has been removed at any point during the algorithm. Suppose for the purpose of
¯ǫ, but then in iteration j + 1, ǫ′ > ¯ǫ.
establishing a contradiction that in iteration j, ǫ′ ≤
¯ǫ/2.
Then on step j, we can apply lemma 2, and from the analysis above, ǫ′ ≤
However, in any single iteration, the maximum probability mass the algorithm might throw
out is 1/γ2, as can be seen from the proof of lemma 1:

ǫ/2

≤

a2

1

≤

−

γ2p

0

1

≤

−

γ2p

⇒

⇒

1/γ2

p

≤

≤
¯ǫ
2 , and so on step j + 1, we

Thus in one step ǫ′ increase by at most ǫ/[4n(ln(R/r) + 1)]
still have ǫ′ ≤

¯ǫ. This concludes the proof of theorem 2.

We now give an alternate proof of theorem 2 using the construction given by Algorithm 1.
We begin by proving an analogue to lemma 1.

Lemma 3 (Restriction to an Ellipsoid) Let γ be ﬁxed, and let µ be a full-dimensional
isotropic distribution. Let S =

S]. Then

γ2

x : (xT x)
{

≤

V ol(W (MS))

≥

}

and p = Pr[x /
∈
epγ2/2V ol(W (M ))

8

Proof: First we establish the tradeoﬀ for a radially symmetric distribution, and then we
show that a radially symmetric distribution is the worst case for the tradeoﬀ we want.

Let µ′ be a radially symmetric distribution, and deﬁne M ′, S, and p as above. We then
calculate the increase in V ol(W (M ′)). Let a2 = Eµ′[(wT x)2 : x
S] for any
w,
= 1. From the center of an n-dimensional sphere of radius γ, the projection of
the sphere on to any direction is sharply concentrated around γ/√n, and the squared
expectation is exactly γ2/n. Using the identity

S] Pr[x

w
|

∈

∈

|

E[(wT x)2] = Ex /
∈

S[(wT x)2] Pr[x /
∈

S] + Ex

S[(wT x)2] Pr[x

∈

S]

∈

as in the proof of lemma 1, but now for any w, we deduce 1

a2 + γ2p/n, and thus

As in the proof of lemma 1, we observe that W (M ′S) includes a vector of length 1/a in the
direction of w. Since this is now true for every w, the dual ellipsoid volume increases by at
least a factor of (1/a)n. This shows that in the case of a radially symmetric distribution,

≥

γ2p
n

1

−

an

≤

(cid:18)

n/2

≤

(cid:19)

γ2p/2

e−

V ol(W (MS))

≥

epγ2/2V ol(W (M ))

Now we show that a radially symmetric distribution is the worst case for the tradeoﬀ we
want. Suppose there were some isotropic, full-dimensional distribution µ for which the
statement of the lemma was not true. We construct a new isotropic, full-dimensional and
radially symmetric distribution µ′ for which the statement is also false.

We begin by noting that every point thrown out from µ is also thrown out from any rotation
of µ – this just follows from the fact that µ is isotropic. Let µ′ be the expectation of µ
under a random rotation. That is, µ′ is a radially symmetric distribution such that the
probability of choosing x from µ′ at distance less than r from the origin is exactly the same
as the probability of choosing x from µ at distance less than r from the origin, for every r.
Let M ′ correspond to µ′.

Consider an axis direction wi of E(MS),
∈
S]. For E(M ′S), denote the axis length for any axis (also just the radius of E(M ′S)) by ¯a.
We ﬁnd from the construction of µ′ that

S] Pr[x

wi|
|

= 1. We have a2

i = E[(wT

i x)2 : x

∈

¯a2 =

E[(wT

i x)2 : x

S] Pr[x

S] =

∈

∈

1
n

n

Xi=1

1
n

n

a2
i

Xi=1

One way to visualize this equality is to take µ and simply consider ˜µ achieved by averaging
over rotations of the axes of µ onto the other axes of µ; since this is a discrete set of
rotations, it is clear that the squared axis lengths of ˜µ are just the arithmetic averages of
the squared axis lengths of µ. Then we can make ˜µ into µ′ by taking a continuous set of
rotations, without aﬀecting the axis lengths from ˜µ.

We now consider the volume of E(M ′S). We have

V ol(E(M ′S)) = f (n)

¯a = f (n)

f (n)

ai = V ol(E(MS)

n

Yi=1

n

a2
i 

≥

n

Xi=1







v
u
u
t

n

Yi=1

1
n

9

using the arithmetic mean-geometric mean inequality. This implies that V ol(W (MS))
V ol(W (M ′S)). This concludes the proof of lemma 3.

≥

Finally, we prove that Algorithm 1 terminates with S satisfying theorem 2.
Proof of Theorem 2: As in the proof of theorem 2 using Algorithm 2, let β = 4 n
r +1).
Lemma 2 still holds. The rate of increase in the dual volume as we throw out probability
mass (lemma 3) is the same as before (lemma 1). The only thing we need to address is
what we called “the one remaining catch” in the proof using Algorithm 2. Our bound on
the amount of probability mass that can be thrown out in a single step is no longer 1/γ2,
¯ǫ
but is now n/γ2. However, n/γ2 = ǫ/[4(ln(R/r) + 1)]
2 just as before. This concludes
the analysis of Algorithm 1.

ǫ (ln R

≤

The following connection shows that the success of either algorithm implies that they both
succeed. If our criterion for a point x to be a β-outlier in a direction w were instead that

(wT x)2 > βE[(wT x)2 : x

P ] Pr[x

P ]

∈

∈

then Algorithms 1 and 2 both throw out the exact same points, and so must yield the same
bound on β as a function of ǫ. To see this, note that any β-outlier under this deﬁnition
remains a β-outlier as further points are removed, and so will have to be removed itself
eventually. Also, no point is ever removed unless it currently is a β-outlier. Thus the
two algorithms throw out exactly the same set of points in the end under this alternative
deﬁnition of an outlier. In section 7, we develop this observation into an approximation
algorithm for the problem of outlier removal using the standard deﬁnition of a β-outlier
(not this alternative deﬁnition).

We pause to stress what we have gained by allowing some points of the distribution to
be removed. If we force ǫ = 0, then even under the hypothesis of theorem 2, β may be
unbounded. Even a radially symmetric distribution satisfying the hypothesis with support
in

, where BR denotes the ball of radius R, might have β as large as

BR \
{

Br√n}

By allowing ǫ > 0, we have achieved

β =

R2
r2

β = 4

(ln

+ 1)

n
ǫ

R
r

4 Outlier Removal over Discrete Support

While theorem 2 might suﬃce for many applications, it is indeed possible that during outlier
removal on an arbitrary set, the full-dimensional condition might be violated (indeed, the
dimensionality of the remaining set might decrease). In this section we prove the following
theorem, which shows that for distributions over integers, the full-dimensional condition is
entirely unnecessary.

10

Theorem 1 (Outlier Removal over Discrete Support) Let µ be a probability distri-
bution on

n
b . Then for every ǫ > 0, there exists S and

Z

such that
(i) µ(S)
(ii) max

1

ǫ

−

≥
(wT x)2 : x
{

S

∈

} ≤

β = O

(b + log

n
ǫ

)

(cid:17)

n
ǫ

(cid:16)

∈

βE[(wT x)2 : x

S] for all w

n
∈ R

The proof of this theorem presents two diﬃculties that were not present in the proof of
theorem 2. First, µ might initially lie entirely on a lower-dimensional subspace, or µ might
lie on a lower-dimensional subspace after the removal of a few points. Secondly, even if
the distribution does not lie on a lower-dimensional subspace, we do not have the same
lower bound on the smallest singular value of the distribution (singular value of the matrix
M associated with µ). While we insisted in the hypothesis of theorem 2 that the smallest
b in the discrete
singular value must be at least 1/r, which will be roughly equivalent to 2−
nb, as the following example
case, it may be that the smallest singular value is actually 2−
makes clear.

Example 1 Let B = 2b, and let each row of the matrix below represent a point in space.
Denote the ﬁrst n

n
i=1 and denote the last row by p.
−

1 rows by

1

−

vi}
{

B



1
−
B

1
−
B

1
−
. . .



1












This set of points is clearly full-dimensional, and in most directions the singular values are
1, 1], we ﬁnd that
on the order of B. However, in the direction w = [B−
nb. Since w > 1, the singular value is actually
(wT vi)2 = 0 while (wT p)2 = B−
slightly less than 2−

n+1, . . . , B−

n = 2−

n, B−

nb.

Example 1 shows that even disregarding issues of the distribution not being full-
dimensional, we cannot use theorem 2 to treat the distribution with integer support unless we
are willing to settle for β = ˜O( n2b
ǫ ). In extending our techniques to prove theorem 1, we will
show that although one singular value may be small, they are not all small simultaneously
in an appropriate amortized sense.

The ﬁrst thing we shall deﬁne is a potential function that generalizes the dual ellipsoid
volume we used in the proof of theorem 2. This potential function will account for the
distribution µ being concentrated on a lower dimensional subspace, or even the possibility
that µ is simply quite close to a lower dimensional distribution. We begin by deﬁning the α-
core of a distribution to be that subset of the distribution which lies on a subspace spanned
by every large subset of the distribution. It will help to deﬁne the indicator function of E
to be

where E is a logical statement. The α-core is then given by

χE =

1
0
(

if E is true
if E is false

11

Deﬁnition 1 (α-core) Deﬁne the α-core of µS to be µT , where T
maximum such that

⊂

S is chosen to be

w

∀

∈

span(µT ) such that w

= 0,

wT x

=0

χ{

}µ(x)

α

≥

x
µT
X
∈

We now establish some characteristics of the α-core, including that the α-core is well-deﬁned.

Lemma 4 (Characterization of α-core)

(i) For any µT ,

if and only if

w

∀

∈

span(µT ) such that w

= 0,

wT x

=0

χ{

}µ(x)

α

(a)

w, wT x
∀

= 0 for some x

µT ⇒

∈

wT x

=0

χ{

}µ(x)

α

(b)

x
µT
X
∈

x
µT
X
∈

≥

≥

⊂

⊂

∈

(ii) Q

S =

α-core(µQ)

α-core(µS)

⊂

(iii) Q

S =

α-core(µQ) = α-core(µQ ∩

α-core(µS))

⇒

⇒

(iv) Suppose that µT = α-core(µS), and that dim(span(µS)) = k, dim(span(µT )) = k′.

Then µ(S

T )

(k

k′)α

\

≤

−

Proof: We ﬁrst establish (i). Let µT be arbitrary. Assume (b) does not hold, i.e., there
}µ(x) < α. Writing
exists x

µT and a direction w such that wT x

= 0 and

wT x

χ{

=0

w = w1 + w2, w1 ∈
χ{
= 0, while

P
span(µT ), w2 ⊥
w′T x

=0

}µ(x) =

wT

1 x

=0

χ{

}µ(x) < α. Since

1 x

span(µT ), (a) does not hold.
P

we ﬁnd wT x = wT
w1 ∈
Now suppose that (b) does hold. If w
=0
hence (b) implies that

wT x

µT

∈

x

χ{

x

µT

∈

∈

}µ(x)

≥

To show (ii), we give an algorithm for constructing α-core(µS):

P

span(µT ), then wT x

= 0 for some x

µT , and

∈

α, so (a) holds too.

x

µT

∈

span(µT )

x

µT

∈

P

1. If there exists x

µS and a direction w such that wT x

= 0 but

wT x

=0

χ{

}µ(x) <

α, remove x from µS.

∈

x

µS

∈

P

2. Repeat until there does not exists such an x.

To argue the correctness of this algorithm, it suﬃces to show that if x meets the cri-
terion of step 1, then x cannot be in any µR, R

S such that

= 0 =

⇒
}µ(x) < α. But this is obvious, since the w associated with x in step 1
}µ(x) < α. Therefore

= 0 and yet

}µ(x)

wT x

wT x

χ{

χ{

⊂

=0

=0

wT x

=0

χ{

x

µR

∈

satisﬁes wT x
P

w, wT x
∀

x

µR

∈

P

≤

x

µS

∈

P

12

6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
any x identiﬁed in step 1 cannot be in α-core(µS). Since the algorithm stops when it has
arrived at µT satisfying (b), and no point has been removed that could be in α-core(µS),
µT = α-core(µS). This establishes that the α-core is well-deﬁned.

Now consider the order in which points are identiﬁed in step 1 when the algorithm is applied
to µS. Considering points in the same order (and omitting points that are in µS but not
in µQ), the algorithm run on µQ would always remove the points as well, simply because
}µ(x), where Q′ and S′ are Q and S minus the

}µ(x)

wT x

wT x

′ χ{

′ χ{

=0

=0

µQ

x

∈

≤

µS

x

∈

points that the algorithm has removed prior to the iteration under consideration.
P
We prove (iii) using (ii).

P

µQ ∩
α-core(µQ ∩

⊂

α-core(µQ)
α-core(µQ)
α-core(µQ))

α-core(µS)
µQ ∩
α-core(µQ ∩
α-core(µQ)) = α-core(µQ). Now

⊂

⊂

⇒

α-core(µS)

⇒

α-core(µS))

We note that α-core(µQ ∩

µQ ∩
α-core(µQ ∩

α-core(µS)
α-core(µS))

⊂

µQ ⇒
α-core(µQ)

⊂
α-core(µS)).

Combining these yields α-core(µQ) = α-core(µQ ∩
To see (iv), construct µT from µS in the following greedy manner. If dim(span(µT )) <
dim(span(µS)), then

w

∃

∈

span(µS)

such that

wT x

=0

χ{

}µ(x) < α

x
µS
X
∈

If w were not

to span(µT ), we could write

⊥

w = w1 + w2, w1 ⊥

span(µT ), w2 ∈

span(µT )

wT x

=0

wT x

=0

x

∈

µS

χ{

}µ(x)

µT
span(µT ). Remove every point x

χ{
α.
and then argue
= 0 (a less than α
Hence w
fraction of the total probability mass), and note that this causes dim(span(µS)) to drop by
k′) times, and hence
at least 1. Therefore this construction can be iterated at most (k
µ(S

µT
∈
µS such that wT x

}µ(x) =

}µ(x)

T )

χ{

P

P

P

(k

⊥

≥

≥

−

∈

∈

x

x

k′)α.

wT

2 x

=0

\

≤

−

We can now deﬁne the potential φ of a distribution (or a subset of a distribution).

Deﬁnition 2 (Potential Function: φ) Let µT be the α-core of µS. Let φ(µS) be
V ol(W (MT )), the volume of the dual ellipsoid of µT . If µT is not full dimensional, but
instead lies in a space of dimension k, let φ(µS) be V olk(W (MT )), the k-dimensional volume
(within the span of µT ) of the dual ellipsoid of µT .

We now prove upper and lower bounds on φ(µS), analogous to lemma 2, for the case that the
α-core of µS is full-dimensional. Although a tighter version of this lemma may be possible,
the analysis here is suﬃcient to show the asymptotic result of theorem 1.

13

6
6
6
6
6
6
6
Lemma 5 (Bounds on φ) Denote the α-core of µS by µT and suppose that µT is full-
dimensional (and hence µT = µS). Then

φ(µS)

≥
φ(µS)

≤

(2b√n)−

nf (n)

(n/α)nf (n)

Proof: We lower bound φ by showing that for any vector v satisfying
dual ellipsoid. Using that no element x of µ has length greater than √n2b, we ﬁnd that

| ≤

v
|

2−b
√n , v is in the

vT MSv =

(xT v)2µ(x)

x
µS
X
∈

x2v2µ(x)

1

≤

≤

x
µS
X
∈

The claimed lower bound now follows from the fact that W (MS) contains a ball of radius
2−b
√n .
To upper bound φ(µS), we will use that µT is full-dimensional. Because the volume of an
ellipse is equal to the product of the axis lengths times a factor that depends only on the
xxT µ(x). We now
dimension, we have that φ(µS) = f (n)/Det(MS) where MS =
show that we can decompose MS into a set of simpler components plus some extra points,

µS

∈

x

P

MS =

λiMi +

yyT

Xi

y

X

0.

P

λi ≥

α/n, λi ≥

µS. Now pick any point x2 ∈

span(x1). Now pick any point x3 /
∈

where each Mi is a positive deﬁnite nxn matrix of integers and
µS
To see this decomposition, begin by picking any point x1 ∈
such that x2 /
span(x1, x2). Continuing, we can always
∈
make such a choice by considering any direction w perpendicular to the span of the previous
points — any point with non-zero inner product with this w, guaranteed to exist by the
n
deﬁnition of α-core, lies oﬀ the span of the previous points. This ﬁrst set of points
j=1
j with λ1 = minj µ(xj). To form M2, we must restrict ourselves to
yields M1 =
(using slight overloading of notation). By the deﬁnition of
µS \
picking points from
{
λi < α/n, we will always be able to form a new Mi because we have
α-core, as long as
subtracted oﬀ less than an α fraction of the probability mass from the distribution thus
far. The process can be seen to terminate in a ﬁnite number of steps because the support
of µS is initially a ﬁnite number of points, and at every step the cardinality of the support
decreases by at least one. The
which we referred to as “extra points” above are simply
the points remaining in µS when this operation, having formed a suﬃcient number of Mi,
comes to an end.

λ1M1}

j xjxT

xj}
{

y
{

P

P

}

Note that each matrix Mi satisiﬁes Det(Mi)
1 because it is the sum of the products of
many integer terms and it is positive (because Mi is positive deﬁnite). We now show that we
may ignore the y terms in establishing a lower bound for Det(MS). Another consequence of
Mi being positive deﬁnite is that Mi = AiAT
for some Ai. Since the determinant of MS is
i
y(yT wj)2
the product of the eigenvalues, and each eigenvalue ej is equal to
i +
for some unit vector wj, Det(

i λi(AT
yyT ).

i λiAiAT
i )

i λiAiAT

i wj)2+

Det(

≥

P

≤

We have from fact 2 in section 10 (and since the geometric mean is at least the min) that
for

P

P

λ′i = 1,

P
P

P

Det(

λ′iMi)

min
i {

Det(Mi)
}

≥

Xi

14

The last step is to write Det(ξM ) = ξnDet(M ), which implies Det(MS)
yields the claimed upper bound on φ(µS).

≥

(α/n)n. This

Note that lemma 5 implies that the log of the ratios between the upper and lower bounds on
φ is at most n(b + 1.5(log n
α )). (The relevant setting of α for the proof of theorem 1 will be
α = ǫ/(3n).) This compares favorably with the corresponding ratio in the continuous case,
n ln R
r , and suggests that we have not introduced much slack while extending our techniques
to amortize over the singular values.

We now address the issue of dimension dropping. We refer to non-monotone growth in
the title of lemma 6 because now φ may drop when we remove some of the distribution.
To see this, consider example 1 again: φ is initially about f (n), but after removing the
point p, φ becomes roughly 2−
In the proof of theorem 2, we bounded the
drop in probability mass by bounding the increase in the volume of the dual ellipsoid.
Because φ may decrease greatly during the course of the algorithm (when the α-core drops
in dimension), a bound on φ’s ﬁnal value is no longer enough to bound the drop in probability
mass. Happily, we can still bound the growth of φ in the following sense:

1)f (n).

b(n

−

Lemma 6 (Non-Monotone Growth of φ) Over the course of either algorithm on dis-
tribution µ, let (∆φ)i denote the relative increase in φ while α-core(µ) spans a subspace of
dimension i (or 1 if α-core(µ) is never concentrated on a subspace of dimension i). Then

2n(b+3 log n

α +1)

(∆φ)i ≤

Yi

Proof: Suppose that initially the α-core of µ is full-dimensional, and that
i(∆φ)i = V .
Under a simplifying assumption, we construct a distribution µ′ such that the α-core of µ′ is
full-dimensional and φ(µ′)/φ(µ)
V . (If the result of applying the oulier removal algorithm
to µ is µS that has full-dimensional α-core, then µ′ = µS and there is nothing to do.) By
lemma 5, φ(µ′) and φ(µ) cannot diﬀer by a factor of more than 2n(b+1.5 log n
α ), and thus this
suﬃces to prove the bound on V . We then remove the simplifying assumption. We defer
the issue that the α-core of µ might not initially be full-dimensional to the very end of the
proof.

Q

≥

Suppose that the algorithm goes from µR of dimension (i + 1) to µS of dimension i, and
then runs for a while to produce µT (still of dimension i). The simplifying assumption we
mentioned above is that the dimension of the α-core has only fallen by 1 on this step. For
ease of exposition, assume that each distribution is equal to its α-core. This is without
loss of generality because φ is deﬁned in terms of the α-core, and so the points outside the
α-core are irrelevant for this lemma. We will construct µ′S′ and µ′T ′ of dimension (i + 1)
such that φ(µ′S′)

φ(µ′
φ(µS ) . Then we will have
S

φ(µ′
φ(µT ) =
T

φ(µR) and

′ )

′ )

≥

φ(µ′T ′) =

φ(µ′S′)

(∆φ)iφ(µR)

≥

φ(µT )
φ(µS)

Applying this construction iteratively over all the dimensions yields µ′ of dimension n
satisfying φ(µ′)/φ(µ)

V .

≥

15

Let us now construct µ′S′ and µ′T ′. Deﬁne pj = µ(xj) for all xj ∈
Then

\

µR

S and let P =

j pj.

MR = MS +

pjxjxT

j =

(MS + P xjxT
j )

P

pj
P

Xi

Xj

Deﬁne Xj to be (MS + P xjxT

j ). By fact 2 (section 10),

there exists j such that Det(Xj)

Det(MR). Denote this particular xj by x, and let

Det(

λjXj)

Det(Xj)
min
,
}
{

≥

λj = 1

X

X

≤
µ′S′ =

µ′T ′ =

µS + x with weight α
{

}
µT + x with weight α
{

}
Det(Xj). Thus φ(µ′S′)

′ )

′ )

≤

Note that P

α, and so Det(M ′S′)
φ(µ′
φ(µT ) =
T

≥
φ(µ′
φ(µS ) . Rotate the distributions so that span(µS) and span(µT ) are
We now show
S
equal to the ﬁrst i coordinate axes, and x lies in the span of the ﬁrst i + 1 coordinate axes.
Denote the vector formed from the ﬁrst i coordinates of x by x[1 . . . i], and the (i + 1)st
coordinate of x by x[i + 1]. Then the distance of x to span(µS) is just x[i + 1], and this is
also the distance of x to span(µT ). We have φ(µT ) = f (i)/Det(MT ), while

φ(µR).

≥

φ(µ′T ′) = f (i + 1)/Det

MT
0

(cid:18)(cid:20)

0
0

(cid:21)

+ α2

(cid:20)

x[1 . . . i]x[1 . . . i]T x[i + 1]x[1 . . . i]
x[i + 1]x[1 . . . i]T

x[i + 1]2

(cid:21)(cid:19)

where the upper left matrix block (MT + α2x[1 . . . i]x[1 . . . i]T ) is ixi. For any matrix A,
subtracting a scalar multiple of some row of A from another row of A does not change the
determinant of A. To calculate φ(µ′T ′), we subtract x[l]/x[i + 1] times the last row of the
matrix from the lth row for every l

i. This yields

Det

MT
0

(cid:18)(cid:20)

0
0

(cid:21)

+ α2

x[i + 1]x[1 . . . i]T x[i + 1]2
(cid:20)

(cid:21)(cid:19)

0

= Det(MT )α2x[i + 1]2

≤

0

′ )

Therefore
′ )

φ(µ′
φ(µT ) = (αx[i + 1])−
T
f (i)
φ(µ′
′ )
φ(µT ) =
T

φ(µ′
φ(µS ) . This shows that
S

for

2 f (i+1)

φ(µ′
′ )
φ(µS ) .
S

. An identical calculation yields an identical result

We now remove the simplifying assumption and extend this construction to the case that at
some step the α-core falls in dimension by more than 1. If µR and µS diﬀer by k dimensions,
we construct µ′S′ by adjoining k points from µR
S, each with weight α/k. We now show
how to ﬁnd these points. Since µR is an α-core, and span(µS) is a subspace of k dimensions
less, we can use the construction of lemma 5 to write

\

MR = MS +

λiAiAT

i +

yyT ,

λi = Λ

α
k

≥

Xi
where each Ai is a set of k points such that span(
µS + Ai}
{
Det(MS +ΛAiAT
i )

λiAiAT
i )

Det(MS +

Det(MR)

Xi

X

min
i {

≥

≥

) = span(µR). As above,

min
i {

} ≥

Det(MS +

α
k

AiAT
i )
}

Xi

16

Let A denote the Ai realizing this minimum and let

µ′S′ =

µS + A with weight
{

µ′T ′ =

µT + A with weight
{

α
k }
α
k }

≥

φ(µR) by construction. It remains to show

φ(µ′
φ(µS ) . We do this by
We have φ(µ′S′)
S
by showing that the previous calculation (giving this fact under the simplifying assumption)
can be repeated k times. Let µ′
and
deﬁne µ′

µS + ﬁrst l points of A with weight α
k }
{

(l)
T ′ similarly. Then the previous calculation yields

φ(µ′
φ(µT ) =
T

(l)
S′ denote

′ )

′ )

(1)
φ(µ′
T ′ )
φ(µT )

=

(1)
S′ )
φ(µ′
φ(µS)

,

φ(µ′

φ(µ′

(2)
T ′ )
(1)
T ′ )

=

φ(µ′

φ(µ′

(2)
S′ )
(1)
S′ )

,

. . .

(k)
T ′ )
φ(µ′
φ(µ′T ′)

=

(k)
S′ )
φ(µ′
φ(µ′S′)

⇒

This concludes the construction of µ′ which is full-dimensional and at least an α/n-core.

We now turn to the case that µ is initially only k dimensional, where k < n. In this case,
we adjoin any (n
k) points, each with weight α, to form ¯µ, where ¯µ is full-dimensional.
Then ¯µ may not be a probability distribution, but it has total weight at most 1 + nα, and
so

−

φ(¯µ)

(2b√n√1 + nα)−

nf (n)

≥

by the same construction as in the lower bound of lemma 5. The iterative construction
(n2/α)nf (n), and
above (without the simplifying assumption) yields ¯µ′ such that φ(¯µ′)
so

≤

φ(¯µ′)/φ(¯µ)

2b

n + n2α)n

(2n32b/α)n

2n(b+3 log n

α +1)

≤

≤

(

n2
α

≤

This concludes the proof of the lemma.

p

We now prove that Algorithm 2 applied to a distribution µ over the b-bit integers yields S
satifying theorem 1.
Proof of Theorem 1: Let α = ǫ/(3n) and let β = γ2 = 6 n
ǫ + 3). The only
time that the drop in probability mass due to action by the algorithm does not lead to an
increase in φ is when either the algorithm causes the dimension of the α-core to drop, or
the algorithm removes probability mass that lies outside the α-core.

ǫ (b + 4 log n

We ﬁrst consider the fraction of the distribution that is not part of the α-core. Initially,
dim(span(µ)) is at most n. Suppose dim(span(α-core(µ))) = k. Then by lemma 4:(iv), at
most an α(n
k) fraction of µ lies outside of the α-core of µ. Points only leave the α-core
when they are removed by the algorithm, or when the algorithm takes µS to µS′ in one step
and

−

dim(α-core(µS)) = k1,

dim(α-core(µS′)) = k2,

k2 < k1

In the latter case, the probability mass lost from the α-core (and not removed by the
algorithm) is given by

} \ {
S, by lemma 4:(iii), this is the same as

∩

µS′
{

α-core(µS)

α-core(µS′)
}

Since S′ ⊂

µS′
{

∩

α-core(µS)

α-core(µS′

} \ {

α-core(µS))
}

∩

17

and by lemma 4:(iv) this is no more than α(k1 −
k2). Since the cumulative drop in dimension
of the α-core is no more than n dimensions, no more than an αn fraction of the distribution
ever leaves the α-core (without being removed by the algorithm) over the course of the
algorithm.

We now bound the amount of the distribution removed by the algorithm on steps in which
the α-core drops in dimension. In the proof of theorem 2, we showed that in any single
step, Algorithm 2 throws out no more than a 1/γ2 fraction of the distribution. Since there
are no more than n steps where the α-core drops in dimension, we throw out no more than
an n/γ2 fraction in this way. This yields that at most an nα + n/γ2
2ǫ/3 fraction of the
probability mass that we throw away does not contribute to increasing φ.

≤

We now proceed exactly as we did in the proof of theorem 2 for Algorithm 2. Every time
we remove pi of the probability mass from the α-core and the α-core does not drop in
dimension, we have from lemma 1 that φ must increase by epiγ2/2. If we throw out an ǫ′
fraction of µ, at most a 2ǫ/3 fraction does not contribute to φ increasing, so by application
of lemma 1

Lemma 6 then yields

Y

(∆φ)i ≥

e

γ2
2 (ǫ′

2ǫ
3 )

−

ǫ +3)

2n(b+4 log n
n
ǫ

n(b + 4 log

≥

+ 3)

⇒

γ2
2 (ǫ′

e

2ǫ
3 )

−

γ2
2

(ǫ′

≥

2ǫ
3

)

1

⇒

≥

3
ǫ

(ǫ′

−

2ǫ
3

)

ǫ′

⇒

≤

−

ǫ

This concludes the proof of theorem 1 using Algorithm 2.

We now prove theorem 1 using Algorithm 1. As we noted previously, this may be obtained
as a corollary of the success of Algorithm 2, but a direct proof raises an additional issue
that we explore below. The resolution of this issue leads to a bound on β with smaller
leading constant.
Proof of Theorem 1: Let α = ǫ/(3n) and let β = γ2 = 3 n
ǫ + 3). The only
new issue is bounding the amount of probability mass removed by the algorithm on steps in
which the α-core drops in dimension. We might remove up to an n/γ2 fraction in a single
step, but our asymptotic bound would not stand up if we could remove up to an n2/γ2
fraction of the probability mass over the course of the algorithm.

ǫ (b + 4 log n

| ≤

x
|

x :
{

γ after rounding
}

Suppose the α-core falls by k dimensions in one step of the algorithm. Rather than con-
sidering all the points outside S =
as being removed at once,
imagine instead that the probability mass on every point is uniformly decreased. Then, φ
increases continuously except for at most k discrete time steps, when the dimension of the
α-core drops. Apart from the steps on which the α-core drops, φ increases as a function
of the probability mass removed exactly as implied by lemma 3. Every time the α-core
drops by i dimensions, at most an iα amount of probability mass leaves the α-core (by
lemma 4:(iv)). Therefore at most a kα amount of probability mass is removed without an
increase in φ. In this thought experiment, no probability mass in the α-core is removed by
the algorithm without an increase in φ. Thus at most an nα = ǫ/3 amount of probability

18

mass is removed without an increase in φ. We apply lemma 3 and lemma 6 as before to
obtain

e

(∆φ)i ≥
Y
2n(b+4 log n
ǫ +3)
n
ǫ

+ 3)

≥

n(b + 4 log

⇒

⇒

γ2
2 (ǫ′

ǫ
3 )

−

γ2
2 (ǫ′

e

ǫ
3 )

−

γ2
2

(ǫ′

≥

ǫ
3

)

−

ǫ

1

⇒

≥

3
2ǫ

(ǫ′

−

ǫ
3

)

ǫ′

⇒

≤

This concludes the proof of theorem 1 using Algorithm 1.

5 Time and Sample Complexity

In this section we describe polynomial time versions of both algorithms. The computational
model is to allow multiplications and additions in unit time.

5.1 Point sets

Suppose the distribution µ is speciﬁed explicitly as a set of m points with weights corre-
sponding to probabilities. Then we can achieve exactly the stated value of β with either
algorithm deterministically. The running time for either algorithm is given by the time to
compute M (O(mn2)), the time to round the distribution (O(n3 + mn2)), the time to ﬁnd
an outlier (O(mn)), and the need to repeat the whole process up to m times. This yields a
time bound of O(m2n2 + mn3).

In the above discussion we made the worst case assumption that only one data point was
thrown out in each iteration of rounding and looking for outliers. In the case that a single
If the
data point is throw out, rounding the distribution can be done more eﬃciently.
pvvT
distribution is initially isotropic, and v of probability p is removed, then M ′ = I
gives the new inertial ellipsoid. We can factor M ′−

1 symbolically as

−

M ′−

1 = BBT =

I
 

1
−  

−

1

vT vp !

1

−

2

vvT
vT v !

where we have chosen B to be symmetric. To verify this calculation, note that

p

BM ′BT = (I

bvvT )(I

pvvT )(I

bvvT ) = [I

(2b

b2v2)vvT ][I

pvvT ]

−

−

−

−

−

and we have used that the matrices commute. We calculate

where b = 1
v2

1

(cid:18)

−

√1

v2p

−

1

−

(cid:19)

1
v2

b2v2 =

2b

−

(2

−

 

)

(1

1

pv2

−

−

1

pv2

2

−

+

1
pv2 )
!

1

−

2

−

p

p

19

Plugging this in completes the veriﬁcation

=

1
v2

1

(cid:18)

−

1

−

1
pv2

p
= −
pv2
−

1

(cid:19)

[I

(2b

−

−

b2v2)vvT ][I

pvvT ] = [I +

−

p
pv2 vvT ][I

−

pvvT ]

1

−

If the old distribution was
for B yields

Bx
, where our formula
}
{

p2v2

p
pv2 −

= [I + (

1

p

pv2 )vvT ] = I
x
, the new isotropic distribution is
}
{

−

−

−

1

Bx = x

1
−  

−

1

vT vp !

1

−

v(vT x)

vT v

p

which is computable in time O(n) for any point x. Another explanation for this formula is
that we are just correcting the inertial ellipsoid in the direction of v; this type of update
step is sometimes referred to as a rank-1 update. Using this observation, we can compute
M from scratch once (O(mn2)), round the distribution from scratch once (O(n3 + mn2)),
and then ﬁnd an outlier (O(mn)) and reround using our formula above (O(mn)) a total of
at most m times. This yields the improved time bound of O(m2n + mn2 + n3). If we throw
away less than an ǫ fraction of the point set, the time bound is just O(ǫm2n + mn2 + n3).

Z

n
b and the case that the distribution has full-dimensional
If we specialize our analysis to
α-core throughout the algorithm, we can obtain a running time with a diﬀerent dependance
on the relevant parameters. Suppose that on some step of Algorithm 1 with parameter β
we remove all β-outliers and φ (equivalently, the dual ellipsoid volume) increases by a factor
of no more than (1 + δ) — then the remaining data set is (1 + δ)β-outlier free. Because
we may have removed many points, we cannot use the technique just developed above, and
our time bound is O(mn2 + n3) per iteration. However, by our upper and lower bounds on
φ, there are at most log(1+δ) 2 ˜O(nb) = ˜O( nb
δ ) iterations where φ increases by (1 + δ) or more.
The ﬁnal bound on the running time is then ˜O( mn3b+n4b
) to obtain a (1 + δ)β-outlier free
set.

δ

5.2 Arbitrary distributions

Now suppose that we are not given µ explicitly, but rather only the ability to sample from
n
µ. For ease of exposition, we will refer only to the case that the support of µ is in
b . The
outlier-free restriction of µ will be speciﬁed as the part of µ contained in an ellipsoid. The
algorithm for distributions is:

Z

1. Get a set P =

x1, . . . , xm}
{

of m samples from µ.

2. Run the outlier removal algorithm on the discrete point set P with parameter Γ2.

3. Let P ′ be the outlier-free subset of P . Then the outlier-free restriction of P is given
i . The outlier-free restriction of µ is given by

by ΓE(M ′), where M ′ = 1
m
(0, 1/4) is an accuracy parameter.
(1 + δ)ΓE(M ′), where δ
P

P ′ xixT

∈

xi

∈

The main theorem of this section is the following.

20

Theorem 3 (Sample Complexity) Let

m = O

n log

+ log

γ2
δ2

(cid:18)

(cid:18)

n
δ

n(b + log n)

δ

= ˜O

nγ2
δ2

(cid:19)(cid:19)

(cid:18)

(cid:19)

Then with high probability, either outlier removal algorithm run with parameter Γ2 = (1 +
δ)2γ2 returns an ellipsoid T = ΓE(M ′) satisfying
(i) µ((1 + δ)T )
(ii) (1 + δ)T has no (1 + δ)O(1)γ2-outliers
where (γ2, ǫ) is achieved by the deterministic omniscient algorithm (omniscient in that it
knows the distribution exactly).

−

≥

1

ǫ

For the remainder of this section, assume that the deterministic omniscient algorithm with
ǫ, and µS has no γ2-outliers. The
parameter γ2 ﬁnds a subset S such that µ(S)
statement “µS has no γ2-outliers”, or simply “S has no γ2-outliers” (since µ is implicit), is
exactly that

≥

−

1

w, max

∀

(wT x)2 : x
{

S

∈

} ≤

γ2E[(wT x)2 : x

S] Pr[x

S] = γ2

(wT x)2µ(x)

∈

∈

S
Xx
∈

T

S

∈

∈

∈

} ≤

max

µS, but rather x

(wT x)2 : x
{

, we will be able to conclude that S
}

S. This is an important subtlety. Since S and
The max is not over x
(wT x)2 :
T constructed by the algorithm are always convex, whenever we have
{
T . If we had instead
x
T ) = 0 (i.e.,
deﬁned the max over x
although S might not be a subset of T , µ does not assign positive probability to any point
in S that lies outside of T ); this alternative deﬁnition would have increased the length of
the proof.
We know that γ2 = ˜O( bn
bound on running time is proved for arbitrary values of γ2.

ǫ ) is always achievable, but in some cases we may do better. Our

∈
µS, we would only be able to conclude that µ(S

w, max

⊆

∈

∀

\

±

δ in every direction. Let
Suppose that at some step we can estimate E(M ) to within 1
Γ2 = (1 + δ)2γ2. Then every point that we perceive to be a Γ2-outlier will be at least a
γ2-outlier with respect to the true distribution, and so removing them does not throw away
any point that the deterministic algorithm keeps. Similarly, if we perceive the distribution
to have no Γ2-outliers, the true distribution will have no (1+δ)2Γ2-outliers. Before removing
outliers, we may not have that our working estimate of M , ¯M , is within 1
δ of M . However,
whenever we are wrong by more than 1 + δ, there is some true outlier with respect to the
original distribution that we throw out even using our ﬂawed estimate ¯M . This line of
reasoning (made rigorous) will allow us to ﬁnd a (1 + δ)O(1)γ2-outlier-free subset in space,
where γ2 is achieved by the deterministic version of the algorithm. In lemma 7 we show
this for a particular direction in a particular iteration. In lemma 8 we extend this to all
iterations, and in the proof of theorem 3 we extend this to all directions and all iterations,
at every step bounding the sample complexity.

±

Lemma 7 (Outlier Detection, One Iteration) Fix a direction w. Let S be a subset of
space. Let our number of samples be

m = O(

γ2
δ2 )

21

and consider the sample distances in direction w given by
variance of S and ¯y denote the sample variance,

wT xi}
{

. Let y denote the true

y =

(wT x)2µ(x)

¯y =

(wT xi)2

1
m

S
Xxi
∈

S
Xx
∈
Then with constant probability
γ2y
(wT x)2 : x
(i) max
{
(wT x)2 : x
γ2y and T =
(ii) max
{

S
} ≤
S
} ≤

∈
∈

⇒

(1

−

δ)y

¯y

(1 + δ)y.
Γ2 ¯y

≤

≤
x : (wT x)2
{

≤

S

T .

} ⇒

⊂

Proof: Property (i) says that we do correctly estimate the variance of an outlier-free
restriction of the distribution, and property (ii) assures us that any outlier-free restriction
of the distribution has no probability mass past Γ2 times the sample variance (i.e., we can
always safely throw away probability mass using the sample variance). Both claims are for
a ﬁxed direction w. Note that S is assumed to be γ2-outlier-free in the hypotheses of both
(i) and (ii). Lemma 8 will not rely upon part (ii) explicitly, but it will involve a similar
argument.

Let Xi be the random variable representing the squared distance of xi along the direction w,
Xi = (wT xi)2, or 0 if xi /
S
= 1
∈
∈
1
(by an appropriate scaling). First we show (i). Since µS has no γ2-outliers, we have y
γ2 .
Applying the Chernoﬀ bound to determine the probability that ¯y is not a good estimate
for y, we have

(wT x)2 : x
S. Without loss of generality, assume max
{

}
≥

δmy]
≤
This occurs with constant probability for m = O( γ2
δ2 ).

Pr[
m¯y
|

my

| ≥

−

δ2my/3

e−

(wT x)2 : x
Now we show (ii). Let T be as above, and again assume max
{
loss of generality. If S has no γ2-outliers, then y
accurate estimate by the analysis in the previous paragraph. In this case, (1
γ2y
y
then implies S

(1 + δ)2 ¯y, and S has no γ2-outliers implies max

(wT x)2 : x
{

= 1 without
1
γ2 , and we would have found ¯y to be an

δ)y
⇒
Γ2 ¯y. This

−
≤

} ≤

T .

≤

≥

≤

∈

∈

S

S

¯y

}

⊆

Lemma 8 (Outlier Detection, Many Iterations) Fix w. Assume S is
full-dimensional. Let

m = O

γ2
δ2 log

(cid:18)

n(b + log n)

δ

= ˜O

γ2
δ2

(cid:19)

(cid:18)

(cid:19)

Then with constant probability either outlier removal algorithm restricted to w with param-
eter Γ2 produces a subset of space

T =

x : (wT x)2
{

t

}

≤

for some value t such that
(i) For any subset of space S that has no γ2-outliers along w, S
(ii) (1 + δ)T has no (1 + δ)8γ2-outliers along w.

T .

⊆

22

Proof: By “either outlier removal algorithm restricted to w”, we simply mean the one-
dimensional version of the two algorithms. Consider S achieved by the deterministic omni-
scient version of the algorithm (restricted to w). Since our outlier removal algorithm only
throws away probability mass when necessary, this S is the largest possible restriction that
is γ2-outlier free. Deﬁne y and ¯y as in lemma 7. By lemma 7 part (i), we have that ¯y
is a good approximation to y. This ensures that with good probability, we identify S as
Γ2-outlier-free, and so (i) is proved.
It remains to show that, if our algorithm for some
reason chooses a substantially larger set T , then (1 + δ)T has no (1 + δ)8γ2-outliers.

Deﬁne Tα =
has no (1 + δ)2Γ2-outliers. This follows from the fact that

. Suppose
}

x : (wT x)2
{

α such that Tα has no Γ2-outliers. Then T(1+δ)α
∃

≤

α

max

(wT x)2 : x
{

T(1+δ)α} ≤

∈

(1 + δ)2 max

(wT x)2 : x
{

Tα}

∈

∈

∈

Tα] Pr[x

Tα] is a monotonically increasing function of α.

and E[(wT x)2 : x
Suppose we estimate that some set T = Tt has no Γ2-outliers (in which case the algorithm
might return T as an answer). Then our sample also leads us to calculate that Tα has no
(1 + δ)2Γ2-outliers for α
[t, (1 + δ)t] by the same reasoning as in the preceding paragraph.
For every t, we will show that for some nearby (within a factor of (1 + δ)) value of α,
we estimate the sample variance of the restriction of µ to Tα with suﬃcient accuracy. We
proceed to analyze what values of α we need to consider.

∈

n

}

yj

≥

yj}
{

yj}
{

is uniform and the

j=1[(wT yj)2] where the probability distribution on the

Assume without loss of generality that w is a unit vector. An easy upper bound on
max(wT x)2 is 2b√n. To develop a lower bound, we will need to use the assumption that
S is full-dimensional. For any µS, we can write max(wT x)2
E[(wT x)2]. By decom-
posing µS in the manner of lemma 5, we can obtain the stronger statement max(wT x)2
≥
E
are
yj}
{
{
full-dimensional. The term E[(wT yj)2] is lower bounded by the smallest singular value of
. We have previously shown that the product of the singular values of such a distri-
the
2n. Since no individual singular value is more than 2b√n, we have that
bution is at least n−
2n 2n(b+.5 log n) = 2 ˜O(nb). Therefore we can restrict our attention to
the smallest is at least n−
α = (1+δ)k for k an integer and union bound over the at most log(1+δ) 2 ˜O(nb) = O( n(b+log n)
)
possible values for k.
We now show that if we estimate Tα to have no (1+δ)2Γ2-outliers, then with good probability
Tα actually has no (1 + δ)6Γ2-outliers with respect to the true distribution, and by our
reasoning above, since there is an α within (1 + δ) of t, T(1+δ)t is (1 + δ)8Γ2 outlier-free.
We do this by showing that if Tα has a (1 + δ)6Γ2-outlier, then with good probability our
sample shows Tα to have at least a (1 + δ)2Γ2-outlier. Let Xi be the random variable
representing the squared distance of xi along the direction w, Xi = (wT xi)2, or zero if
Tα. Without loss of generality, assume α = 1. Deﬁne y and ¯y as in lemma 7 (but with
xi /
∈
Tα in place of S). Then by assumption on Tα, y = E[Xi]
(1+δ)6Γ2 . The condition that
our samples show Tα to have at least a (1 + δ)2Γ2-outlier is ¯y = 1
(1+δ)2Γ2 . We
m
apply the Chernoﬀ bound,

Xi ≤

≤

1

1

δ

P

Pr[¯y > (1 + ∆)y] < e−

∆2my/3

23

where we have stated the Chernoﬀ bound for the case that ∆ < 1. Let ∆ =

(this yields the event that ¯y >

1

(1+δ)2Γ2 in our probability calculation). If ∆ < 1, then

1

y(1+δ)2Γ2 −

1

∆2y =

1

(1 + δ)2Γ2 −

y ≥

(1 + δ)6Γ2

y ≥

(cid:18)

(cid:19)

(cid:18)

4δ

2 1

δ2
Γ2

2 1

y

(cid:19)

and the probability we do not correctly identify the furthest outlier is at most e−
O(1) for m = O( Γ2

1, then

δ2 ). If ∆

≥

∆2my/3 =

and the applicable alternate form of the Chernoﬀ bound

1

∆y =

(1 + δ)2Γ2 −

≥

y

δ
Γ2

Pr[¯y > (1 + ∆)y] < e−

∆my/3

∆my/3 = O(1) for the same setting of m.

yields that e−
Since there are only O( n(b+log n)
m = O( Γ2
) allows us to union bound over all the possible values of α. This
shows that with constant probability, if we estimate T to have no Γ2-outliers (in which case
our algorithm might return T ), then (1 + δ)T has no (1 + δ)8Γ2-outliers. This implies (ii).

) diﬀerent values of α to consider,

δ2 log n(b+log n)

δ

δ

We extend the analysis of lemmas 7 and 8 from a ﬁxed direction to all directions and argue
the correctness of the entire algorithm by proving theorem 3.

Proof of Theorem 3: Let S be the ellipsoid found by the deterministic algorithm (i.e. the
outlier-free subset of points lies in this ellipsoid). Assume initially that S is full-dimensional.
Rather than considering the original space, consider the transformed space where S is the
unit sphere.
Consider the many directions w given by a δ′-grid in the unit cube, δ′ = δ
6n . We form this
0, δ′, 2δ′, . . . , 1
. By our
grid by choosing every w such that the coordinates of w lie in
}
{
choice of m, we can apply lemma 8 part (i) to each of these ( 6n
δ )n directions simultaneously
(wT x)2 :
and then union bound. Then with good probability, for every w in the δ′-grid, max
{
x
(i.e., in this direction T contains S). We now show that for
an arbitrary direction w, (1 + δ)T contains S.

(wT x)2 : x
{

max

} ≥

∈

∈

S

T

}

wi}
{

Consider an arbitrary unit vector w. By rounding every coordinate of w up or down to an
integer multiple of δ′ we obtain a point on the δ′-grid. The set of all possible such roundings
forms a box surrounding w, and some (not neccessarily unique) subset of n of these points,
, satisfy that w is in the convex cone of the
. Since w is a unit
wi}
which we denote
{
is within 2δ′√n of w.
δ′√n), and so ˆwi = wi/
wi|
vector, each wi has length
|
Deﬁne T (y) to be the distance to the boundary of T along the direction y. Since T is convex
1, the quantity T (w) is lower bounded by the minimum distance of points on
and T (wi)
the convex hull of the
, so is the
projection of w to their convex hull. Since the point on the convex hull is at most 2δ′√n
δ/3. Since S is within 1 of the origin
away from ˆwi for any i, T (w)
everywhere, (1 + δ)T contains S. This concludes the proof of (i).

to the origin. Since w is within 2δ′√n of each

wi| ∈
|

ˆwi}
{

ˆwi}
{

2δ′√n

(1

±

≥

≥

−

≥

−

1

1

24

⊂

Now consider (ii). Since S
(1 + δ)T , T is full-dimensional as well. For every w in our δ′-
grid, we have that (1+δ)T is (1+δ)8Γ2-outlier-free along w by lemma 8 part (ii). As before,
consider the transformed space in which (1 + δ)T is the unit sphere. Let R = E(MT ) be the
actual inertial ellipsoid of µT . Let w be an arbitrary unit vector and deﬁne
as before.
(1+δ)9Γ2 .
We have that R(wi)
−
Therefore (1 + δ)T is (1 + δ)9Γ2-outlier-free.

(1+δ)8Γ2 and we reason as above that R(w)

wi}
{
δ/3
1
(1+δ)8Γ2 ≥

≥

≥

1

1

w′i}
{

to ζ yields

wi}
{
wi}
{

that are within δ′√n of the

We now remove the assumption that S is full-dimensional. Suppose S is not full-dimensional,
but rather spans a subspace ζ. It suﬃces to consider w
ζ. For such a w, the projection of
the associated
(because they don’t
and w just
move further than the distance to w upon projection). We can compare the
δ/3)
as we did the
of the max along wi, and the max along w′i was lower bounded in lemma 8, the max along
wi is similarly lower bounded even though wi /
ζ (the change in the lower bound on the
∈
max is asymptotically negligible). Therefore we can apply lemma 8 part (i) to wi. Thus
2δ
3 . Thus (1 + δ)T contains S. This establishes part
T (wi)
(i).

and w previously. Because the max along w′i is within a factor (1

δ/3, and so T (w)

wi}
{

w′i}
{

−

≥

≥

−

−

∈

1

1

We can extend the proof of part (ii) to the case that T is not full-dimensional in an identical
manner. This concludes the proof of theorem 3.

Corollary 1 (Running Time) The algorithm runs in time ˜O( b2n5

ǫδ4 ).

Proof: We have from section 3 that β = γ2 is at most ˜O(bn/ǫ), and so we never need more
than m = ˜O( bn2
ǫδ2 ) samples. Plugging in this value for m to our bounds from section 5.1
yields that the algorithm runs in time ˜O( b2n5
ǫδ4 ), which is the bound we referred to in the
introduction. In this time we achieve a (1 + δ)O(1) = 1 + O(δ) approximation to the optimal
value of β.

We now pose a related problem: Suppose that we are not given the parameter γ2, but
rather only ǫ, and asked to ﬁnd the appropriate γ2. Lemma 9 will show that we can at
any point determine within a factor of (1 + δ) how much of the probability mass is within
a ﬁxed ellipsoid. Since γ2
) values
of γ2 to consider (with a loss of at most a factor of (1 + δ) in the value we ﬁnd for γ2).
Therefore we can simply try them all, estimating for each one whether this γ2 requires us
to throw away more than a (1 + δ)ǫ fraction of the distribution.

ǫ )], there are at most log(1+δ)

ǫ ) = O(

[1, ˜O( bn

log( bn
ǫ )

˜O( bn

∈

δ

Thus, if the parameters (γ2, ǫ) are achievable for the deterministic algorithm, and we are
only given ǫ, we can ﬁnd a subset of space T satisfying parameters ((1+O(δ))γ2, (1+O(δ))ǫ).
Our asymptotic running time is still ˜O( b2n5

ǫδ4 ).

Lemma 9 (Probability Mass Location) Let E be an ellipsoid. Let our number of sam-
ǫδ2 ). Then with constant probability, if we estimate a (1+δ)ǫ fraction
ples
of our samples to be outside of E, at most a (1 + δ)2ǫ fraction is outside of E, and at least
an ǫ fraction is outside of E.

i=1 be m = O( 1
m

xi}
{

25

Proof: Round E. Let Yi be a random variable, Yi = 1 iﬀ x2
Yi and
y = E[yi]. The event that we estimate a (1 + δ)ǫ fraction of the sample to be outside E
when less than an ǫ fraction truly lies outside E, is y < ǫ, ¯y
(1 + δ)ǫ. We can upper
bound the probability of this event using the Chernoﬀ bound

i > 1. Let ¯y = 1
m

P

≥

Pr[

Yi ≥

X

m(1 + ∆)E[Yi]]

e−

≤

∆2mE[Yi]/3

where ∆ = (1+δ)ǫ

1. Then

y −

∆2y = (

(1 + δ)ǫ

y

−

1)((1 + δ)ǫ

y)

−

(

≥

(1 + δ)ǫ

ǫ

−

1)((1 + δ)ǫ

ǫ) = δ2ǫ

−

and so the upper bound on the probability is constant for m = O( 1
case the alternate form of the Chernoﬀ bound is applicable, we ﬁnd ∆y
number of samples is still suﬃcient.

ǫδ2 ). If ∆
≥

1, in which
≥
δǫ, and so the

A similar calculation for the event that y > (1 + δ)2ǫ, ¯y

(1 + δ)ǫ using

Pr[

Yi ≤

m(1

∆)E[Yi]]

e−

−

≤

≤
∆2mE[Yi]/3

involves setting ∆ = 1

, which yields

X
(1+δ)ǫ
y

−

∆2y = (y

(1 + δ)ǫ)(1

−

(1 + δ)ǫ

−

y

δ2ǫ

)

≥

and similarly for the alternate form of the Chernoﬀ bound if ∆
1. Therefore the proba-
bility of signiﬁcantly underestimating the amount of probability mass outside E is at most
a constant for the same value of m.

≥

One consequence of the theorems in this section is that a sample of size ˜O( n2b
to estimate the inertial ellipsoid of any distribution on Zn
fraction) and thus bring it into nearly isotropic position.

ǫ ) is enough
b (after removing at most an ǫ

6 A Matching Lower Bound

We show that for any ǫ < 1/4 there exists a distribution µ with support
for any S satisfying µ(S)

ǫ, there exists w such that

1

⊂ Z

n
b such that,

≥

−

max

(wT x)2 : x
{

S

∈

} ≥

¯βE[(wT x)2 : x

S] Pr[x

S]

∈

≥

∈

E[(wT x)2 : x

S]

∈

¯β
2

where ¯β = Ω( n
on β in the case that we can’t throw out more than half the distribution

ǫ )). Based on the comparison between our upper and lower bounds

log 1

ǫ (b

−

O

(b + log

vs. Ω

n
ǫ

(cid:16)

n
ǫ

)

(cid:17)

n
ǫ

(cid:18)

(b

log

−

1
ǫ

)
(cid:19)

we describe theorem 1 as asymptotically optimal.

26

A:

B:

C:

20

21

1

2ǫ

−
1

22

2ǫ

. . .

2b

<−weight

1/√ǫ

<−position

D:

Figure 2: Lower Bound Constructions

We motivate the construction of the worst case distribution by constructing three simpler
distributions, each of which proves a weaker lower bound. The strong lower bound will
follow from examining a distribution that is a composite of the three distributions showing
the weaker lower bounds.

20, 21, ...2b
{

To prove the ﬁrst weak lower bound, let µ be the uniform distribution on the one-dimensional
. An illustration of this µ is given in ﬁgure 2, part A. We claim that
points
}
for any ǫ < 1
4 , the best achievable (i.e. smallest) β satisﬁes β = Ω(b). The proof is simple:
suppose the largest data point we keep is 2k. Then (ignoring the factor w since we are in
x2 : x
one dimension), max
b ). Since
{
β = max

∈
, we ﬁnd β = Ω(b).

= 22k, while E[x2 : x

ǫ) = O( 22k

20+...22k
(b+1)(1

S]

≤

∈

S

}

−

{·}E[
]

·

To prove the next weak lower bound, we construct a distribution as in ﬁgure 2, part B. Let µ
√ǫ ) = 2ǫ.
be the probability distribution on one-dimensional points given by µ(1) = 1
Then for ǫ < 1
ǫ , while
E[x2 : x
S] = 3

x2 : x
4 , neither point can be thrown away. Thus max
{

2ǫ, yielding β = Ω( 1

2ǫ, µ( 1
= 1
S

−

∈

}

ǫ ).

∈

−

For the third weak lower bound, we let µ be a distribution on n-dimensional space.
In
particular, let µ be the uniform distribution on n points, one on each coordinate axis, each
one at unit distance from the origin, as illustrated in ﬁgure 2, part C. For ǫ < 1
2 , we do
not throw away any points on at least n/2 of the axes. Then for w a unit vector along
(wT x)2 : x
one of the axes where the point is not thrown away, we have max
= 1,
{
E[(wT x)2 : x

4
n , and thus β = Ω(n).

S]

∈

S

}

∈

≤

The composite construction that we use to prove our strong lower bound in illustrated in
ﬁgure 2, part D. We obtain the composite distribution by taking the distribution of part A,
and making two copies that are weighted and translated as the two points are that compose
the distribution of part B. We then place a copy of this new one-dimensional distribution
along each axis, as in the distribution of part C. We now restate this construction formally
and proceed to analyze it.

27

Fix n, ǫ and b′ = b
Let there be 2b′ points at distances

4 log 1

2 −

1

ǫ . Let µ be a copy of the following distribution along each axis.

20, 21, . . . , 2b′

1,

−

2b′
√ǫ

,

2b′+1
√ǫ

, . . .

1

22b′
−
√ǫ

and consider the distribution that places a (1
2ǫ) fraction of the probability mass uniformly
on the ﬁrst b′ points and a 2ǫ fraction uniformly on the remaining b′ points. This distribution
satisifes that the maximum bit length along an axis is log 22b

−

′

√ǫ = b.

There are many ways of choosing a subset S of this distribution, but we can quickly restrict
the set of interesting choices to ones that treat each axis symmetrically. For the purpose of
establishing a contradiction, suppose that it helped to treat the diﬀerent axes diﬀerently.
We begin by noting that for a distribution concentrated on the axes and ﬁxed S, the vector
w that maximizes

max

(wT x)2 : x
{

S
S] Pr[x

∈

E[(wT x)2 : x

∈

}
∈

S]

always occurs on an axis — to see this, note that the rounding transformation need only
scale the axes, the maximizing w after rounding is in the direction of some point (i.e., along
an axis), and therefore the maximizing w before rounding is also along an axis. Let µ1 be
a distribution concentrated on the axes and symmetric on each axis on which it is possible
to throw out an ǫ fraction of the distribution and achieve parameter ¯β. Further suppose
that this ǫ is the minimum such that this ¯β is achievable, and the only S achieving ¯β is
asymmetric. Let axis i be an axis that a maximum outlier occurs on, and suppose that
along axis i we throw out an ǫi fraction of the total distribution. If

ǫ/n

ǫi ≤

ǫi > ǫ/n

then let S′ be the subset of µ1 where we throw out the same points along every axis that
ǫ, and yet S′ achieves ¯β along each
we threw out along axis i in S. Then we have ǫ′ = nǫi ≤
axis, contradicting the assumption that there was no symmetric subset we could throw out
achieving the same (ǫ, ¯β). If

then there is some other axis j such that along axis j we throw out an ǫj < ǫi fraction of
the probability distribution, but achieving ¯βj ≤
} ≤
¯βjE[x2
S]). Constructing S′′ by taking S and replacing our choice of points
to throw out along axis i with the points thrown out along axis j then yields a contradiction
because ǫ′′ < ǫ. Thus we can restrict our attention to S symmetric.

¯β along that axis (i.e. max
xj : x
{

S] Pr[x

j : x

∈

∈

∈

S

For any direction w along an axis, the projection onto w of any point on the other n
axes is 0, so we obtain

1

−

E[(wT x)2] =

E[x2, µ one-dimensional]

1
n

We ignore the factor of n for the rest of the proof and restrict our attention to a single
coordinate axis. Suppose the furthest point kept by S achieving parameters (ǫ, ¯β) is the
point with exponent k. By our choice of distribution, we cannot have thrown out more than
half the points with a 1
ǫ , k > b′. Calculating
the expectation

x2 : x
√ǫ factor, and so we have max
{

= 22k

∈

S

}

E[x2 : x

S] Pr[x

S]

∈

≤

∈

1

2ǫ

−
2b′

(20 + 22 + . . . + 22b′

2) +

−

(22b′

+ 22b′+2 + . . . + 22k)

2ǫ
2b′

1
ǫ

28

b′
yields that ¯β = max[
4ǫ (1
·
E[
]
·
our lower bound in the n-dimensional case is

]
= max[
·
] Pr[
·

E[

≥

]

]

·

ǫ)

−

≥

b′
8ǫ for the one-dimensional case. Thus

+

22k+1

22k+2

b′ ≤

b′

1

≤

22b′
−
2b′
Pr[
]
·

¯β

≥

n
16ǫ

(b

log

−

1
ǫ

)

7 An Approximation Algorithm

ǫ (b + log n

We showed earlier in the paper that for any distribution µ, and any ǫ we can achieve
β = O( n
ǫ )). A question that naturally arises is how well we can do on a particular
distribution compared to the best possible on that particular distribution. Formally, given
µ and ǫ, we seek S minimizing β subject to the constraints that

(i) µ(S)

1

ǫ

(ii)

w, max

∀

≥

−
(wT x)2 : x
{

S

∈

} ≤

βE[(wT x)2 : x

S]

∈

This is really a bicriteria approximation problem with parameters (β, ǫ). Note that in this
case, we are looking for the normalized probability distribution to be β-outlier free. We
show this problem to be NP-hard even for one-dimensional data by a reduction from the
subset-sum problem. We then exhibit a ( 1
ǫ , 1)-approximation algorithm for this task in
1
−
the case that we are given the distribution explicitly.
If we can only sample from the
distribution µ, our algorithm yields a ( 1
ǫ + δ, 1 + δ)-approximation for any constant δ > 0
1
−
with high probability.
The subset-sum problem is: given pi ∈
I pi ≤
to the constraint that
i
∈
i pi, ǫ = 1
removal problem, let P =
P

I pi subject
1. To form a corresponding instance (µ, ǫ) of the outlier

1, ...n
, ﬁnd I maximizing
}

2P , and let µ be given by

(0, 1), i

∈ {

P

i
∈

a point at 1 with probability mass 1
2

P

i, a point at 0 with probability mass p′i = pi

2P

•

• ∀

Let S be a possible solution to this instance of the outlier removal problem. Since P >
1 (otherwise the subset-sum problem is trivial), the point at 1 cannot be removed, and
S] =
hence maxx
∈
ǫ′)
(1) 1
2 −
2ǫ′, and minimizing this subject to ǫ′ ≤

S = 1. If we remove probability mass ǫ′ of the points at 0, E[x2 : x
= 1
−

2ǫ′ . Thus the ratio max[
E[

is exactly the problem of ﬁnding the optimal solution I to the subset-sum problem.

2 +(0)( 1
ǫ′

= 2

−

∈

−

·
]

ǫ

2

1

]

·

We now prove a lemma that enables the approximation result.

Lemma 10 (Preservation of Outliers) Let µ be a distribution. Any β-outlier for µ is
at least a β(1

ǫ)-outlier with respect to any subset S satisfying µ(S)

ǫ.

1

≥

−

Proof: Let x be a β-outlier in the original distribution. Then for some w, (wT x)2 >
βE[(wT x)2] For any S, we have E[(wT x)2 : x
E[(wT x)2] and so x satisﬁes
(wT x)2 > β(1

ǫ)E[(wT x)2 : x

S] Pr[x

S]

S]

≤

∈

∈

∈

−

−

29

The approximation algorithm is simply either algorithm described in section 5, with error
parameter δ in the case that we are sampling from µ. We could determine the optimal β for
a ﬁxed ǫ through a binary search. Suppose the value βOP T is achievable by the restriction of
µ to some S satisfying µ(S)
ǫ. Anytime our algorithm sees a point that is a β′-outlier
with respect to the unnormalized distribution, β′ > βOP T
ǫ , we know that this cannot be a
−
βOP T )-outlier under any restriction of µ by lemma 10. So this point will have to be
(
≤
thrown out by the optimal solution. Thus running our algorithm with β = βOP T
forces us
ǫ
−
to throw away no points that the optimal solution does not also throw away. This yields
that we achieve a 1
ǫ -approximation in the case of an explicitly provided distribution. As
1
−
before, the running time is O(m2n) for m > n.

−

≥

1

1

1

The outlier removal algorithm in fact ﬁnds an approximation to β for every ǫ in one pass.
The algorithms of section 2 can be used to deﬁne an outlier ordering of a point set, namely,
the ﬁrst point that is an outlier, the second point, etc. Now to approximate the best possible
β for a particular value of ǫ we simply remove the initial ǫ fraction of the points in the outlier
ordering one at a time, and then look back to see the lowest value of β achieved by any
ǫ′ < ǫ.

8 Standard Deviations from the Mean

We prove a variant of our theorem that shows we can ﬁnd a large subset of the original
probability distribution where no point is too many standard deviations away from the
mean.

Corollary 2 (Standard Deviations from the Mean) Let µ be a probability distribu-
n
b . Let S be a subset of space. Denote by µ(S) the probability that x chosen
tion on
according to µ is in S. Let ¯x = E[x : x
S]. Then for
every ǫ > 0, there exists S and

w = E[(wT (x

S] and σ2

¯x))2 : x

−

Z

∈

∈

β = O

(b + log

n
ǫ

(cid:16)

n
ǫ

)

(cid:17)

such that
(i) µ(S)
(ii) max

1

ǫ

≥
−
wT (x
{

−

¯x) : x

S

∈

} ≤

√βσw for all w

n
∈ R

Proof: The proof of the corollary is much like the proof of theorem 1. The appropriately
modiﬁed outlier removal algorithm for constructing S is simply to translate the data set so
that the origin coincides with the mean before each removal step. We can easily show that
translating µ so that the origin coincides with the mean never decreases the volume of the
dual ellipsoid of µ. We then explain how a variation on our potential function φ, and the
upper and lower bounds on φ, imply that the modiﬁed algorithm does not throw out more
than an ǫ fraction of the data set.
To analyze the volume of the dual ellipsoid, consider a ﬁxed direction w and let 1
r2 =
E[(wT x)2] (r is the length of the dual ellipsoid in this direction). If we translate the origin
to a value z along w, then have 1
z))2]. Single variable calculus shows that
the value maximizing r is z = E[wT x/
], which is just the mean. Thus translating our
w
|
|

r2 = E[(wT (x

−

30

origin to ¯x maximizes the length of the dual ellipsoid in every direction simultaneously.
Thus the tradeoﬀ between drop in probability mass and growth of the dual ellipsoid shown
in lemmas 1 and 3 also holds for the modiﬁed algorithm.

⊂

To describe our modiﬁed φ, we need to deﬁne the α-aﬃne-core of a distribution µS to be
S is chosen to be maximum subject to the requirement that the aﬃne hull of
µT where T
is not of lower dimension than the aﬃne hull of µT for any
µT minus an α fraction of µT }
{
choice of the α fraction. Under this deﬁnition, an appropriately modiﬁed version of lemma 4
is still true. Deﬁne φ′(µS) to be an appropriately modiﬁed φ, φ′(µS) = V ol(W (MT )) where
µT is the α-aﬃne-core of µS. We now explain how to derive upper and lower bounds on φ′
analogous to lemma 5 in the case that the α-aﬃne-core of µS is full-dimensional.

The lower bound is immediately implied by the argument above that translating the origin
to the mean does not decrease the dual volume. To derive the upper bound, consider a set
whose aﬃne hull is full-dimensional. In lemma 5, we argued that
A of n + 1 points
Det(AAT ) was a positive integer, not zero by choice of A, and thus Det(AAT )
1. Letting
¯a = 1
¯a)T ). Writing
n+1

ai, we must lower bound Det(

ai}
{

n+1
i

n+1
i

≥

(ai −

¯a)(ai −

P

n+1

(n + 1)2nDet(

Xi

(ai −

¯a)(ai −

¯a)T ) = Det(

(n + 1)¯a)((n + 1)ai −

(n + 1)¯a)T )

n+1

P
((n + 1)ai −

Xi

we have that the second term is the positive non-zero determinant of an integer matrix, and
hence the original determinant is at least
(n+1)2n . Because the origin corresponding to the
mean of a set of points maximizes the dual volume, this bound holds for all possibilities for
the origin. The upper bound on φ′ is then ( n

α )n(n + 1)2nf (n).

1

To prove a statement analogous to lemma 6 for the cumulative drop in φ′, we revisit the
construction of µ′T ′, µ′S′ from µR, µS, µT . Deﬁne these objects just as in the proof of lemma 6.
We have that Det(M ′S′)
φ′(µR) to at least the same extent. We now calculate
to the mean of µT , we have φ′(µT ) = f (i)/Det(MT ) where MT =
mean of µ′T ′ is given by ¯x = αx

≥
φ′(µ′
φ′(µT ) . Letting the origin correspond
T
T yyT µ(y). The

Det(MR) when the origin is the mean of µR, and so φ′(µ′S′)

α+µ(T ) . Then

≤

′ )

∈

y

P

M ′T ′ =

(y

¯x)(y

−

−

¯x)T µ(y) + α2(x

¯x)(x

¯x)T =

−

−

T
Xy
∈

−

T
Xy
∈

yyT µ(y)

¯xyT µ(y)

y¯xT µ(y) + ¯x¯xT µ(T )

+ α2(x

¯x)(x

¯x)T =





T
Xy
∈

T
Xy
∈

yyT µ(y) + ¯x¯xT µ(T ) + α2(x

¯x)(x

¯x)T =

yyT µ(y) +

T
Xy
∈

−

−

−

T
Xy
∈

−

−





µ(T )2α2 + µ(T )α2

(µ(T ) + α)2

xxT

Performing the same analysis using Gaussian elimination as we did previously and then
computing the ratio yields

φ′(µ′T ′)
φ′(µT )

=

f (i + 1)

(µ(T ) + α)2

f (i)

µ(T )α2(1 + µ(T ))x[i + 1]2

φ′(µ′T ′)
φ′(µT )

φ′(µS)
φ′(µ′S′)

=

⇒

(µ(T ) + α)2

µ(S)α2(1 + µ(S))

µ(T )α2(1 + µ(T ))

(µ(S) + α)2

31

We will now assume that we never remove more than an ǫ fraction of the probability mass.
This is not circular reasoning — just as in the proof of theorem 2 using algorithm 2, the
upper bound on φ′ under this assumption will imply that we never remove more than an
ǫ/2 fraction of the probability mass, and since we never remove more than an ǫ/2 fraction
on any one step, the assumption will always hold. Using this assumption, we calculate

(µ(T ) + α)2
(µ(S) + α)2 ≥

(1

ǫ)2

−
12

1
4

,

≥

µ(S)α2(1 + µ(S))
µ(T )α2(1 + µ(T )) ≥

1

Multiplying these factors together over the at most n steps in the iterative construction
yields an additional cumulative factor of at most 22n, which is negligible. Combining this
bit of additional slack with the new bound on φ′ in the full dimensional case and the
possibility that we only have an (α/n)-aﬃne-core (as at the end of the proof of lemma 6),
we ﬁnally arrive at a bound on the total cumulative drop in φ′ of

This immediately implies the claimed value for β in corollary 2.

2n(b+3 log n

α +3)

1

1
1

ǫ
3ǫ

We now show that the

ǫ -approximation algorithm of section 7 naturally extends to a
-approximation algorithm in the setting where we measure outlierness with respect
to the mean, rather than a ﬁxed origin. To establish this, it suﬃces to prove the following
(cid:16)
analogue of lemma 10.

−
−

(cid:17)

−

1

Lemma 11 (Outlier Preservation Variant) Let µ be a distribution. As in Corollary 2,
measure outlierness by squared distance from the mean rather than from a ﬁxed origin.
Suppose x0 is a β-outlier for µ, and no other point is a β′-outlier for β′ > β. Then x0 is
at least a β 1
−
1
−

3ǫ
ǫ -outlier with respect to any subset S satisfying µ(S)

≥

−

ǫ.

1

Proof: As in the proof of lemma 10, consider a unit vector w such that (wT x0)2 >
βE[(wT x)2], and let β = γ2. The diﬀerence between this bound and the bound of lemma 10
will result from the mean possibly moving closer to x0 after removing other points
xi}
.
{
Without loss of generality, let the mean of µ be the origin, and let E[(wT x)2] = 1.

Suppose that to reach S we remove points

of total probabilty mass ǫ′ ≤

ǫ. Then

E[(wT x)2 : x

S] Pr[x

(wT xi)2µ(xi)

∈

∈

E[(wT x)2 : x

S] = (1

(wT xi)2µ(xi))/(1

ǫ′)

−

⇒

We calculate the new mean as

E[(wT x) : x

S] Pr[x

S] = 0

(wT xi)µ(xi)

∈

∈

E[(wT x) : x

S] = (0

(wT xi)µ(xi))/(1

⇒

∈

ǫ′)

−

xi}
{
S] = 1

∈

−

Xi

−

Xi

−

Xi

−

Xi

32

Therefore the new distance of x0 to the mean is
calculate

γ

(0

−

−

i(wT xi)µ(xi))/(1

ǫ′)

. We

−

(cid:1)

P

2 =

γ′

distance2
variance

=

2

(cid:0)

=

γ + Pi(wT xi)µ(xi)
Pi(wT xi)2µ(xi)

ǫ′

−

1

1

(cid:16)

−

ǫ′

1

−

(cid:16)

(cid:17)

(cid:17)

((1
(1

ǫ′)γ +
ǫ′)(1

−

−
−

i(wT xi)µ(xi))2
i(wT xi)2µ(xi))

P
P

Let ¯x = P wT xiµ(xi)
numerator by the same amount, and ¯x2ǫ′ ≤
decrease. The derivation of ¯x2ǫ

ǫ′

, the average of the points. Then removing ¯x of weight ǫ′ changes the
(wT xi)2µ(xi), so the denominator cannot

x2
i µ(xi) follows from

P

which follows from

≤
λixi)2

P
≤

(

X

λix2
i ,

λi = 1,

λi ≥

0

X
1
2

a +

(

1
2

b)2

≤

X
1
a2 +
2

b2

1
2

along the same lines that fact 2 follows from fact 1 in section 10. Now we have shown we
2. We
may consider removing only a single point ¯x of weight ǫ′ in order to lower bound γ′
may view this as a constrained maximization problem over ¯x, with constraints
γ, and
¯x2ǫ′ ≤

1. The expression for f (¯x) = γ′

2 is

¯x
|

| ≤

2 =

γ′

((1
(1

−
−

ǫ′)γ + ǫ′ ¯x)2
ǫ′ ¯x2)
ǫ′)(1

−

If the constraint ¯x2ǫ′ ≤
would be 0, which would imply γ′

1 were tight, then the variance of the distribution after removing ¯x
γ were tight, we would have

2 = 1. If the constraint

2 =

γ′

((1
(1

−
−

ǫ′)γ
ǫ′)(1

−
−

γǫ′)2
γ2ǫ′)

= γ2 (1
−
1
−

2ǫ′)2
ǫ′

1

−

1
γ2ǫ′ ≥

γ2

2

2ǫ′
ǫ′ (cid:19)

γ2

≥

1
−
1
−

3ǫ′
ǫ′ (cid:19)

(cid:18)

If neither constraint is tight, we may solve the unconstrained optimization problem by
setting df (¯x)

d¯x = 0 to ﬁnd the local maximum, and then evaluating f (¯x) at this maximum.

¯x
|

| ≤
1
−
1
−

(cid:18)

f (¯x) = γ′

2 =

1

u(¯x)2
v(¯x)

ǫ

1

−
u(¯x)2v′(¯x)

df (¯x)

d¯x

=

1

2u(¯x)u′(¯x)

1

ǫ

(cid:18)

−
2v(¯x)u′(¯x)

v(¯x)

−

v(¯x)2

u(¯x)v′(¯x) = 0

= 0

⇒

(cid:19)

⇒

2(1

ǫ′ ¯x2)(ǫ′)

−

((1
−
ǫ′ ¯x2) + ((1

−

−
ǫ′γ ¯x = 0

−

(1

−

1 + γ ¯x

−

ǫ′)γ

−
ǫ′)(1

((1

(1

−

(1

1
ǫ′)γ ǫ′)2
−
1
ǫ′)2γ2 ǫ′)
ǫ′
γ2

(1
−
ǫ′)2

−

−
(1

−
(1

−
ǫ′)

−

γ2

f (¯x) =

ǫ′)γ + ǫ′ ¯x)(

2ǫ′ ¯x) = 0

−

⇒

ǫ′)γ + ǫ′ ¯x)¯x = 0

¯x =

−

(1

⇒

=

((1

ǫ′)2γ2

ǫ′)2

−

ǫ′)((1

−

ǫ′)2γ2

=

ǫ′)

−

(1

−

⇒

1
ǫ′)γ

−

−

γ2

≥

1
−
1
−

3ǫ′
ǫ′ (cid:19)

(cid:18)

which proves the lemma.

33

9 A Robust Statistic

In robust statistics, the choice of the median as the quintessential robust statistic is com-
monly motivated by describing it as a “robust version of the mean.” In particular, it is
noted that for any data set, the mean of the data set can be changed by an arbitrary
amount simply by moving one of the data points to inﬁnity. In contrast, the median does
not “go to absurdity,” as the literature commonly puts it, until at least half of the data has
been so changed by an adversary.

For a one-dimensional data set, deﬁne a δ-median to be a point such that at least a δ
fraction of the data lies to the left of the point and at least a δ fraction to the right. In
n-dimensions, call a point a δ-median if, for every direction w, it satisﬁes the deﬁnition of
the one-dimensional δ-median under projection to w.

1
Using Helly’s theorem, one can prove that
n+1 -medians exist for any n-dimensional data set
(or distribution), and this is best possible. Such a point is called a centerpoint. Centerpoints
were proposed by Donoho and Gasko[DG 92] as a robust estimator for high-dimensional
data. Donoho and Gasko showed centerpoints to have a high breakdown point, which is a
technical criterion of “robustness” that we shall not discuss further here.

Teng et al [CEMST 93] gave the ﬁrst polynomial time algorithm for computing an approx-
imate center point (polynomial in n). Their algorithm produces Ω( 1
n2 )-medians. We show
n
b , this
that the algorithm of section 8 produces
yields ˜Ω( 1

ǫ) -medians. For a distribution on

1
2γ2(1

Z

−

nb )-medians.

Theorem 4 Let µ be a distribution, let ¯x = E[x : x
(i) µ(S)
(ii) max
Then ¯x is a

ǫ
≥
−
(wT (x
{

¯x))2 : x
S
∈
ǫ) -median.

γ2E[(wT (x

−
1
2γ2(1

} ≤

−

1

∈

−

S], and suppose S satisﬁes

¯x))2 : x

S] for all w

∈

n
∈ R

Proof: Suppose initially that µ(S) = 1. Without loss of generality, consider a particular
direction given by the unit vector w, and assume that wT ¯x = 0 and E[(wT x)2] = 1. Since
we are restricting our attention to w for the rest of the proof, we may deﬁne yi = wT xi.
denote the distribution µ on S, and let I denote the index set. We partition I and
Let
deﬁne δ± via

yi}
{

I − =

i : xi < 0
}
{
µ(xi)
δ− =

I + =

δ+ =

i : xi ≥
{
µ(xi)

0
}

I −
Xi
∈

xiµ(xi) +

xiµ(xi) = 0

x2
i µ(xi) = 1

I +
Xi
∈

I
Xi
∈

Then we have

I −
Xi
∈
xi|
, we obtain
|

γ

I +
Xi
∈

Using that x2

i ≤
x2
i µ(xi)

1 =

I
Xi
∈

γ

xi|
|

≤

I
Xi
∈

From this we conlude that δ+
µ(S) = 1 turns our lower bound into

≥

µ(xi) = γ(

xiµ(xi)

xiµ(xi)) = γ(2

xiµ(xi))

I +
Xi
∈

−

I −
Xi
∈

I +
Xi
∈

2γ2δ+

≤

1
2γ2 , and similary for δ−. Dropping the assumption that

1
2γ2(1

ǫ) .

−

34

10 Some Properties of Matrices

The proof in section 4 relied on fact 2, which we speculate to be well-known. We present
the proof of this fact here since it uses techniques that are otherwise not necessary in the
rest of section 4.

Fact 1 For X, Y positive deﬁnite

Proof: This statement is equivalent to (clearing denominators and squaring twice)

Det((X + Y )/2)

Det(X)Det(Y )

≥

p

Det(XY )

Det2((X + Y )/2)

≤

which is equivalent to

Det2((X + Y )/2)

1

≤

= Det(

(X + Y ))Det(X −

1)Det(X + Y )Det(Y −

1)

= Det(

(X + Y )(X −

1)(X + Y )(Y −

1))

Det(XY )
1
4
1
4
1
4
1
4
1
4

(Y X −

= Det(

(I + Y X −

1)(XY −

1 + I))

= Det(

1 + 2I + XY −

1))

= Det(

(A + 2I + A−

1))

where we let A = Y X −
case of showing that Det(B)
B is at least 1. Consider an arbitrary (eigenvector, eigenvalue)-pair of A, (e, λ). Then

. We have reduced to the
1. We will show the stronger claim that every eigenvalue of

1 at the very end. Also let B = A+2I+A−1

≥

4

Be =

(λ + 2 +

)e

1
4

1
λ

4 (λ + 2 + 1
λ )

Since 1
1, we have that e is an eigenvector of eigenvalue at least 1 for B (this
used that λ
0, which is true since A is positive deﬁnite). Since the eigenvectors of A form
an orthonormal basis of the whole space, all of B’s eigenvectors are also eigenvectors of A.

≥

≥

Fact 2 For positive deﬁnite Xi and

0,

λ′i = 1, λ′i ≥
P
λ′iXi)

Det(Xi)λ′

i

Det(

Xi

≥

Yi

Proof: This is a straightforward generalization of fact 1.

35

Suppose ﬁrst that for each i, λ′i is exactly equal to pi/2k for some integer pi. In this case,
we may apply fact 1 iteratively to ﬁnd

Det(

X ′j)

≥

2k

Xi=1

2k

Yi=1

Det(X ′j)(1/2k)

Equating pi of the
, we
have that the theorem must hold for any k-bit binary approximation to the λ′i; fact 2 then
follows from standard continuity arguments.

to Xi for each i, we recover fact 2 exactly. For general

X ′j}
{

λ′i}
{

References

[BG 97]

[DG 93]

[BFKV 99]

[DV 01]

[LKS 95]

[LKS 97]

[MY]

[CEMST 93]

C. Becker and U. Gather, “The Maximum Asymptotic Bias of Outlier
Identiﬁers,” Technical Report TR1998/03, University of Dortmund.

P. Davies and U. Gather, “The identiﬁcation of multiple outliers,” In
Journal of the American Statistical Association, 88, 1993, p. 782-801.

A. Blum, A. Frieze, R. Kannan and S. Vempala, “A Polynomial-Time Al-
gorithm for Learning Noisy Linear Threshold Functions,” In Algorithmica,
22(1), 1999, pp35-52.

J. Dunagan and S. Vempala, “Optimal Outlier Removal
in High-
Dimensional Spaces,” In Proceedings of the 33rd ACM Symposium on the
Theory of Computing (STOC ’01), Crete, 2001, pp627-636.

L. Lov´asz, R. Kannan and M. Simonovits, “Isoperimetric problems for
convex bodies and a localization lemma,” In Discrete Computational Ge-
ometry 13, 1995, pp541-559.

L. Lov´asz, R. Kannan and M. Simonovits, “Random walks and an O∗(n5)
volume algorithm for convex bodies,” In Random Structures and Algo-
rithms 11(1), 1997, pp1-50.

R. Maronna and V. Yohai, “The behaviour of the Stahel-Donoho robust
multivariate estimator,” In Journal of the American Statistical Association
90(429), pp330-341, 1995.

K. L. Clarkson, D. Eppstein, G. L. Miller, C. Sturtivant, and S. Teng,
“Approximating center points with iterated radon points,” In Proceedings
of the 9th ACM Symposium on Computational Geometry (SOCG ’93), San
Diego, CA, 1993, pp91-98. To appear in International Journal of Compu-
tational Geometry & Applications.

[DG 92]

D. L. Donoho and M. Gasko, “Breakdown properties of location estimates
based on halfspace depth and projected outlyingness,” In The Annals of
Statistics, 20(4), 1992, pp1803-1827.

36

11 An Implementation

×

Let X be an m
n matrix whose rows are the points of our distribution. Let m, n, beta,
epsilon be the values for m, n, β, ǫ, and let the boolean variable done indicate whether we
are ﬁnished removing outliers. A complete implementation is given by the following matlab
code:

%% requires X,m,epsilon,beta
done = 0
while(˜done)

done = 1
M = cov(X) %% M is the covariance matrix of X
Y = X/cholinc(sparse(M),’inf’) %% Y is the isotropic version of X
for i = 1:m, %% remove current outliers

if Y(:,i)’*Y(:,i) > beta, X(:,i)=0, done = 0, end

end

end

As of the Spring of 2002, a java applet illustrating the outlier removal algorithm is available
at
http://theory.lcs.mit.edu/˜jdunagan/

37

