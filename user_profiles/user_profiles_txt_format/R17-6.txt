37th International Conference on Parallel Processing

Achieving Multi-level Parallelism in the
Filter-labeled Stream Programming Model

George Teodoro1 Daniel Fireman1 Dorgival Guedes1 Wagner Meira Jr.

Renato Ferreira1,2

1Department of Computer Science

Universidade Federal de Minas Gerais, Brazil

2Department of Biomedical Informatics

The Ohio State University, USA

{george, fireman, dorgival, meira, renato}@dcc.ufmg.br

Abstract—New architectural trends in chip design
resulted in machines with multiple processing units as
well as efﬁcient communication networks, leading to
the wide availability of systems that provide multiple
levels of parallelism, both inter- and intra-machine.
Developing applications that efﬁciently make use of
such systems is a challenge, specially for application-
domain programmers.

In this paper we present a new version of the
Anthill programming environment that efﬁciently ex-
ploits multi-level parallelism and experimental results
that demonstrate such efﬁciency. Anthill is based on
the ﬁlter-stream model; in this model, applications
are decomposed into a set of ﬁlters communicating
through streams, which has already been shown to be
efﬁcient for expressing inter-machine parallelism. We
replaced the ﬁlter run-time environment, originally
process-oriented, with an event-oriented version. This
new version allow programmers to efﬁciently express
opportunities for parallelism within each compute
node through a higher-level programming abstrac-
tion.

We evaluated our solution on dual- and quad-
core machines with two data mining applications:
Eclat and KNN. Both had drops in execution time
nearly proportional to the number of cores on a
single machine. When using a cluster of dual-core
machines, speed-ups were close to linear on the
number of available cores for both applications,
conﬁrming event-oriented Anthill performs well both
on the inter- and intra-machine parallelism levels.

I. INTRODUCTION

Many current computer applications involve ex-
pensive computations on very large data. At the
same time, demand for interactive response times
is very high, which creates a demand for compute
power way beyond that of a single computer.
Fueled by the continuous drop of cost per unit and

This work was partially supported by CNPq, CAPES, FINEP,

and FAPEMIG.

increase in the power of the shelf components, the
current trend in high performance computing is to
make use of very large sets of machines with fast
networks interconnects. On the other hand, the lat-
est developments in the micro-processor arena have
brought multiprocessing into all the newest CPU
chips. Those chips, when used in the machines of
large clusters and other computer aggregates, lead
to multiple levels of parallelism in the system. To
take full advantage of such systems, applications
must be built with parallelism in mind both at the
inter-machine level and at the single machine level.
Programming efﬁcient parallel systems, in par-
ticular for such heterogeneous environments, re-
mains a complex problem. One common environ-
ment used for that
is the combination of MPI
(for inter-machine parallelism) with POSIX threads
(for intra-machine/multi-core parallelism). While
a number of computer scientists demonstrate the
capacity of such environments to achieve high
computing rates for many applications, as com-
puters are becoming a widely used tool in many
ﬁelds, such tasks are more and more laid on the
shoulders of less experienced programmers. These
are the application domain specialists, the actual
originators of the demand for high-performance
computing. Bridging the programming gap that
arises has, therefore, become a central issue in
computer science.

Several programming frameworks and middle-
wares have been proposed aiming to ease the devel-
opment and efﬁcient execution of parallel applica-
tions. Several of them, however, assume dedicated
resources, ﬁxed resource allocation strategies or
a very simpliﬁed application semantics (embar-
rassingly parallel), failing to provide the desired
performance that applications expect from large

0190-3918/08 $25.00 © 2008 IEEE
DOI 10.1109/ICPP.2008.72

287

pools of resources.

Anthill [1] is a runtime framework which makes
it possible for a broad class of applications to
take advantage of parallelism in loosely coupled,
heterogeneous environments. It allows paralleliza-
tion of applications in several dimensions by par-
titioning the applications into a set of ﬁlters that
communicate using streams (Filter-Stream model).
These ﬁlters are then instantiated on the distributed
platform for execution, allowing multiple replicas
of each ﬁlter to be created transparently. Previous
results have shown that efﬁcient and scalable im-
plementations of applications in domains ranging
from complex parallel image processing [2], [3] to
parallel data mining [1], [4], [5], can be achieved
using Anthill [1].

Although conceptually the ﬁlter-stream model
is already suited for expressing multi-level paral-
lelism, the original implementation does a better
job at the inter-machine level. On a single machine,
the programming interface forces the programmer
to build each ﬁlter instance as a separate, sequential
process that goes through a loop reading new data,
processing it and forwarding results.

In this paper we present a novel version of the
Anthill’s ﬁlter programming interface that allows
the system to better utilize resources available
within one compute node. Our approach is to
change the original process/loop oriented run-time
system into a demand driven one, better suited
for the data ﬂow approach. Instead of creating a
process loop which gets called once and that must
then become responsible for the whole control ﬂow,
programmers are now responsible for writing just
the operations necessary to process a data unit
upon arrival. The run-time system uses an event-
oriented architecture to issue the processing code
as new data units become available and resources
are available. With that new interface, the ﬁlter
can spawn multiple execution threads transparently,
to take advantage of multiple cores that may be
available.

This new interface changes the programming
interface exposed by the runtime environment, pro-
viding a higher level abstraction that simpliﬁes the
programmer’s task. Several complicated issues can
be addressed in a more general way. For example,
previously it was hard for the programmer to
code a ﬁlter that should be able to receive data
from any one of multiple streams at any time,
requiring that all of them be multiplexed into one
stream and that the ﬁlter implement internally a
demultiplexing function to recover the notion of
independent streams. Now, programmers have just

to indicate what processing must take place when
a data unit arrives in any stream, and the run-
time systems does the rest. That avoid situations
where programmer generated code may contain
unexpected, unnecessary blocking points that might
lead to deadlocks or other forms of contention and,
therefore, to loss of efﬁciency. If data process-
ing may have in itself data dependencies between
different data units, programmers are still able to
express such dependencies. The run-time system
takes care of scheduling and synchronization when-
ever such dependencies are found.

To evaluate our solution we implemented two
data mining applications using the new event-
oriented interface: “Eclat” and “K Nearest Neigh-
bors”. The results show that the new system can
take full advantage of multiple cores available on
the processing nodes. Scalability experiments also
show that Anthill exposes enough parallelism in
order to make full utilization of large number of
multi-core nodes.

This paper is organized as follows: Section II
describes the ﬁlter-stream programming model and
the Anthill runtime framework; Section III presents
the event-driven Anthill developed in this work;
Section IV presents the experimental evaluation;
related works are presented in Section V; and,
ﬁnally, we discuss our results and some future work
in Section VI.

II. ANTHILL

Anthill is a runtime framework inspired on the
ﬁlter-stream programming model implemented in
Datacutter [6]. This model uses a dataﬂow ap-
proach to decompose the applications into a set
of processing units referred to as ﬁlters. Data is
transferred through the ﬁlters using communication
streams which allow ﬁxed-size untyped data buffers
to be transferred from one ﬁlter to the next.

The application decomposition process leads to
the possibility of task parallelism at runtime, as
the application becomes a set of ﬁlters connected
like a directed graph. At execution time, mul-
tiple (transparent) copies of each of the ﬁlters
that compose the application are instantiated on
several machines of the system and the streams
are connected from sources to destinations. The
transparent copy mechanism allows any vertex of
the application’s graph to be replicated over many
nodes of the execution platform and the data that
goes through each ﬁlter may be partitioned among
the copies creating data parallelism, as shown in
Figure 1.

288

For several applications, the natural decomposi-
tion is a cyclic graph, where the execution consists
of multiple iterations over the ﬁlters. An applica-
tion starts with data representing an initial set of
possible solutions and as these pass through the
ﬁlters, new candidate solutions are created. Those,
in turn, have to be passed through the network
so they can also be processed. In our experience
with developing applications in Anthill we noticed
that this behavior also leads to asynchronous execu-
tions, in the sense that several candidate solutions
(possibly from different iterations) are processed
simultaneously at run-time. This is similar to soft-
ware pipelining or the unrolling of a parallel loop.
Anthill, therefore, tries to exploit the maximum
parallelism in applications by using all three pos-
sibilities discussed above: task parallelism, data
parallelism and asynchrony. By dividing the com-
putation into multiple pipeline stages, each one
replicated multiple times, we can have a very ﬁne-
grained parallelism and, since all this is happening
asynchronously, the execution is mostly bottleneck
free.

Fig. 1. Anthill

With regard to the transparent copy mechanism,
care should be taken when the ﬁlter being repli-
cated has state. Ideally, that state should be parti-
tioned. For that to happen, the incoming messages
to any ﬁlter need to be sorted with respect to the
state variables they update and delivered to the
appropriate transparent copies. The mechanism that
guarantees it in Anthill is called Labeled Stream. A
label is associated to each message as they are sent
down the stream and the stream then checks that
label against a bijective (e.g., hash) function which
selects one copy to deliver the message to. This
guarantees that messages associated with the same
label, and consequently the same state variables,

289

are delivered always to the same transparent copy.
The current programming interface exposed to
the ﬁlters is a set of three functions that need
to be implemented on each: Initialize, Process,
and Finalize. The runtime system invokes each of
these three methods once, appropriately. This has
been the same interface exposed by Datacutter. One
problem with that interface is that it forces the
ﬁlter programmer to write a loop within the method
Process that will pretty much iterate while there
is work to be done. Having such a loop creates
complications to the application programmers who
now should be able to deal with data dependencies
and synchronization problems that may arise from
the interactions among the several ﬁlters compris-
ing the application. Moreover, it turns the ﬁlter
into a big loop, that cannot easily beneﬁt from
the available hardware, unless it is written by an
experienced programmer.

III. EVENT-ORIENTED ANTHILL

In this Section we describe the novel version
of Anthill that provides the new ﬁlter interface
discussed previously, which alleviates some of the
problems mentioned and brings Anthill closer to
the data ﬂow model. The idea is to create a
demand driven version of Anthill in which the
control loop moves from the application code (the
ﬁlter code) to the run-time-system code (within
Anthill). The programmer provides a function to be
invoked whenever data is available for processing.
Scheduling and dependencies are handled within
the run-time system. Since processing is event-
oriented, if resources are available, multiple events
may be triggered in parallel effectively unrolling
the original loop into the multiple processing units
available for computation. This feature is very
useful for exploiting the full capability of the
current multicore architectures without forcing the
application programmers to keep all parallelism
levels in mind explicitly.

A. The beneﬁts of events

In the process-oriented version of Anthill, orig-
inally, there is no notion of intra-machine paral-
lelism. The ﬁlter Process function is declared as
a sequential loop and as such it is executed. If
desired, the programmer may use another abstrac-
tion for exploiting multiple execution units, if they
are available. That is what programmers do when
they rely on MPI for inter-machine communication
and on POSIX threads for intra-machine paral-
lelism, for example. That, however, has some major

drawbacks, like the possible incompatibility of the
primitives used by the different frameworks or of
the abstractions each of them provide/require.

On making Anthill event-oriented, we are not
adding a different, conﬂicting abstraction, but ac-
tually closing an implementation gap between the
model and the programming interface. A ﬁlter pro-
cesses data as it ﬂows through, possibly changing
the nature of the data that is passed to the next ﬁlter.
On those pretenses, a programmer, when deﬁning
a ﬁlter, should have to worry only with the trans-
formation function process(newData), ran for
each data unit in turn, and that may internally
cause some data to be forwarded to the next ﬁlter.
Deciding when such processing takes place, except
for explicitly indicated data dependencies,
is a
task of the ﬁlter framework, not a concern of
the programmer. That was not possible with the
process-oriented implementation, but is true when
we use events.

When the run-time system controls events, it
can easily decide, once an event is raised, when
it may be processed, given the hardware resources
available. The programmer is relieved from the
task of determining the level of parallelism to be
employed: as long as there are resources available
and events pending, execution may continue.

Event-orientation also removes another limi-
tation of
the original Anthill: data multiplex-
ing/demultiplexing. A ﬁlter may be built so that it
receives input from multiple, independent, streams.
In such case, if data must be processed as soon
as received, independent of its source, the pro-
grammer must resort to asynchronous communi-
cation primitives (non-blocking I/O) to be able
to probe continuously different sources in search
for data. That is, clearly, an undesirable burden
on the programmer. Another possibility is to have
the system multiplex data from all streams in one
communication entity, requiring the programmer to
implement a demultiplexing function to determine
which stream originated the data, if that informa-
tion is needed. That is also not desirable, since it
adds an (otherwise unnecessary) overhead.

With events, the programmer may indicate what
should be done upon arrival of data on any stream,
even with different actions for each source. The
run-time system is in better condition to control
non-blocking I/O, if necessary, and choose the right
processing function for each event based on the
programmer’s instructions. Such approach derives
heavily from the message-oriented programming
model pioneered by the x-kernel [7] and later
extended to include explicit, user-deﬁned events in

Coyote [8].

We should emphasize the expressive power of
this parallelization strategy. The developer, by us-
ing a single abstraction (the ﬁlter-stream model),
with a simple programming interface (Anthill
events), may create applications that exploit two-
level parallelism (inter- and intra-machine, or clus-
ter and processor), by just focusing on how ﬁlters
communicate and what ﬁlters do upon the arrival
of data.

B. Implementation

[1],

In order to support event-oriented programming,
we extended Anthill
incorporating several
modules. As depicted in Figure 2, we created an
event layer that is divided into three components:
Communication module, Event queue controller,
and Event handler. We discuss each of them next.

Fig. 2. Architecture

• Communication module:

responsible for
monitoring all input ﬂows and sending mes-
sages for the target ﬁlter instances. It is im-
plemented by a separate thread that deals with
the multiple communication end-points. Its
operation essentially consists of creating an
event in the respective event queue, whenever
a message associated with a stream arrives in
the ﬁlter instance.

• Event queue controller: responsible for con-
trolling the event queues associated with each
ﬁlter instance. This component determines in
which order and when the events in each
queue should be processed and calls the ap-
propriate ﬁlter processing function.

• Event handler: it contains the application-
speciﬁc code. For each input stream, the de-
veloper creates a function that processes the

290

data received through it and registers it in the
system. Whenever the event associated with a
given stream is triggered, it is that registered
function which gets executed, receiving as
input the event to be handled (i.e., the data
to be processed).

By keeping those elements apart, the application
programmer is isolated from low-level abstractions
and control is assigned to the run-time system. That
makes it easier for scheduling and load balancing
to be implemented without affecting the program-
mer’s code. Special scheduling can be created by
monitoring the communications module and the
event queue controller, and by actuating back on
those modules, without requiring changes to the
application code. That makes the event-oriented
Anthill a good environment for experiments on
dynamic scheduling and new load balancing so-
lutions.

IV. EXPERIMENTAL EVALUATION

We used two different data mining applications
to evaluate the performance and impact of the
ideas proposed in this paper: Eclat and K Nearest
Neighbors (KNN). Before we describe our experi-
mental setup and present the results we will brieﬂy
describe each of those applications.

A. Association analysis

Association analysis consists

in identifying
causal relationships between frequent co-occurring
items in a database. Such relations are named asso-
ciation rules, and the ﬁrst step to compute them is
to determine all co-occurring sets that appear in the
database more frequently than a given threshold. A
traditional algorithm to solve this problem is named
Eclat [9], which has been previously parallelized
using Anthill [4].

The Anthill implementation of Eclat, which is
depicted in Figure 3, is based on the principle
that, in order for an itemset (set of co-occurring
items) with k items to be frequent, all of its subsets
with k − 1 items also need to be frequent. The
algorithm then starts with an initial set of itemset
candidates, which includes all itemsets of size 1.
It
then computes the actual frequency of each
of those candidates in the database. The itemset
candidates which are found to be frequent, are
then combined to produce new candidates of size
2 which are tested in further interactions. The
algorithm continues creating new candidates in that
fashion until there are no more new candidates to
be checked.

Itemsets partial occurrence

C o u n t e r

V e r i f i e r

Candidate itemsets

 Frequent itemsets

C a n d i d a t e
g e n e r a t o r

Fig. 3. Eclat Filters

In Figure 3 we show the 3 ﬁlters that we
developed to parallelize the Eclat algorithm. The
Counter ﬁlter is responsible for counting the fre-
quency of each candidate. The Veriﬁer ﬁlter checks
whether or not the overall frequency of a particular
itemset is above the given threshold and the Candi-
date Generator ﬁlter creates new candidates based
on previous frequent itemsets. The Eclat algorithm
implemented in Anthill exploits asynchrony in such
a way that each Counter ﬁlter can process different
itemsets, typically several of them simultaneously,
using different transparent copies of the Counter
ﬁlter.

The MPI

implementation ﬁrst partitions the
transaction database over the available processes.
During candidate itemset frequency checking, each
process generates a local count and exchanges this
information with the other processes to generate
a global frequency count for that candidate. After
completing the candidate frequency checking each
process locally generates the itemset candidates
of K + 1 size. The application has found all of
the frequent itemsets when none of the candidate
itemsets were found to be frequent, or when there
is only one candidate itemset in the last iteration,
frequent or not.

B. Classiﬁcation

Classiﬁcation is a data mining predictive task
where we want to assign classes (or categories)
to unknown objects (which are usually called the
test set) using a model that was based on pre-
classiﬁed objects (the training set). A classic al-
gorithm for object classiﬁcation is the k-Nearest
Neighbor (KNN), ﬁrst introduced by E. Fix and
J. Hodges [10], which is based on the intuition
that the neighborhood of an object contains similar
objects. Thus, by analyzing its neighborhood, we
should be able to determine its class. In practice,
since deﬁning neighborhood may be hard, we use
the k closest objects in terms of their attributes,
which explains the algorithm’s denomination.

The Anthill implementation of KNN uses three
ﬁlters: Reader, Classiﬁer and Merger, shown in ﬁg-

291

ure 4. The Reader ﬁlter is responsible for sending
the training dataset and the objects to be classi-
ﬁed to the Classiﬁer ﬁlter. The training dataset
is divided among the Classiﬁer instances while
the test dataset
is broadcasted to all of them.
This strategy does not affect the performance of
the algorithm, since the training dataset is usually
much larger than the test dataset. Each instance of
the Classiﬁer ﬁlter then computes, for each object
of the test base, the distance to the objects in the
training dataset that are assigned to that instance,
and selects the k-closest objects, which are sent to
the merger ﬁlter. The Merger ﬁlter receives the k-
closest objects from each Classiﬁer instance, sort
them and selects the global k-closest objects to
assign the object to be classiﬁed to one class.

Fig. 4. KNN Filters

We have also implemented the KNN using MPI
to compare with the Anthill version. The MPI
version of KNN reads the training set from one
node and replicates it to the other ranks, and then
performs the same test set partitioning, and send
the ﬁnal classiﬁcation to one process.

C. Experimental setup

For KNN we used synthetic datasets: the training
dataset contains 1,500,000 objects with 50 dimen-
sions and the test dataset contains 3000 objects
to be classiﬁed into 10 classes. We also used a
synthetic dataset for Eclat, consisting of 1,000,000
transactions each one with 200 attributes, generated
using the procedure described in [4]. This dataset
mimics the transactions in a retailing environment.
The speedup experiments were run on a cluster
of 13 PCs, which are connected using a Fast Ether-
net Switch. Each node has an Intel(R) Core(TM)2
CPU 2.13GHz, 2 GB of main memory and runs
the Linux 2.6 operating system. We have also used
one Intel(R) Xeon(R) CPU X3220 2.40GHz with
4GB of main memory and Linux 2.6 to evaluate
our solution in a quad core machine.

In our experiments, we varied just the number
of instances of the Counter (Eclat) and Classiﬁer
(KNN) ﬁlters, which are the CPU intensive ﬁlters.
In both cases, the remaining ﬁlters run on a single
machine, which never saturated during our experi-
ments.

D. Results

In this section we ﬁrst analyze the application
execution times on a single machine using a dual-
and a quad-core and after that we show the speedup
using a cluster of dual-core machines.

1) Single Machine Analysis: Figure 5 shows the
execution times for Eclat and KNN using dual and
quad core machines. The Figures 5(a) and 5(b)
have the execution of the two algorithms, varying
the number of threads, for a dual core machine.
Both applications reduce their execution time when
we added a second thread and there are no gains
with a third thread, since there are only two cores
available. In the two-thread conﬁguration, the sin-
gle threaded execution times for Eclat and KNN
are reduced by 48% and 50%, respectively.

The quad-core execution times are shown in
Figures 5(c) and 5(d). The execution times reduce
as we increase the number of threads, until the
latter reaches the number of cores. After that,
there are no signiﬁcant gains, and, in some cases,
the scheduling overhead actually leads to longer
execution times. With four threads, execution times
are reduced to 30% and 25% of the single threaded
version for Eclat and KNN, respectively.

The difference w.r.t. performance gains in KNN
and Eclat can be explained by the relation between
their data access patterns and the design of the quad
core processor. In an architecture like the Xeon
processor, which has two sets of two cores, each
set sharing an L2 cache, if two threads running
in different sets access the same datum, it may be
duplicated in both caches. In that case, applications
that present a high degree of reference locality
may not scale linearly with the number of cores.
It occurs because the available cache space is not
optimized due to cache entry replications.

2) Application Speedup: Figure 6 shows the
speedup of the two applications, implemented us-
ing Anthill and MPI. The KNN results are nearly
linear, as expected for an algorithm with such a
regular communication and computation pattern,
and it still scales well up to 30 cores. The Eclat
results show Anthill outperforming the MPI im-
plementation. The two implementations are very
similar in theory, although the Anthill implemen-
tation leverages the time parallelism inherent in
the application exposed by the use of the ﬁlter-
stream programming model. As such, the Anthill
implementation scales extremely well, while the
MPI implementation is plagued by the irregular,
data-dependent nature of the computation.

These results show that our event-oriented im-
plementation of Anthill is very successful in pro-

292

)
s
d
n
o
c
e
s
(
 

e
m
T

i

)
s
d
n
o
c
e
s
(
 

e
m
T

i

 14000

 12000

 10000

 8000

 6000

 4000

 2000

 0

 0

 3000

 2500

 2000

 1500

 1000

 500

 0

 0

)
s
d
n
o
c
e
s
(
 

e
m
T

i

)
s
d
n
o
c
e
s
(
 

e
m
T

i

 0

 0

 4000

 3500

 3000

 2500

 2000

 1500

 1000

 500

 3500

 3000

 2500

 2000

 1500

 1000

 500

 0

 0

p
u
d
e
e
p
S

l

e
u
a
V

 30

 25

 20

 15

 10

 5

 0

 30

 25

 20

 15

 10

 5

 0

 1

 2

 3

 4

 1

 2

 3

 4

Number of threads

Number of threads

(a) Eclat - Dual core execution time

(b) KNN - Dual core execution time

 1

 2

 3

 4

 5

 6

 1

 2

 3

 4

 5

 6

Number of threads

Number of threads

(c) Eclat - Quad core execution time

(d) KNN - Quad core execution time

Fig. 5. Different architectures evaluation

viding a transparent, ﬂexible, and scalable strat-
egy for exploiting multi-level parallelism, even for
irregular applications such as Eclat. On the other
hand, we found that the development of locality-
conscious techniques may improve the applications
scalability even more and, again, Anthill seems
to be a good environment for implementing such
techniques, given the level of abstraction in which
applications are implemented.

V. RELATED WORK

There has been several efforts on exploiting
multi-level parallelism, although many of them
focus on speciﬁc applications and not at the level
of programming environments. In this section we
discuss some of these efforts.

Ruoming Jin et al. have created a middle-
ware to develop data-mining applications, called
FREERIDE, which exploits environments with dis-
tributed and shared memories [11]. That frame-
work can be used by a number of data mining
algorithms because it computes generalized dis-
tributed reductions, a common operation for this
class of algorithms. The authors also evaluate dif-
ferent strategies to synchronize data access and
avoid errors in the reduction operation. Although
the framework achieves good performance for the
shown applications its programming model is not

Linear Speedup #cores
Anthill Speedup
MPI Speedup

 0

 5

 10

 15

 20

 25

 30

Number of cores

(a) Eclat speedup

Linear Speedup #cores
Anthill Speedup
MPI Speedup

 0

 5

 10

 15

 20

 25

 30

Number of cores

(b) KNN speedup

Fig. 6. Applications Speedup with the number of cores

as generic as the one presented in this paper,
since it can exploit shared and distributed memory
systems for all applications that can be mapped

293

in the ﬁlter-stream model, not only those based
on reductions. For the applications considered, the
speedups achieved with Anthill are linear on the
number of available cores, which is not the case
with FREERIDE.

Blagojevic et al. [12] present an event-driven
multi-threading scheduler for task-level parallelism
on the Cell processor. They propose different
scheduling policies to map and execute tasks from
MPI process on the Cell exploring multilevel par-
allelism. Although interesting, the scalability of the
scheduler is bounded and the solution can only
be used for embarrassingly parallel applications,
while the ﬁlter-stream programming model is not
restricted in that sense.

Chai, Gao and Panda [13] present a detailed
evaluation of multi-core architectures, measuring
the different communication costs between cores in
a cluster of multi-core machines and evaluating its
overall performance. Although it does not present
a solution for the problem of efﬁciently exploring
multi-level parallelism, the study presented in that
paper is very usefull to help understand and design
multicore-aware programming frameworks.

VI. CONCLUSIONS AND FUTURE WORK

In this paper, we presented an event driven
version of Anthill that allows the system to better
utilize resources available within a compute node,
which is particularly important in the scenario of
multi-core processors. The results show that we
have achieved good performance for two classic
data mining applications, Eclat and KNN. Our
event-oriented strategy to exploit multi-core pro-
cessing have succeed for both applications in dual
and quad core machines. The Eclat execution time
in a single machine has been reduced to 52%
(dual core) and 30% (quad core) of the execution
time using one thread. For the KNN, the execution
time is 50% (dual core) and 25% (quad core)
of the single threaded execution. We also have
achieved almost linear speedups, w.r.t. the number
of available cores, for both applications.

For future work, we are currently investigating a
mechanism to schedule events and threads. Using
this mechanism we intend to better exploit the
reference locality through out-of-order execution of
the events and also avoid undesirable migrations of
the threads between cores. Other future work refers
to the exploration of heterogeneous environment
using ﬁlters that can automatically adapt the num-
ber of processing threads according to the power
of the available machines.

REFERENCES

[1] R. Ferreira, W. M. Jr., D. Guedes, L. Drummond,
B. Coutinho, G. Teodoro, T. Tavares, R. Araujo, and
G. Ferreira, “Anthill:a scalable run-time environment for
data mining applications,” in Symposium on Computer
Architecture and High-Performance Computing (SBAC-
PAD), 2005.

[2] G. Teodoro, T. Tavares, R. Ferreira, T. Kurc, W. Meira,
D. Guedes, T. Pan, and J. Saltz, “Run-time support for
efﬁcient execution of scientiﬁc workﬂows on distributed
environmments,” in International Symposium on Com-
puter Architecture and High Performance Computing,
Ouro Preto, Brazil, October 2006.

[3] S. Hastings, M. Ribeiro, S. Langella, S. Oster,
U. Catalyurek, T. Pan, K. Huang, R. Ferreira, J. Saltz, and
T. Kurc, “Xml database support for distributed execution
of data-intensive scientiﬁc workﬂows,” SIGMOD Rec.,
vol. 34, no. 3, pp. 50–55, 2005.

[4] A. Veloso, J. Wagner Meira, R. Ferreira, D. G. Neto, and
S. Parthasarathy, “Asynchronous and anticipatory ﬁlter-
stream based parallel algorithm for frequent itemset min-
ing,” in PKDD ’04: Proceedings of the 8th European
Conference on Principles and Practice of Knowledge
Discovery in Databases. New York, NY, USA: Springer-
Verlag New York, Inc., 2004, pp. 422–433.

[5] R. Araujo, G. Ferreira, G. Orair, W. Meira Jr., R. Ferreira,
and D. Guedes, “Partricluster: A scalable parallel algo-
rithm for gene expression analysis,” International Journal
of Parallel Programming, 2008.

“Datacutter: Middleware

[6] M. Beynon, R. Ferreira, T. M. Kurc, A. Sussman, and
for ﬁltering
storage
Storage
[Online]. Available:

J. H. Saltz,
very
systems,”
Systems,
citeseer.ist.psu.edu/beynon00datacutter.html

on archival
IEEE Symposium on Mass

in
2000,

119–134.

scientiﬁc

datasets

large

pp.

[7] S. W. O’Malley and L. L. Peterson, “A dynamic network
architecture,” ACM Trans. Comput. Syst., vol. 10, no. 2,
pp. 110–143, 1992.

[8] N. T. Bhatti, M. A. Hiltunen, R. D. Schlichting, and
W. Chiu, “Coyote: a system for constructing ﬁne-grain
conﬁgurable communication services,” ACM Trans. Com-
put. Syst., vol. 16, no. 4, pp. 321–366, 1998.

[9] R. Agrawal and R. Srikant, “Fast algorithms for mining
association rules,” in Proc. of the Int. Conf. on Very Large
Databases, VLDB. SanTiago, Chile: VLDB, June 1994,
pp. 487–499.

[10] E. Fix and J. Hodges, “Discriminatory analysis, non-
parametric discrimination, consistency properties,” School
of Aviation Medicine, Randolph Field, Texas, Computer
Science Technical Report, 1951.

[11] R. Jin and G. Agrawal, “Combining distributed memory
and shared memory parallelization for data mining algo-
rithms,” in 6th International Workshop on High Perfor-
mance Data Mining: Pervasive and Data Stream Mining
(HPDM: PDS03), 2003.

[12] F. Blagojevic, D. S. Nikolopoulos, A. Stamatakis, and
C. D. Antonopoulos, “Dynamic multigrain parallelization
on the cell broadband engine,” in PPoPP ’07: Proceedings
of the 12th ACM SIGPLAN symposium on Principles and
practice of parallel programming. New York, NY, USA:
ACM, 2007, pp. 90–100.

impact

of multi-core

[13] L. Chai, Q. Gao, and D. K. Panda, “Understanding
cluster
the
dual-core
computing: A case
the Grid,
system,”
2007. CCGRID 2007. Seventh IEEE International
Symposium on, 2007, pp. 471–478. [Online]. Available:
http://portal.acm.org/citation.cfm?id=1251971.1252256

in Cluster Computing

study with

intel
and

architecture

in

294

