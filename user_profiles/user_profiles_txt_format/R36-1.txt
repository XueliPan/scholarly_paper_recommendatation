TalkMiner: A Search Engine for Online Lecture Video

John Adcock1 Matthew Cooper1 Laurent Denoue1 Hamed Pirsiavash2

Lawrence A. Rowe1

1FX Palo Alto Laboratory

3400 Hillview Ave.

Palo Alto, CA 94304 USA
{last name}@fxpal.com

2Dept. of Computer Science
University of California, Irvine

Irvine, CA 92697 USA
hpirsiav@ics.uci.edu

ABSTRACT
TalkMiner is a search engine for lecture webcasts. Lecture
videos are processed to recover a set of distinct slide images
and OCR is used to generate a list of indexable terms from
the slides. On our prototype system, users can search and
browse lists of lectures, slides in a speciﬁc lecture, and play
the lecture video. Over 10,000 lecture videos have been in-
dexed from a variety of sources. A public website now allows
users to experiment with the search engine.

Categories and Subject Descriptors

H.5.1 [Information Interfaces and Presentation]: Mul-
timedia Information Systems

General Terms
Algorithms, Experimentation

1 Overview
Lecture webcasts are readily available on the Internet. These
webcasts include class lectures (e.g., Berkeley Webcast, MIT
Open Courseware, etc.), research seminars (e.g., Google Tech
Talks, PARC Forums, etc.), product demonstrations, or train-
ing materials.

Conventional web search engines can be used to locate
these lecture videos, but only when supporting text appears
on the hosting web page, or the media has been tagged or
otherwise authored within a purposed hosting system. But
users, especially students, need to ﬁnd the locations within
a video when an instructor discusses a speciﬁc topic. Ad-
dressing this need requires a search engine that can identify
relevant material with the content of the webcast.

TalkMiner provides an enhanced search and browsing sys-
tem designed to improve the usefulness of lecture webcasts
by enabling keyword search of slides appearing within a pre-
sentation video, and video playback directly from the time a
chosen slide appears (see Figure 1). The system analyzes the
videos to identify and extract unique slide images along with
their time codes. This allows seeking the embedded video
to the appearance of each detected slide. OCR is applied to
detected slide images and a text search index is built from
the recovered words.

TalkMiner builds its search index and interface from com-
monly recorded video. It requires neither dedicated lecture-
capture systems, nor careful post-capture authoring, nor

Copyright is held by the author/owner(s).
MM’10, October 25–29, 2010, Firenze, Italy.
ACM 978-1-60558-933-6/10/10.

Figure 1: Viewing slides and using them to seek the
embedded player. Slides matching the user-speciﬁed
search terms are highlighted.

even constraints on the style of the video capture. Thus,
the system can scale to include a greater volume and variety
of existing and newly created content at a much lower cost
than would otherwise be possible. Also, by leveraging ex-
isting online video distribution infrastructure to embed we-
bcasts within an enhanced interface the system minimizes
storage and bandwidth requirements, further aiding scala-
bility and portability.

The system currently indexes lecture videos from three
sites: YouTube [7], U.C. Berkeley [1], and blip.tv [2]. The
current number of talks indexed is over 11,000.

2 System Architecture
An overview of the architecture appears in Figure 2. The
system is composed of two main components:
the back-
end video indexer and the front-end web server. The video
indexer searches the web for lecture webcasts and indexes
them. It automatically identiﬁes slide images and processes
them to create the text search index.

The method for identifying and accessing the presenta-
tion media varies slightly for each source site, but generally
videos are identiﬁed by parsing RSS feeds to which the sys-
tem is subscribed. Once downloaded, it takes roughly 6
minutes to process a 60 minute talk (i.e., 10 percent of the
real time talk duration), so the system is generally limited
by download speed rather than processing speed.

The TalkMiner web-based front end is implemented in
Java server pages (JSP) running on an industry standard

1507Figure 2: System architecture showing separate
back-end, front-end, and content hosting compo-
nents.

Apache/Tomcat combination. The indexing and search fra-
mework, implemented with DBSight [4], runs in the same
Tomcat instance. At runtime, searches are performed thr-
ough the DBSight web application from previously comput-
ed Lucene indexes of the MySQL talk database to render
the search results lists. The detailed talk page reads the
detailed slide text and timing information directly from the
MySQL database. Embedded player control and within-talk
text searches are performed in javascript, freeing the system
from futher web or database access until the next search.

3 User Interaction
The TalkMiner search interface resembles typical web search
interfaces. The user enters one or more search terms and a
list of talks that include those terms in the title, abstract or
the presentation slides are listed as shown in Figure 3.

The information displayed for each talk on the search re-
sults pages includes a representative key frame, the title of
the lecture, and the channel or source of the talk. Other
metadata displayed includes the duration of the talk, the
number of slides, the publication date, and the date it was
indexed by TalkMiner. An attribution and link to the orig-
inal video source is also provided.

The user can browse a list of talks and alter the sorting
and ﬁltering criteria for the listing. By default, talks are
sorted by relevance to the query terms as computed by the
Lucene text retrieval software [6]. Other available sort at-
tributes include publication date, number of slides, channel,
and rating. The ﬁrst column on the left side of the results
page includes interface controls to ﬁlter results according to
speciﬁc criteria (e.g., the year of publication, the channel,
etc.). It also includes a list of recent search queries to allow
users to re-execute a recent query.

Search results link to the detailed talk view as depicted in
Figure 1. Slides matching the query are highlighted, and the
user can control the playback position of the embedded video
player by selecting the slide thumbnail with the content of
interest.

4 Slide Detection
We adapted a straightforward frame-diﬀerence based anal-
ysis from ProjectorBox [5] to identify keyframes for Talk-
Miner. We extract a keyframe for nearly stationary seg-

Figure 3: Search results. Each lecture shows a
representative keyframe, attribution, title, and de-
scription when available. Results can be ﬁltered or
sorted based on metadata including the date, dura-
tion, number of slides, and source of the webcast.

ments of a minimum length; these generally correspond to
slides. We have extended this baseline approach to ad-
dress several frequently occurring cases in which it often
selects spurious keyframes, or misses slide appearances alto-
gether.

• full-frame shots of the speaker with neither navigational

or informational value.

• shots of slides that contain “picture-in-picture” streams.

• shots from the back of the room that include the audience

in the foreground and/or the speaker.

To improve our slide detection we have incorporated spatial
ﬁltering to ameliorate the eﬀects of insigniﬁcant motion, and
self-bootstrapping visual models to ﬁlter out shots of the
speaker.

5 Copyright Considerations
Video on the web exists under a wide variety of copyrights
and terms of use and the implementation of TalkMiner has
taken this into account. University lecture material usu-
ally has an explicit Creative Commons [3] license, but even
these vary in their scope, in particular not always allowing
for the creation of derivative works which puts various de-
sirable modiﬁcations on shaky legal ground. The potential
value of TalkMiner is much higher when using content with-
out copyright restrictions, such as would be the case if the
system were deployed by the copyright holder proper.

6 References

[1] Berkeley Webcasts. http://webcast.berkeley.edu/.
[2] Blip TV. http://www.blip.tv/.
[3] Creative Commons. http://creativecommons.org, 2007.
[4] DBSight. http://www.dbsight.net/.
[5] L. Denoue, D. Hilbert, D. Billsus, and M. Cooper.

Projectorbox: Seamless presentation capture for classrooms.
In World Conf. on E-Learning in Corporate, Government,
Healthcare, and Higher Education, 2005.

[6] Apache Lucene. http://lucene.apache.org/java/docs/.
[7] YouTube. http://www.youtube.com/.

1508