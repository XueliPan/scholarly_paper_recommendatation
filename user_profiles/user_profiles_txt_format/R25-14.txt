Interacting with Computers 17 (2005) 187–206

www.elsevier.com/locate/intcom

Findex: improving search result use through

automatic ﬁltering categories

Mika Ka¨ki*, Anne Aula1

Department of Computer Sciences, University of Tampere, Kehruukoulunkatu 1, FIN-33014 Tampere, Finland

Received 24 June 2004; revised 6 August 2004; accepted 10 January 2005

Abstract

Long result lists from web search engines can be tedious to use. We designed a text categorization
algorithm and a ﬁltering user interface to address the problem. The Findex system provides an
overview of the results by presenting a list of the most frequent words and phrases as result
categories next to the actual results. Selecting a category (word or phrase) ﬁlters the result list to
show only the results containing it. An experiment with 20 participants was conducted to compare
the category design to the de facto standard solution (Google-type ranked list interface). Results
show that the users were 25% faster and 21% more accurate with our system. In particular,
participants’ speed of ﬁnding relevant results was 40% higher with the proposed system. Subjective
ratings revealed signiﬁcantly more positive attitudes towards the new system. Results indicate that
the proposed design is feasible and beneﬁcial.
q 2005 Elsevier B.V. All rights reserved.

Keywords: Web search; Search user interface; Categorization; Clustering; Information access

1. Introduction

Web search engines are one of the most popular means of ﬁnding information from the
World Wide Web. The huge amount of documents requires the users to describe their
information need very precisely in order to avoid too long result lists. Indeed, formulating
the information need accurately in the search query is known to be hard for typical web

* Corresponding author. Tel.: C358 3 215 6181; fax: C358 3 215 6070.

E-mail addresses: mika.kaki@cs.uta.ﬁ (M. Ka¨ki), anne.aula@cs.uta.ﬁ (A. Aula).

1 Tel. C358 3 215 8871; fax: C358 3 215 6070.

0953-5438/$ - see front matter q 2005 Elsevier B.V. All rights reserved.
doi:10.1016/j.intcom.2005.01.001

188

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

users. Studies show that people enter very few search terms, typically one or two (Jansen
et al., 1998). Such queries result in huge result sets which are hard to understand and slow
to browse through.

Very long result lists are clearly a major usability problem and a challenge for the
search engine user interfaces. To solve this problem, information scientists look for better
retrieval algorithms to get better results in the ﬁrst place and human–computer interaction
practitioners work for improved user interfaces for result handling. We follow the latter
path and state the ﬁrst question of the study: how to present the search results so that users
are able to ﬁnd the needed information efﬁciently?

We propose a new ﬁltering user interface based on automatic result categories for
accessing the results. The system is called Findex. The user interface presents an overview
of the results to help the users identify and access the interesting results quickly. The
overview is constructed by computing the most frequent words and phrases in the results
and presenting them to the user as a list of categories next to the result list. The user can
then select an interesting category from the list and the user interface ﬁlters the result list to
show the corresponding search results.

In order to test the feasibility and the usefulness of the design, we implemented it and
conducted an experiment with 20 participants. The experiment compared our solution to a
currently widely accepted search user interface model (ranked list). The experiment aimed
to answer the second question of this study: can users understand the user interface and is it
beneﬁcial in accessing the search results? The results show that the new user interface was
faster and more accurate compared to the conventional one and users expressed positive
attitudes towards it.

In the following, we will take a brief look of related work in the ﬁeld. After that, the
algorithm and the user interface of the proposed system are thoroughly described followed
by a description of the experiment and its results. Finally, ﬁndings are summed up in the
conclusions.

2. Related work

Three areas of research are relevant for this study. Firstly, research in categorization of
the background for our categorization
textual documents (web or otherwise) set
mechanism. Secondly, work in understanding and improving web search user interfaces
provides us with information on search interface usability. Thirdly, studies on usability
evaluation of web search engines give examples of how to study this phenomenon. We
concentrate on papers that cover several of these areas as they are the closest references.
Document categorization has long traditions in the information retrieval (IR)
community. The incompleteness and impreciseness of simple lexical text matching
methods have been identiﬁed and document categorization is regarded as one option to
overcome these difﬁculties. We can identify two commonly used categorization
techniques: document classiﬁcation means putting documents into predeﬁned categories
whereas document clustering refers to dividing a set of documents into groups based on
their similarity (Maarek et al., 2000). Various implementation techniques have been
proposed for both.

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

189

Fig. 1. Categorizing web search user interface: Grouper (picture after Zamir and Etzioni, 1998).

Dumais et al. (1988) were among the ﬁrst in the HCI community to suggest the use of
clustering techniques (in their case based on Latent Semantic Indexing, LSI) for improving
the access to textual information. Later, Scatter/Gather (Cutting et al., 1992) was one of
the ﬁrst real systems where the clustering approach was usability tested. The ﬁrst results
were not promising as the clustering interface seemed both slower and less precise when
searching for relevant articles on a given topic (Pirolli et al., 1996). However, in a follow-
up study, Scatter/Gather was found to have potential because users were able to identify
and use the most relevant categories in information gathering tasks (Hearst and Pedersen,
1996).

Later on, Zamir and Etzioni (1998) demonstrated the technical feasibility of clustering
techniques in web environment using their own algorithm. They have also proposed a
search engine user interface—Grouper (Zamir and Etzioni, 1999) (Fig. 1)—based on the
clustering algorithm. The interface presents the titles of sample result pages grouped in
clusters and resembles the user interface of Scatter/Gather. In addition, Grouper lets the
user reﬁne the query by selecting keywords from a category. Unfortunately, controlled
usability tests on Grouper have not been published, but evaluation has been based on log
studies and measuring the properties of the algorithm.

The DART (Cho and Myaeng, 2000) system provides another type of user interface for
a clustering system. DART uses the same clustering algorithm as Grouper. The
contribution of DART is a dartboard-like user interface (Fig. 2) which visualizes the result
set in relation to the clusters. The system has been usability tested, but the results are hard
to interpret. For example, the number of participants was not reported.

190

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

Fig. 2. User interface of DART (picture after Cho and Myaeng, 2000).

One problem with clustering is the naming of the clusters. Typically, the most frequent
or most distinctive word(s) found in the documents of the cluster are used as the name.
This, however, may lead to rather long, uninformative, and incomprehensible names. A
solution to this problem is to use document classiﬁcation instead. When document
categories are predeﬁned, the names can also be predeﬁned to correctly express the
intended meaning.

Chekuri et al. (1997) used this classiﬁcation approach in a document classiﬁer for web
searches. They envisioned that the user could choose one or multiple categories along with
the search terms when submitting a query. Unfortunately neither the user interface nor a
usability evaluation are reported for this solution.

A similar solution has been in the focus of recent SWISH prototype (Chen and Dumais,
2000; Dumais and Chen, 2001; Dumais et al., 2001). SWISH has a classiﬁer for web
search engine results and a special user interface for it (Fig. 3). The proposed solutions
have also been thoroughly evaluated (both algorithmically and for usability). The user
studies conclude that categories are indeed faster and more efﬁcient for a particular type of
tasks. The studies compared multiple user interface solutions and found that categories are
most effective when presented with some sample results. The examples seem to help the
users to understand the meaning of a category.

In addition to text-based clustering and classiﬁcation, other categorization methods have
also been identiﬁed. For instance, Cha–Cha (Chen et al., 1999) and AMIT (Wittenburg and
Sigman, 1997) use hypertext
link structure as the basis for the categorization of
the documents. The usability of Cha–Cha has been under investigation and answers to

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

191

Fig. 3. Result classifying SWISH web search interface (picture after Dumais et al., 2001).

a questionnaire showed positive attitudes towards the system. However, without more
objective measures, the results are rather speculative. Another example of a different
categorization scheme is DynaCat (Pratt and Fagan, 2000), which uses domain knowledge
(man-made taxonomies) in its classiﬁcation process. DynaCat has an overview-based user
interface (Fig. 4) and a user test showed positive results about its usefulness.

The digital library project in New Zealand adopted yet another way of providing
summaries of textual material. The project produced multiple examples of user interfaces
that are based on key phrase extraction. The technique has been used to ﬁnd related
documents (Jones and Paynter, 1999) as well as to categorize search results on small
devices (Jones et al., 2004).

A commercial product called Vivı´simo (http://www.vivisimo.com) uses categorization,
but neither a description of the categorization algorithm nor usability test results have been
published. The user interface of Vivı´simo resembles closely that of ours as it initially
displays 10 categories besides the actual results. The biggest difference is that Vivı´simo
utilizes hierarchical categorization scheme whereas ours is based on a simpler list. In the
Vivı´simo user interface, the initial top-level categories can be further explored by looking
at sub categories of them. The actual categorization scheme of Vivı´simo is unknown, but
seems to utilize frequently occurring words, words that occur frequently together in the
same result, and frequently occurring word stings (phrases). The hierarchy is achieved
assumingly by applying the same categorization scheme recursively to the top-level
categories.

192

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

Fig. 4. DynaCat system that uses domain knowledge for result categorization (picture after Pratt and
Fagan, 2000).

In summary, the problems in the past studies and systems that are relevant here are:

† Lack of thorough user experiments. We have only limited knowledge on the usability of

† Utilization of too complex categorization techniques that are arguably not understood

result categorization systems.

by the users.

Our work aims to solve these problems.

3. System description

Technically our solution is something in between the Grouper and the user interface
developed by Chen and Dumais (2000). It does not use predeﬁned categories like Chen
and Dumais’ system, but it does not use classical clustering techniques, either. Instead we
simply seek for the most frequent words or phrases among the results and use them as the
categories. The categories are shown in a separate list beside the results. Selecting a
category displays the corresponding results in the result list, that is, ﬁlters the result set.
The actual searches are done through Google Web API (http://www.google.com). We
shall ﬁrst describe the categorization algorithm.

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

193

3.1. Result categorization

One of the restrictions in the web environment is that the whole document text body is
not available for the categorization process. As others have demonstrated (Dumais et al.,
2001; Zamir and Etzioni, 1998), clustering and classiﬁcation methods can be used to
categorize web search results based solely on the short text summaries (snippets) returned
by the search engine. However, we believe that the naming problem associated with
clustering, the limitations of classiﬁcation, and the complexity of both could be avoided
with a different approach. In order to make the system understandable and users to feel in
control, a simpler solution is desirable.

Our categorization is solely based on word frequencies in the result listing, i.e. in titles
and short text summaries (snippets). We basically select the n most frequent words and use
them as the categories. Such a category contains then all the results where the word
appears. It is commonly known that simply selecting the most frequent words does not
work, because articles and other very frequent words (like ‘and’) do not carry much
meaning on their own. We use a stop word list to exclude such words from the category
list.

The second problem in simple lexical word matching is that simple inﬂections of the
words make them different (e.g. ‘car’ and ‘cars’ would be two different words). In order to
reduce this problem, we use a word stemmer (Snowball stemmer by Martin Porter, http://
snowball.tartarus.org/). The stemmer removes the word endings so that the simple
inﬂections of a word map to the same word stem.

Both the previous techniques are language-sensitive. Our software has been built in a
way that enables us to easily add more languages as desired. For a new language, we need:
(1) a stemmer, (2) a word frequency list of the language (corpus), and (3) a stop word list.
The corpus is used for language detection and it could also be used to approximate the stop
word list automatically, but a human-made list is preferred for better accuracy. For the
testing purposes we have implemented the needed functions for English and Finnish.
Language detection of the results is automatic and is based on word frequencies in the
corpora.

With this simple logic, we get a list of words capturing the major topics of the results
fairly well. However, some words acquire considerably more meaning when presented in
context. For example, the word ‘states’ does not convey as much meaning as the phrase
‘united states’. To present the user with more meaningful categories we search for the most
frequent phrases in the results as well. A phrase is deﬁned to be any string of words inside a
sentence (between periods).

Categories are computed right after the search engine has returned the results. Each
word in the results, except the stop words, is stemmed and stored with information on the
result item that contained the word. For the phrases the procedure is similar. As each
sentence is broken into phrases, each word comprising a phrase is stemmed and the
resulting stemmed phrase is stored with information about which result contains it.

As the category candidates are processed, unique words and phrases and so called sub
phrases are removed. Sub phrases are part of a longer phrase (super phrase). For example,
if we have a super phrase ‘united states’, there will be corresponding sub phrases ‘united’
and ‘states’ among the candidates. All sub phrases, which are part of a super phrase in

194

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

Table 1
Categories calculated for a few popular queries

Query

Challenger

Categories

Space shuttle challenger
Challenger disaster
Mission
Challenger learning center
January
Crew
Nasa
Tragedy
Challenger accident
Information
Science
Reagan
History
Description
Dodge challenger

Sars

Health
Information
Global
China
Outbreak
Public
World
Cdc
Latest
Sars virus
Asia
April
Diseases
Sars epidemic
Government

Jaguar

Club
Jaguar cars
Information
Jaguar panthera onca
Atari jaguar
Mac jaguar
Reviews
Performance
Wildlife
Virtual
Largest cat
First
Apple
Powerful
Homepage

the same result, are removed. Candidates are sorted according to the frequency, and n
(currently 15) ﬁrst candidates are selected as the categories. We are currently preparing
another paper describing the algorithm in more detail. Some examples of the resulting
categories for a few queries can be seen in Table 1.

3.2. Properties of the categorization technique

The calculation of categories is computationally intensive if the number of candidate
phrases is large. The dominant factor is the number of results to be categorized. By
experimentation we found that 150 ﬁrst results seem to capture the most frequent
categories. Increasing the number of results beyond that makes practically no difference in
the categories. The current implementation needs about 2 seconds to form categories for
these 150 search results making it a feasible solution. Part of the calculation could be made
in parallel while waiting for the search results.

As the categories are computed from the ﬁrst 150 results returned by a search engine,
the underlying ranking method has a considerable effect on the outcome. It determines
(1) which results are categorized (150 ﬁrst ones), (2) what is the order of results within the
calculated categories (rank order), and (3) which categories are selected in a tie situation
(category with higher ranks in the original result list).

There are a few properties of the frequency-based categorization technique that require
further attention. First, the selected categories are not exclusive meaning that one result
can sometimes be accessed through multiple categories. We consider such an overlap in
the categories to be a desirable feature, because the meaning of the information depends on
the context and the overlapping categorization may help users to realize some of those
meanings. For example, let us consider the ‘challenger’ query in Table 1. The following
two results seem to discuss roughly the same topic (space shuttle Challenger disaster).

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

195

Online Ethics Center: Roger Boisjoly and the Challenger Disaster
The space shuttle Challenger disaster recounted by Roger Boisjoly who attempted to
get the mission cancelled.. Roger Boisjoly and the Challenger Disaster..

The Space Shuttle Challenger Disaster—a NASA Tragedy
The Space Shuttle Challenger Disaster, a NASA Tragedy. When the space shuttle.
Related Resources to Space Shuttle Challenger Disaster..

The categorization algorithm places both of these results into two categories:
‘challenger disaster’ and ‘space shuttle challenger’. For users looking for information
about the accident the former is more relevant while the latter one will attract those
generally interested in the space shuttle. The listed results are relevant for both.

The second feature of the technique is that not all results are guaranteed to belong to
any category. This could be very undesirable as some relevant results could not be
accessed at all. We provide a special built-in category for viewing all the results in the
normal rank order list to overcome this problem. According to our experience, this
solution works ﬁne.

The third property of the technique is that it may produce categories that are out
of context or hard to understand for
the user. Because the technique is based
solely on statistical analysis,
it does not consider the words as concepts having
a well-deﬁned meaning. As a result,
is not
guaranteed and it can vary between different result sets. We believe that users
will understand this and that
they are able to discard the possible low-quality
categories.

the usefulness of

the categories

3.3. User interface

The current prototype is implemented in Java as a standalone application. This
approach enabled us to easily experiment with the user interface mechanics and made
implementation of the experiment easy and robust. However, we have also implemented
the same functionality as a standard web service to be used through a standard web
browser. This implementation makes the solution attractive to a wider user population and
is seen to be feasible.

The user interface follows the basic idea used in most graphical email clients and
in Windows Explorer. These programs display a set of collections on the left of the
window while the right side shows the contents of the selected collection (like ﬁles
in a folder). The same holds true here as well: the left side of the user interface
shows the list of categories (words and phrases) and the right side displays the
corresponding results (Fig. 5). This familiar design was assumed to be easy to
understand and adopt.

The category list and the result display are tightly coupled in the user interface so that
changing the category selection immediately changes the contents of the result view. In the
result view, the selected keyword is highlighted in pale yellow to make the connection
between a selected keyword and a result evident.

196

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

Fig. 5. The proposed category user interface. The category list is on the left and the corresponding results are
shown on the right.

If no category is selected, the result view is empty. However, this is not likely to happen
as the special built-in ‘All results’ category is automatically selected after the query has
been completed. As the name of the built-in category suggests, selecting that category the
user will see all the results retrieved for the query (by default, 150 ﬁrst results). It is also
possible to select many categories. In this case, the results are required to belong to all the
selected categories (intersection of the categories).

The order of the results is determined by the search engine. When all the results are
shown, the result listing is the same as that returned by the search engine. When a category
is selected, the relative order of the results is determined by the search engine, although the
results are not likely to be sequential in the original list. The results have an ordinal that
shows the position of the result in the rank order of the search engine.

Other parts of the user interface are fairly obvious. On the top, there is a ﬁeld for
entering the query and ‘Search’ and ‘Cancel’ buttons for controlling the search engine.
The status bar on the bottom shows additional information like the number of documents
found in total and the state of the system (on/off-line).

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

197

We conducted an experiment to evaluate the categorization algorithm and the user
interface described in the previous sections. The experiment tested if the new solution
differs from a widely accepted solution, in this case the search engine user interface
displaying results in a ranked list, in terms of speed, accuracy, and subjective satisfaction.

There were 20 volunteer participants (8 male, 12 female) in the study. The participants’
average age was 35 years varying from 19 to 57 years. The participants were recruited
from the local university. They were students and personnel from seven organizational
units. The participants had relatively long histories of general computer use (on average
11.5 years) as well as web use (on average 6 years). Almost all the participants can be
regarded as experienced computer users.

4. Experiment

4.1. Participants

4.2. Apparatus

Two user interfaces were used to access the search results:

1. Category interface (category UI, Fig. 6, right window) was the Findex user interface
described above with two modiﬁcations: (1) multiple selections of keywords was not

Fig. 6. Desktop set-up in the test. Task window on the left and search window on the right. The screen size was
1200!1024 pixels and the search window size was 800!900 pixels.

198

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

Fig. 7. Reference user interface showing 10 results on a page and radio buttons to navigate between the pages.
Note the checkboxes for selecting results.

allowed, and (2) automatic selection of ‘All results’ category was disabled. The latter
modiﬁcation means that
list was initially empty in each task. Both
modiﬁcations were made after pilot tests to make the experiment set-up more robust
and focused. Category computation produced 15 categories and it was based on the ﬁrst
150 results.

the result

2. Reference interface (reference UI, Fig. 7) was a Google web search engine imitation
showing results in a ranked list on separate pages, 10 results per page. In the bottom of
the window, there were controls to browse the pages in order (Previous and Next
buttons) or in random order (radio buttons). The participant had access to 15 pages
(150 ﬁrst results).

Both user interfaces showed the results in the same visual format (Fig. 8). The format
resembles closely the format of Google, omitting size, category, cached pages, and similar
pages features found in Google.

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

199

Fig. 8. Visual format of the individual result elements in the experiment.

The reason why we did not use the publicly available Google interface was the
controllability of the experiment variables. By using Google, we would have been faced
with possible network problems and changing content of the Google database. The
instrumentation of different systems could also have caused errors in timings, for instance.
the computer
desktop contained two windows: one displayed the test tasks in textual format (task
window) while the other was the user interface studied (search window) as shown in Fig. 6.
The size and location of both windows were predetermined and ﬁxed.

The experiment procedure was automated. During the experiment

The experiment had search user interface as the only independent variable with two
values: category UI and reference UI. The values of the independent variable were varied
within the subjects and thus the analysis was done using repeated measures tools.

As dependent variables we measured: (1) time to accomplish a task in seconds,
(2) number of results selected for a task, (3) relevance of each selected result on a three-
step scale (relevant, related, not relevant), and (4) subjective attitudes towards the systems.

4.3. Design

4.4. Procedure

The experiments were carried out in a usability laboratory where participants were
invited one at a time. Before the experiment the whole procedure was explained to the
participants and any questions regarding the set-up were answered. One experiment lasted
about 45 min and contained 18 information seeking tasks and three claim rating tasks (one
for each condition and one comparing the two).

The experiment was divided into two parts, each consisting of nine information seeking
tasks and a claim answering task. One part was carried out with the category interface and
other using the reference interface. The order of the parts and the tasks were
counterbalanced between the participants. Before each part
the participants were
explained how the user interface worked and they were allowed to try it by themselves.
The search tasks were based on predeﬁned queries. We adopted this approach from an
experiment by Dumais et al. (2001) where the participants did not formulate the queries
themselves either. This approach makes perfect sense, because it removes the vast
variability caused by different search capabilities of the participants and lets us measure
the performance in the result evaluation phase, which we aim to improve. The tasks were
selected to cover multiple interests (e.g. astronomy, cooking, movies, cars, gardening,
etc.). The queries were balanced in terms of (1) number of obviously relevant categories,

200

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

and (2) the position of them in the category list. In addition, a few queries did not have any
obviously relevant categories.

For each task, the participants were instructed to ﬁrst read the task description, then
push the ‘Start’ button on the task window and promptly proceed to accomplish the task
using the search window. While the participant read the task, the test apparatus fetched the
results of the corresponding predeﬁned query, but the user interface was hidden. When the
participant pushed the ‘Start’ button, the search window was enabled and the task
execution could start
immediately. The actual queries were executed before the
experiment and saved locally for fast and equal access.

Upon task completion, the participants were instructed to push the ‘Done’ button on the
task window. The time between ‘Start’ and ‘Done’ button presses was measured as the total
time for the task. The participants were told about this timing scheme. After each task, the
participant used the ‘Next’ button in the task window to see the next one. Between the tasks,
the participants were in control of the situation being able to take a short break if desired.
The actual task of the participant was to ‘collect as many relevant results for the
information seeking task as possible as fast as you can’. The task has two competing goals
(speed and accuracy) to simulate realistic settings. In a real situation, users balance
between these two goals intuitively based on the various factors (time, importance of the
task, available resources, etc.), but in test situation such limiting factors have to be created
artiﬁcially. In the pilot tests we observed that even if the task had these two competing
goals, users tented to favor thoroughness using extended amount of time for a task. To
enforce more realistic (faster) behavior, the time for each task was limited to 1 min. The
participants were encouraged to utilize their own personal habits in selecting the results.
When the 1 min time limit passed, the search window was automatically disabled and
the clock was automatically stopped. The participants were able to proceed to the next task
before this time limit if they thought that they had found enough results.

Collecting the results was implemented by adding a check box beside each result item
(see Figs. 6 and 7). Participants were instructed to check the corresponding check box
when a result seemed relevant. If a mistake happened, it was possible to clear the mark by
clicking the checkbox again.

After the two sets of tasks and ratings, comparison ratings and demographic
information were elicited by online forms in the task window. After this the experiment
was over.

5. Results

5.1. Speed measures

As the total time reserved for each task was limited to 1 min, the plain task times do not
tell the whole story about the speed. In both conditions (category and reference user
interface), it was very common that the participant used all the time reserved for a task.
Thus, the mean task times were very close to 1 min, being 56.6 s (sdZ5.5) for the category
interface and 58.3 s (sdZ3.5) for the reference interface. Due to the experiment set-up, it
is understandable that we did not observe a statistically signiﬁcant difference in task time

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

201

Fig. 9. There is a statistically signiﬁcant difference in speed of use between the compared user interfaces in favor
of the category UI. Note also the greater proportion of relevant results found with it.

between the two conditions. Repeated measures analysis of variance (ANOVA) gave
F(1,19)Z3.65, n.s.

The number of results that the participants collected revealed an interesting ﬁnding as
Fig. 9 shows. On average the participants were able to ﬁnd 5.5 results per minute (sdZ2.0)
using the category interface contrasted with 4.1 per minute (sdZ1.1) found with the
reference interface. This difference in speed is statistically signiﬁcant (F(1,19)Z12.13,
p!.01).

However, the raw speed of result acquisition may not be the best measure to evaluate
the efﬁciency of a search engine user interface. It may be that the selected results do not
really answer the user’s initial information need. In order to estimate the usefulness of the
obtained results, we need to consider their accuracy.

5.2. Accuracy measures

For measuring the quality of the results the participants collected in the experiment, we
judged the relevancy of the selected results for each task. Each selected result was assigned
one of three relevance values: relevant, related, and not relevant. Ranking was based only
on the snippets as were the participants’ selections, because we did not want to include the
snippet–document relationship as a factor in the study.

A result was ranked relevant if the snippet clearly indicated that the corresponding page
referred had desired content. In practice, for a result to be relevant we required the
existence of multiple concepts of the task in the snippet. In contrast, a related snippet was
required to refer to the same overall topic, but not to other aspects of the task description.
Finally, the not relevant snippets differed from the overall topic of the task. For example,
in a task to ﬁnd pictures of the planet Jupiter, a snippet referring to images of Jupiter was
rated as relevant, a snippet referring generally to the planet Jupiter was rated as related,
and ﬁnally a snippet referring to the Jupiter Research Company was rated as not relevant.
Relevance was measured in four ways. Firstly, recall states which portion of all
relevant results were found. In this study the recall measure was calculated from all the
relevant results found by all the participants, not from all the relevant results in the result

202

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

sets, which is the conventional way. Secondly, precision tells the proportion of relevant
results among selected results. The third and fourth relevance measures concern the
accuracy in relation to speed.

Recall and precision measures show a difference between the user interfaces. When
using the category interface 62% of the results (sdZ13), on average, were relevant
whereas the precision with the reference user interface was 49% (sdZ15). The difference
is statistically signiﬁcant: F(1,19)Z14.49, p!.01. The recall measure revealed a similar
difference: with the category user interface the participants found on average 33% (sdZ4)
and with the reference user interface 19% (sdZ7) of the relevant results for each task
(F(1,19)Z29.88, p!.01).

The breakdown of the speed measures according to relevance shows also signiﬁcant
differences (see Fig. 9). Using the category user interface, the participants were able to ﬁnd
3.5 relevant results per minute on average (sdZ1.5) whereas the use of the reference user
interface yielded 2.0 relevant results per minute (sdZ0.8; F(1,19)Z8.20, p!.01). The
speed of acquiring related result (on average 1.4) is the same for both systems, but the
category user interface reduces the number of not relevant results (0.4 vs. 0.7 not relevant
results per minute, sdZ0.3 for both cases; F(1,19)Z11.24, p!.01).

5.3. Immediate success measures

In the real world, web users are not typically looking for as many results as possible, but
in many cases they are looking for the ﬁrst good enough answer. To measure this kind of
behavior, we analyzed the success of the ﬁrst few selections. Fig. 10 shows the cumulative

Fig. 10. Cumulative proportion of cases where at least one relevant result has been found with the nth selection.
The participants found the ﬁrst relevant result sooner with the category UI than with the reference UI.

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

203

Fig. 11. The precision (proportion of relevant results) of the ﬁrst selections were higher when using the category
user interface.

percentage of the cases where users have found at least one relevant result with the nth
selection. This measure is called immediate accuracy (Ka¨ki, 2004). The most interesting
difference is produced already with the ﬁrst selection, where in 56% of the cases users ﬁnd
a relevant result with the category UI whereas for the reference UI the ﬁgure is 40%. The
difference is statistically signiﬁcant, F(1,19)Z12.5, p!.01. Note that the difference stays
virtually the same after the ﬁrst selection.

The same effect can be seen in the precision of the ﬁrst selections in Fig. 11. With the
category UI 59% of the ﬁrst selections and 70% of the second selections are relevant. The
corresponding numbers for the reference UI are 42 and 46%. In both cases, the difference
is signiﬁcant as ANOVA gives: F(1,19)Z13.1, p!.01 and F(1,19)Z25.6, p!.001,
respectively.

The comparison of the corresponding times, however, does not show notable
differences. Average time used by the participants to ﬁnd the ﬁrst relevant result was
about 21 s with both user interfaces. The second relevant result was found in 27 and 35 s
(sdZ6 and 8) while ﬁnding the third relevant result took 30 and 36 s for the category and
reference user interfaces, respectively. The difference in acquiring the second relevant
result is signiﬁcant (F(1,19)Z17.70, p!.01). It is notable, however, that when using the
reference UI there were more cases where not a single relevant result was found for a task.
Thus the average times for ﬁnding the ﬁrst relevant result are not completely comparable.

5.4. Subjective measures

The alternative user interfaces were also evaluated using subjective measures. To
achieve this, the participants were presented a set of claims (e.g. ‘It was easy to ﬁnd the
results’ and ‘The user interface was confusing’) after using each user interface.

204

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

Fig. 12. Distribution of the subjective ratings shows more positive attitudes towards the category UI than towards
the reference UI.

The participants responded to the claims on a six point scale from agree (0) to disagree (5).
In the end of the experiment, there was another set of claims where participants had to
compare the two user interfaces against each other (e.g. ‘Functionality was easier to
understand’ and ‘Task execution was harder’). The responses were also here collected
along a six point scale but the range was from category interface (0) to reference
interface (5).

Fig. 12 shows the results of the claim answers. In the picture the scales have been
normalized to have positive answers on the left and negative ones on the right. As Fig. 12
suggests, there is a difference in the subjective ratings of the systems. The analysis of the
responses shows a statistically signiﬁcant difference in the attitudes toward the systems.
The median score (medianZ1) for the category interface indicates more positive
attitudes towards it than towards the reference interface (medianZ2) and Wilcoxon
matched-pairs signed-ranks test gives ZZK2.51, p!.02 (see Fig. 12 for variability).
Similarly, when comparing the two systems together we see a statistically signiﬁcant bias
for the category interface. On a six-point scale where 0 stands for the reference UI and 5
for the category UI, the median score was 3.5. One-sample Wilcoxon singed-ranks test
gives VZ188, p!.01.

6. Discussion and conclusions

In the beginning, we had two questions in mind. The ﬁrst one was ﬁnding ways to
present search results. The answer was Findex, a system with automatically formed
categories that provide an overview of the results in association with a ﬁltering user
interface. The second question was whether the proposed solution would work in practice
or not. A range of measures collected in the experiment gives us a good reason to believe
that Findex does, indeed, perform better than a conventional ranked list 10-results-per-
page user interface. The belief is supported by four measurements collected in the
experiment.

First, with the category user interface it is possible to browse through more results than
with the conventional user interface because the searching speed is higher (5.5 vs. 4.1
results per minute). This is important, since many times the web search results are rather
unreliable and it is thus desirable to be able to access alternative results quickly.

Second, and more importantly, the proposed interface not only gives the users more
options but gives them more relevant options. Results showed that the increase in the
number of results was due to increased number of relevant results. The measured speed of

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

205

ﬁnding relevant results was about 40% higher with the proposed system compared to the
reference solution.

Third, for users in the real situations, the immediate success of the search is also very
important. The results show that when using our system, the users found the ﬁrst relevant
result earlier (with fewer selections) than with the reference user interface. The result is
very important although we did not measure the time difference in ﬁnding the ﬁrst relevant
result. The selection with which the ﬁrst relevant result is found is crucial, since people
tend to visit very few result pages in the web environment (Spink et al., 2001), typically
one or two. According to the results, the proposed system performs better in this kind of
search tactic.

Fourth, the results showed that users had positive attitudes towards the system.
Although subjective ratings are rather soft measures, they do grasp a very critical issue in a
user interface. Even the most sophisticated and efﬁcient user interface is useless if people
do not like it. Based on the questionnaire and informal discussions with the participants we
have good reasons to believe that the proposed interface could ﬁnd its audience.

Although the results are very promising, we must bear in mind that improvements come
with a certain price. In order to compute the categories, a large number of results must be
fetched from a search engine. This takes time in addition to the actual computation of the
categories. However, the system is built in a way that it can display ﬁrst 10 results
immediately and then compute the categories in the background while the user can
evaluate the ﬁrst results. This reduces the perceived cost for the operations, but delays can
still have an effect on subjective ratings.

7. Future work

The basic functionality of the category user interface is promising, and we are planning
to continue to investigate it. First, the number of categories presented to the user is an
interesting question and it was left completely untouched in the present study. Even the
number of categories (15) was somewhat arbitrarily chosen. We plan to study what is the
effect of the number of categories on the users’ performance.

Second, we are executing a longitudinal study of the usability of the category user
interface. In the study we aim to ﬁnd out if the categories are beneﬁcial in the long run
when their use is truly voluntary and happens in natural settings. In addition, it is
interesting to see if the category user interface alters the users’ behavior in some ways, for
example, could it stimulate the users to make better reformulation of the queries? For the
study, an implementation of the user interface has been done for the web environment.

Acknowledgements

This work was supported by the Graduate School in User-Centered Information
Technology (UCIT). We would like to thank Poika Isokoski, Johanna Ho¨ysniemi
and Kari-Jouko Ra¨iha¨ for comments and discussions that contributed to this study,
Tomi Heimonen, Natalie Jhaveri, Harri Siirtola.

206

M. Ka¨ki, A. Aula / Interacting with Computers 17 (2005) 187–206

References

Chekuri, C., Goldwasser, M., Raghavan, P., Upfal, E., 1997. Web search using automatic classiﬁcation,

Proceedings of the Sixth International World Wide Web Conference WWW6, Santa Clara, USA .

Chen, H., Dumais, S., 2000. Bringing order to the web: automatically categorizing search results, Proceedings of

CHI’2000, The Hague, Neatherlands. ACM Press, New York, pp. 145–152.

Chen, M., Hearst, M., Hong, J., Lin, J., 1999. Cha–Cha: a system for organizing Intranet search results,

Proceedings of the Second USENIX Symposium on Internet Technologies and SYSTEMS (USITS) .

Cho, E., Myaeng, S., 2000. Visualization of retrieval results using DART, Proceedings of RIAO 2000, Paris,

France .

Cutting, D., Karger, D., Pedersen, J., Tukey, J., 1992. Scatter/Gather: a cluster-based approach to browsing large
document collections, Proceedings of SIGIR 1992, Copenhagen, Denmark. ACM Press, New York, pp.
318–329.

Dumais, S., Chen, H., 2001. Hierarchical classiﬁcation of web content, Proceedings of SIGIR 2000, Athens,

Greece. ACM Press, New York, pp. 256–263.

Dumais, S., Furnas, G., Landauer, T., Deerwester, S., Harshman, R., 1988. Using latent semantic analysis to
improve access to textual information, Proceedings of CHI’88, Washington DC, USA. ACM Press, New York
pp. 281–285.

Dumais, S., Cutrell, E., Chen, H., 2001. Optimizing search by showing results in context, Proceedings of

CHI’2001, Seattle, USA. ACM Press, New York, pp. 277–284.

Hearst, M., Pedersen, J., 1996. Reexamining the cluster hypothesis: Scatter/Gather on retrieval results,

Proceedings of ACM SIGIR’96, Zu¨rich, Switzerland. ACM Press, New York.

Jansen, B., Spink, A., Bateman, J., Saracevic, T., 1998. Searchers, the subjects they search, and sufﬁciency: a
study of a large sample of excite searchers, 1998 World Conference on the WWW and Internet, Orlando, USA
1998.

Jones, S., Paynter, G., 1999. Topic-based browsing within a digital library using keyphrases, Proceedings of the

ACM Conference on Digital Libraries, Berkeley, USA. ACM Press, New York, pp. 114–121.

Jones, S., Jones, M., Deo, S., 2004. Using keyphrases as search result surrogates on small screen devices. Personal

Ka¨ki, M., 2004. Proportional Search Interface usability measures. Proceedings of Nordi CHI 2004, Tampere,

and Ubiquitous Computing 8 (1), 55–68.

Finland. ACM Press, New York, pp.365–372.

IBM Research Report RJ. 10186, April 2000.

Maarek, Y., Fagin, R., Ben-Shaul, I., Pelleg, D. (2000). Ephemeral document clustering for web applications.

Pirolli, P., Schank, P., Hearst, M., Diehl, C., 1996. Scatter/Gather browsing communicates the topic structure of a
very large text collection, Proceedings of CHI’96, Vancouver, Canada. ACM Press, New York, pp. 213–220.
Pratt, W., Fagan, L., 2000. The usefulness of dynamically categorizing search results. Journal of the American

Medical Informatics Association 7 (6), 605–617.

Spink, A., Wolfram, D., Jansen, M., Saracevic, T., 2001. Searching the web: the public and their queries. Journal

of the American Society for Information Science and Technology 52 (3), 226–234.

Wittenburg, K., Sigman, E., 1997. Integration of browsing, searching, and ﬁltering in an applet for web

information access. CHI’97 Electronic Publications: Late Breaking/Short Talks, Atlanta, USA.

Zamir, O., Etzioni, O., 1998. Web document clustering: a feasibility demonstration, Proceedings of the 19th
International SIGIR Conference on Research and Development in Information Retrieval (SIGIR’98). ACM
Press, New York, pp. 46–54.

Zamir, O., Etzioni, O., 1999. Grouper: a dynamic clustering interface to web search results, Proceedings of the

Eighth International World Wide Web Conference WWW8, Toronto, Canada. Elsevier, Amsterdam.

