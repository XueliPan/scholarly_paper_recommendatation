Learning Thin Junction Trees via Graph Cuts

Dafna Shahaf

Anton Chechetka

Carlos Guestrin

Carnegie Mellon University

{dshahaf, antonc, guestrin}@cs.cmu.edu

Abstract

Structure learning algorithms usually focus on
the compactness of the learned model. However,
for general compact models, both exact and ap-
proximate inference are still NP-hard. Therefore,
the focus only on compactness leads to learning
models that require approximate inference tech-
niques, thus reducing their prediction quality. In
this paper, we propose a method for learning
an attractive class of models: bounded-treewidth
junction trees, which permit both compact repre-
sentation of probability distributions and efﬁcient
exact inference.

Using Bethe approximation of the likelihood, we
transform the problem of ﬁnding a good junction
tree separator into a minimum cut problem on a
weighted graph. Using the graph cut intuition,
we present an efﬁcient algorithm with theoretical
guarantees for ﬁnding good separators, which we
recursively apply to obtain a thin junction tree.
Our extensive empirical evaluation demonstrates
the beneﬁt of applying exact inference using our
models to answer queries. We also extend our
technique to learning low tree-width conditional
random ﬁelds, and demonstrate signiﬁcant im-
provements over state of the art block-L1 regu-
larization techniques.

1

Introduction

Structure learning algorithms have traditionally focused on
learning compact probabilistic graphical models (PGMs).
However, compactness by itself is not sufﬁcient to guaran-
tee that the learned models are useful in practice. PGMs

Appearing in Proceedings of the 12th International Confe-rence
on Artiﬁcial Intelligence and Statistics (AISTATS) 2009, Clearwa-
ter Beach, Florida, USA. Volume 5 of JMLR: W&CP 5. Copyright
2009 by the authors.

are mainly used for inference, but even approximate infer-
ence is NP-hard in compact graphical models. Therefore,
one usually has to resort to approximate inference methods,
which can yield unreliable results.

Luckily, there exist exact inference algorithms with com-
plexity exponential only in treewidth of the model, so for
models with low treewidth, exact inference can be per-
formed efﬁciently. In this paper, instead of taking the com-
mon approach of learning complicated models that ﬁt the
data well and running approximate inference on them, we
will learn simpler models (speciﬁcally, low-treewidth junc-
tion trees (Bach and Jordan [2002])) and use exact infer-
ence.

There has been a substantial amount of work on struc-
ture learning (Chow and Liu [1968], Karger and Srebro
[2001], Bach and Jordan [2002]). For n variables and
desired treewidth k, existing methods for learning low-
treewidth structures either have complexity of at least
O(nk) (Chechetka and Guestrin [2008]), or take local,
myopic steps (Bach and Jordan [2002]). Unlike previous
methods, our algorithm takes steps that are based on a more
global criterion, while having complexity polynomial not
only in n, but also in k. In addition to attractive asymptotic
complexity, our approach is also very fast in practice.

In this paper, we show that under Bethe approximation of
likelihood (Yedidia et al. [2000]), ﬁnding the optimal JT
separator is equivalent to ﬁnding a minimum-weight cut in
a weighted graph for a certain metric. Based on the graph
cut formulation, we present an efﬁcient principled algo-
rithm for ﬁnding good separators, which we recursively ap-
ply to obtain a thin junction tree. We evaluate our method
on different synthetic and real-world datasets and show it
to be faster than existing algorithms while in most cases
learning models of similar or better quality.

As our algorithm only requires a weighted graph as input,
it can be easily adapted to criteria other than Bethe approx-
imation. As an example, in this paper we extend our ap-
proach to learning low-treewidth conditional random ﬁelds
(Lafferty et al. [2001]) and demonstrate signiﬁcant empiri-
cal improvements over the state of the art techniques.

Learning Thin Junction Trees via Graph Cuts

The main contributions of this paper are:

3 Maximizing Bethe Approximation to

• Formulating the problem of structure learning via

Likelihood

graph cuts.

• A fast randomized algorithm with theoretical guaran-
tees for ﬁnding good separators and combining them.
• Extending the algorithm to conditional random ﬁelds.

2 Learning Junction Trees

In this paper, we learn a certain class of PGMs, namely JTs
with low treewidth, for which exact inference can be per-
formed efﬁciently. In this section, we brieﬂy review these
models (for more details, see Cowell et al. [2003]).

Figure 1: Left: a junction tree. Ellipsoids are cliques, rectangles
are separators. Right: the corresponding Markov Network.

Let C = {C1, ..., Cm} be a collection of subsets of V . El-
ements of C are called cliques. Let T be a set of edges
connecting pairs of cliques such that (T, C) is a tree.

Deﬁnition 2.1 (Junction Tree (JT)) (T, C) is a junction
tree iff it satisﬁes the running intersection property:
∀Ci, Cj ∈ C and ∀Ck on the (unique) simple path between
Ci and Cj , x ∈ Ci ∩ Cj ⇒ x ∈ Ck.

A set Sij ≡ Ci ∩ Cj is called the separator corresponding
to an edge (i − j) ∈ T . The size of a largest clique in a
junction tree minus one is called the treewidth. A k-JT is a
junction tree of treewidth at most k. For example, the tree
on the left Figure 1 is a 2-JT. On the right is the correspond-
ing Markov network: there is an edge between every two
vertices that appear together in a JT clique.
A distribution P (V ) is representable using (T, C) if in-
stantiating all variables in any separator Sij renders the
variables on different sides of Sij independent.
In this
case, a projection of P on (T, C), deﬁned as P(T,C) =
((cid:81)
(i−j)∈T P (Sij)) is equal to P itself.
Otherwise, projection P(T,C) provides the best possible ap-
proximation (in a Kullback−Leibler sense) for a JT with
structure (T, C).

Ci∈C P (Ci))/((cid:81)

In this paper, we address the following problem: given data
D and an integer k, we treat each datapoint as a complete
instantiation of the random variables V and seek to ﬁnd a
good tractable approximation of P (V ). Speciﬁcally, we try
to ﬁnd a junction tree of treewidth at most k that maximizes
the log likelihood of the data, max(T,C) log P (D|(T, C)).

We want to efﬁciently ﬁnd a low-treewidth model with high
log-likelihood. Maximizing a model’s likelihood is equiva-
lent to minimizing its empirical entropy H(V ). Given a JT,
we can compute its entropy efﬁciently: since entropy de-
composes as a sum over clique entropies minus separator
entropies, we only need to compute entropy of limited-size
sets. Methods for structure learning (e.g., Chechetka and
Guestrin [2008]) typically need to explore a large number
of candidate cliques and separators, and thus are forced to
compute many such entropies, potentially over all possi-
ble O(nk+1) sets. To avoid such a costly dependence on
the treewidth k, we propose a scoring method based on
a Bethe free energy approximation of the entropy Yedidia
et al. [2000].

Deﬁnition 3.1 (Bethe Entropy) Let G = (V, E) be an
undirected Markov network. The Bethe entropy, Hβ, is the
sum of edge entropies, minus the entropies of over-counted
intersections.

Hβ(V, E) =

H(u, v) −

(deg(v) − 1)H(v)

(cid:88)

(u,v)∈E

(cid:88)

v∈V

A JT can be represented by a Markov net, by creating an
edge between every pair of variables that appear in the
same JT clique, (e.g., Figure 1). We will thus focus on
ﬁnding a JT with low Hβ.

Claim 3.2 Minimizing Hβ is equivalent to maximizing
(cid:80)

(i,j)∈E I(Vi, Vj).

Proof sketch: (cid:80)
(cid:80)
i H(Vi), and (cid:80)

(i,j)∈E I(Vi, Vj) = −Hβ(V, E) +
i H(Vi) is constant.

We need a JT with edges that are as strong as possible,
in terms of mutual information. One natural way to get
a high-likelihood, low-treewidth model is to start from a
dense model and remove weak edges (weighted by the mu-
tual information) until the treewidth is low enough. Moti-
vated by this, we reformulate the problem from Section 2:

Deﬁnition 3.3 (Edge removal for low-treewidth learn-
ing problem)
Input: G = (V, E), an undirected graph, weight function
w : E → R+ and an integer k.
Goal: remove the lowest-weight set of edges such that the
resulting graph has treewidth ≤ k.

settings,

In our
edge weights come from mutual-
information estimations. Weights could also come from L1
regularization, or other methods that estimate pairwise de-
pendence. Later we learn conditional random ﬁelds, and
edges are weighted by conditional mutual information.

d,fc,d,fa,d,eabedcfd,e,fd,eb,d,ed,eABSShahaf, Chechetka, Guestrin

4 Finding a Low-Weight Separator

Given a dense, weighted graph, our goal is to remove weak
edges until the treewidth is low enough. We achieve this
goal by identifying potentially good JT separators, one by
one. Later we can recover the tree from the list of sepa-
rators and their corresponding connected components. We
notice that if S is a separator in a JT, it cuts the graph into
two connected components.

Deﬁnition 4.1 (Cut) A cut (A, B|S) is a partition of V
into disjoint subsets V = A ∪ B ∪ S, such that A, B (cid:54)= ∅.
S is called a separator.

We want to ﬁnd a cut such that removing separator S ren-
ders A and B almost independent. By the Bethe approxi-
mation, we want the weight of the cut, (cid:80)
a∈A,b∈B w(a, b),
to be low. Cut edges are the edges we remove, and thus
their weight corresponds to loss in Bethe entropy. Figure 2

Figure 2: Left: A mutual-information graph. Dashed edges have
lower weight. Right: A partition with a separator of size 2, s.t.
edges between A and B are as weak as possible. Note, the actual
JT cliques are A ∪ S, B ∪ S.

illustrates this idea. A weighted graph G is on the left;
dashed edges are weaker (have low weight). There is no
separator of size 2 that cuts the G into independent com-
ponents; however, there is a separator S that partitions the
graph into components that have only weak edges between
them, A and B. Note that we only pay for edges from A to
B directly. The edges that go through S might be strong,
but we do not pay for them. In our approach, we will use
S as a separator in our JT and recurse on both sides, S ∪ A
and S ∪ B. In the following, we formalize this intuition.

Deﬁnition 4.2 (min k-Node-Removal Edge Cut Problem)
Input: an undirected graph G = (V, E), a non-negative
weight function w : E → R+ and an integer k.
Goal: ﬁnd a separator set S of at most k nodes, and a
partition (A, B) of the remaining variables, such that the
weight of cut (A, B|S) = (cid:80)

a∈A,b∈B wab, is minimized.

4.1 Solution via Integer Programming

In this section we encode the problem as an integer pro-
gram. We later relax it to a linear program and round the LP
solution. We want a low-cost cut (A, B|S), with |S| ≤ k.
Suppose we know in advance some a ∈ A, b ∈ B. We can
formulate the problem as an integer program:

Figure 3: Rounding demonstrated. Left: LP solution. Numbers
denote edge/node internal distance. Note that ds = 0, dt = 1.
Two cuts are shown, for different radii. For ρ = 0.6, there is one
node in the separator.

min

s.t.

(cid:88)

e∈E
(cid:88)

we · ce

sv ≤ k

v∈V
sa = 0, sb = 0
da = 0, db = 1
du ≤ dv + sv + c(v,u)
ce, sv, dv ∈ {0, 1}

(1)

(2)
(3)
(4)
(5)

ce = 1 indicates that we drop edge e (i.e., e is a cut-edge).
The objective, (cid:80)
e∈E we · ce, is the cost of dropped edges.
sv = 1 indicates that node v is in the separator S. Con-
straints (1), (2) assert that |S| ≤ k and that a, b /∈ S. In ad-
dition, the IP deﬁnes an indicator variable dv = 1 if v ∈ B
for every node v. Constraint (3) asserts that a /∈ B, b ∈ B.
Finally, constraint (4) is a transition constraint: for every
edge (u, v), if u ∈ B and v /∈ B, either v ∈ S or we re-
move the edge (u, v). In other words, on every path from
A to B there is either a node from S, or a cut-edge which
we remove. An integer solution to this problem speciﬁes a
cut (A, B|S), such that |S| ≤ k.

2

Recall that we assumed knowing in advance some a ∈
A, b ∈ B. However, this is not the case. The naive way
(cid:1) pairs of vertices;
would consider running the IP for all (cid:0)n
instead, we note that we can arbitrarily pick one vertex to
be a, and test the other n − 1 vertices as b. In order to guar-
antee a in not in the separator, we may need to pick k + 1
potential sources, for a total of O(nk) checks (see Alg. 1).
In practice, we can often stop after a smaller number of
checks, especially for sparse graphs, using the following
property: the IP objective is non-negative, so if the IP ﬁnds
a 0-weight cut, it is an optimal solution, and cannot be im-
proved.

4.2 LP Relaxation and Rounding

Integer programming is NP-hard.
Instead, we solve the
linear programming (LP) relaxation, obtained by replac-
ing integrality constraints (5) by range constraints 0 ≤
ce, sv, dv ≤ 1. LPs are solved in polytime, but may return
a fractional solution. The next step is to derive a separa-
tor from the optimal LP solution (Alg. 2). We describe our
randomized rounding procedure as follows:

The LP returns ce, sv, dv ∈ [0, 1]. We interpret those as
distances: ce is the length of an edge, and sv is the length
of a node (some nodes are bigger than others). The length

d,fc,d,fa,d,eabedcfd,e,fd,eb,d,ed,eABSab.20.80.5.5ab.20.80.5.5ab.20.80.5.5r=.2r=.6Learning Thin Junction Trees via Graph Cuts

Algorithm 1: ﬁndSeparator (G, w, k)

Input: G = (V, E): undirected graph, w: weight function,

k: max separator size.

Output: (A, B|S) cut of V
if |V | ≤ k + 1 then return (V, φ|φ) ; // found a small
clique, no need to separate
for i = 1 to k + 1 do

Pick a ∈ V
forall b ∈ V, b (cid:54)= a do
(cid:88)

minimize

we · ce

subject to

e∈E
0 ≤ ce, sv, dv ≤ 1
(cid:80)
da = 0, db = 1, sa = 0, sb = 0
du ≤ dv + sv + c(v,u)

v∈V sv ≤ k

Pick a∗, b∗ with the minimal LP value
return roundCut(G, w, k, (cid:104)a∗, b∗, c∗

e, s∗

v, d∗

v(cid:105))

Algorithm 2: roundCut(G, w, k, (cid:104)a, b, ce, sv, dv(cid:105))

Input: G = (V, E) graph, w weight, k max separator size,

the LP solution for a, b

Output: (A, B|S) cut of V
distances= uniqueSet({dv} ∪ {dv + sv})
distances= distances \{db}
forall ρ ∈distances ;
do

// derandomize (Section 4.2)

// Find best k-node cut

A = {v ∈ V | dv + sv ≤ ρ} ;
for A
B(cid:48) = V \ {A ∪ b} ; valb(cid:48) = (cid:80)
Pick S ⊆ B(cid:48) with highest (cid:80)
B = V \ {A ∪ S}
weight(A, B|S)= (cid:80)
checkValid (G,w,S) ;
check if S can be added to current JT (Section 5)

a(cid:48)∈A,b(cid:48)∈B w(a(cid:48),b(cid:48))

s∈S vals, |S| ≤ k

// when called recursively, need to

if no ﬁnite-weight valid (A, B|S) exists then return some
∞-cost cut
else return the minimum-weight valid (A, B|S)

1

2
3
4

5

6
7

1
2
3
4
5

6

7

8

9

10

11

12

of a path from u to v is the sum of its edge lengths and
internal node lengths. Under this interpretation, dv is the
distance from vertex a to vertex v (i.e., the minimal path
length). Constraints (3), (4) thus assert that the distance
from a to itself is 0, the distance from a to b is 1.

We pick a radius ρ ∈ [0, 1) uniformly at random, and par-
tition the graph into sets (A, B, S) s.t. a ∈ A, b ∈ B. Let
A = {v | dv + sv ≤ ρ}, the nodes completely inside the
ρ-ball centered at s. S = {v | dv < ρ < dv + sv} are the
nodes cut by ρ. B are the rest, ρ ≤ dv (see Figure 3).

Proposition 4.3 The expected cost of roundCut is at most
OPTLP, the optimal objective value of the LP relaxation:
Eρ(weight of cut) ≤ (cid:80) we · ce = OPTLP ≤ OPTIP, where
OPTIP is the optimal value of the integer program. The
expected size of S is at most k: Eρ(|S|) ≤ (cid:80) su = k

of ρ typically the cut cost would be higher, or S would be
too big, so we need to ﬁnd a Pareto choice. Fortunately,
we have a derandomization scheme that provides the entire
approximate Pareto frontier. We note that although ρ can
take any value in [0, 1), there are only 2n critical points:
transitions happen only around {dv}, {dv + sv}. By the
probabilistic method, if we try each one of those radii we
will ﬁnd a solution with either |S| ≤ k, or value ≤ OPTLP.
This frontier can allow us to regularize, if we wanted to
and pick a smaller (or even larger) S. However, in this pa-
per, we are only interested in |S| ≤ k. Therefore, we only
explore that part of the frontier, and pick the weakest cut.

A variant of this derandomization scheme (Alg. 2) guaran-
tees that |S| ≤ k. Given a radius ρ, compute A as before.
By deﬁnition, S is a subset of A’s neighbours. Each neigh-
bour b can either belong to S or to B. If b ∈ B, we pay for
all the edges between b and A. If b ∈ S, we do not need
to pay for these edges. We compute the weight of edges
between A and b, valb, and pick up to k nodes greedily for
S. This results in the optimal separator |S| ≤ k for this A.

Claim 4.4 The greedy derandomization scheme is guaran-
teed to ﬁnd a cut that does not weight more than any cut
(Aρ, Bρ|Sρ) generated by some sampled ρ s.t. |Sρ| ≤ k.

Now we have a procedure for ﬁnding a partition of the
graph, where both sides A, B are connected by weak edges.
We can use S as a separator in our JT, and recurse on both
sides of the partition – A∪S, B∪S, stopping when the set is
of size k + 1. However, this simple approach will not work.
The cliques and separators generated might not guarantee
the existence of a k-JT: if we add an edge between every
two variables which appear in the same clique, the Markov
network generated might have a treewidth higher than k.
By deﬁnition, this means that the cliques and separators
cannot be combined to form a k-JT.

See Figure 4 for an example. We try to ﬁnd a 2-JT over
10 variables: 1,..,4,a,..,f. Suppose in the ﬁrst 6 steps the
algorithm chooses a unique pair in {1, 2, 3, 4} for S, and
splits one of a,...,f from the rest. After six steps, the clique
{1, 2, 3, 4} is too big, but cannot be split anymore; if we
split any two variables that have previously appeared in a
separator, we lose the running intersection property.

Recall
the problem deﬁnition (4.2). We are given a
weighted clique; our goal is to remove a subset of the
edges, such that the resulting graph has a low treewidth,
and the weight of the removed edges is minimal. At the be-
ginning, all of the edges are undecided. When we choose
a cut (A, B|S), we commit to removing edges between A
and B, and keeping all the internal edges of S.

a(cid:48)∈A w(a(cid:48),b(cid:48)) ∀b(cid:48) ∈ B(cid:48)

5 Recursive Procedure

Proof sketch: P (e in cut) ≤ ((ρ + ce) − ρ) = ce.
P (u ∈ S) ≤ ((ρ + su) − ρ) = su. Proposition 4.3 pro-
vides a guarantee in expectation. However, for each value

In other words, the set of edges E can be partitioned into
three subsets of edges, E+ ∪ E− ∪ E? : edges we commit
to keep, edges we remove, and edges that are undecided.

Shahaf, Chechetka, Guestrin

Figure 4: A sequence of six separation steps (left to right) re-
sulting in cliques that cannot be made into a JT of treewidth 2.

Our algorithm successively picks cuts in the graph. We
start with E? = E, and as the algorithm proceeds it moves
edges from E? into E+, E− until eventually E? = ∅.

In general, there are two situations that might prevent us
from getting a k-JT: We can end up with E+ ∩ E− (cid:54)= ∅,
i.e. a separator formed in the earlier part of the recursion
is cut in a latter step, which violates the running intersec-
tion property (Sep-RIP). Alternatively, the treewidth of the
graph induced by E+ may be larger than k (High-TW).

In order to ensure the running intersection property for sep-
arators (Sep-RIP), after choosing a separator we make it
into an ∞-weighted clique (see ﬁndJT ). This way, ver-
tices of S might be used in other separators, but splitting
them would cost ∞. Therefore, any ﬁnite-weight cut does
not split previous separators; in other words, E+ ∩E− = ∅.

Claim 5.1 After choosing a separator S for the JT, S will
not be split in latter iterations.

In order to solve the High-TW problem, we need to look
at the graph induced by E+. By construction, those are
exactly the ∞-edges.

Claim 5.2 Let G∞ = (V, E(cid:48)), such that E(cid:48) = {e ∈ E |
we = ∞} (the ∞-edges subgraph, E+). If the treewidth of
G∞ is ≤ k throughout the (recursive) computation, ﬁndJT
returns a k-JT.

Note that we cannot allow the treewidth of G∞ to exceed
k at any stage of the algorithm. As E+ can only grow, the
treewidth can only increase from that point.

Claim 5.3 If the treewidth of G∞ is ≤ k and E? (cid:54)= ∅,
there exists a ﬁnite-weight cut (A, B|S) which keeps the
treewidth of G∞ at most k and reduces the size of E?.

If the treewidth of G∞ is ≤ k, there is a triangulation where
all separators are of size ≤ k. Choosing S from this tri-
angulation maintains the treewidth of G∞ at most k. For
example, refer to Figure 4 again: after picking the last sepa-
rator, {3, 4}, G∞ had treewidth 4. Looking at the triangula-
tion in the previous step, we see cliques {1, 2, 3}, {1, 2, 4}
and separator {1, 2}. And indeed, picking S = {1, 2} and
splitting 3 from 4 leads to a valid 2-JT.

In order to ﬁnd the cut guaranteed by Claim 5.3, we use
the min-ﬁll heuristics to bound the treewidth of G∞ from

Algorithm 3: ﬁndJT (G, w, k)

Input: G = (V, E): undirected graph, w: weight function,

k: max separator size.

Output: a list of cliques that can form a k-JT
if G has several connected components then recurse on
each, return
if |V | ≤ k + 1 then record V in the JT. return
(A, B, S) = ﬁndSeparator (G, w, k)
if S has ∞-weight ; // no ﬁnite separator found
then

Maintain Claim 5.2: Find valid S using min-ﬁll heuristic
run min-cuts to ﬁnd corresponding A and B

1

2
3
4
5
6
7

8
9
10

Change G, w such that S is a clique with ∞-weighted edges
ﬁndJT (G[A ∪ S], w, k, S) ; // got a valid S, recurse
ﬁndJT (G[B ∪ S], w, k, S)

Algorithm 4: checkValid (G, w, S)

Input: G = (V, E): undirected graph, w: weight function,

k: max separator size, S: potential separator.

Output: Does there still exist a k-JT if we add S?
Change G, w such that S is a clique with ∞-weighted edges
G(cid:48) := G∞ = (V, E(cid:48)) , E(cid:48) = {e ∈ E | we = ∞}
k(cid:48) =Use min-ﬁll heuristic to bound tree-width of G(cid:48)
if k(cid:48) ≤ k, return true else return false

1

2

3

4

above (see Alg. 4). We can use any other triangulation
method insead of min-ﬁll.
If the LP separator violates
High-TW, we drop it and look for another one (e.g. con-
straining the LP to use at most |S| − 1 of S’s vertices,
(cid:80)

v∈S sv ≤ |S| − 1).

If we were to solve the IP, ﬁndJT would be guaranteed to
ﬁnd a ﬁnite weight separator at every step, and thus a k-
JT. However, the rounding may not ﬁnd a ﬁnite cut, even
if one exists. If ﬁndSeparator returns an inﬁnite cut, we
discard it and instead use a separator we got from min-ﬁll
in the previous iteration. A, B are obtained using the min-
cuts algorithm. This situation seems to arise very rarely in
practice.

Proposition 5.4 ﬁndJT is guaranteed to ﬁnd a k-JT.

5.1 Objective Function

Our objective function in Section 4 minimizes the total
weight of removed edges. Since we only handled ﬁnding
a single cut, all of the edges not removed were assumed to
stay in the graph. However, this is no longer the case: the
undecided edges E? may be removed later. For this reason,
we suggest changing the objective function, taking into ac-
count both the edges we remove and the edges we keep:

(cid:32)

(cid:88)

e∈E

min

we · ce −

we · insepe

(cid:88)

e∈E.we<∞

(cid:33)

The ﬁrst component of the sum is the previous objective:
we is the edge weight and ce indicates a cut-edge. insepe
are new variables indicating that edge e is an internal edge

1,2,34,a,b,c,d,e,f1,2,3,4,b,c,d,e,f1,2,a1,21,2,3,4,c,d,e,f1,3,b1,31,2,a1,21,23,41,3,b1,31,2,a1,22,3,d2,31,4,c1,42,4,e2,43,4,f3,4…t=0126Learning Thin Junction Trees via Graph Cuts

of S. We achieve this by adding constraint (6): insepe ≤
su, insepe ≤ sv. The edges with insepe = 1 are those
with both ends in S, i.e. exactly the edges which are added
to E+. We need to be careful not to double count edges that
are already in E+, so we sum over ﬁnite-weight edges only.
The second component of the new objective is therefore the
total weight of newly added edges (alternatively, we can
also add a tradeoff parameter λ between the sums).

This objective still minimizes the weight of E− (the sum
of weights of E− and E+ is ﬁxed). By taking into account
the edges of the separator, the new objective also has the
advantage of picking strongly-connected separators.

5.2 Running Time and Complexity

Claim 5.5 ﬁndJT is polynomial in n and k.

The size of the graphs G passed to ﬁndSeparator decreases
with every iteration; therefore, the number of recursive
calls is linear in n. ﬁndSeparator calls the LP solver k + 1
times. Each LP involves O(n2) variables and constraints,
and thus is solved in polytime. Rounding the cut is again
polynomial in n (we consider 2n radii). Thus, the total run-
ning time is polynomial in n and k.

We note that rounding is a fast process in practice: very
often the LP solver returns an integer solution, or one with
few distinct values, so the number of radii we need to con-
sider is much less than 2n. Also, note the LP solution is
always non-negative. When the LP solver returns a zero-
cost solution, we do not need to continue looking for more
pairs of vertices a, b to separate.

5.3 A Note on Regularization

Regularization is often used for estimation of sparse mod-
els in order to avoid overﬁtting. We considered several
types of regularization. As described, ﬁndJT learns max-
imal JTs (all separators are of size k). However, smaller
cliques and separators lead to simpler models.
Instead
of stopping when clique size reaches k + 1, we can let
ﬁndSeparator split the clique further, if the cost is small,
as deﬁned by some threshold. A drawback of this tech-
niques is the assumption that each node has the same
amount of regularization (in our experiments, this constant
was set by cross validation).

Secondly, we deﬁne a multiplicative-ratio threshold: after
computing the best k-separator S, pick the smallest separa-
tor that costs at most 1 + (cid:15) times the cost of S.

Finally, we can restrict our models further by reducing the
number of parameters. Instead of learning full clique po-
tentials for the resulting JT, we can assume, e.g., a pairwise-
Markov structure and learn its parameters. Regularization
reduced overﬁtting, especially on small datasets.

6 Experimental Results

We have used ﬁndJT to learn various low-treewidth graph-
ical models, from synthetic and real-world datasets.

6.1 Utility of Approximation

Approximate inference is a common solution when the
treewidth is too large. In contrast, our approach in this pa-
per is to perform exact inference on an approximate model.
In this section we compare these approaches.

We have generated several artiﬁcial networks with a
relatively-large treewidth (between 10 and 30).
The
treewidth was picked so that exact inference on the model
takes a long time, but is still feasible. We compared
ﬁndJT to a state-of-the-art structure learner for compact
BN (order-based search, OBS). First, we computed log
likelihood for corresponding test sets (Figure 5a). Our al-
gorithm did slightly worse, but comparable to OBS. This
is not a surprise, since OBS directly tries to minimize like-
lihood. In addition, it does not try to learn a thin junction
tree, and thus has less constraints (indeed, the models that
OBS learned had high treewidth).

The real goal of structure learning is the ability to run in-
ference efﬁciently and accurately. Therefore, as another
way to evaluate the quality of our learned models, we have
compared the quality of inference. We ﬁxed a random sub-
set of variables as evidence, and conditioned the models
on them. We ran (1) exact inference on the thin models
we learned, and (2) fast approximate inference algorithm
(residual loopy belief-propagation) on the OBS model. We
compared those to the ground truth, exact inference on the
real model Pt. For each variable v we computed the KL-
Divergence from v’s ground truth conditional to the cor-
responding conditional in the approximate model Pa. We
averaged over the variables:

Scorea = 1
n

(cid:80)

v DKL(Pt(v|evidence)||Pa(v|evidence))

We let loopy OBS+BP run for the same time our algorithm
took, which was usually a few minutes (note that our imple-
mentation is in Matlab, while loopy BP is in C++). Refer
to Figure 5ab for the results. Each point represents a single
network: the x axis is the score of our algorithm, and y –
of approximate inference. Most points are above the x = y
line, or very close to it, meaning our algorithm was closer
to the ground truth.

6.2 Classiﬁcation Accuracy

In addition to the synthetic datasets, we have tested ﬁndJT
on several real-world domains. As baselines for compar-
ison, we used LPACJT (Chechetka and Guestrin [2008]),
and algorithms of Karger and Srebro [2001] (denoted KS)
and Teyssier and Koller [2005] (denoted OBS), all repro-
duced from Chechetka and Guestrin [2008]. The necessary
entropies were cached in advance. KS and LPACJT learn

Shahaf, Chechetka, Guestrin

Figure 5: (a),(b): Likelihood and KL-divergence for our algorithm vs. OBS+loopy BP on synthetic data.
lh experiments. (c),(d):
runtime comparison, vs. separator size and domain size. Lightening signs indicate sudden death of the algorithm, usually running out
of memory. (e)-(g): llh experiments on real data. Cuts+Local is running local search from the structure found by the Cut algorithm.
Cuts+Comb is the algorithm with the combined objective of Section 5.1. (h) classiﬁcation accuracies, same experiment.

k-JTs, and OBS learns compact Bayes nets with at most k
parents for every variable. The datasets we used were:

Trafﬁc measured every 5 minutes in 32 locations in Cali-
fornia for a month Krause and Guestrin [2005]. Discretized
into 4 bins, learned models of treewidth 3.

Temperature 2-month deployment of 54 sensors (15K dat-
apoints) Deshpande et al. [2004]. Discretized into 4 bins,
learned models of treewidth 2. The sensor locations have
an ∞-like shape, so learning a thin JT is hard.

Alarm Discrete data sampled from a known Bayesian net-
work Beinlich et al. [1988], treewidth 4. Due to com-
putational limitations of other methods, we only learned
treewidth 3 models.

Our method ran signiﬁcantly faster than others, taking no
more than a few minutes. Algorithms of O(nk) complex-
ity could not handle separators larger than 3 on the full
datasets, or took hours to complete. Refer to Figure 5cd for
running-time comparisons between the algorithms Another
interesting point illustrated in Figure 5 is that our algorithm
sometimes takes less time on larger k’s: the number of LP
calls increases per iteration of ﬁndSeparator , but the num-
ber of iterations is smaller.

We started by comparing log-likelihoods. The results are
shown in Figure 5efg: ﬁndJT does well on Trafﬁc, but not
as well as OBS on Temperature and Alarm. This is reason-
able, because OBS learns compact BNs. BNs allow com-
puting likelihood exactly, so the structure may look very
good before inference. Like before, we tested classiﬁca-
tion accuracy. The real structure of Trafﬁc and Temper-
ature is unknown, so we had no ground truth to compare
to. Instead, we ﬁxed a random subset of variables as ev-
idence, E; for each test point xj, we computed the MPA,
given the evidence variables xj[E]. The accuracy is the ra-
tio of variables we classiﬁed correctly. We compared the

classiﬁcation accuracies of our model and the OBS model.
The results are shown in Figure 5h. Despite the likelihood
results, our accuracy is comparable or better in most cases.

7 Structure Learning for Conditional

Random Fields

CRFs Lafferty et al. [2001] are undirected graphical models
that compactly represent conditional distributions, P (y |
x) (x: observations, y: labels). Learning CRFs is a harder
than learning Bayes Nets; even scoring a structure re-
quires inference. Therefore, most work on CRFs assumes a
known structure, often a linear chain or a 2D lattice. There
has been very little work on learning CRF’s topology from
data. A lot of the previous work focuses on special cases,
or different objectives. A notable exception is Schmidt
et al. [2008], who proposed a method for jointly learning
the (sparse) structure and parameters of CRFs, formulating
these tasks as a convex optimization problem. They con-
sidered block-L1 regularization and found the global min-
imum of the resulting pseudo-likelihood objective. We ex-

Figure 6: Left: error rate in CRF learning. Right: a learned
model. Blue edges appear only in the learned model (due to trian-
gulation), red – only in the real model.

tended our algorithm to CRFs. We assume potentials are
of the form f (Yi, Yj, X). Initial edge-weights are condi-
tional mutual information, evaluated by logistic regression

-65-60-55-50-45-40-35-30-25-2050500Log likelihoodTraining set sizeTraffic: Log LikelihoodKSLPACJTOBSCutsCuts-CombCuts-75-70-65-60-55-50-45-40100100010000Log likelihoodTraining set sizeTemperature: Log LikelihoodKSLPACJTOBSCutsCuts+localCuts +localCuts-31-29-27-25-23-21-19-17-15505005000Log likelihoodTraining set sizeAlarm: Log LikelihoodKSLPACJTOBSCutsCuts-CombCuts0.020.040.060.080.10.120.030.050.070.090.11OBS distanceCuts distanceKL-Divergence to Ground TruthCutsbetterOBS better-41-36-31-26-21-41-36-31-26-21Cuts llhOBS+BP llhLogLikelihood on Synthetic DataCutsbetterOBS+BP better11010010001000012345Running Time (sec)Separator SizeRunning Time vs. Separator SizeOBSLPACJTCutLocal0.111010010001000001020304050Running Time (sec)#VariablesRunning Time vs. Domain SizeLPACJTCutLocalOBSCutsbetterOBS+BPbetterOBS AccuracyCut AccuracyCutsbetterCO betterClassification Accuracy on Real-World Datasets(a)(b)(c)(d)(e)(f)(g)(h)-65-60-55-50-45-40-35-30-25-2050500Log likelihoodTraining set sizeTraffic: Log LikelihoodKSLPACJTOBSCutsCuts-CombCuts-75-70-65-60-55-50-45-40100100010000Log likelihoodTraining set sizeTemperature: Log LikelihoodKSLPACJTOBSCutsCuts+localCuts +localCuts-31-29-27-25-23-21-19-17-15505005000Log likelihoodTraining set sizeAlarm: Log LikelihoodKSLPACJTOBSCutsCuts-CombCuts0.020.040.060.080.10.120.030.050.070.090.11OBS distanceCuts distanceKL-Divergence to Ground TruthCutsbetterOBS better-41-36-31-26-21-41-36-31-26-21Cuts llhOBS+BP llhLogLikelihood on Synthetic DataCutsbetterOBS+BP better11010010001000012345Running Time (sec)Separator SizeRunning Time vs. Separator SizeOBSLPACJTCutLocal0.111010010001000001020304050Running Time (sec)#VariablesRunning Time vs. Domain SizeLPACJTCutLocalOBSCutsbetterOBS+BPbetterOBS AccuracyCut AccuracyCutsbetterCO betterClassification Accuracy on Real-World Datasets(a)(b)(c)(d)(e)(f)(g)(h)Learning Thin Junction Trees via Graph Cuts

for every pair of Y ’s.
In order to compare our structure
learning to Schmidt et al. [2008], we have generated sev-
eral synthetic CRFs, whose size ranges from 8 to 100. To
obtain a fair comparison, we used their parameter-learning
code on our structures as well. That is, we ran Schmidt’s
algorithm twice: once for learning a structure and parame-
ters, and one with a ﬁxed structure that we learned. Figure
6 compares the error rate of the approaches. Our algorithm
wins for all structures, although it does not try to minimize
that error measure. One of those structures learned is also
shown in Figure 6. Edges are false positive (due to triangu-
lation), false negative, or true.

8 Relation to prior work

Except for learning the most likely JT of treewidth 1, which
can be solved in O(n3) time (Chow and Liu [1968]), the
problem of learning an optimal junction tree of treewidth
at most k is NP-complete for most formulations Karger
and Srebro [2001]. It is thus not surprising that the exist-
ing algorithms for learning limited-treewidth JTs that pro-
vide any theoretical guarantees on the quality of the solu-
tion (Karger and Srebro [2001], Chechetka and Guestrin
[2008]) have complexity at least O(nk) and are usually not
practical for k > 3. Other popular approaches are based
on local search (Bach and Jordan [2002]), and only guar-
antee the resulting structure to be a local optimum. Our
approach is as fast as the local search-based algorithms and
provides an important advantage: our algorithm selects an
entire separator at every time step, while most existing lo-
cal approaches take a more myopic view and only add a
single edge.

Recently, Lowd and Domingos [2008] have proposed
learning arithmetic circuits, a class of models that does not
necessarily have low treewidth, but still admit tractable ex-
act inference. Their approach, too, has only local guaran-
tees on the quality of the result.

9 Conclusions and Future Work

Learning high-treewidth models requires using approxi-
mate inference methods, which can yield unreliable results.
In this paper, we have proposed to learn instead simpler
models which allow exact inference. We have proposed a
new method for learning bounded-treewidth junction trees,
an attractive class of probabilistic graphical models that
permit both compact representation and efﬁcient exact in-
ference. By employing Bethe approximation of the model
entropy, we have formulated the problem of selecting an
optimal junction tree separator in terms of ﬁnding a low-
weight cut in a graph. We presented a fast randomized algo-
rithm with theoretical guarantees for ﬁnding good separa-
tors and showed how to combine the separators to obtain a
low-treewidth model. We have also extended our approach
to learning the structure of discriminative models (CRFs).
In addition to theoretical guarantees, we have demonstrated

empirically that our approach produces high-quality results
while being faster than the state of the art methods.

An interesting future work direction is extending our tech-
niques to other model formalisms, such as arithmetic cir-
cuits (Lowd and Domingos [2008]) and relational net-
works (Getoor et al. [2001]). We also plan to exploring
more general approximations, e.g. Kikuchi (Yedidia et al.
[2000]). Our algorithm is currently limited to testing pair-
wise mutual information; taking higher entropies into ac-
count might improve the likelihood. In the future, we plan
to enhance our algorithm to handle this case, perhaps by
considering hyperedge-cuts. We believe that our general
approach can be naturally extended along both of the above
directions, which will lead to wider applicability of the al-
gorithm and better quality of the models.

Acknowledgements: This work is supported in part
by the ARO under MURI W911NF0710287 and
W911NF0810242, and by NSF Career
IIS-0644225.
We wish to thank Anupam Gupta for helpful discussions.

References

F. R. Bach and M. I. Jordan. Thin junction trees. In NIPS, 2002.

C. Chow and C. Liu. Approximating discrete probability distribu-
tions with dependence trees. IEEE Transactions on Informa-
tion Theory, 1968.

D. Karger and N. Srebro. Learning Markov networks: Maximum

bounded tree-width graphs. In SODA, 2001.

A. Chechetka and C. Guestrin. Efﬁcient principled learning of

thin junction trees. In NIPS. 2008.

J. Yedidia, W. Freeman, and Y. Weiss. Bethe free energy, Kikuchi

approximations, and belief propagation algorithms, 2000.

J. Lafferty, A. McCallum, and F. Pereira. Conditional random
ﬁelds: Probabilistic models for segmenting and labeling se-
quence data. In ICML, 2001.

R. Cowell, P. Dawid, S. Lauritzen, and D. Spiegelhalter. Proba-

bilistic Networks and Expert Systems. 2003.

M. Teyssier and D. Koller. Ordering-based search: A simple and
In UAI,

effective algorithm for learning Bayesian networks.
2005.

A. Krause and C. Guestrin. Near-optimal nonmyopic value of

information in graphical models. In UAI, 2005.

A. Deshpande, C. Guestrin, S. Madden, J. Hellerstein, and
W. Hong. Model-driven data acquisition in sensor networks.
In VLDB, 2004.

I. Beinlich, J. Suermondt, M. Chavez, and G. Cooper. The
ALARM monitoring system: A case study with two probab-
In European
listic inference techniques for belief networks.
Conference on Artiﬁcial Intelligence in Medicine, 1988.

M. Schmidt, K. Murphy, G. Fung, and R. Rosales. Structure learn-
ing in random ﬁelds for heart motion abnormality detection. In
CVPR, 2008.

D. Lowd and P. Domingos. Learning arithmetic circuits. In UAI,

2008.

L. Getoor, N. Friedman, D. Koller, and B. Taskar. Learning prob-

abilistic models of relational structure. In ICML, 2001.

