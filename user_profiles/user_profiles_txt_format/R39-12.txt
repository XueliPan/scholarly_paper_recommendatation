The StageNet Fabric for Constructing Resilient

Multicore Systems

Shantanu Gupta

Shuguang Feng

Amin Ansari

Jason Blome

Scott Mahlke

Advanced Computer Architecture Laboratory

University of Michigan
Ann Arbor, MI 48109

{shangupt, shoe, ansary, jblome, mahlke}@umich.edu

ABSTRACT
Scaling of CMOS feature size has long been a source of dramatic
performance gains. However, the reduction in voltage levels has not
been able to match this rate of scaling, leading to increasing oper-
ating temperatures and current densities. Given that most wearout
mechanisms that plague semiconductor devices are highly depen-
dent on these parameters, signiﬁcantly higher failure rates are pro-
jected for future technology generations. Consequently, high relia-
bility and fault tolerance, which have traditionally been subjects of
interest for high-end server markets, are now getting emphasis in
the mainstream desktop and embedded systems space. The popular
solution for this has been the use of redundancy at a coarse gran-
ularity, such as dual/triple modular redundancy. In this work, we
challenge the practice of coarse-granularity redundancy by identi-
fying its inability to scale to high failure rate scenarios and investi-
gating the advantages of ﬁner-grained conﬁgurations. To this end,
this paper presents and evaluates a highly reconﬁgurable multicore
architecture, named StageNet (SN), that is designed with reliability
as its ﬁrst class design criteria. SN relies on a reconﬁgurable net-
work of replicated processor pipeline stages to maximize the use-
ful lifetime of a chip, gracefully degrading performance towards
the end of life. Our results show that the proposed SN architec-
ture can perform nearly 50% more cumulative work compared to a
traditional multicore.

1.

INTRODUCTION

Technological trends into the nanometer regime have lead to in-
creasing current and power densities and rising on-chip tempera-
tures, resulting in both increasing transient, as well as permanent,
failures rates. Leading technology experts have warned designers
that device reliability will begin to deteriorate from the 65nm node
onward [8]. Current projections indicate that future microproces-
sors will be composed of billions of transistors, many of which will
be unusable at manufacture time, and many more which will de-
grade in performance (or even fail) over the expected lifetime of
the processor [10]. In an effort to assuage these concerns, industry
has initiated a shift towards multicore and GPU inspired designs
that employ simpler cores to limit the power and thermal envelope
of the chips [42, 21, 23]. However, this paradigm shift also leads
towards core designs that have little inherent redundancy and are
therefore incapable of performing the self-repair possible in big su-
perscalar cores [32, 38]. Thus, in the near future, architects must
directly address reliability in computer systems through innovative
fault-tolerance techniques.

The sources of computer system failures are widespread, rang-
ing from transient faults, due to energetic particle strikes [47] and
electrical noise [43], to permanent errors, caused by wearout phe-
nomenon such as electromigration [13] and time dependent dielec-

tric breakdown [46].
In recent years, industry designers and re-
searchers have invested signiﬁcant effort in building architectures
resistant to transient faults [33, 33, 34, 44].
In contrast, much
less attention has been paid to the problem of permanent faults,
speciﬁcally transistor wearout due to the degradation of semicon-
ductor materials over time. Traditional techniques for dealing with
transistor wearout have involved extra provisioning in logic cir-
cuits, known as guard-banding, to account for the expected per-
formance degradation of transistors over time. However, the in-
creasing degradation rate projected for future technology genera-
tions implies that traditional margining techniques will be insufﬁ-
cient.

The challenge of tolerating permanent faults can be broadly di-
vided into three requisite tasks: fault detection, fault diagnosis, and
system recovery/reconﬁguration. Fault detection mechanisms [9,
26, 5] are used to identify the presence of a fault, while fault di-
agnosis techniques [25, 15, 12] are used to determine the source
of the fault, i.e. the broken component(s). System recovery needs
to leverage some form of a spatial or temporal redundancy to keep
the faulty component isolated from the design. As an example,
many computer vendors provide the ability to repair faulty mem-
ory and cache cells through the inclusion of spare memory ele-
ments [36]. Recently, researchers have begun to extend these tech-
niques to support sparing for additional on-chip resources [38],
such as branch predictors [11] and registers [32]. The granularity
at which spares/redundancy is maintained determines the number
of failures a system can tolerate. The focus of this work is to un-
derstand the issues associated with system recovery and to design a
fault tolerant architecture that is capable of tolerating a large num-
ber of failures.

Traditionally, system recovery in high-end servers and mission
critical systems has been addressed by using mechanisms such as
dual and triple-modular redundancy (DMR and TMR) [7, 36]. How-
ever, such approaches are too costly and therefore not applicable to
desktop and embedded systems. With the recent popularity of mul-
ticore systems, these traditional core-level approaches have been
able to leverage the inherent redundancy present in large chip mul-
tiprocessors (CMPs) [35, 1, 39]. However, both the historical de-
signs and their modern incarnations, because of their emphasis on
core-level redundancy, incur high hardware overhead and can only
tolerate a small number of defects. With the increasing defect rate
in semiconductor technology, it will not be uncommon to see a
rapid degradation in throughput for these systems as single device
failures cause entire cores to be decommissioned, often times with
the majority of the core still intact and functional.

In contrast, this paper argues the case for reconﬁguration and
redundancy at a ﬁner granularity. To this end, this work presents
the StageNet (SN) fabric, a highly reconﬁgurable and adaptable

978-1-4244-2837-3/08/$25.00 ©2008 IEEE141multicore computing substrate. SN is a multicore architecture de-
signed as a network of pipeline stages, rather than isolated cores in
a CMP. The network is formed by replacing the direct connections
at each pipeline stage boundary by a crossbar switch interconnec-
tion. Within the SN architecture, pipeline stages can be selected
from the pool of available stages to act as logical processing cores.
A logical core in the StageNet architecture is referred to as a Sta-
geNetSlice (SNS). A SNS can easily isolate failures by adaptively
routing around faulty stages. The interconnection ﬂexibility in the
system allows SNSs to salvage healthy stages from adjacent cores
and even makes it possible for different SNSs to time-multiplex
a scarce pipeline resource. Because of this added ﬂexibility, a
SN system possesses inherent redundancy (through borrowing and
sharing pipeline stages) and is therefore, all else being equal, ca-
pable of maintaining higher throughput over the duration of a sys-
tem’s life compared to a conventional multicore design. Over time
as more and more devices fail, such a system can gracefully de-
grade its performance capabilities, maximizing its useful lifetime.
The reconﬁguration ﬂexibility of the SN architecture has a cost
associated with it. The introduction of network switches into the
heart of a processor pipeline will inevitably lead to poor perfor-
mance due to high communication latencies and low communica-
tion bandwidth between stages. The key to creating an efﬁcient SN
design is re-thinking the organization of a basic processor pipeline
to more effectively isolate the operation of individual stages. More
speciﬁcally, inter-stage communication paths must either be re-
moved, namely by breaking loops in the design, or the volume of
data transmitted must be reduced. This paper starts off with the
design of an efﬁcient SNS (a logical StageNet core) that attacks
these problems and reduces the performance overhead from net-
work switches to an acceptable level. It further presents the com-
plete SN architecture that stitches together multiple such SNSs to
form a highly reconﬁgurable architecture capable of tolerating a
large number of failures. In this work, we take a simple in-order
core design as the basis of the SN architecture. This is motivated
by the fact that thermal and power considerations are pushing de-
signs towards simpler cores. Furthermore, they are being adopted
by designs targeting massively multicore chips [42, 24] and are
suitable for low-latency and high throughput applications [23, 21].
At the same time, we believe that the proposed design methodol-
ogy of ﬁne-grained reconﬁguration can also be effectively applied
to deeper and more aggressive pipeline designs.

The primary contributions of this paper include: 1) A design
space exploration of reconﬁguration granularities for resilient sys-
tems; 2) Design and evaluation of a StageNetSlice, a networked
pipeline microarchitecture; and 3) Design and evaluation of Sta-
geNet, a resilient multicore architecture, composed using multiple
SNSs.

2. RECONFIGURATION GRANULARITY
For tolerating permanent faults, architectures must have the abil-
ity to reconﬁgure, where reconﬁguration can refer to a variety of ac-
tivities ranging from decommissioning non-functioning, non-critical
processor structures to swapping in cold spare devices.
In a re-
conﬁgurable architecture, recovery entails isolating defective mod-
ule(s) and incorporating spare structures as needed. Support for
reconﬁguration can be achieved at various granularities, from ultra-
ﬁne grain systems that have the ability to replace individual logic
gates to coarser designs that focus on isolating entire processor
cores. This choice presents a trade-off between complexity of im-
plementation and potential lifetime enhancement. This section shows
experiments studying this trade-off and draws upon these results to
motivate the design of the SN architecture.

(a) Overlay of ﬂoorplan

OR1200 Core
Area
Power
Clock Frequency
Data Cache Size
Instruction Cache Size
Technology Node

1.28 mm2
92.22 mW
200 MHz
8 KB
8 KB
130 nm

(b) Implementation details

Figure 1: OpenRisc 1200 embedded microprocessor.

2.1 Experimental Setup

In order to effectively model the reliability of different designs,
a Verilog model of the OpenRISC 1200 (OR1200) core [27] was
used in lifetime reliability experiments. The OR1200 is an open-
source core with a conventional 5-stage pipeline design, represen-
tative of commercially available embedded processors. The core
was synthesized, placed and routed using industry standard CAD
tools with a library characterized for a 130nm process. The ﬁnal
ﬂoorplan along with several attributes of the design is shown in
Figure 1.

To study the impact of reconﬁguration granularity on chip life-
times, the mean-time-to-failure (MTTF) was calculated for each in-
dividual module in the OR1200. MTTF was determined by estimat-
ing the effects of a common wearout mechanism, time-dependent
dielectric breakdown (TDDB) on a OR1200 core running a rep-
resentative workload. Employing an empirical model similar to
that found in [37], Equation 1 presents the formula used to calcu-
late per-module MTTFs. The temperature numbers for the modules
were generated using HotSpot [19].

M T T FT DDB ∝ (

1
V )(a−bT )e

(X+

Y
T +ZT )
kT

(1)

where V = operating voltage, T = temperature, k = Boltzmann’s
constant, and a,b,X,Y,and Z are all ﬁtting parameters based on [37].

2.2 Granularity Trade offs

The granularity of reconﬁguration is used to describe the unit
of isolation/redundancy for modules within a chip. Various options
for reconﬁguration, in order of increasing granularity, are discussed
below.

1. Gate level: At this level of reconﬁguration, a system can re-
place individual logic gates in the design as they fail. Unfor-
tunately, such designs are typically impractical because they
require both precise fault diagnosis and tremendous overhead
due to redundant components and wire routing area.

2. Module level: In this scenario, a processor core can replace

broken microarchitectural structures such as an ALU or branch
predictor. Such designs have been active topics of research [16,
32]. The biggest downside of this reconﬁguration level is that

142Module-granularity replacement
Stage-granularity replacement
Core-granularity replacement

F
T
T
M
n

 

i
 

e
s
a
e
r
c
n

I
 
t

n
e
c
r
e
P

 700

 600

 500

 400

 300

 200

 100

 0

 0

 50

 100

 150

 200

 250

 300

Percent Area Overhead

Figure 2: Gain in MTTF from the addition of cold spares at
the granularity of micro-architectural modules, pipeline stages,
and processor core. The gains shown are cumulative, and spare
modules are added (denoted with markers) in the order they
are expected to fail.

maintaining redundancy for full coverage is almost imprac-
tical. Additionally, for the case of simple cores, even fewer
opportunities exist for isolation since almost all modules are
unique in the design.

3. Stage level: Here, the entire pipeline stages are treated as sin-
gle monolithic units that can be replaced. Reconﬁguration
at this level is challenging because: 1) pipeline stages are
tightly coupled with each other (reconﬁguration can cause
performance loss), and 2) cold sparing pipeline stages is ex-
pensive (area overhead).

4. Core level: This is the coarsest level of reconﬁguration where
entire processor cores are isolated from the system in the
event of a failure. Core level reconﬁguration has also been
an active area of research [39, 1], and from the perspective
of a system designer, it is probably the easiest technique to
implement. However, it has the poorest returns in terms of
lifetime extension, and therefore might not be able to keep
up with increasing defect rates.

While multiple levels of reconﬁguration granularity could be ut-
lized, Figure 2 demonstrates the effectiveness of each applied in
isolation (gate-level reconﬁguration was not included in this study).
The ﬁgure shows the potential for lifetime enhancement (measured
as MTTF) as a function of how much area a designer is willing to
allocate to cold spares. The MTTF of a n-way redundant structure
is taken to be n times its base MTTF. And, the MTTF of the overall
system is taken to be the MTTF of the fastest failing module in the
design. This is similar to the serial model of failure used in [37].
The ﬁgure overlays three separate plots, one for each level of re-
conﬁguration. The redundant spares were allowed to add as much
as 300% area overhead.

The data shown in Figure 2 demonstrates that going towards
ﬁner-grain reconﬁguration is categorically beneﬁcial as far as gains
in MTTF are concerned. But, it overlooks the design complexity
aspect of the problem. Finer-grain reconﬁguration tends to exacer-
bate the hardware challenges for supporting redundancy, e.g. mux-
ing logic, wiring overhead, circuit timing management, etc. At the
same time, very coarse grained reconﬁguration is also not an ideal
candidate since MTTF scales poorly with the area overhead. There-
fore, a compromise solution is desirable, one that has manageable
reconﬁguration hardware and a better life expectancy.

2.3 Implications on StageNet

Stage level reconﬁguration is positioned as a good candidate for
system recovery as it scales well with the increase in area avail-
able for redundancy (Figure 2). Logically, stages are a convenient
boundary because pipeline architectures divide work at the level of
stages (e.g., fetch, decode, etc.). Similarly, in terms of circuit im-
plementation, stages are an intuitive boundary because data signals
typically get latched at the end of every pipeline stage. Both these
factors are helpful when reconﬁguration is desired with a minimum
impact on the performance. However, there are two major obsta-
cles that must be overcome before stage level reconﬁguration is
practical:

1. Pipeline stages are tightly coupled with each other and are

therefore difﬁcult to isolate/replace.

2. Maintaining spares at the pipeline stage granularity is very

area intensive.

One of the ways to allow stage level reconﬁguration is to decou-
ple the pipeline stages from each other. In other words, remove
all direct point-to-point communication between the stages and re-
place them by a switch based interconnection network (Figure 8).
We call this design StageNet (SN). Processor cores within SN are
designed as part of a high speed network-on-a-chip, where each
stage in the processor pipeline corresponds to a node in the net-
work. A horizontal slice of this architecture is equivalent to a log-
ical processor core, and we call it a StageNetSlice (SNS). The use
of switches allows complete ﬂexibility for a pipeline stage at depth
N to communicate with any stage at depth N+1, even those from
a different SNS. The SN architecture overcomes both of the major
obstacles for stage level reconﬁguration. Pipeline stages are decou-
pled from each other, and hence faulty ones can be easily isolated.
Furthermore, there is no need to exclusively devote chip area for
cold sparing. The SN architecture exploits the inherent redundancy
present in a multicore by borrowing/sharing stages from adjacent
cores. As nodes (stages) wearout and eventually fail, SN will ex-
hibit a graceful degradation in performance, and a gradual decline
in throughput.

Along with its beneﬁts, SN architecture has certain area and
performance overheads associated with itself. Area overhead pri-
marily arises from the switch interconnection network between the
stages. And depending upon the switch bandwidth, a variable num-
ber of cycles will be required to transmit operations between stages,
leading to performance penalties. The remainder of this paper in-
vestigates the design of a practical SN architecture starting with a
single SNS.

3. SNS MICROARCHITECTURE

3.1 Overview

The SNS is a basic building block for the SN architecture.

It
consists of a decoupled pipeline microarchitecture that allows con-
venient reconﬁguration at the granularity of stages. As a basis for
SNS design, a simple embedded processor core is used, consisting
of ﬁve stages namely, fetch, decode, issue, execute/memory, and
writeback [3, 4, 27] (see Figure 3(a)). Although the execute/memory
block is sometimes separated into multiple stages, it is treated as a
single stage in this work.

Starting with the basic pipeline design (Figure 3(a)), we will
go through the steps of its transformation into a SNS. As the ﬁrst
step, pipeline latches are replaced with a combination of a cross-
bar switch and buffers. A graphical illustration of the resulting
pipeline design is shown in Figure 3(b). The shaded boxes inside
the pipeline stages are microarchitectural additions that will be dis-
cussed in detail later in this section. To minimize the performance
loss from inter-stage communications, we propose the use of full

143branch feedback

forwarding

register writeback

Fetch

h
c
t
a
l

Decode

Issue

h
c
t
a
l

Execute/Mem

Writeback

h
c
t
a
l

Register

File

h
c
t
a
l

Gen PC

Branch
Predictor

To I$

To D$

(a) A 5-stage in-order pipeline

branch feedback

register writeback

SID

e
l
b
u
o
d

r
e
f
f
u
b

Fetch

Gen PC

Branch
Predictor

To I$

e
l
b
u
o
d

r
e
f
f
u
b

Packer

SID

Decode

e
l
b
u
o
d

r
e
f
f
u
b

e
l
b
u
o
d

r
e
f
f
u
b

Register

File

Scoreboard

SID

Issue

e
l
b
u
o
d

r
e
f
f
u
b

e
l
b
u
o
d

r
e
f
f
u
b

Bypass $

SID

Execute/Mem

e
l
b
u
o
d

r
e
f
f
u
b

To D$

(b) The SNS. Pipeline stages are interconnected using a full crossbar switch. The shaded portions highlight mod-
ules that are not present in a regular pipeline

Figure 3: Traditional in-order pipeline design and SNS.

crossbar switches [28] since a) these allow non-blocking access to
all of their inputs and b) for a small number of inputs and outputs
they are not prohibitively expensive. The full crossbar switches
have a ﬁxed channel width and, as a result, transfer of an instruc-
tion from one stage to the next can take a variable number of cycles.
However, this channel width of the crossbar can be varied to trade-
off performance with area.
In addition to the forward data path
connections, pipeline feedback loops in the SNS also need to go
through similar switches. These are needed because, in the con-
text of the complete SN architecture, different SNSs can share their
stages with each other and thus require an exchange of branch mis-
predict and writeback results. For instance, the result from, say,
SNS A’s execute stage, might need to be directed to SNS B’s is-
sue stage for the writeback. Due to the introduction of the crossbar
switches, the SNS has three fundamental challenges to overcome:
1. Global Communication: Global pipeline stall/ﬂush signals
are fundamental to the functionality of a pipeline. Stall sig-
nals are sent to all the stages for cases such as multi-cycle op-
erations, memory access, and other hazards. Similarly, ﬂush
signals are necessary to squash instructions that are fetched
along mispredicted control paths. In a SNS, all the stages are
decoupled from each other, and global broadcast is infeasi-
ble.

2. Forwarding: Data forwarding is a crucial technique used in a
pipeline for avoiding frequent stalls that would otherwise oc-
cur because of data dependencies in the instruction stream.
The data forwarding logic relies on precisely timed (in an ar-
chitectural sense) communication between execute and later
stages using combinational links. With variable amounts of
delay through the switches, and the presence of intermediate
buffers, forwarding logic within a SNS is not feasible.

3. Performance: Lastly, even if the above two problems are
solved, communication delay between stages is still expected
to result in a hefty performance penalty.

The rest of this section will discuss how the SNS design over-
comes these challenges (Section 3.2) and also propose techniques
that can recover the expected loss in performance (Section 3.3).

3.2 Functional Needs

Stream Identiﬁcation: The SNS pipeline lacks global commu-
nication signals. Without global stall/ﬂush signals, traditional ap-
proaches to ﬂushing instructions upon a branch mispredict are not

applicable. The ﬁrst addition to the basic pipeline, a stream identi-
ﬁcation register, targets this problem.

The SNS design shown in Figure 3(b) has certain components
that are shaded in order to distinguish the ones that are not found
in a traditional pipeline. One of these additional components is
a stream identiﬁcation (sid) register in all the stages. This is a
single bit register and can be arbitrarily (but consistently across
stages) initialized to 0 or 1. Over the course of program execu-
tion, this value changes whenever a branch mispredict takes place.
Every in-ﬂight instruction in a SNS carries a stream id, and this is
used by the stages to distinguish the instructions on the correctly
predicted path from those on the incorrect path. The former are
processed and allowed to proceed, and the latter are squashed. A
single bit sufﬁces because the pipeline model is in-order and it can
have only one resolved branch mispredict outstanding at any given
time. All other instructions following this mispredicted branch can
be squashed. In other words, the stream id works as a cheap and ef-
ﬁcient mechanism to replace the global branch mis-predict signal.
The details of how and when the sid register value is modiﬁed are
discussed below on a stage-by-stage basis:

• Fetch: Every new instruction is stamped with the current
value stored in the sid register. When a branch mis-predict
is detected (using the branch update from execute/memory
stage), it toggles the sid register and ﬂushes the program
counter. From this point onwards, the instructions fetched
are stamped with the updated stream id.

• Decode: The sid register is updated from the stream ids of
the incoming instructions. If at any cycle, the old stream id
stored in decode does not match the stream id of an incom-
ing instruction, a branch mispredict is implied and decode
ﬂushes its instruction buffer.

• Issue: It maintains the sid register along with an additional
1-bit last-sid register. The sid register is updated using
the stream id of the instruction that performs register write-
back. And, the last-sid value is updated from the stream
id of the last successfully issued instruction. For an instruc-
tion reaching the issue stage, its stream id is compared with
the sid register. If the values match, then it is eligible for is-
sue. A mismatch implies that some branch was mispredicted,
in the recent past, and further knowledge is required to deter-
mine whether this new incoming instruction is on the correct
path or the incorrect path. This is where the last-sid

144register becomes important. A mismatch of the new instruc-
tion’s stream id with the last-sid indicates that the new
instruction is on the corrected path of execution and hence it
is eligible for issue. A match implies the otherwise and the
new instruction is squashed. The complete signiﬁcance of
last-sid will be made clear later in this section.

• Execute/Memory: compares the stream id of the incoming
instructions to the sid register. In the event of a mismatch,
the instruction is squashed. A mispredicted branch instruc-
tion toggles its own stream id along with the sid register
value stored here. This branch resolution information is sent
back to the fetch stage, initiating a change in its sid register
value. The mispredicted branch instruction also updates the
sid in the issue stage during writeback. Thus, the cycle of
updates is completed.

To summarize, under normal operating conditions (i.e. no mis-
predicts), instructions go through the switched interconnection fab-
ric, get issued, executed and write back computed results. When a
mispredict occurs, using the stream id mechanism, instructions on
the incorrect execution path can be systematically squashed in time.
Scoreboard: The second component required for proper func-
tionality of the SNS is a scoreboard that resides in the issue stage.
A scoreboard is essential in this design because a forwarding unit
(that normally handles register value dependencies) is not feasible.
More often than not, a scoreboard is already present in a pipeline’s
issue stage for hazard detection.
In such a scenario, only minor
modiﬁcations are needed to tailor a conventional scoreboard to the
needs of a SNS pipeline.

The SNS pipeline needs a scoreboard in order to keep track of the
registers that have results outstanding and are therefore invalid in
the register ﬁle. Instructions for which one or more input registers
are invalid can be stalled in the issue stage. The SNS scoreboard ta-
ble has two columns (see Figure 7(c)), the ﬁrst to maintain a valid
bit for each register, and second to store the id of the last modi-
fying instruction. In case of a branch mis-predict, the scoreboard
needs to be wiped clean since it gets polluted by instructions on
the wrong path of execution. To recognize a mis-predict, the issue
stage maintains a last-sid register that stores the stream id of
the last issued instruction. Whenever the issue stage ﬁnds out that
the new incoming instruction’s stream id differs from last-sid,
it knows that a branch mis-predict has taken place. At this point,
the scoreboard waits to receive the writeback, if it hasn’t received
it already, for the branch instruction that was the cause of the mis-
predict. This branch instruction can be easily identiﬁed because it
will bear the same stream id as the new incoming instruction. Fi-
nally, after this waiting period, the scoreboard is cleared and the
new instruction is issued.

Network Flow Issues: In a SNS, the stalls are automatically
handled by maintaining network back pressure through the switched
interconnection. A crossbar does not forward values to the buffer of
a subsequent stage if the stage is stalled. This is similar to the way
network queues handle stalls. In our implementation, we guarantee
that an instruction is never dropped (thrown away) by a buffer.

For a producer-consumer based system, where the transfer la-
tency is variable, double buffering is a standard technique used to
make the transfer latency overlap with the job cycles of a producer
or consumer. In a SNS, all stages have their input and output latches
double buffered to enable this optimization.

3.3 Performance Enhancement

The additions to the SNS discussed in the previous section brings
the design to a point where it is functionally correct. In order to
compare the performance of this basic SNS design to an in-order
pipeline (Figure 3(a)), we conducted some experiments using a cy-

baseline
SNS

SNS + bp$ 2
SNS + bp$ 4

SNS + bp$ 6
SNS + bp$ 8

e
m

i
t

n
u
R
d
e
z

 

i
l

a
m
r
o
N

 6

 5

 4

 3

 2

 1

 0

3

d

e

s

g

g

7

2

7

2

1

d

1

e

id

c
t

p

c

1

r
a

r
a

w

w

r
c

4

rijn

s

o

A

v

b

el

e
r
a

d

a

el

g

e

1

8

1
.

m

e

q

n

g
r
e

p

w

A

c

v

c
f

e
r
a

g

e

e

c

n

c

o

o

d

e

d

e

c

a

d

a

u

u

dio

dio

Embedded

Desktop

Figure 4: SNS pipeline compared to the baseline pipeline with
and without the bypass cache. The slowdown, seen because of
the issue stage stalls, reduces as the size of bypass cache is in-
creased.

cle accurate simulator developed in the Liberty Simulation Envi-
ronment [41]. Basic here implies a SNS conﬁgured with the stream
identiﬁcation logic, scoreboard, and double buffering. Interested
readers can ﬁnd the details of our simulation setup and benchmarks
in Section 5.1. The performance of the SNS in comparison to the
baseline is shown as a comparison between the ﬁrst and second bars
in Figure 4. The results are normalized to the runtime of the base-
line in-order processor. On average, a 4X slowdown was observed,
which is a signiﬁcant price to pay in return for the reconﬁguration
ﬂexibility. However, in this version of the SNS design, much is left
on the table in terms of performance. Most of this performance is
lost in the stalls due to 1) the absence of forwarding paths and 2)
transmission delay through the switches.

Bypass Cache: Due to the lack of forwarding logic in a SNS,
frequent stalls are expected for instructions with register depen-
dencies. To alleviate the performance loss, we add a bypass cache
in the execute/memory stage (see Figure 7(d)). This cache stores
values generated by recently executed instructions within the ex-
ecute/memory stage. The instructions that follow can use these
cached values and need not stall in issue waiting for writeback. In
fact, if this cache is large enough, results from every instruction
that has been issued, but has not written back, can be retained. This
would completely eliminate the stalls arising from register depen-
dencies emulating forwarding logic.

A FIFO replacement policy is used for this cache because older
instructions are less likely to have produced a result for an incoming
instruction. The scoreboard unit in the issue stage is made aware of
the bypass cache size when the system is ﬁrst conﬁgured. When-
ever the number of outstanding registers in the scoreboard becomes
equal to this cache size, instruction issue is stalled.
In all other
cases, the instruction can be issued as all of its input dependencies
are guaranteed to be present within the bypass cache. Hence, the
scoreboard can accurately predict whether or not the bypass cache
will have a vacancy to store the output from the current instruction.
Furthermore, the issue stage can perform selective register operand
fetch for only those values that are not going to be available in the
bypass cache. By doing this, the issue stage can reduce the number
of bits that it needs to transfer to the execute/memory stage.

As evident from the experimental results (Figure 4), the addition
of the bypass cache results in dramatic improvements in the overall
performance of the SNS. The biggest improvement comes between
the SNS conﬁguration without any bypass cache (second bar) to
the one with a bypass cache of size 2 (third bar). This improvement
diminishes after a while, and saturates beyond 8 entries. The aver-
age slowdown hovers around 2.1X with the addition of the bypass
cache, which is still high.

Crossbar Width: The crossbar channel width is the number of

145baseline
SNS + bp$ + 32-bit Xbar
SNS + bp$ + 64-bit Xbar

SNS + bp$ + Inf. bit Xbar

baseline
SNS + bp$
SNS + bp$ + MOP 3 live-ins/outs
SNS + bp$ + MOP 4 live-ins/outs

SNS + bp$ + MOP 5 live-ins/outs
SNS + bp$ + MOP 6 live-ins/outs

e
m

i
t

n
u
R
d
e
z

 

i
l

a
m
r
o
N

 2.5

 2

 1.5

 1

 0.5

 0

e
m

i
t

n
u
R
d
e
z

 

i
l

a
m
r
o
N

 1.6

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

3

d

g

7

g

7

e

s

2

2

1

d

1

e

id

c
t

p

c

1

r
a

r
a

w

w

r
c

4

rijn

s

o

A

v

b

el

e
r
a

d

a

el

g

e

1

8

e

q

1
.

m

n

g
r
e

p

w

A

c

v

c
f

e
r
a

g

e

3

d

e

s

g

g

7

2

7

2

1

d

1

e

id

c
t

p

c

1

r
a

r
a

w

w

r
c

4

rijn

s

o

A

v

b

el

e
r
a

d

a

el

g

e

1

8

1
.

m

e

q

n

g
r
e

p

w

A

c

v

c
f

e
r
a

g

e

Desktop

Desktop

e

c

n

c

o

o

d

e

d

e

c

a

d

a

u

u

dio

dio

Embedded

e

n

c

o

c

o

d

e

d

e

c

a

d

a

u

u

dio

dio

Embedded

Figure 5: SNS pipeline, with variation in the transmission
bandwidth. As expected the performance improves with higher
transmission bandwidth.

Figure 6: A SNS with a bypass cache and the capability to han-
dle MOPs, compared to the baseline in-order pipeline. The sec-
ond bars are for MOP sizes ﬁxed at 1, while the other bars have
constraint on the number of live-ins and live-outs.

bits that can be transferred to/from the crossbar in a single cycle. In
the context of a SNS, it determines the number of cycles it will take
to transfer an instruction between the stages. The results presented
so far in this section have been with a crossbar channel width of
32-bits. Figure 5 illustrates the impact of varying this width on per-
formance. Three data points are presented for every benchmark: a
32-bit channel width, a 64-bit channel width, and inﬁnite channel
width. A large performance gain is seen when going from 32-bit
width to 64-bit width. Inﬁnite bandwidth essentially means elim-
inating all transfer latency between the stages, resulting in perfor-
mance comparable to the baseline. With a 64-bit crossbar switch,
SNS has an average slowdown of about 1.35X.

Macro Operations: The performance of the SNS design suf-
fers signiﬁcantly from the overhead of transferring instructions be-
tween stages, since every instruction has to go through a switched
network with a variable amount of delay. Here, a natural optimiza-
tion would be to increase the granularity of communication to a
bundle of multiple operations (a MOP). There are two advantages
of doing this:

1. More work (multiple instructions) is available for the stages

to work on while the next MOP is being transmitted.

2. Macro-ops can eliminate the temporary intermediate values
generated within small sequences of instructions, and there-
fore give an illusion of data compression to the underlying
interconnection fabric.

These collections of operations can be identiﬁed both statically
(at compile time) or dynamically (in the hardware). To keep the
overall hardware overhead low, we form these statically in the com-
piler. Our approach involves selecting a subset of instructions be-
longing to a basic block, while bounding two parameters: 1) the
number of live-ins and live-outs and 2) the number of instructions.
We use a simple greedy policy, similar to [14], that maximizes the
number of instructions, while minimizing the number of live-ins
and live-outs. When forming MOPs, as long as the computation
time in the stages can be brought closer to the transfer time over
the interconnection, it is a win.

The compiler embeds the MOP boundaries, internal data ﬂow,
and live-in/live-out information in the program binary. During run-
time, the decode stage’s Packer structure is responsible for identify-
ing and assembling MOPs. Leveraging hints for the boundaries that
are embedded in the program binary, the Packer assigns a unique
MOP id (MID) to every MOP ﬂowing through the pipeline. All
other stages in the SNS are also slightly modiﬁed in order to work
with these MOPs instead of simple instructions. This is particularly
true of the execute/memory stage where a controller cycles across
the individual instructions that comprise a MOP, executing them in
sequence.

The performance results shown in Figure 6 are for a SNS with

the bypass cache, 64-bit switch channel width and MOPs. The var-
ious bars in the plot are for different conﬁgurations of the MOP
selection algorithm. The results show that beyond a certain limit,
relaxing the MOP selection constraints (live-ins and live-outs) does
not result in performance improvement. Prior to reaching this limit,
relaxing constraints helps in forming longer MOPs, thereby balanc-
ing transfer time with computation time. Beyond this limit, relax-
ing constraints does not result in longer MOPs. Instead it produces
wider MOPs that have more live-ins/outs, which increases transfer
time without actually increasing the number of distinct computa-
tions that are encoded. The best case result for embedded appli-
cations has about 1.1X slowdown, and the desktop equivalent has
about 1.2X slowdown. The worst performers were the benchmarks
that had very poor branch prediction rates. In fact, the performance
on a SNS was found to be strongly correlated with the number of
mispredicts per thousand instructions. This is expected because the
use of MOPs, and the additional cycles spent for data transfer be-
tween stages, causes the SNS to behave like a very deep pipeline.

3.4 Stage Modiﬁcations

This section goes over the pipeline stages in the SNS, and sum-

marizes the modules added to each of them.

• Fetch: The modiﬁcations made here are restricted to the ad-
dition of sid register and a small amount of logic to toggle
it upon branch mis-predicts (Figure 7(a)).

• Decode: The decode stage (Figure 7(b)) collects the fetched
instructions in a buffer. An instruction buffer is a common
structure found in most pipeline designs, and to that we add
our sid register. For an incoming instruction with a different
stream id, this register is toggled and the instruction buffer is
ﬂushed. The decode stage is also augmented with the Packer.
The Packer logic reads instructions from the buffer, identiﬁes
the MOP boundaries, assigns them a MID, and ﬁlls out the
MOP structure attributes such as length, number of opera-
tions and live-in/out register names.

• Issue: The issue stage (Figure 7(c)) is modiﬁed to include
a Scoreboard that tracks register dependencies. For a MOP
that is ready for issue, the register ﬁle is read to populate
the live-ins. The issue stage also maintains two 1-bit regis-
ters: sid and last-sid, in order to identify branch mis-
predicts and ﬂush the Scoreboard at appropriate times.

• Execute/Memory: The execute/memory stage (Figure 7(d))
houses the bypass cache that emulates the job of forwarding
logic. This stage is also the ﬁrst to update its sid register
upon a branch mis-predict. In order to handle MOP execu-
tion, the execute/memory controller is modiﬁed to walk the
MOP instructions one at a time (one execution per cycle). At

146S
I
D

SID

Instruction Buffer

Incoming OP

Branch
resolution

Mispredict

Handler

Mispredict ?

Exec Next PC

Predicted Next PC

M
U
X

PC

Update BP

Branch
Predictor

Fetch

Controller

OP

Icache response

Icache request

(a) Fetch

MID

Packer

Decoder
Logic

Outgoing (Macro) OP

(b) Decode

Branch Resolution

Incoming (Macro) OP

(Macro) OP latch

Incoming (Macro) OP

(Macro) OP latch

REG ID Valid Last Wr. MID

SID

Last SID

0
1

2

59

60
61

62
63

0
1
1

0
1

1
0
1

17
−
−

21
−
−

17
−

Writeback Registers

Issue

Controller

(c) Issue

SCOREBOARD

REGISTER FILE

Outgoing (Macro) OP

SID

OP Counter

Ex / Mem 
Controller

Functional Unit

REG ID

REG Value

Outgoing (Macro) OP

BYPASS $

Dcache Response

Dcache Request

(d) Execute/Memory

Figure 7: Pipeline stages of the SNS. Gray blocks highlight the modules added for transforming a traditional pipeline into a SNS.

the same time, the computed results are saved into the bypass
cache for later use.

switch routing tables.

4. STAGENET MULTICORE FABRIC

The SNS presented in the last section is in itself a complete mi-
croarchitectural solution to allow pipeline stage level reconﬁgura-
tion. By maintaining cold spares for stages that are most likely
to fail, a SNS-based design can achieve the lifetime enhancement
targets projected in Figure 2. However, these gains can be greatly
ampliﬁed, without the cold sparing cost, by using multiple SNSs as
building blocks for the SN architecture.

The high level abstraction of the SN, in combination with the
SNS design, forms the basis of the SN fabric (Figure 8). This ﬁnal
SN design is a highly reconﬁgurable multicore fabric capable of
arbitrarily using pipeline stages to form logical SNSs. The two
prominent hardware structures within the SN architecture are:

1. Interconnection Switch: The role of the switch is to direct the
incoming MOP to the correct destination stage. For this task,
it maintains a static routing table that is addressed using the
thread id of the MOP. The thread id uniquely determines the
destination stage because, in its current form, SN does not
allow a thread to use two stages of the same type. To circum-
vent the danger of having them as single points of failure,
multiple switches are maintained by the SN fabric.

2. Conﬁguration Manager: Given a pool of stage resources,
the conﬁguration manager divides them into logical SNSs.
The logic for the conﬁguration manager is better suited to
a software implementation since: 1) it will be accessed in-
frequently, and 2) more ﬂexibility is available in software to
experiment with resource allocation policies. The conﬁgu-
ration manager can be designed as a ﬁrmware/kernel mod-
ule. When failures occur, a trap can be sent to the virtual-
ization/OS interface, which can then initiate updates for the

In the event of any stage failure, the SN architecture can initiate
recovery by combining live stages from different slices, i.e. sal-
vaging healthy modules to form logical SNSs. We refer to this as
the stage borrowing (Section 4.1). In addition to this, if the un-
derlying stage design permits, stages can be time-multiplexed by
two distinct SNSs. For instance, a pair of SNSs, even if one of
them loses its issue stage, can still run separate threads while shar-
ing the single live issue stage. We refer to this as stage sharing
(Section 4.2).

4.1 Stage Borrowing

A pipeline stage failure in the system calls upon the conﬁgu-
ration manager to determine the maximum number of full logical
SNSs that can be formed using the pool of live stages. Full SNS
here implies a SNS with exclusive access to exactly one stage of
each type. The number of such SNSs that can be formed by the
conﬁguration manager is determined by the stage with the fewest
live instances. For example, in Figure 8, the top three SNSs have
a minimum of two stages alive of each type, and, thus, two logi-
cal SNSs are formed. The logical slices are highlighted using the
shaded path indicating the ﬂow of the instruction streams.

It is noteworthy that all top three pipelines in Figure 8 have at
least one failed stage, and therefore, a multicore system in a similar
situation would have lost all three pipelines. Hence, SN’s ability to
efﬁciently borrow stages from different slices, gives it the compet-
itive edge over a traditional multicore.

4.2 Stage Sharing

Stage borrowing is good, but it is not enough in certain failure
situations. For example, the ﬁrst stage failure in the SN fabric re-
duces the number of logical SNSs by one. However, if the stages
can be time-multiplexed by multiple SNSs, then the same number
of logical SNSs can be maintained. Figure 8 has the bottom two

147Figure 8: A SN architecture formed using ﬁve SNSs. As an example scenario where lightning bolts indicate broken stages, four
operational SNSs are extracted as shown by the transparent paths.

SNSs sharing the issue stage. The number of logical SNSs that can
share a single stage can be tuned in our implementation.

The sharing is beneﬁcial only when the threads involved present
opportunities to interleave their execution. Therefore, threads with
very high IPC (instructions per cycle) are expected to derive lesser
beneﬁt compared to low IPC threads. Furthermore, as the degree of
stage sharing is increased, the beneﬁts are expected to shrink since
more and more threads will contend for the available stage. In order
for the stages to be shareable, certain hardware modiﬁcations are
also required:

• Fetch: It needs to maintain a separate program counter for
each thread and has to time-multiplex the memory accesses.
The instruction cache, in turn, will also be shared implicitly
by the executing threads

• Decode: The instruction buffer has to be partitioned between

different threads.

• Issue: The scoreboard and the register ﬁle are populated with
state values speciﬁc to a thread, and it is not trivial to share
them. There are two ways to handle the sharing for these
structures: 1) compile the thread with fewer registers or 2)
use a hardware structure for register caching [29].
In our
evaluation, we implement the register caching in hardware
and share it across multiple threads.

• Execute/Memory: The bypass cache is statically partitioned
between the threads. Similarly, the data cache gets shared by
the threads.

The conﬁguration manager is invoked whenever any stage re-
source develops a defect in the SN system. Depending upon the
degree of stage sharing, it determines the number of logical SNSs
that can be formed. The conﬁguration manager also conﬁgures
the stages that need to be shared and partitions their resources ac-
cordingly between threads. While working with higher degrees of
sharing, the conﬁguration manager employs a fairness policy for
resource allocation, so that the work (threads) gets evenly divided
among the stages. For example, if there are ﬁve threads that need
to share three live stages of same type, the fairness policy prefers
a 2-2-1 conﬁguration (two threads each to stages 1 and 2 and re-
maining one to stage 3) over a 3-1-1 conﬁguration (three threads to
stage 1, one each to stages 2 and 3). Further details of regarding

the resource allocation are omitted for space reasons.

5. RESULTS AND DISCUSSION

5.1 Simulation Setup

The evaluation infrastructure for the SN fabric consisted of three
major components: 1) the compilation framework, 2) an architec-
tural simulator, and 3) a Monte Carlo simulator for lifetime through-
put estimations. A total of 14 benchmarks were selected from the
embedded and desktop application domains. For these evaluations,
the emphasis was on the embedded benchmarks because the SNS
is based on an in-order embedded core. A variety of these were
used including several encryption (3des, pc1, rc4, rijndael), audio
processing (g721encode, g721decode, rawcaudio and rawdaudio),
and image/video processing (idct, sobel) benchmarks. In addition,
four desktop benchmarks (181.mcf, eqn, grep, wc) were also in-
cluded in order to exhibit the potential of this architecture for other
domains.

We use the Trimaran compilation system [40] as our ﬁrst compo-
nent. The MOP selection algorithm is implemented as a compiler
pass on the intermediate code representation. During this pass, the
code is augmented with the MOP boundaries and other miscella-
neous attributes. The ﬁnal code generated by the compiler uses the
HPL-PD ISA [20].

The architectural simulator for the SN evaluation was developed
using the Liberty Simulation Environment (LSE) [41]. A func-
tional emulator was also developed for the HPL-PD ISA within the
LSE system. Two ﬂavors of the microarchitectural simulator were
implemented in sufﬁcient detail to provide cycle accurate results.
The ﬁrst simulator modeled a simple ﬁve stage pipeline, which is
also the baseline for our experiments. The second simulator imple-
mented the SN architecture with all the proposed enhancements.
Table 1 lists the common attributes for our simulations.

The third component of our simulation setup is the Monte Carlo
engine that we employ for lifetime throughput study. Each iteration
of the Monte Carlo process simulates the lifetime of the SN archi-
tecture. The conﬁguration of the SN architecture is speciﬁed in Ta-
ble 1. The MTTF for the various stages and switches in the system

148was calculated using Equation 11. The crossbar switch peak tem-
perature was taken from [31] that performs interconnection model-
ing for the RAW multicore chip [24]. The stage temperatures were
extracted from HotSpot simulations of the OR1200 core with the
ambient temperature normalized to the one used in [31]. The cal-
culated MTTFs are used as the mean of the Weibull distributions for
generating time to failure (TTF) for each module (stage/switch) in
the system. For each iteration of the Monte Carlo, the system gets
reconﬁgured over its lifetime whenever a failure is introduced. The
instantaneous throughput of the system is computed for each new
conﬁguration using the architectural simulator on multiple random
benchmark subsets. From this, we obtain the system throughput
over the lifetime. Nearly 200 such iterations are run for conducting
the Monte Carlo study.

Base core
SNS

Base multicore
SN

Branch pred.

L1 I$, D$
L2 $ uniﬁed
Memory

5-stage in-order pipeline
4-stage in-order, with double buffering
and 64-bit 5×5 crossbar switches
5 baseline cores
5 SNSs, ﬁve 2-way redundant 64-bit crossbar
switches, and a conﬁguration manager
global, 16-bit history, gshare predictor
BTB size of 2KB
4-way, 16 KB, 1 cycle hit latency
8-way, 64 KB, 5 cycle hit latency
40 cycle hit latency

Table 1: Architectural attributes.

5.2 Simulation Results

Lifetime performance beneﬁts: The simulation results were
gathered separately for the embedded and desktop benchmarks.
Figure 9(a) shows the lifetime throughput results for a baseline
multicore compared against various conﬁgurations of the SN ar-
chitecture running the embedded benchmarks. The baseline sys-
tem starts with a performance advantage over the SN architecture.
However, as failures accumulate over time, the throughput of SN
overtakes the baseline performance and thereafter remains domi-
nant. For instance, at year 7, the IPC of the SN is nearly 4X the
baseline IPC. The shaded portion in this ﬁgure depicts the cumu-
lative distribution function (CDF) of the combined MTTF Weibull
distributions. For instance, this plot shows that after 10 years, on
an average, there are 12 failed modules in the system. Similar
throughput improvements were seen for the desktop benchmarks
(Figure 9(c)).

Figure 9(b) shows the cumulative performance (total work done)
for various SN conﬁguration compared against the baseline. By the
end of the lifetime, we achieve as much as 50% improvement in
the work done for the SN fabric. About 40% of this is achieved
by stage borrowing only, and the additional 10% beneﬁt is a re-
sult of stage sharing. Furthermore, going from 2-way sharing to
3-way and higher sharing yields negligible improvement. This is
an expected result because the opportunities to overlap executions
diminish as the contention for a stage increases. For the desktop
benchmarks (Figure 9(d)), the cumulative performance improve-
ments stand at about 40%. Overall, the desktop benchmarks ex-
hibit lesser beneﬁts than their embedded counterparts. This result
follows the trend seen in Figure 6.

Area overhead: The area overhead in the SN arises from the
additional microarchitectural structures that were added and the

1The fetch stage was qualiﬁed to have a MTTF of 10 years. This
is a conservative estimate and no actual module level MTTF values
are available from any public source.

interconnection fabric composed of the crossbar switches. Area
overhead is shown using an OR1200 core as the baseline (see Sec-
tion 2.1). The area numbers for the scoreboard, bypass cache and
the register cache are estimated by taking similar structures from
the OR1200 core and resizing them appropriately. The scoreboard
area estimate is based on the register ﬁle. Bypass cache and register
cache areas are based on the TLB area, which is also an associative
look-up structure. And ﬁnally, the area of double buffers is based
on the maximum macro-op size they have to store. The sizing of
all these structure was done in accordance with the SNS conﬁgura-
tion that achieved the best performance. The crossbar switch area
is based on the Verilog model from [28]. The total area overhead
for the SN design is 15%.

All the design blocks were synthesized, placed and routed us-
ing industry standard CAD tools with a library characterized for
a 130nm process. The area overhead for separate modules and the
crossbar switches is shown in Figure 10(a). The total area overhead
of the SN system compared to the multicore baseline is shown in
Figure 10(b).

Although, we have not investigated the impact of our microarchi-
tectural changes to the circuit critical paths, our changes primarily
impact the pipeline depth (due to the additional buffers). Hence, we
don’t expect a measurable inﬂuence on the cycle time in the SNS
as compared to the baseline.

Design block
Scoreboard
Bypass cache
Register cache
Double buffers

64-bit crossbar switch

Area (mm2) Percent overhead

0.018
0.044
0.028
0.067
0.028

1.4%
3.4%
2.2%
5.3%
2.1%

(a) Area for SNS modules and crossbar switch

Conﬁguration

Percent overhead

SN without sharing

SN with sharing

14.1%
16.3%

(b) Total area overhead for SN

Figure 10: Area overhead for the SN architecture.

6. RELATED WORK

Concern over reliability issues in future technology generations
has spawned a new wave of research in reliability-aware microar-
chitectures. Recent work has addressed the entire spectrum of re-
liability topics, from fault detection and diagnosis to system repair
and recovery. This section focuses on the most relevant subset of
recent work, those that propose architectures that tolerate and/or
adapt to the presence of faults.

High-end server systems designed with reliability as a ﬁrst-order
design constraint have been around for decades but have typically
relied on coarse grain replication to provide a high degree of re-
liability [7, 36], such as Tandem NonStop [6], Teramac [17, 2],
Stratus [45], and the IBM zSeries [6]. However, dual and triple
modular redundant systems incur signiﬁcant overheads in terms of
area and power. Furthermore, these systems still remain suscep-
tible to wearout-induced failures since they cannot tolerate a high
failure rate.

ElastIC [39] is a slightly different high-level architectural vision
for multiprocessor fault tolerance. Exploiting low-level circuit sen-
sors for monitoring the health of individual cores, the authors pro-
pose a dynamic reliability management that can throttle and even-

149Baseline multicore
StageNet without sharing
StageNet with 2-way sharing
StageNet with 3-way sharing
StageNet with 4-way sharing

Baseline multicore
StageNet without sharing
StageNet with 2-way sharing
StageNet with 3-way sharing
StageNet with 4-way sharing

 5

 10

 15

 20

 5

 10

 15

 20

 25

 0

 25

 0

 0

(a) Monte Carlo results for embedded benchmarks

Time (years)

(b) Cumulative performance results for embedded
benchmarks

Time (years)

Baseline multicore
StageNet without sharing
StageNet with 2-way sharing
StageNet with 3-way sharing
StageNet with 4-way sharing

Baseline multicore
StageNet without sharing
StageNet with 2-way sharing
StageNet with 3-way sharing
StageNet with 4-way sharing

 35

 30

 25

 20

 15

 10

 5

s
e
h
c
t
i

w
s
 
d
n
a

 
s
e
g
a
t
s
 
d
e

l
i

a

f
 
f

o

 
r
e
b
m
u
N

 35

 30

 25

 20

 15

 10

 5

s
e
h
c
t
i

w
s
 

d
n
a
 
s
e
g
a

t
s
 

d
e

l
i

a

f
 
f

o
 
r
e
b
m
u
N

 2

 1.5

 1

 0.5

e
c
n
a
m
r
o

f
r
e
p
e
v
i
t

 

l

a
u
m
u
c
 

d
e
z

i
l

a
m
r
o
N

 2

 1.5

 1

 0.5

e
c
n
a
m
r
o
f
r
e
p

 

e
v
i
t

l

a
u
m
u
c
 

d
e
z

i
l

a
m
r
o
N

)

C
P

I
(
 
t

u
p
h
g
u
o
r
h
T

)

C
P

I
(
 
t

u
p
h
g
u
o
r
h
T

 3.5

 2.5

 1.5

 3

 2

 1

 0.5

 0

 0

 3.5

 2.5

 1.5

 3

 2

 1

 0.5

 0

 0

 5

 10

 15

 20

 5

 10

 15

 20

 25

 0

 25

 0

 0

(c) Monte Carlo results for desktop benchmarks

Time (years)

(d) Cumulative performance results for desktop
benchmarks

Time (years)

Figure 9: Throughput and cumulative performance results for the SN architecture. Plots (a) and (c) also show (shaded portion) the
expected number of failed modules (stages/switch) until that point in the lifetime.

tually turn off cores as they age over time. ElastIC relies upon
redundancy management at the core level similar to the Conﬁg-
urable Isolation [1]. Within such a framework, the system must
be provisioned with a massive number of redundant cores or face
the prospect of rapidly declining processing throughput as single
faults disable entire cores. Much work has also been done in ﬁne-
grained redundancy maintenance such as Bulletproof [16], sparing
in array structures [11], and other such microarchitectural struc-
tures [32]. These schemes typically rely on inherent redundancy
of superscalar cores, and it is also extremely hard to achieve full
coverage with them. Nevertheless, ideas from such approaches can
be leveraged in our system for additional beneﬁts as they apply re-
dundancy management at a ﬁner granularity.

Other research has focused on building reliable substrates out
of future nanotechnologies that are expected to be inherently fault-
prone. The NanoBox Processor Grid [22] was designed as a recur-
sive system of black box devices, each employing their own unique
fault tolerance mechanisms. While this project does boast a signiﬁ-
cant amount of defect tolerance, it comes at a 9X overhead in terms
of redundant structures.

SNS differs dramatically from solutions previously proposed in
that our goal is to minimize the amount of hardware used solely
for redundancy. More importantly, we enable reconﬁguration at
the granularity of a pipeline stage, making it possible for a single
core to tolerate multiple failures at a much lower cost. The work
here is an extension of [18] where we explored the potential of
pipeline stage level reconﬁgurability in the context of a single SNS.
In parallel to our efforts, Romanescu et al. [30] have proposed a
multicore architecture, Core Cannibalization Architecture (CCA),
that also exploits stage level reconﬁgurability. CCA allows only a
subset of pipelines to lend their stages to other broken pipelines,

thereby avoiding full crossbar interconnection. Unlike SNS, CCA
pipelines maintain all feedback links and avoid any major changes
to the microarchitecture. Although these design choices reduce the
overall complexity, fewer opportunities of reconﬁguration exist for
the CCA as compared to SN.

7. CONCLUSION

As CMOS technology continues to evolve, so too must the tech-
niques that are employed to counter the effects of ever more de-
manding reliability challenges. Efforts in fault detection, diagno-
sis, and recovery/reconﬁguration must all be leveraged together to
form a comprehensive solution to the problem of unreliable sili-
con. This work contributes to the area of recovery and reconﬁgura-
tion by proposing a radical architectural shift in processor design.
Motivated by the need for ﬁner-grain reconﬁguration, networked
pipeline stages were identiﬁed as the effective trade-off between
cost and reliability enhancement. Although performance suffered
at ﬁrst as a result of the changes to the basic pipeline, a few well-
placed microarchitectural enhancements were able to reclaim much
of what was lost. Ultimately, the SN fabric exchanged a modest
amount of area overhead (15%) in return for a highly resilient mul-
ticore fabric that yielded about 50% more work during its lifetime
than a traditional multicore.

For mainstream process technologies (like 45nm), SN can be
used to combat wearout failures over time. For upcoming technol-
ogy generations with high defect rates at manufacture time (32nm
and beyond), SN can also be employed for yield improvements.
Finally, for more distant future technologies (such as carbon nan-
otubes), SN will likely need to be used in conjunction with other
methods to combat high failure rates. Hence, the SN fabric is well
positioned to withstand the rapidly increasing device failure rates

150expected in future technology nodes.

8. ACKNOWLEDGEMENTS

We thank David Penry for his assistance with the Liberty Sim-
ulation Environment and Visvesh Sathe for his help with Cadence
Encounter. Also, our gratitude goes to the anonymous referees who
provided excellent feedback on this work. This research was sup-
ported by ARM Ltd., the National Science Foundation grant CCF-
0347411, and the Gigascale Systems Research Center, one of ﬁve
research centers funded under the Focus Center Research Program,
a Semiconductor Research Corporation program.

9. REFERENCES

[1] N. Aggarwal, P. Ranganathan, N. P. Jouppi, and J. E. Smith. Conﬁgurable

isolation: building high availability systems with commodity multi-core
processors. In Proc. of the 34th Annual International Symposium on Computer
Architecture, pages 470–481, 2007.

enable a compact energy efﬁcient chip multiprocessor. ACM SIGPLAN Notices,
41(11):117–128, 2006.

[22] A. KleinOsowski, K. KleinOsowski, and V. Rangarajan. The recursive nanobox

processor grid: A reliable system architecture for unreliable nanotechnology
devices. In International Conference on Dependable Systems and Networks,
page 167, June 2004.

[23] P. Kongetira, K. Aingaran, and K. Olukotun. Niagara: A 32-way multithreaded

SPARC processor. IEEE Micro, 25(2):21–29, Feb. 2005.

[24] W. Lee, R. Barua, M. Frank, D. Srikrishna, J. Babb, V. Sarkar, and

S. Amarasinghe. Space-time scheduling of instruction-level parallelism on a
RAW machine. In Eighth International Conference on Architectural Support
for Programming Languages and Operating Systems, pages 46–57, Oct. 1998.

[25] M.-L. Li, P. Ramachandran, S. Sahoo, S. Adve, V. Adve, and Y. Zhou.

Trace-based microarchitecture-level diagnosis of permanent hardware faults. In
Proc. of the 2008 International Conference on Dependable Systems and
Networks, June 2008.

[26] A. Meixner, M. Bauer, and D. Sorin. Argus: Low-cost, comprehensive error

detection in simple cores. IEEE Micro, 28(1):52–59, 2008.

[27] OpenCores. OpenRISC 1200, 2006.

http://www.opencores.org/projects.cgi/web/ or1k/openrisc 1200.

[28] L.-S. Peh and W. Dally. A delay model and speculative architecture for

pipelined routers. In Proc. of the 7th International Symposium on
High-Performance Computer Architecture, pages 255–266, Jan. 2001.

[2] R. Amerson, R. J. Carter, W. B. Culbertson, P. Kuekes, and G. Snider. Teramac

[29] M. Postiff, D. Greene, S. Raasch, and T. Mudge. Integrating superscalar

– conﬁgurable custom computing. In Proc. of the 1995 International
Symposium on FPGA’s for Custom Computing Machines, pages 32–38, 1995.

processor components to implement register caching. In Proc. of the 2001
International Conference on Supercomputing, pages 348–357, 2001.

[3] ARM. Arm11.

http://www.arm.com/products/CPUs/families/ARM11Family.html.

[4] ARM. Arm9. http://www.arm.com/products/CPUs/families/ARM9Family.html.
[5] T. Austin. Diva: a reliable substrate for deep submicron microarchitecture

design. In Proc. of the 32nd Annual International Symposium on
Microarchitecture, pages 196–207, 1999.

[6] W. Bartlett and L. Spainhower. Commercial fault tolerance: A tale of two

sytems. IEEE Transactions on Dependable and Secure Computing, 1(1):87–96,
2004.

[7] D. Bernick, B. Bruckert, P. D. Vigna, D. Garcia, R. Jardine, J. Klecka, and
J. Smullen. Nonstop advanced architecture. In International Conference on
Dependable Systems and Networks, pages 12–21, June 2005.

[8] K. Bernstein. Nano-meter scale cmos devices (tutorial presentation), 2004.
[9] J. Blome, S. Feng, S. Gupta, and S. Mahlke. Self-calibrating online wearout

detection. In Proc. of the 40th Annual International Symposium on
Microarchitecture, pages 109–120, 2007.

[10] S. Borkar. Designing reliable systems from unreliable components: The

challenges of transistor variability and degradation. IEEE Micro, 25(6):10–16,
2005.

[11] F. A. Bower, P. G. Shealy, S. Ozev, and D. J. Sorin. Tolerating hard faults in

microprocessor array structures. In Proc. of the 2004 International Conference
on Dependable Systems and Networks, page 51, 2004.

[12] F. A. Bower, D. J. Sorin, and S. Ozev. A mechanism for online diagnosis of

hard faults in microprocessors. In Proc. of the 38th Annual International
Symposium on Microarchitecture, pages 197–208, 2005.

[13] A. Christou. Electromigration and Electronic Device Degradation. John Wiley

and Sons, Inc., 1994.

[14] N. Clark, A. Hormati, S. Mahlke, and S. Yehia. Scalable subgraph mapping for
acyclic computation accelerators. In Proc. of the 2006 International Conference
on Compilers, Architecture, and Synthesis for Embedded Systems, pages
147–157, Oct. 2006.

[15] K. Constantinides, O. Mutlu, T. Austin, and V. Bertacco. Software-based online

detection of hardware defects: Mechanisms, architectural support, and
evaluation. In Proc. of the 40th Annual International Symposium on
Microarchitecture, pages 97–108, 2008.

[16] K. Constantinides, S. Plaza, J. Blome, B. Zhang, V. Bertacco, S. Mahlke,
T. Austin, and M. Orshansky. Bulletproof: A defect-tolerant CMP switch
architecture. In Proc. of the 12th International Symposium on
High-Performance Computer Architecture, pages 3–14, Feb. 2006.

[17] W. Culbertson, R. Amerson, R. Carter, P. Kuekes, and G. Snider. Defect

tolerance on the teramac custom computer. In Proc. of the 1997 International
Symposium on FPGA’s for Custom Computing Machines, pages 116–123, 1997.

[18] S. Gupta, S. Feng, A. Ansari, J. Blome, and S. Mahlke. Stagenetslice: A

reconﬁgurable microarchitecture building block for resilient cmp systems. In
Proc. of the 2008 International Conference on Compilers, Architecture, and
Synthesis for Embedded Systems, 2008.

[19] W. Huang, M. R. Stan, K. Skadron, K. Sankaranarayanan, and S. Ghosh.

Hotspot: A compact thermal modeling method for cmos vlsi systems. IEEE
Transactions on Very Large Scale Integration (VLSI) Systems, 14(5):501–513,
May 2006.

[20] V. Kathail, M. Schlansker, and B. Rau. HPL-PD architecture speciﬁcation:

Version 1.1. Technical Report HPL-93-80(R.1), Hewlett-Packard Laboratories,
Feb. 2000.

[21] T. Kgil, S. D’Souza, A. Saidi, N. Binkert, R. Dreslinski, T. Mudge,

S. Reinhardt, and K. Flautner. Picoserver: using 3d stacking technology to

[30] B. F. Romanescu and D. J. Sorin. Core cannibalization architecture: Improving
lifetime chip performance for multicore processor in the presence of hard faults.
In Proc. of the 17th International Conference on Parallel Architectures and
Compilation Techniques, 2008.

[31] L. Shang, L. Peh, A. Kumar, and N. K. Jha. Temperature-aware on-chip

networks. IEEE Micro, 2006.

[32] P. Shivakumar, S. Keckler, C. Moore, and D. Burger. Exploiting

microarchitectural redundancy for defect tolerance. In Proc. of the 2003
International Conference on Computer Design, page 481, Oct. 2003.
[33] D. Siewiorek and R. Swarz. Reliable Computer Systems: Design and

Evaluation, 3rd Edition. AK Peters, Ltd., 1998.

[34] J. Smolens, J. Kim, J. Hoe, and B. Falsaﬁ. Efﬁcient resource sharing in

concurrent error detecting superscalar microarchitectures. In Proc. of the 37th
Annual International Symposium on Microarchitecture, pages 256–268, Dec.
2004.

[35] J. C. Smolens, B. T. Gold, B. Falsaﬁ, and J. C. Hoe. Reunion:

Complexity-effective multicore redundancy. In Proc. of the 39th Annual
International Symposium on Microarchitecture, pages 223–234, 2006.

[36] L. Spainhower and T. Gregg. IBM S/390 Parallel Enterprise Server G5 Fault

Tolerance: A Historical Perspective. IBM Journal of Research and
Development, 43(6):863–873, 1999.

[37] J. Srinivasan, S. V. Adve, P. Bose, and J. A. Rivers. The case for lifetime

reliability-aware microprocessors. In Proc. of the 31st Annual International
Symposium on Computer Architecture, pages 276–287, June 2004.

[38] J. Srinivasan, S. V. Adve, P. Bose, and J. A. Rivers. Exploiting structural

duplication for lifetime reliability enhancement. In Proc. of the 32nd Annual
International Symposium on Computer Architecture, pages 520–531, June 2005.

[39] D. Sylvester, D. Blaauw, and E. Karl. Elastic: An adaptive self-healing
architecture for unpredictable silicon. IEEE Journal of Design and Test,
23(6):484–490, 2006.

[40] Trimaran. An infrastructure for research in ILP, 2000. http://www.trimaran.org/.
[41] M. Vachharajani, N. Vachharajani, D. A. Penry, J. A. Blome, S. Malik, and D. I.

August. The liberty simulation environment: A deliberate approach to
high-level system modeling. ACM Transactions on Computer Systems,
24(3):211–249, 2006.

[42] S. Vangal, J. Howard, G. Ruhl, S. Dighe, H. Wilson, J. Tschanz, D. Finan,

A. Singh, T. Jacob, S. Jain, V. E. C. Roberts, Y. Hoskote, N. Borkar, and
S. Borkar. An 80-tile sub-100-w teraﬂops processor in 65-nm cmos. In 2008
IEEE International Solid-State Circuits Conference, pages 29–41, Jan. 2008.

[43] S. Vrudhula, D. Blaauw, and S. Sirichotiyakul. Estimation of the likelihood of
capacitive coupling noise. In Proc. of the 39th Design Automation Conference,
pages 653–658, 2002.

[44] C. Weaver and T. M. Austin. A fault tolerant approach to microprocessor

design. In Proc. of the 2001 International Conference on Dependable Systems
and Networks, pages 411–420, Washington, DC, USA, 2001. IEEE Computer
Society.

[45] D. Wilson. The stratus computer system. Resilient Computing Systems,

1:208–231, 1986.

[46] E. Wu, J. M. McKenna, W. Lai, E. Nowak, and A. Vayshenker. Interplay of
voltage and temperature acceleration of oxide breakdown for ultra-thin gate
oxides. Solid-State Electronics, 46:1787–1798, 2002.

[47] J. Zeigler. Terrestrial cosmic ray intensities. IBM Journal of Research and

Development, 42(1):117–139, 1998.

151