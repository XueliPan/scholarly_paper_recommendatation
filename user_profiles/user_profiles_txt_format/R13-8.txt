Learning Partially Observable Action Schemas

Dafna Shahaf

and Eyal Amir

Computer Science Department

University of Illinois, Urbana-Champaign

Urbana, IL 61801, USA
dshahaf2,eyal
}

@uiuc.edu

{

Abstract

We present an algorithm that derives actions’ effects and pre-
conditions in partially observable, relational domains. Our
algorithm has two unique features: an expressive relational
language, and an exact tractable computation. An action-
schema language that we present permits learning of precon-
ditions and effects that include implicit objects and unstated
relationships between objects. For example, we can learn that
replacing a blown fuse turns on all the lights whose switch is
set to on. The algorithm maintains and outputs a relational-
logical representation of all possible action-schema models
after a sequence of executed actions and partial observations.
Importantly, our algorithm takes polynomial time in the num-
ber of time steps and predicates. Time dependence on other
domain parameters varies with the action-schema language.
Our experiments show that the relational structure speeds up
both learning and generalization, and outperforms proposi-
tional learning methods. It also allows establishing apriori-
unknown connections between objects (e.g. light bulbs and
their switches), and permits learning conditional effects in re-
alistic and complex situations. Our algorithm takes advan-
tage of a DAG structure that can be updated efﬁciently and
preserves compactness of representation.

1 Introduction

Agents that operate in unfamiliar domains can act intelli-
gently if they learn the world’s dynamics. Understanding
the world’s dynamics is particularly important in domains
whose complete state is hidden and only partial observations
are available. Example domains are active Web crawlers
(that perform actions on pages), robots that explore build-
ings, and agents in rich virtual worlds.

Learning domain dynamics is difﬁcult in general partially
observable domains. An agent must learn how its actions
affect the world as the world state changes and it is un-
sure about the exact state before or after the action. Cur-
rent methods are successful, but assume full observability
(e.g., learning planning operators (Gil 1994; Wang 1995;
Pasula et al.
2004) and reinforcement learning (Sutton
and Barto 1998)), or do not scale to large domains (re-
inforcement learning in POMDPs (Jaakkola et al. 1994;
Littman 1996; Even-Dar et al. 2005)), or approximate the
problem (Wu et al. 2005).

Copyright c
(cid:13)
gence (www.aaai.org). All rights reserved.

2006, American Association for Artiﬁcial Intelli-

In this paper we present a relational-logical approach to
scaling up action learning in deterministic partially observ-
able domains. Focusing on deterministic domains and the
relational approach yields a strong result. The algorithm that
we present learns relational schema representations that are
rich and surpass much of PDDL (Ghallab et al. 1998). Many
of the beneﬁts of the relational approach hold here, includ-
ing faster convergence of learning, faster computation, and
generalization from objects to classes.

Our learning algorithm uses an innovative boolean-circuit
formula representation for possible transition models and
world states (transition belief states). The learning algo-
rithm is given a sequence of executed actions and perceived
observations together with a formula representing the initial
transition belief state. It updates this formula with every ac-
tion and observation in the sequence in an online fashion.
This update makes sure that the new formula represents ex-
actly all the transition relations that are consistent with the
actions and observations. The formula returned at the end
includes all consistent models, which can be retrieved then
with additional processing.

We show that updating such formulas using actions and
observations takes polynomial time, is exact (it includes all
consistent models and only them), and increases the for-
mula size by at most a constant additive (without increas-
ing the number of state variables). We do so by updating
a directed acyclic graph (DAG) representation of the for-
mula. We conclude that the overall exact learning problem is
tractable, when there are no stochastic interferences; it takes
pk+1), for t time steps, p predicates, and k the
time O(t
maximal precondition length. Thus, this is the ﬁrst tractable
relational learning algorithm for partially observable rela-
tional domains.

·

These results are useful in deterministic domains that in-
volve many objects, relations, and actions, e.g., Web min-
ing, learning planning operator schemas from partially ob-
served sequences, and exploration agents in virtual domains.
In those domains, our algorithm determines how actions af-
fect the world, and also which objects are affected by ac-
tions on other objects (e.g., associating light bulbs with their
switches). The understanding developed in this work is also
promising for relational structure in real-world partially ob-
served stochastic domains. It might also help enabling rein-
forcement learning research to extend its reach beyond ex-

913plicit or very simply structured state spaces.

Related Work Our approach is closest to (Amir 2005).
There, a formula-update approach learns the effects (but not
preconditions) of STRIPS actions in propositional, deter-
ministic partially observable domains. In contrast, our algo-
rithm learns models (preconditions and effects) that include
conditional effects in a very expressive relational language.
Consequently, our representation is signiﬁcantly smaller,
and the algorithm scales to much larger domains. Finally,
our algorithm can generalize across instances, resulting in
signiﬁcantly stronger and faster learning results.

Another close approach is (Wu et al. 2005), which learns
action models from plans. There, the output is a single
model, which is built heuristically in a hill-climbing fash-
ion. Consequently, the resulting model is sometimes incon-
sistent with the input. In contrast, our output is exact, and
the formula that we produce accounts for exactly all of the
possible transition models (within the chosen representation
language). Furthermore, our approach accepts observations
and observed action failures.

Another related approach is structure-learning in Dy-
namic Bayes Nets (Friedman et al. 1998). This approach
addresses a more complex problem (stochastic domain), and
applies hill-climbing EM. It is a propositional approach, and
consequently it is limited to small domains. Also, it could
have unbounded errors in discrete deterministic domains.

In recent years, the Relational Paradigm enabled impor-
tant advances (Friedman et al. 1999; Dzeroski and Luc
De Raedt 2001; Pasula et al. 2004; Getoor 2000). This
approach takes advantage of the underlying structure of the
data, in order to be able to generalize and scale up well. We
incorporate those ideas into Logical Learning and present a
relational logical approach.

We present our problem in Section 2, propose several rep-
resentation languages in Section 3, present our algorithm in
Section 4, and evaluate it experimentally in Section 5.

2 A Relational Transition Learning Problem
Consider the example in Figure 1. It presents a three-room
domain. It is a partially observable domain – the agent can
only observe the state of his current room. There are two
switches in the middle room, and light bulbs in the other
rooms; unbeknownst to our agent, the left and right switches
affect the light bulbs in the left and right rooms, respectively.
The agent performs a sequence of actions: switching up
the left switch and entering the left room. After each action,
he gets some (partial) observations. The agent’s goal is to
determine the effects of these actions (to the extent he can),
while also tracking the world. Furthermore, we want our
agent to generalize: once he learns that switching up the
left switch causes it to be up, he should guess that the same
might hold for the other switch.

We deﬁne the problem formally as follows.

Deﬁnition 2.1 A relational transition system is a tuple
Obj,Pred,Act,P,S,A,R
h
i
•

Obj, Pred, and Act are ﬁnite sets of objects in the world,
predicate symbols, and action names, respectively. Pred-
icates and actions also have an arity.

t=1

t=2

t=3

SwUp(lSw)

GoTo(lRoom)

Figure 1: An action-observation sequence. The left part presents
the actions and actual states timeline, and the right illustrates some
possible
pairs at times 1,2,3, re-
spectively. Every row is a transition-relation fragment related to
the action sequence. A star indicates the agent’s location.

World-State,Transition-Relation
i

h

•

•

•

•

Obj.

∈
⊆

Pred, c1, .., cm ∈
Pow(P) is the set of world states; a state s

P is a ﬁnite set of ﬂuents of the form p(c1, ..., cm), where
p
S
subset of P containing exactly the ﬂuents true in s.
Act, ¯c = (c1, .., cn), ci ∈
A
∈
instances of Act.
S is the transition relation.
R
R means that state s0 is the result of per-

⊆ {
S

, ground

S is the

|
A

Obj,

a(¯c)

×

∈

a

}

⊆

×
s, a(¯c), s0
h

i ∈

forming action a(¯c) in state s. In our light bulb world,

lSw, rSw, lBulb, rBulb, lRoom,...

GoTo(1),
Obj=
SwUp(1), SwDown(1)
, P =
On(lBulb), On(rBulb), Up(lSw), Up(rSw), At(lRoom),..

On(1), Up(1), At(1)

, Pred=

, Act=

}

}

{

{

}

{

{
}

Our agent cannot observe the state of the world completely,
and he does not know how his actions change it. One way
to determine this is to maintain a set of possible world-states
and transition relations that might govern the world.

Deﬁnition 2.2 (transition belief state) Let
transition relations on S, A. A transition belief state ρ
S
transition relation.

⊆
, where s is a state and R a
i

is a set of pairs

be the set of

s, R
h

× R

R

The agent updates his transition belief state as follows after
he performs actions and receives observations.

S

∈

× R

Deﬁnition 2.3 Simultaneous Learning and Filtering of
a transition belief state,
Schemas (SLAFS) ρ
⊆
A an action. We assume that observations ¯o are
a(¯c)
logical sentences over P .
1. SLAFS[(cid:15)](ρ) = ρ ((cid:15): an empty sequence)
2. SLAFS[a(¯c)](ρ) =
{h
3. SLAFS[o](ρ) =
s, R
{h
4. SLAFS[
aj(¯cj), oj ii
j
h
SLAFS[
aj(¯cj ), ojii<j
h

i | h
ρ
|
i ∈
t](ρ) =
≤
t] (SLAFS[oi](SLAFS[ai(¯ci)](ρ)))
We call step 2 progression with a(¯c) and step 3 ﬁltering with
o. The intuition here is that every pair
transitions to
a new pair
after an action. If an observation discards
a state ˜s, then all pairs involving ˜s are removed from the set.

s,a(¯c),s’
o is true in s
}

s0, R
h

˜s, R
h

s,R
h

s’,R

i ∈

i ∈

R,

ρ

}

≤

≤

i

i

914We conclude that R is not possible when all pairs including
it have been removed. Note that we are interested in deter-
ministic domains, i.e. for every s, a(¯c) there is exactly one
s0; if the action fails, we stay in the same state. An extension
to domains in which there is at most one s0 is easy.

EXAMPLE

The right part of Figure 1 illustrates a
(simpliﬁed) transition belief state, consisting of three
s, R
h
i
pairs, and the way it is updated. At the beginning, the agent
considers three possibilities: all pairs agree on the initial
state (the light is currently off in the left room), but they sug-
gest different transition relations. Each chain illustrates the
way the state of the world changes according to one of the
relations (with respect to the speciﬁc action sequence): The
ﬁrst pair suggests that both actions do not affect the light, the
second suggests that entering the room turns on the light, and
the third associates the light with ﬂipping the switch. After
performing both actions, the agent observes that the light is
on in the left room. This observation contradicts the ﬁrst
pair, so we eliminate it from his belief state.

3 Representing Transition Belief States

The na¨ıve approach for representing transition belief states,
enumeration, is intractable for non-trivial domains. We ap-
ply logic to represent transition belief states more compactly
and to make learning tractable; later, we show how to solve
SLAFS as a logical inference problem, while maintaining a
compact representation of our transition belief state.

For humans, the action of opening a door and opening a
book is the same meta-action; in both cases, the object will
be opened. We try to capture this intuition.

Our logical languages represent transition belief states,
using ground relational ﬂuents from P (representing the
state of the world), and action-schemas, which are propo-
sitions that represent the possible transition relations.
In-
formally, schemas correspond to if-then rules;
together,
they are very similar to actions’ speciﬁcation in PDDL
(Ghallab et al.
For example, a schema in
our language is causes(SwUp(x), Up(x), TRUE) (switch-
if TRUE). This
ing up an object causes it
instances– ground transi-
schema represents a set of
causes(SwUp(lSw), Up(lSw), TRUE) and
tion rules, e.g.
causes(SwUp(rSw), Up(rSw), TRUE).

to be up,

1998).

Deﬁnition 3.1 (Schemas) A schema is a proposition of the
form causes(a(x1, ..., xn), F, G) (read: a(¯x) causes F if
G). a
Act is an n-ary action name, ¯x are n different
symbols, F (the effect) is a literal and G (the precondition)
is a sentence, both over PPat which we now deﬁne. W.l.g., G
is a conjunction of literals; otherwise, we can take its DNF
form and split it to several schemas of this form.

∈

. PPat

{

x1, x2, ...
}
.

Pat
}

Let Pat be a set of symbols that includes

is the set of patterned ﬂuents over Pat:

PPat =

p(y1, ..., ym)

p

Pred, y1, .., ym ∈

∈

|

{

In other words, a schema is a transition rule containing
variables. Its instances can be calculated by assigning ob-
jects to these variables; the result is a ground transition rule
causes(a(¯c), F, G) for a
A,F, G over P . That is, every
patterned ﬂuent (Up(x)) becomes a ﬂuent (Up(lSw)) after the
assignment. In order to compute the instances, we may need

∈

to know some relations between objects, e.g. which switch
controls which bulb. The set of possible relations is denoted
by RelatedObjs (see SL-H below).

Deﬁnition 3.2 (Transition Rules Semantics) Given
a
state s and a ground action a(¯c), the resulting state s0
satisﬁes every literal F which is the effect of an activated
rule (a rule whose precondition held in s). The rest of the
ﬂuents do not change– in particular, if no precondition held,
the state stays the same.
If two rules with contradicting
effects are activated, we say that the action is not possible.

{

).

We now present several languages to represent schemas,
starting from our most basic language.
In this language, Pat =
SL0: The Basic Language
. For any schema causes(a(x1, ..., xn), F, G),
x1, x2, ...
{
}
F and G include only symbols from x1, ..., xn. An instance
is an assignment of objects to x1, ..., xn. No related objects
are needed (we set RelatedObjs =
TRUE
}
Examples include causes(SwUp(x1), Up(x1), TRUE),
causes(PutOn(x1, x2), On(x1, x2), Has(x1)
Clear(x2))
(you can put a block that you hold on another, clear one).
Any STRIPS domain can be represented in SL0. Note that
SL0 can only describe domains in which every action a(¯c)
can affect only the elements in ¯c; the following extensions
are more expressive.
SL-V: Adding Quantiﬁed Variables Some domains
in effects and precondi-
permit quantiﬁed variables
tions;
than
For example, causes(sprayColor(x1),
its parameters.
Color(x2, x1), RobotAt(x3)
At(x2, x3)) (spraying color x1
causes everything at the robot’s location to be painted).

there, an action can affect objects other

∧

∧

{

Pat is still

, but F, G can include any symbol
x1, x2, ...
}
{
from Pat.
are the action’s parameters, ¯c (thus,
x1, ..., xn}
they are speciﬁed by the action a(¯c)).
represent
free variables. Similarly to PDDL, free variables that ap-
pear in the effect part of the schema are considered to be
universally quantiﬁed, and those that appear only in the pre-
condition part are considered existentially quantiﬁed. In the
previous example, x2 is universally quantiﬁed and x3 is ex-
istentially quantiﬁed. No related objects are needed.

xn+1, ...
}

{

In a more expressive variant of SL-V, the variables range
only over objects that cannot be described any other way–
that is, they do not range over the action’s parameters, ¯c (and
in richer languages, not over their functions). This allows
us to express defaults and exceptions, as in ”blowing a fuse
turns every light bulb off, except for a special (emergency)
bulb, which is turned on”, or ”moving the rook to (c1, c2)
causes it to attack every square (x, c2) except for (c1, c2)”.
If Pred includes equality, we can use a simpler variant.
SL-H: Adding Hidden Object Functions Pat =
. We write h1 as a short-
x1, x2, ...
{
} ∪ {
This extension can handle hidden
hand for h1(¯x).
objects– objects that are affected by an action, although
they do not appear in the action’s parameters.
For
example, the rules causes(SwUp(lSw), On(lBulb), TRUE),
causes(SwUp(rSw), On(rBulb), TRUE) are instances of the
schema causes(SwUp(x1), On(h1(x1)), TRUE) (ﬂipping up

h1, h2, ...
}

915∈

switch c1 causes its light bulb, h1(c1), to turn on. Note
that h1 is a function of the action’s parameters, which does
not change over time). SL-H includes related object propo-
sitions, which specify these functions:
di ∈
relobjs

|
means ’undeﬁned’. Every
RelatedObjs completely speciﬁes those functions.

hj( ¯d) = d0

Obj, d0

∪ ⊥}

Obj

⊥

∈

{

.

Other Possible Extensions: Extended Hidden Objects: in
SL-H, the hidden objects depended only on the action’s pa-
rameters. We add to the language new functions, that can
depend on the quantiﬁed variables as well. We add their
speciﬁcations to RelatedObjs. (example schema: OpenAll
causes all the doors for which we have a key to open)

Invented Predicates:

sometimes the representation of
the world does not enable us to learn the transition
model. For example, consider a Block-world with predicates
On(x,y),Has(x); this sufﬁces for describing any world state,
but we cannot learn the precondition of Take(x): it involves
On(y,x). If we add a predicate
universal quantiﬁcation,
Clear(x), it is easy to express all of the transition rules (in-
cluding those that affect Clear) in our language. This idea is
similar to the ones used in Constructive Induction and Pred-
icate Invention (Muggleton and Buntine 1988).

y.

¬

∀

We can also combine the languages mentioned above. For

example, SL-VH allows both variables and hidden objects.

4 Learning Via Logical Inference

In this section we present a tractable algorithm that solves
SLAFS exactly. The algorithm maintains a formula that rep-
resents the agent’s transition belief state. In order to main-
tain compactness, the formula is represented as a DAG (di-
rected acyclic graph). The algorithm can be applied to any
schema language.

4.1 Update of Possible Transition Models
Algorithm Overview (see Figure 2): We are given ϕ, a
formula over P and a schema language. ϕ represents the
initial belief state (if we know nothing, ϕ =TRUE). For ev-
ery ﬂuent, f , we maintain a formula, explf (intuition: the
explanation of f ’s value). This formula is updated every
time step, s.t. it is true if and only if f currently holds. An-
other formula, kb, stores the knowledge gained so far (by
ϕ, the observations, and the actions that were performed).
We make sure that those formulas do not involve any ﬂuent
(proposition from P ). To do this, we add new propositions
to the language, initf . Those propositions represent the ini-
tial state of each ﬂuent. kb and explf can only involve those
propositions and schema propositions.

At the beginning (steps 1-2 in DAG-SLAFS), we initial-
ize kb and explf according to ϕ, using those new proposi-
tions. Then we iterate: every time step, we progress with
the action and ﬁlter with the observation. Procedure DAG-
SLAFS-STEP updates explf according to successor-state
axioms (see below, and in procedure ExplAfterAction), and
adds the assertion that the action was possible (procedure

1If the language does not involve related objects, assume Re-

latedObjs=

TRUE

{

.
}

2Implementation depends on the schema language used.

ai(¯ci), oi

PROCEDURE DAG-SLAFS(
input: an action-observation sequence and a formula over P
1: for f
2: kb = replace every occurrence of f

P do explf = a new proposition initf

∈
∈
Lines 1-2: Preprocessing of ϕ

P by initf in ϕ

t, ϕ)

0<i<

≤

i

h

{

3: for i=1...t do
4:
5: return Vf

DAG-SLAFS-STEP(ai(¯ci), oi)

Process Sequence

}

P (f

∈

↔

explf )

kb

∧

∧

}

{
base

∈

P do

PossibleAct(f,a(¯c))

PROCEDURE DAG-SLAFS-STEP(a(¯c), o)
input: a(¯c) an action, o an observation
1: for f
2:
3:
4:
5: ReplaceFluents(kb)
6: for f
∈
7: kb = kb
8: ReplaceFluents(kb)

kb = kb
expl0f = ExplAfterAction(f,a(¯c))
ReplaceFluents(expl0f )

P do explf = expl0f {
∧

∧

o

P , a(¯c) an action

PROCEDURE PossibleAct(f,a(¯c))
input: f
1: ψ = TRUE
2: for relobjs
3:

RelatedObjs 1 do

∈

∈

1-6: Progress with action

}

7-8: Filter with observation

{

}

Compute all schema-instance pairs with effect f ,
(sch+,inst+)
{
regarding action a(¯c) and relobjs. 2
ψ = ψ

, and those with effect

relobjs

f ,

¬

{

}

∧

[(W sch+
∧
the action is possible if relobjs is true

(W sch-

→
prec(inst+))

∧

∧

(sch-,inst-)

,
}

prec(inst-))]

}

4:

¬
{

5: return ψ

P , a(¯c) an action

PROCEDURE ExplAfterAction(f,a(¯c))
input: f
1: ψ = TRUE
2: for relobjs
3:

RelatedObjs do

∈

∈

Compute all schema-instance pairs with effect f ,
(sch+,inst+)
{
regarding action a(¯c) and relobjs.
ψ = ψ

, and those with effect

relobjs

f ,

¬

{

}

∧

→

[W sch+
∧
f ’s value after the action if relobjs is true

prec(inst+)]

(W sch-

∧ ¬

[f

∨

∧

(sch-,inst-)

,
}

prec(inst-))]

}

4:

5: return ψ

{

PROCEDURE ReplaceFluents(ψ)
input: ψ a formula
1: for f

∈

P do replace f by a pointer to explf in ψ

Figure 2: DAG-SLAFS

PossibleAct) to kb. Both updates insert ﬂuents into our for-
mulas; we use ReplaceFluents to replace the ﬂuents for their
(equivalent) explanations. This is done using pointers to the
relevant node, so there is no need to copy the whole formula.
DAG-SLAFS-STEP also adds the observation to kb, and

uses ReplaceFluents to eliminate ﬂuents.

After every iteration, the updated ϕ is kb

↔
explf ). At the end, we return it conjoined with base, which
is a formula that each transition relation must satisfy; we use
it to ensure that we return only legal relations.
base := baserelobjs

V

∧

∈

P (f

f

∧

916(causes(a, F, G)

∧
[causes(a, F, G0)

causes(a,

F, G))

¬

causes(a, F, G) ]

→

a,F,G ¬
a,F,G

G0

→

V
V

and baserelobjs is a formula that the related objects must sat-
isfy. It depends on the schema language used.

(EXAMPLE– BUILDING THE DAG:) In Figure 3 we see
how explOn(lBulb) is updated after the ﬁrst action, SwUp(lSw).
The DAG in Figure 3 is the formula expl’On(lBulb) (after up-
date). The node labeled ”expl” is the root of the DAG before
the update. The bottom nodes (the leaves) are the propo-
sitions: This is a simpliﬁed example, so we only show two
relobjs nodes– (p1,p2), and two schemas– tr1,tr2. tr1 claims
that switching up an object x causes its hidden object, h(x)
to become on. tr2 claims that it turns off everything that is
currently on. p1,p2 relate the left switch with the left and
right light bulbs, respectively.
nodes (second layer) correspond to different cases of
The
relobjs. The
node is the explanation of On(lBulb) in case
p1 holds. Its left branch describes the case that the action
caused the ﬂuent to hold– tr1 is true, and its preconditions
hold; the right branch deals with the case that On(lBulb) held
before the action, and the action did not change it (that is,
either tr2 is false, or its precondition does not hold). The
formula in Figure 3 can be simpliﬁed, but this is an opti-
mization that is not needed for our algorithm.

→

∨

→

→

.....

expl’
∧

∨

∧

∧

¬

∧

.....

.....

p1

tr1

T

F

tr2

p2

h(lSw)=

lBu

SwUp(x)
causes
On(h(x))

 if T

expl

init

On(lBu)

h(lSw)=

rBu

SwUp(x)
causes
¬On(y)
 if On(y)

Figure 3: a (simpliﬁed) update of On(lBulb) after the ﬁrst action,
SwUp(lSw)

UPDATING THE FORMULAS– A CLOSER LOOK: Given
relobjs, a ground action a(¯c) and a ﬂuent f , we want to up-
date explf and kb. To do this, we ﬁrst identify the instances
that can affect ﬂuent f , and the schemas they correspond to.
Denote by (sch+,inst+) a schema-instance pair that can
inst+ is a transition rule with effect f , which is
cause f .
an instance of schema sch+. In other words– if the schema
is true in our domain, and the precondition of the instance
(prec(inst)) holds, f will hold after the action. Similarly,
(sch-,inst-) is a pair that can cause
f . We need relobjs and
¬
a(¯c) to match schemas and instances.

Fluent f is true after the action if either (1) a schema sch+
is true and the precondition of its instance holds, or (2) f
holds, and for every schema sch- that is true, no precondition
holds. kb asserts that the action was possible; it cannot be
the case that there are two schema-instance pairs, such that

their effects are f and

f , and both preconditions hold.

We assume that the sequence consists of possible actions;
if the agent has a way to know whether the action was pos-
sible, we do not need this assumption.

¬

Theorem 4.1 DAG-SLAFS is correct. For any formula ϕ
and a sequence of actions and observations

=

{h

s, R

that satisfy DAG-SLAFS(
ai(¯ci), oii0<i<
h
s, R
{h

SLAFS[
ai(¯ci), oii0<i<
h

that satisfy ϕ
}

t, ϕ)
}
).

t](

≤

≤

i

i

PROOF OVERVIEW we deﬁne an effect model for action
a(¯c) at time t, Teff(a(¯c), t), which is a logical formula con-
sisting of Situation-Calculus-like axioms (Reiter 2001). It
describes the ways in which performing the action at time
t affects the world. We then show that SLAFS[a(¯c)](ϕ) is
equivalent so consequence ﬁnding in a restricted language
of ϕ
Teff(a(¯c), t). Consequence ﬁnding can be done by re-
solving all ﬂuents that are not in the language; we show that
DAG-SLAFS calculates exactly those consequences.

∧

COMPLEXITY

In order to keep the representation
compact and the algorithm tractable, we implement the algo-
rithm to maintain a DAG instead of a ﬂat formula. This way,
when ReplaceFluents replaces f by explf , we only need to
update a pointer, rather than copying the expression again.
This allows us to recursively share subformulas.
the total length of
Let ϕ0 be the initial belief state,
the observations (if observations are always conjunctions of
literals, we can omit it), t is the length of the sequence.
The maximal precondition length, k, is at most min
{
|
. Let pairs be the maximal num-
preconditions are k’-DNF
}
ber of schema-instance pairs for an action a(¯c). Let ra, rp be
the maximal arities of actions and predicates, respectively.

Obs
|
|

k0

Theorem 4.2 With the DAG implementation, DAG-SLAFS’s
time and space (formula size) complexities are O(
+
ϕ0|
|
pairs) We can maintain an NNF-DAG (no
Obs
|
|
negation nodes) with the same complexity.

+ t

k

·

·

| ·

P
|

Pred
|

Pred
(2
|
·
Pred
|
| ·

If we allow preprocessing (allocating space for the leafs):
rp)k+1. In SL-H with f func-
ra
(ra + f )rp)k+1. In
RelatedObjs
(2
|
|
rp
(ra +
|
(2
(ra + (k +
If we add invented predicates, we increase

In SL0, pairs = (2
Pred
tions, pairs =
| ·
|
SL-V without existential quantiﬁers,
P
|
rp)rp)k+1, and with them-
(k+1)rp
|
1)rp)rp)k+1.
Pred
|
|
Interestingly, SL0 (and some cases of SL-H) allow run-
time that does not depend on the domain size (requiring
Importantly, SL0 in-
a slightly different implementation).
cludes STRIPS.
If there are no preconditions (always executable actions), we
can maintain a ﬂat formula with the same complexity.

Since ra, rp and k are usually small, this is tractable.

accordingly.

| ·

·

Note: The inference on the resulting DAG is difﬁcult
variables). The related problem of tempo-
(SAT with
ral projection is coNP-hard when the initial state is not fully
known.

P
|

|

Using the model Our algorithm computes a solution to
SLAFS as a logical formula; we can use a SAT solver in
order to answer queries about the world state and the tran-
sition model. If the formula is represented as a DAG, we

917Time \ Size as SLAFS Proceeds

Inference on the DAG

Time (991)
Time (271)
Time (55)
Size (991)
Size(271)
Size (55)

1500

1200

 
e
z
i
S
 
a
l
u
m
r
o
F

)
0
0
0
1
*
s
e
d
o
n
(

900

600

300

 
,
)
0
0
0
1
/
c
e
s
(
 
e
m
T

i

 
l
a
t
o
T

0

0

Find a Model

False Schema
True Schema
False Fluent
True Fluent

2000

1750

1500

1250

1000

750

500

250

)
0
0
0
1

/

c
e
s
(
 

e
m
T

i

 
l

a
t
o
T

0

0

2000

4000

6000

8000

10000

Number of Steps

1000

2000

3000

4000

Number of Steps

Figure 4: Left: Time and space for several Block-Worlds (numbers represent
). As can be seen, the time and space do not depend on the
|
size of the domain. Slight time differences are due to a hash table. Right: Inference time on DAG-SLAFS’ output, for several simple queries.

P
|

use an algorithm which adapts DPLL for DAGs (we have
created such an implementation– see Section 5). Note that
the number of variables in the formula is independent of the
length of the sequence. Therefore, we can use DPLL-DAG
and SAT solvers for very long sequences. We can also use
bias (McCarthy 1986) to ﬁnd minimal models. Preferential
bias is well studied and ﬁts easily with logical formula.

5 Experimental Results

We implemented and tested DAG-SLAFS for SL0 and for
a variant of SL-V; we also implemented a version for the
case of always-executable actions, which returns a ﬂat for-
mula. In addition, we implemented a DPLL SAT-search al-
gorithm for DAGs. It ﬁnds satisfying assignments for the
algorithm’s output. We tested the SLAFS algorithms on
randomly generated partially observable action sequences,
including STRIPS domains (Block-Worlds, Chess, Driver-
log), and ADL, PDDL domains (Briefcase, Bomb-In-Toilet,
Safe, Grid) of various size, ranging from tens to thousands
of propositional ﬂuents.

Figures 4, 6 present some of our results. We measured
the time, space, knowledge rate (percentage of schemas that
were learned, out of all the schemas currently in the knowl-
edge base), and learning rate (percentage of schemas that
were learned, out of all of the schemas that could have been
learned from the given sequence). A schema is learned, if
all models assign the same truth value to it.

As expected, the algorithm takes linear time and space in
the sequence length, and does not depend on the domain’s
size (Figure 4 ). Importantly, simple SAT queries return rela-
tively fast, especially regarding schemas which were contra-
dicted by the sequence. Naturally, more complicated queries
take longer time.

Decreasing the number of observations also resulted in
long inference time: in the Bomb-In-Toilet domain, we gen-
erated several action-observation sequences, with different
degrees of observability (from full observability to 10% of
the ﬂuents). Then, we then chose 40 transition rules at
random, and checked how many of them were learned for
each sequence. Not surprisingly, both learning and inference
were faster when the number of observations was higher. If
there are no observations we can still eliminate some mod-
els, since we know that the actions were possible.

Another important point is that, most of the schemas that
can be learned are learned very quickly, even for larger
domains. In most domains, more than 98% of schemas were
learned after 200 steps (Figure 6). This is mainly because
the number of action schemas does not depend on the size
of the domain, e.g. all Block-Worlds have exactly the same
number of schemas. Compare this with the decreasing
knowledge rate in the propositional approach of (Amir
2005). The latter does not generalize across instances, and
the number of encountered (propositional) transition rules
grows faster than those that are learned.

(dunk ?bomb ?toilet) causes (NOT (armed
?bomb)) if (NOT (clogged ?toilet))
(dunk ?bomb ?toilet) causes (clogged
?toilet) if (TRUE)
(dunk ?bomb ?toilet) causes (toilet
?toilet) if (TRUE)
(flush ?toilet) causes (not (clogged
?toilet)) if (TRUE)
-----------------
(dunk ?bomb ?toilet) causes (NOT (armed
?bomb)) if (AND (bomb ?bomb) (toilet
?toilet) (NOT (clogged ?toilet)))
(dunk ?bomb ?toilet) causes (clogged
?toilet) if (TRUE)
(flush ?toilet) causes (not (clogged
?toilet)) if (toilet ?toilet)

Figure 5: Possible Models of the Bomb-Toilet World (Top:
after 5 steps. Bottom: after 20 steps)

In another comparison, we ran sequences from (Wu et al.
2005), and each one took our algorithm a fraction of a sec-
ond to process. We also cross-validated our output and the
output of (Wu et al. 2005) with a known model. We found
that several outputs of (Wu et al. 2005) were inconsistent
with the action-observation sequence, while the true model
was consistent with our ﬁnal transition belief state.

Note that

their algorithm returns one (approximate)
model, whereas our algorithm return a formula that repre-
sents all consistent models. Figure 5 shows two models of
Bomb-In-Toilet world. Those models were found by run-

918Learning Curve

SLAF vs. SLAFS

100

 

d
e
n
r
a
e
L
s
a
m
e
h
c
S

 
f
o
%

 

75

50

25

0

0

89 Fluents

181 Fluents

461 Fluents

991 Fluents

1

 

d
e
n
r
a
e
L
s
a
m
e
h
c
S

 
f
o
%

 

0.8

0.6

0.4

0.2

0

0

SLAFS Learning

SLAFS Knowledge

SLAF Learning

SLAF Knowledge

100

200

300

400

Number of Steps

30

60

90

120

Number of Steps

Figure 6: Block Worlds. Left: SLAFS learning rate. Right: SLAF (Amir 2005) and SLAFS knowledge and learning rates.

ning our DPLL algorithm on the resulting DAG after 5 and
20 steps, respectively, and returning the ﬁrst satisfying as-
signment. The second model (20 steps) is more reﬁned than
the ﬁrst one, and is quite close to the real model.
Trying a schema language that is too weak for the model (for
example, trying SL0 for the Briefcase World) resulted in no
models, eventually.

6 Conclusions

We presented an approach for learning action schemas in
partially observable domains. The contributions of our work
are a formalization of the problem, the schema languages,
and the tractable algorithm. Our results compare favorably
with previous work, and we expect to apply and specialize
them to agents in text-based adventure games, active Web
crawling agents, and extensions of semantic Web services.

Signiﬁcantly, our approach is a natural bridge between
machine learning and logical knowledge representation. It
shows how learning can be seen as logical reasoning in com-
monsense domains of interest to the KR community. It fur-
ther shows how restrictions on one’s knowledge represen-
tation language gives rise to efﬁcient learning algorithms
into that language. Its use of logical inference techniques
(especially resolution theorem proving served in proving
correctness of our theorems) and knowledge representation
techniques makes it applicable to populating commonsense
knowledge bases automatically.

6.1 Criticism and Future Directions

Firstly, our work is not robust to noise (because of its logical
nature). This limits its potential utility. Na¨ıve ways to handle
noise will affect the efﬁciency of the inference.

Several other topics need to be addressed: How does ob-
servability affect learning? How does the choice of schema
language affect it? Also, a more detailed analysis of conver-
gence to the correct underlying model is needed.

Acknowledgements This work was supported by a De-
fense Advanced Research Projects Agency (DARPA) grant
HR0011-05-1-0040, and by a DAF Air Force Research
Laboratory Award FA8750-04-2-0222 (DARPA REAL pro-
gram).

References

E. Amir. Learning partially observable deterministic action mod-
els. In IJCAI ’05. MK, 2005.
Saso Dzeroski and K. Driessens Luc De Raedt. Relational rein-
forcement learning. Machine Learning, 43(1-2):7–52, 2001.
E. Even-Dar, S. M. Kakade, and Y. Mansour. Reinforcement
learning in POMDPs. In IJCAI ’05, 2005.
N. Friedman, K. Murphy, and S. Russell. Learning the structure
of dynamic probabilistic networks. In Proc. UAI ’98. MK, 1998.
Nir Friedman, Lise Getoor, Daphne Koller, and Avi Pfeffer.
In IJCAI ’99, pages
Learning probabilistic relational models.
1300–1307. MK, 1999.
Lise Getoor. Learning probabilistic relational models. Lecture
Notes in Computer Science, 1864:1300–1307, 2000.
M. Ghallab, A. Howe, C. Knoblock, D. McDermott, A. Ram,
M. Veloso, D. Weld, and D. Wilkins. PDDL – The Planning Do-
main Deﬁnition Language, version 1.2. Technical report, Yale
center for computational vision and control, 1998.
Y. Gil. Learning by experimentation: Incremental reﬁnement of
incomplete planning domains. In Proc. ICML-94, 1994.
T. Jaakkola, S. P. Singh, and M. I. Jordan. Reinforcement learning
algorithm for partially observable Markov decision problems. In
Proc. NIPS’94, volume 7, 1994.
Algorithms for sequential decision making.
M. L. Littman.
PhD thesis, Department of Computer Science, Brown University,
1996. Technical report CS-96-09.
J. McCarthy. Applications of circumscription in formalizing com-
mon sense knowledge. Artiﬁcial Intelligence, 28:89–116, 1986.
S. Muggleton and W. Buntine. Machine invention of ﬁrst-order
predicates by inverting resolution. In Proc. ICML-88, 1988.
H. M. Pasula, L. S. Zettlemoyer, and L. P. Kaelbling. Learning
probabilistic relational planning rules. In Proc. ICAPS’04, 2004.
R. Reiter. Knowledge In Action: Logical Foundations for De-
scribing and Implementing Dynamical Systems. MIT Press, 2001.
R. S. Sutton and A. G. Barto. Reinforcement Learning: an intro-
duction. MIT Press, 1998.
X. Wang. Learning by observation and practice: an incremental
In Proc. ICML-95,
approach for planning operator acquisition.
pages 549–557. MK, 1995.
K. Wu, Q. Yang, and Y. Jiang. Arms: Action-relation modelling
system for learning action models. Proc. ICAPS’05, 2005.

919