Visual Comput (2005) 21: 559–568
DOI 10.1007/s00371-005-0318-y

O R I G I N A L A R T I C L E

Jiaping Wang
Xin Tong
John Snyder
Yanyun Chen
Baining Guo
Heung-Yeung Shum

Capturing and rendering geometry details for
BTF-mapped surfaces

Published online: 31 August 2005
© Springer-Verlag 2005

J. Wang∗ ((cid:1))
Institute of Computing Technology,
Chinese Academy of Science
Graduated School of Chinese Academy of
Science
e_boris2002@hotmail.com
X. Tong · Y. Chen · B. Guo · H.-Y. Shum
Microsoft Research Asia
{xtong, yachen, bainguo,
hshum}@microsoft.com

J. Snyder
Microsoft Research
johnsny@microsoft.com
∗ This work was done while J. Wang was
visiting Microsoft Research Asia.

Abstract Bidirectional texture
functions, or BTFs, accurately model
reﬂectance variation at a ﬁne (meso-)
scale as a function of lighting and
viewing direction. BTFs also capture
view-dependent visibility variation,
also called masking or parallax,
but only within surface contours.
Mesostructure detail is neglected at
silhouettes, so BTF-mapped objects
retain the coarse shape of the under-
lying model.
We augment BTF rendering to obtain
approximate mesoscale silhouettes.
Our new representation, the 4D
mesostructure distance function
(MDF), tabulates the displacement
from a reference frame where a ray
ﬁrst intersects the mesoscale geom-
etry beneath as a function of ray
direction and ray position along that
reference plane. Given an MDF,
the mesostructure silhouette can be
rendered with a per-pixel depth peel-

ing process on graphics hardware,
while shading and local parallax are
handled by the BTF. Our approach
allows real-time rendering, handles
complex, non-height-ﬁeld mesostruc-
ture, requires that no additional
geometry be sent to the rasterizer
other than the mesh triangles, is
more compact than textured visibility
representations used previously,
and, for the ﬁrst time, can be easily
measured from physical samples. We
also adapt the algorithm to capture
detailed shadows cast both by and
onto BTF-mapped surfaces. We
demonstrate the efﬁciency of our
algorithm on a variety of BTF data,
including real data acquired using our
BTF–MDF measurement system.

Keywords Bidirectional texture
functions · Reﬂectance and shad-
ing models · Rendering · Shadow
algorithms · Texture mapping

1 Introduction

Real-world textures arise from both spatially varying sur-
face reﬂectance and mesostructure, which represents ﬁne-
scale geometric details [14]. The 6D bidirectional texture
function (BTF) captures these variations [6] and has been
exploited to measure real materials and map their proper-
ties onto arbitrary geometric models [23, 25–27]. Render-
ing BTF-mapped surfaces yields rich visual effects such
as ﬁne-scale shading, shadows, interreﬂections, and occlu-
sions, but it omits detailed silhouettes. This is because the

BTF only provides radiance measurements as a function
of lighting and viewing direction, leaving mesostructure
visibility implicit. Silhouettes are thus identical to that of
the underlying mapped surface, as shown in Fig. 1a.

To solve this problem, we build on recent methods that
explicitly tabulate local visibility. The view-dependent
displacement map (VDM) [29] precomputes the meso-
structure visibility as a function of
texture position,
view direction, and curvature. The result is stored as
a 5D table and allows real-time rendering with graph-
ics hardware. The generalized displacement map (GDM)
[30] also stores a 5D table of displacements, but as

560

J. Wang et al.

Fig. 1. a Existing BTF rendering techniques produce unrealistic silhouettes and shadow boundaries. b Our technique generates detailed
silhouettes and shadow boundaries for BTF-mapped objects

a function of ray direction and 3D position in a vol-
ume. Both representations rely on synthetic visibility
data, and it
is not clear how either can be acquired
from real materials. Moreover, the 5D table required
strains the hardware memory system, making the ap-
proach less practical. Most fundamentally, neither method
was combined with BTF rendering but instead used tradi-
tional normal-mapped shading and two-pass shadowing.
Using a BTF efﬁciently provides local shadowing with-
out a shadow buffer, as well as other more general effects
such as diffuse interreﬂections and spatially varying re-
ﬂectance, which are impractical with traditional shading
techniques.

To add silhouette visibility to BTF rendering, we pro-
pose a new, image-based method that represents mesostruc-
ture as a 4D mesostructure distance function (MDF). It is
used as a companion function for a given BTF and records,
for each BTF viewing direction, the displacement along
that direction of the mesostructure from the reference
plane. Now we can determine whether the mesostruc-
ture is visible at the silhouette by rendering the front
surface and the corresponding back surfaces and test-
ing whether the depth between them exceeds the sum of
the displacements indexed from the MDF at these two
surfaces. Since object surfaces self-occlude, we use a mul-
tipass depth peeling approach to ensure that we can get the
front surface and its corresponding back surfaces in sil-
houette rendering. Moreover, depth peeling ensures that
we see through closer layers where the view ray misses
the mesostructure to more distant ones where it may hit.
The result provides visually important silhouette effects
and can be extended to shadows, as shown in Fig. 1b.

The MDF can be seen as a simpliﬁed VDM with-
out its curvature parameter. However, this simpliﬁcation
is not as straightforward as it may appear. The VDM

rendering method relies on a curvature parameter to ob-
tain silhouette visibility effects. In other words, setting
the curvature parameter to 0 and using the VDM ren-
dering method [29] results in silhouettes from the un-
derlying coarse model, just as with BTF rendering. It
is only by using both our MDF representation and ren-
dering method that we obtain mesoscale silhouette ef-
fects.

Our primary contribution is to add detailed silhouettes
to BTF rendering. We show how our MDF representa-
tion can be obtained by measuring real samples at the
same time the BTF is acquired. We also demonstrate re-
alistic shadowing as well as silhouette effects, including,
for the ﬁrst time, shadowing onto surfaces mapped with
mesoscale geometry. Our results demonstrate an efﬁcient
solution to simulate all signiﬁcant visual effects caused by
geometric details on BTF-mapped surfaces.

2 Related work

A number of techniques measure BTFs from real materials
[5, 6, 8, 9, 23, 31]. Such BTFs can be mapped or synthe-
sized onto geometric models (e.g., [16, 27]). BTFs have
also been used for rendering complex, non-height-ﬁeld
mesostructures such as fabric weaves [23] and fur [8]. The
polynomial texture map [17] may be regarded as a special
form of BTF with a ﬁxed view.

Despite the high-dimensional data required, BTF-
mapped surfaces can be rendered in real time [15, 20, 23,
26, 28]. These methods decompose the 6D BTF data into
a combination of lower-dimensional textures easily loaded
and rendered on graphics hardware. Sloan et al. [25] ap-
ply the precomputed radiance transfer (PRT) technique

Capturing and rendering geometry details for BTF-mapped surfaces

561

to BTF-mapped surfaces. All these approaches ignore the
mesostructure silhouette.

For height-ﬁeld mesostructures [1, 3], many techniques

exist to achieve shading and parallax effects.

Horizon mapping generates mesoscale shadows on
bump maps [19]. It can be accelerated using graphics
hardware [24], and the representation can be captured
from real samples [22]. Precomputed visibility [10] also
renders mesoscale shadows and interreﬂections on bump-
mapped surfaces. These methods neglect visibility effects
entirely (e.g., parallax from bumps) both within surface
contours and at silhouettes. They also lack the generality
and realism of full BTF shading.

The parallax map [12] captures the local parallax
of a height ﬁeld in real time. Most recently, Policarpo
et al. [21] have proposed a technique to render the
self-occlusions, interpenetrations, and shadows of relief-
mapped surfaces in real time. As is the case with BTF
techniques,
the underlying object’s silhouette is un-
changed in these approaches. Shading is computed tradi-
tionally; i.e., is non-BTF.

Hirche et al. [11] propose an algorithm to render a dis-
placed surface on a per-pixel basis. Each triangle of an
underlying mesh is extruded to form a prism. The inter-
section of the ray and extruded triangle is computed at
each pixel of the prism faces. (A similar approach is also
employed by the GDM method and must also rasterize
prism faces rather than just mesh triangles.) Although the
technique can render the detailed silhouette of a displaced
surface in real time, it is limited to height-ﬁeld mesostruc-
tures and so precludes many real-world examples includ-
ing the grass and weave textures in our results. We also
demonstrate an acquisition system that measures both the
BTF and MDF (visibility) simultaneously and a more re-
alistic, BTF-based rendering method.

3 MDF measurement

For a given BTF, the companion MDF records the dis-
tance of the mesostructure surface from a reference sur-
face along a BTF viewing direction. As illustrated in
Fig. 2, the MDF is a 4D function d = D(x, y, θ, φ), where
(x, y) is the texture coordinate and the unit vector (θ, φ) is
a sample viewing direction of the BTF expressed in spher-

ical coordinates. For a given direction (θ, φ), each refer-
ence surface point p = p(x, y) projects to a mesostruc-
ture point p(cid:2) along the direction (θ, φ), and D(x, y, θ, φ)
is deﬁned as the distance from p to p(cid:2). To facilitate the
mesostructure visibility computation, we deﬁne both BTF
and MDF on a reference surface that lies above the top of
the mesostructure details.

Our MDF measurement device is a simple laser scan
unit consisting of a laser source and a controller built with
LEGO MindStorms Robotics Invention System 2.0. The
laser scan unit is added to an existing BTF measure-
ment system to measure a companion MDF for each cap-
tured BTF. Speciﬁcally, we generate a displacement image
Dv(x, y) = D(x, y, v) for every viewing direction v of the
BTF.

3.1 System setup

MDF measurement is closely coupled with BTF meas-
urement. As illustrated in Fig. 3a, our BTF measurement
device mainly consists of a turntable and two concentric
arcs, one for light sources and the other for cameras. The
stationary camera arc shown on the left holds eight Drag-
onﬂy cameras, each providing 1024 × 768 24-bit color
images at 15 Hz. The cameras are calibrated as in [32].
The light arc on the right rotates horizontally, driven by
a stepping motor. Eight halogen lamps are mounted on the
light arc. The light sources are manually calibrated as in
[2]. The turntable in the middle holds material samples to
be measured and is driven by another stepping motor. All
the lamps, cameras, and stepping motors are controlled by
a PC.

BTF capture is a fully automatic process controlled by
a PC. After rotating the light arc and turntable to their
desired sampling positions, the PC sequentially turns the
lamps on/off. With each lamp being turned on, every cam-
era records an image. The captured images are rectiﬁed
and then clipped to form the output BTF data.

Figure 3b shows the laser scan unit, which contains
a laser strip generator and a controller. A miniature struc-
tured light diode laser serves as the light source, with a 60◦
line lens placed in front of the laser to project a laser strip

Fig. 2. Deﬁnition of MDF

Fig. 3. a Our BTF–MDF measurement system. b Laser scan unit

562

J. Wang et al.

on the turntable. The controller includes a robust gear-
driven system that rotates the laser emitter vertically so
that the laser strip scans across the turntable. We adjust the
line lens direction to ensure that the laser scanning direc-
tion on the turntable is orthogonal to the laser strip. The
laser unit is mounted on the light arc as shown in Fig. 3a.
In principle we could build a separate laser range scan-
ner for measuring the MDF. By combining MDF with
BTF acquisition, we use the same cameras for measuring
both the BTF and MDF and avoid cross-registration prob-
lems. Our laser unit is easy to build and add to an existing
BTF device.

3.2 Capturing process

The MDF measurement process involves three steps. First
we project laser strips onto the material sample to be
measured. As a laser strip scans across the sample, each
camera records an image sequence. For a given camera
with viewing direction v, the laser strip is often partly oc-
cluded from the camera by the mesostructure being meas-
ured. For this reason it is necessary to scan the material
sample using laser strips projected from multiple source
locations so that each mesostructure point visible from the
camera is scanned at least once. Thus for each v we cap-
ture a number of laser scan image sequences, one for each
laser source location. Then we apply space–time analysis
to recover a displacement image for each captured image
sequence. Finally, for every viewing direction v we merge
all displacement images of v to generate Dv(x, y).

Scan image sequences. For each viewing direction v we
scan the material sample with the laser from 26 source lo-
cations as illustrated in Fig. 4. Since the laser emitter is far
from the turntable, we regard all the laser planes (a laser
plane is a plane containing the laser strip and laser source
point) in one scan as being parallel. Interreﬂections on
the mesostructure surface can produce noise in the laser
scan images. We resolve this problem by using a quick
shutter speed so that the interreﬂections are underexposed
and can be ﬁltered out by postprocessing. Alternatively

we can spray the material sample with matte paint be-
fore MDF measurement. Before recording laser scan im-
ages for each laser source location, we ﬁrst compute the
angle between the laser plane and the reference plane (the
turntable plane). To do this we project the laser strip onto
two planes with different heights and calculate the off-
set of the laser strip in the two planes from two recorded
laser scan images. From this offset we derive the angle be-
tween the laser plane and the reference plane. Once the
angle is found, laser scan images of the material sample
are recorded automatically. Speciﬁcally, we rotate the light
arc and turntable to the desired sampling position. As the
laser strip scans across the material sample, an image se-
quence is recorded by each BTF device camera at 7.5 Hz.
An MDF capturing session produces a set of laser scan
image sequences. The image sequence for laser source di-
rection l and viewing direction v is {Iv,l(x, y, ti)}.

Initial displacement images. From each laser scan image
sequence {Iv,l(x, y, ti)} we compute the displacement
image Dv,l(x, y) using space–time analysis [4]. A simi-
lar method has been used for shadow scanning [2]. Before
the space–time analysis, we compute the laser plane for
every image Iv,l(x, y, ti) in two steps. The ﬁrst step is to
detect the laser line in the image. Using the bounding box
of the material sample, we divide Iv,l(x, y, ti) into two
parts: the reference region showing only the turntable and
the sample region where the material sample is imaged.
The bounding box is manually speciﬁed on the reference
plane before capturing. The laser strip should be a line
in the reference region since it is ﬂat. As illustrated in
Fig. 5a, to ﬁnd this line, we search the reference region
for candidate pixels on the line along the laser scan di-
rection. Since the candidate pixels are by far the brightest
pixels in the image, it is easy to reject outliers using a pre-

Fig. 4. The laser source locations are marked by orange dots. Cur-
rently we simply remount the laser unit on the light arc manually
(5 times per capturing session). Alternatively, we can mount several
identical units along the light arc or use a mobile unit

Fig. 5a,b. MDF capturing process. a Laser line detection. Bottom
green curve: Intensity of pixels on vertical line. Peak point: candi-
date pixel on laser line. b Space–time analysis. Bottom red curve:
intensity variation of a pixel in whole scanning sequence. Peak
point: time at which this pixel images a mesostructure point

Capturing and rendering geometry details for BTF-mapped surfaces

563

scribed threshold. For noise reduction, we Gaussian-ﬁlter
the image before the search. After ﬁnding all candidate
pixels, we ﬁt a line to them. In the second step, we calcu-
late the laser plane for the image Iv,l(x, y, ti) by projecting
the laser line from Iv,l(x, y, ti) to the 3D reference plane.
Once the laser planes of all the images are found, the
space–time analysis proceeds as follows. As illustrated
in Fig. 5b, for each pixel (xs, ys) in the sample region,
we search the temporal sequence {Iv,l(xs, ys, ti)} to ﬁnd
the pixel (xs, ys, ti0
).
) is greater than the prescribed threshold,
If Iv,l(xs, ys, ti0
) images a mesostructure point
then the pixel (xs, ys, ti0
p. We ﬁnd p by projecting the pixel (xs, ys, ti0
) onto the
laser plane of the image Iv,l(x, y, ti0
). We then compute
the displacement of p from the reference plane and record
) is less than
the result in Dv,l(xs, ys). If the Iv,l(xs, ys, ti0
the prescribed threshold, the pixel (xs, ys, ti0
) corresponds
to an occluded mesostructure point. In this case, we set
Dv,l(xs, ys) = −1.

) that has the brightest Iv,l(xs, ys, ti0

Merging displacement images. For every viewing direc-
tion v, we merge all displacement images Dv,l(x, y) to
generate Dv(x, y):

(cid:2)

Dv(x, y) = 1(cid:1)
wi

wi Dv,li

(x, y)

(Dv,li

(x, y) (cid:4)= −1),

where the wi is the weight of the scan result with laser
direction li. Based on the error analysis in [2], we com-
pute the weight for each scan Dv,l(x, y) as the volume of
the tetrahedron formed by the laser source position, the
camera location, and two points on the laser line passing
through the turntable center.

4 Rendering BTF-mapped surfaces with MDF

After the BTF is mapped or synthesized onto arbitrary sur-
faces [27], the associated MDF texture can be mapped
onto the surface with the same texture coordinates. The
MDF data mapped onto surface are then used for ren-
dering the mesostructure silhouette and mesostructure
shadow silhouette, and also for enhancing shadow cast-
ing for a BTF-mapped surface. All these computations
can be performed in the pixel shader and accelerated with
graphics hardware.

4.1 Visibility computation

With the MDF we can render mesostructure silhouettes
as well as mesostructure shadow silhouettes. These two
types of silhouettes are intimately related in that both are
the results of visibility changes caused by mesostructures.
For silhouettes the visibility is that from the viewpoint;
for shadow silhouettes the visibility is that from the light

Fig. 6. Determining mesostructure silhouettes using MDF

source. The visibility computation algorithm discussed
here is limited to closed objects.

Figure 6 illustrates the algorithm for determining
mesostructure silhouettes. For simplicity we consider the
silhouette determination problem in 2D (we can reduce
the original 3D problem to 2D by performing silhouette
determination for each scanline of the rendered image).
Consider the viewing ray r0 that enters the object sur-
face at point p f on a front face and leaves at point pb on
a back face. With MDF value D(x f , yf , vf ) on p f , the
to the mesostructure surface along r0 is
offset from p f
computed as df = D(x f , yf , vf )sf , where s f is the texture
scale at p f . (x f , yf ) is the texture coordinate at p f , and v f
is the viewing direction of r0 computed in the local coor-
dinate frame at p f . Similarly, the distance from point pb
to the mesostructure surface is db = D(xb, yb, vb)sb. As
illustrated in Fig. 6, if d f + db ≤ || p f − pb||, the ray will
intersect the mapped mesostructure at some point. Other-
wise, the ray will pass through the mesostructure without
intersection. If the ray intersects the object multiple times,
we do the same computation for each pair of pf and pb
to determine the visibility of the mesostructure mapped on
the surface.

To handle the mesostructure silhouette with graphics
hardware, we use the depth peeling technique [7, 18] to
ﬁnd p f and pb for each interval inside the objects. As il-
lustrated in Fig. 7, before peeling we initialize the depth
buffer and stencil buffer. We also need two extra depth
buffers for peeling, Zpeel_now and Zpeel_next, which
store the minimum depth for the current and next peel-
ing pass, respectively. In our algorithm, we execute the
depth peeling from front to back. In each peeling pass,
we ﬁrst render the back faces. The Zpeel_now and the
depth buffer Z work together to obtain the pixels pb of
the back faces in the current peel layer, whose depths are
stored in Zpeel_next. We then offset their depths with the
− db) in the depth
MDF value and store the result (z pb
buffer. After that, we disable depth buffer writing and ren-
der all front faces. For pixels p f that pass the peeling depth
− db
test, we compare their offset depths z pf
− db, the ray
stored in the depth buffer. If z p f
intersects with the mesostructure detail between pf and
pb. Otherwise, the ray passes through this peeling layer
without intersection. For pixels whose rays intersect with
mesostructure details, we shade p f with BTF data accord-

+ df with z pb

+ df ≤ z pb

564

J. Wang et al.

Fig. 7. Pseudocode of silhouette rendering

ing to the viewing and lighting conditions. After that, we
mask the shaded pixel using the stencil buffer so that it
will not be corrupted with the surfaces in the following
peeling layers. Then we swap the peeling depth buffer and
do the next peeling pass. In theory, the peeling operation
should be continued until no pixels on the screen are up-
dated. In our current implementation, the number of the
peeling passes for each scene is speciﬁed by the user.

Determining mesostructure shadow silhouettes is done
in the same way except we render the scene for the light-
ing direction. Instead of shading the surface pixels p f with
BTF, we render their depths to the shadow map buffer.

4.2 Shadow casting on BTF-mapped surfaces

To render the BTF-mapped surface realistically, we also
need to consider the shadows cast on the BTF-mapped
surface. A problem with existing rendering techniques is
that the mesostructure has no effect near shadow bound-
aries. Figure 8a shows an example of shadow testing with
existing BTF rendering algorithms. The two reference sur-
face points p1 and p2 have the same viewing and lighting
directions v and l. Without taking the mesostructure into

Fig. 9. Shadow computation geometry

consideration, we would mistakenly regard both p1 and p2
as lit.

To generate realistic shadows, we need to combine the
effects of mesoscale geometry with shadow maps cast by
macroscale objects, which can be easily done with MDF.
For simplicity, we assume that we have a point light source
casting hard shadows.

As illustrated in Fig. 9, for each surface point p with
texture coordinate (x, y), we ﬁnd the intersection point p(cid:2)
of the viewing ray and the mesostructure surface p(cid:2) = p +
s D(x, y, vp) vp, where s is the texture scale at p and vp
is the viewing direction deﬁned in the local coordinate
frame. Using the shadow map we test whether p(cid:2) is in
the macroscale shadow. Since the self-shadowing effect
of mesostructure has been included in the BTF, we gen-
erate the shadow map for a BTF-mapped surface blocker
such that the depth of each point is measured from the
“base” of the mesostructure. As shown in Fig. 9, at point
p(cid:2)(cid:2) the shadow map depth is d( p(cid:2)(cid:2)) + dmax
, where d( p(cid:2)(cid:2))
cos θl
is the depth from the light source calculated using the
macroscale geometry and dmax is the maximum depth of
the mesostructure.

This algorithm can be easily integrated with the visi-
bility computation algorithm described in the pseudocode.
Speciﬁcally, to generate the shadow map, we write the
z( pf ) + dmax
into the shadow buffer, where θl is the light
cos θl
direction computed in the local coordinate frame at p f . To
render the ﬁnal result, we compute the mesostructure point
with depth as z( pf ) + d( pf ) for p f and test it against the
shadow map. Depending on whether a pixel is in shadow,
we shade point p with either an ambient term La(x, y, vp)
or the BTF value T(x, y, vp, lp), where La(x, y, vp) is ob-
tained by integrating the BTF value for all lighting direc-
tions l = (θl, φl) on the hemisphere Ω:

La( p, vp) =

T(x, y, vp, l) dωl.

(cid:3)

Ω

5 Experimental results

Fig. 8. a Without MDF, both p1 and p2 are mistakenly considered
lit as indicated by red arrows. b With MDF, p1 is lit whereas p2 is
in shadow

Implementation details. We have implemented our new
rendering algorithm on a PC with a PIV 2.8-GHz CPU and
Geforce 6800GT graphics card. In our implementation,

we ﬁrst render the shadow map for the lighting direction.
Then we render the ﬁnal result with the BTF. To fetch
the BTF and MDF data in rendering, we ﬁrst compute
the texture scale s, viewing direction v, lighting direction
l, texture coordinate (x, y), and texture coordinate frame
for each vertex. After rasterization, these vertex attributes
are interpolated to pixels. To render the BTF with graph-
ics hardware, we decompose the 6D BTF into several
lower-dimensional maps using singular value decompos-
ition (SVD) [13]. Speciﬁcally, we reorganized the 6D BTF
data into a 2D matrix A where the rows are indexed by
x, y, θl, and φl and the columns are indexed by view-
ing angle θv,φv. Applying SVD to A produces a set of 4D
eigenmaps Ei(x, y, θl, φl) and corresponding 2D weight
maps Wi(θv, φv). By keeping a small number of eigen-
maps (four for all BTF data used in this paper), the BTF
data are compressed and then can be reconstructed as

BTF(x, y, θv, φv, θl, φl) =

Wi(θv, φv)Ei(x, y, θl, φl).

(cid:2)

i

In our current implementation, the 4D eigenmap is reor-
ganized into a 3D texture for rendering, by the means of
packing lighting directions to the third dimension.

To render the 4D MDF data, we directly reorganize the
data into 3D texture by packing viewing directions to the
third dimension. The trilinear interpolation is used for tex-
ture ﬁltering.

Experimental results. Table 1 lists all MDF and com-
pressed BTF data are used in this paper, where the peanut
and the weave data are captured from real materials with
our BTF–MDF capturing device. The other data are ren-
dered from synthetic mesostructures by ray tracing. For
BTFs, we store each captured 24-bit RGB image. MDF
depth data are quantized to 8 bits per pixel and stored in
MDF for rendering. Note that the 4D MDF data are small
enough, which can be easily loaded in graphics hardware
without compression.

Figure 12 demonstrates a scene rendered with BTFs
and MDFs measured from real materials. Both the coarse
weave BTF/MDF data on the tori and the peanut
BTF/MDF data on the ﬂoor are captured from real materi-

Table 1. Sampling resolution and size of BTF/MDF data

Sample

Image res.

Light & view

resolution

BTF MDF
data
data
(MB)
(MB)

Blue pipe

Brick
Peanut
Weave
Grass
Block

64 × 64
64 × 64
128 × 128
128 × 128
64 × 64
64 × 64

12 × 5 × 12 × 5
12 × 5 × 12 × 5
12 × 8 × 12 × 8
12 × 8 × 12 × 8
12 × 8 × 12 × 8
12 × 5 × 12 × 5

1.3
1.3
7.5
7.5
5.3
1.3

0.9
0.9
6.3
6.3
3.9
0.9

Capturing and rendering geometry details for BTF-mapped surfaces

565

Fig. 10a–c. Comparison of results rendered a by BTF only, b by
BTF and MDF, and c ground truth

als using our device. Two peeling passes are used to render
mesostructure silhouettes and mesostructure shadow sil-
houettes. Note the mesostructure silhouette of the tori, the
detailed shadow boundary on the ﬂoor, and other visual ef-
fects rendered by both the BTF and the MDF. Also, the
detailed shadow boundaries cast on the ﬂoor are consis-
tently rendered with the implicit geometry rendered with
the BTF on the ﬂoor.

Figure 13 demonstrates a scene decorated with syn-
thetic grass and brick BTF/MDFs, where three peeling
passes are used in rendering. The mesostructure silhou-
ette on the torus knot is convincingly rendered. Also ob-
serve the mesostructure shadow silhouette on the ﬂoor.
Another rendering result of BTF/MDF data is exhibited in
Fig. 14, where two peeling passes are used to generate the
mesostructure silhouette effects.

Table 2 lists the rendering performance of our algo-
rithm for different scenes shown in the paper, where the
output image resolution is 800 × 600.

Fig. 11a–d. Rendering results of the same torus mapped with brick
BTF/MDF data in different texture scales a with 8 × 3 tiles, b with
6 × 2 tiles, c with 3 × 1 tiles, and d with 2 × 1 tiles. The base trian-
gle mesh is shown as the yellow wireframes in d

566

J. Wang et al.

Fig. 12. Rendering result of BTF-mapped surfaces with real-world BTF/MDF samples

Fig. 13. BTF-mapped surface with synthetic BTF and MDF data

Fig. 14a–c. Three examples of BTF-mapped surfaces with shadow and silhouette effects a with weave BTF/MDF, b with block BTF/MDF,
and c with brick BTF/MDF

Limitations. One issue is that the BTF is measured under
spatially uniform lighting. Introducing macroscale block-
ers causes the illumination to vary spatially. This has
a subtle and complicated inﬂuence on mesostructure in-
terreﬂections that our method does not account for. Qual-

itatively, our method generates overly sharp shadows
that would be softened if interreﬂections were simulated
accurately. Note that this is a problem for any method
based on BTFs. Another issue is that the BTF and MDF
data are measured from a plane and then mapped onto

Capturing and rendering geometry details for BTF-mapped surfaces

567

Table 2. FPS of BTF/MDF rendering for different scenes

Scene

Triangle
number

BTF
only

With
Object

silhouette

With

object &
shadow
silhouette

Fig. 1
Fig. 12
Fig. 14
Fig. 14a
Fig. 14b
Fig. 14c

11,952
3,200
11,952
4,276
4,276
4,276

186
110
115
186
110
115

75
42
30
75
40
85

47
27
18
49
25
44

curved surfaces. Our peeling-based, depth-interval test
is only an approximation of the true deformation of the
mesostructure geometry onto a curved surface. As shown
in Fig. 11, the method we propose yields consistent re-
sults at different surface curvatures. Even in the extreme
situation, the bottom-right torus mapped in the tile-2
scale, the mesostructure and silhouette boundary appear
matched with each other and consistent as the curva-
ture varies. Figure 10 compares the rendering results of
a cylinder mapped with mesostructure rendered with BTF,
BTF/MDF, and displaced geometry. The displaced geom-
etry mapped onto the cylinder is rendered by ray tracing.

Note that our approach provides convincing visual effects
that are consistent with the appearance rendered with the
surface BTF. In particular, the shadow generated by our
algorithm is very similar to that generated by detailed
geometry. The silhouette generated by our method also
provides a good approximation of the ground truth.

6 Conclusion

We have presented an algorithm that augments BTF ren-
dering to include previously neglected, detailed silhouette
effects using the MDF. The MDF can be measured from
real materials without undue additional work in BTF ac-
quisition. Experimental results demonstrate that our new
algorithm can render realistic mesostructure silhouettes
and shadows, including shadows on BTF-mapped sur-
faces, in real time, and at low storage cost. For future work
we are interested in automatically obtaining geometry in-
formation solely from the BTF.

Acknowledgement We are grateful to the anonymous reviewers,
whose comments were very valuable in reﬁning our paper. Many
thanks to Yasuyuki Matsushita for helping us set up capture de-
vices.

References
1. Blinn, J.F.: Simulation of wrinkled surfaces.

In: Computer Graphics (SIGGRAPH ’78
Proceedings), 12(3), 286–292 (1978)

2. Bouguet, J.Y., Perona, P.: 3d photography

on your desk. In: IEEE International
Conference on Computer Vision (ICCV98),
pp. 43–50 (1998)

3. Cook, R.L.: Shade trees. In: Computer

Graphics (SIGGRAPH ’84 Proceedings),
18(3), 223–231 (1984)

4. Curless, B., Levoy, M.: Better optical

triangulation through spacetime analysis.
In: IEEE International Conference on
Computer Vision (ICCV95), pp. 987–994
(1995)

5. Dana, K.J.: Brdf/btf measurement device.

In: Proceedings of the 8th IEEE
International Conference on Computer
Vision (ICCV), 2, 460–466 (2001)

6. Dana, K.J., van Ginneken, B., Nayar, S.K.,
Koenderink, J.J.: Reﬂectance and texture of
real-world surfaces. ACM Trans. Graph.
18(1), 1–34 (1999)

7. Diefenbach, P.J.: Pipeline rendering:

interaction and realism through
hardware-based multi-pass rendering. Ph.D.
thesis, Computer and Information Science,
University of Pennsylvania. Philadelphia,
PA 19104 US (1996)

8. Furukawa, R., Kawasaki, H., Ikeuchi, K.,

Sakauchi, M.: Appearance based object
modeling using texture database:
acquisition, compression and rendering. In:

Eurographics Workshop on Rendering,
pp. 257–266 (2002)

9. Han, J.Y., Perlin, K.: Measuring

bidirectional texture reﬂectance with
a kaleidoscope. ACM Trans. Graph. 22(3),
741–748 (2003)

10. Heidrich, W., Daubert, K., Kautz, J.,

Seidel, H.P.: Illuminating micro geometry
based on precomputed visibility. In:
Computer Graphics (Proceedings of
SIGGRAPH 2000), pp. 455–464 (2000)

11. Hirche, J., Ehlert, A., Guthe, S., Doggett,

M.: Hardware accelerated per-pixel
displacement mapping. In: GI ’04:
Proceedings of the 2004 Conference on
Graphics Interface, pp. 153–158. Canadian
Human-Computer Communications Society,
School of Computer Science, University of
Waterloo, Waterloo, ON, Canada (2004)

12. Kaneko, T., Takahei, T., Inami, M.,

Kawakami, N., Yanagida, Y., Maeda, T.,
Tachi, S.: Detailed shape representation
with parallax mapping. In: ICAT2001(11th
International Conference on Artiﬁcial
Reality and Tele-Existence), pp. 205–208
(2001)

13. Kautz, J., McCool, M.D.: Interactive

rendering with arbitrary BRDFs using
separable approximations. In: Lischinski,
D., Larson, G.W. (eds.) Rendering
Techniques ’99, Eurographics, pp. 247–260.
Springer, Berlin Heidelberg New York
(1999)

14. Koenderink, J.J., Doorn, A.J.V.:

Illuminance texture due to surface
mesostructure. J. Opt. Soc. Am. 13(3),
452–463 (1996)

15. Liu, X., Hu, Y., Zhang, J., Tong, X.,
Guo, B., Shum, H.Y.: Synthesis and
rendering of bidirectional texture functions
on arbitrary surfaces. IEEE Trans. Visual.
Comput. Graph. 10(3), 278–289 (2004)

16. Liu, X., Yu, Y., Shum, H.Y.: Synthesizing

bidirectional texture functions for
real-world surfaces. In: Proceedings of
SIGGRAPH, pp. 97–106 (2001)

17. Malzbender, T., Gelb, D., Wolters, H.:

Polynomial texture maps. In: Proceedings
of SIGGRAPH, pp. 519–528 (2001)

18. Mammen, A.: Transparency and

antialiasing algorithms implemented with
the virtual pixel maps technique. IEEE
Comput. Graph. Appl. 9(4), 43–55
(1989)

19. Max, N.: Horizon mapping: shadows for

bumped mapped surfaces. Visual Comput.
4(2), 109–117 (1988)

20. Mueller, G., Meseth, J., Klein, R.:

Compression and real-time rendering of
measured BTFs using local PCA. In:
Proceedings of Vision, Modeling and
Visualisation (2003)

21. Policarpo, F., Oliveira, M.M., Comba,

J.L.D.: Real-time relief mapping on
arbitrary polygonal surfaces. In: ACM
SIGGRAPH 2005 Symposium on

568

J. Wang et al.

Interactive 3D Graphics and Games (I3D
2005) (2005)

22. Rushmeier, H., Balmelli, L., Bernardini, F.:

Horizon map capture. In: Proceedings of
Eurographics (2001)

23. Sattler, M., Sarlette, R., Klein, R.: Efﬁcient

and realistic visualization of cloth. In:
Eurographics Symposium on Rendering,
pp. 167–178 (2003)

24. Sloan, P.P., Cohen, M.F.: Interactive
horizon mapping. In: Eurographics
Workshop on Rendering, pp. 281–286
(2000)

26. Suykens, F., vom Berge, K., Lagae, A.,

Dutr´e, P.: Interactive rendering with
bidirectional texture functions. In:
Proceedings of Eurographics (2003)

27. Tong, X., Zhang, J., Liu, L., Wang, X.,

Guo, B., Shum, H.Y.: Synthesis of
bidirectional texture functions on arbitrary
surfaces. ACM Trans. Graph. 21(3),
665–672 (2002)

28. Vasilescu, M.A.O., Terzopoulos, D.:

TensorTextures: multilinear image-based
rendering. ACM Trans. Graph. 23(3),
336–342 (2004)

25. Sloan, P.P., Liu, X., Shum, H.Y., Snyder, J.:

29. Wang, L., Wang, X., Tong, X., Lin, S., Hu,

Bi-scale radiance transfer. ACM Trans.
Graph. 22(3), 370–375 (2003)

S., Guo, B., Shum, H.Y.: View-dependent
displacement mapping. ACM Trans. Graph.
22(3), 334–339 (2003)

30. Wang, X., Tong, X., Lin, S., Hu, S., Guo,
B., Shum, H.Y.: Generalized displacement
maps. In: Eurographics Symposium on
Rendering (2004)

31. Yamauchi, Y., Sekine, M., Yanagawa, S.:
Bidirectional texture mapping for realistic
cloth rendering. In: SIGGRAPH 2003
Sketches and Applications (2003)

32. Zhang, Z.: Flexible camera calibration by

viewing a plane from unknown orientations.
In: IEEE International Conference on
Computer Vision (ICCV99), pp. 666–673
(1999)

JIAPING WANG is a ﬁrst-year Ph.D. student at
the Institute of Computing Technology, Chinese
Academy of Sciences in the graduate school of
the Chinese Academy of Sciences. Before that,
he received his B.S. from Ningbo University in
2002. He has been on an internship at Microsoft
Research Asia (MSRA) since 2003.
His research interests include appearance mod-
eling and capturing, hardware accelerated ren-
dering, high-dimensional
texture analysis, and
compression. He is also interested in early vi-
sion, perception, and machine learning.

receiving his Ph.D.

DR. XIN TONG is a researcher/project
lead of
the Internet Graphics Group, Microsoft Research
Asia. After
in computer
graphics from Tsinghua University, Beijing, in
July 1999, he joined Microsoft Research China
as an associate researcher. Prior to that, he re-
ceived his B.S. and Master’s degree in computer
science from Zhejiang University in 1993 and
1996, respectively.
His research interests include appearance mod-
eling and rendering, image-based rendering, tex-

ture synthesis, and natural phenomena simula-
tion.

DR. JOHN SNYDER is a senior
reseacher at
Microsoft Research. He received his B.S. from
Clarkson University, Postdam, NY, in 1984 and
his Ph.D. from the California Institute of Tech-
nology, Pasadena, CA, in 1991 and has been at
Microsoft Research since 1994. His research in-
terests include geometry representation and pro-
cessing and real-time algorithms for global illu-
mination.

DR. YANYUN CHEN is an associate researcher
at Microsoft Research Asia. His current research
interests include photorealistic and nonphoto-
realistic rendering and image-based rendering.
Dr. Chen received his Ph.D. from the Institute
of Software, Chinese Academy of Sciences, in
2000 and his M.S. and B.S. from the Chinese
University of Mining and Technology in 1996
and 1993, respectively.

DR. BAINING GUO is the research manager of
the Internet Graphics Group at Microsoft Re-

search Asia. Before joining Microsoft, Baining
was a senior staff researcher at Microcomputer
Research Labs at Intel Corp. in Santa Clara, CA,
where he worked on graphics architecture. Bain-
ing received his Ph.D. and M.S. from Cornell
University and his B.S. from Beijing University.
Baining is an associate editor of IEEE Trans-
actions on Visualization and Computer Graph-
ics. He holds over 30 granted and pending US
patents.

DR. HEUNG-YEUNG SHUM is currently the
managing director of Microsoft Research Asia.
Dr. Shum joined Microsoft Research in 1996
after receiving his Ph.D. in robotics from the
School of Computer Science at Carnegie Mel-
lon University. He has authored and coauthored
over 100 papers in computer vision, computer
graphics, and robotics and received more than
20 US patents. He is on the editorial boards
for IEEE Transactions on Circuit System Video
Technology (CSVT), IEEE Transactions on Pat-
tern Analysis and Machine Intelligence (PAMI),
the International Journal of Computer Vision,
and Graphical Models.

