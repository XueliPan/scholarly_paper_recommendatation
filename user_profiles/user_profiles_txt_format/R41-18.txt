A Game-Theoretic Analysis of Information Security Games

Secure or Insure?

Jens Grossklags

UC Berkeley

School of Information
Berkeley, CA 94720

Nicolas Christin

Carnegie Mellon University

INI/CyLab Japan

Kobe, 650-0044 Japan
nicolasc@cmu.edu

John Chuang

UC Berkeley

School of Information
Berkeley, CA 94720

jensg@ischool.berkeley.edu

chuang@ischool.berkeley.edu

ABSTRACT
Despite general awareness of the importance of keeping one’s sys-
tem secure, and widespread availability of consumer security tech-
nologies, actual investment in security remains highly variable across
the Internet population, allowing attacks such as distributed denial-
of-service (DDoS) and spam distribution to continue unabated. By
modeling security investment decision-making in established (e.g.,
weakest-link, best-shot) and novel games (e.g., weakest-target), and
allowing expenditures in self-protection versus self-insurance tech-
nologies, we can examine how incentives may shift between invest-
ment in a public good (protection) and a private good (insurance),
subject to factors such as network size, type of attack, loss proba-
bility, loss magnitude, and cost of technology. We can also char-
acterize Nash equilibria and social optima for different classes of
attacks and defenses. In the weakest-target game, an interesting re-
sult is that, for almost all parameter settings, more effort is exerted
at Nash equilibrium than at the social optimum. We may attribute
this to the “strategic uncertainty” of players seeking to self-protect
at just slightly above the lowest protection level.

Categories and Subject Descriptors
C.2 [Computer Systems Organization]: Computer-Communication
Networks; J.4 [Computer Applications]: Social and Behavioral
Sciences—Economics; K.4.4 [Computers and Society]: Electronic
Commerce—Security

General Terms
Economics, Reliability, Security

Keywords
Economics of the Internet, Game Theory, Public Goods, Incentive-
Centered Design and Engineering, Security, Protection, Self-Insurance

1.

INTRODUCTION

The Internet has opened new and attractive channels to publi-
cize and market products, to communicate with friends and col-
leagues, and to access information from spatially distributed re-
sources. Though it has grown signiﬁcantly, the network’s architec-
ture still reﬂects the cooperative spirit of its original designers [32].
Unfortunately, today’s network users are no longer held together by
that same sense of camaraderie and common purpose. For instance,
concrete evidence of the tragedy of the commons [21] occurring in

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2008, April 21–25, 2008, Beijing, China.
ACM 978-1-60558-085-2/08/04.

peer-to-peer ﬁlesharing networks has been documented for a long
time [2]. Accordingly, studies of networking protocols and user
interaction have been assuming users to be selﬁsh and to act strate-
gically [37].

Selﬁsh users are one thing, but the expansion of the Internet has
also attracted individuals and groups with often destructive moti-
vations; these “attackers” intend to improve on their perceived util-
ity by exploiting or creating security weaknesses and harming or
inconveniencing other network users. Some malicious entities are
motivated by peer recognition, or curiosity, and are often undecided
regarding the ethical legitimacy of their behavior [19, 20]. Others
have clearly demonstrated ﬁnancial goals [17]. Problematic behav-
iors and threats include attacks on the network as a whole, attacks
on selected end-points, undesirable forms of interactions such as
spam e-mail, and annoyances such as Web pages that are unavail-
able or defaced. As a result, users cannot rely and trust other net-
work participants [13].

When asked in surveys, network users say they are interested
in preventing attacks and mitigating the damages from computer
and information security breaches [1, 40]. Researchers and indus-
try have responded by developing numerous security technologies
to alleviate many of the aforementioned problems [4], thereby ex-
pecting to help improving individual security practices.

Nevertheless, security breaches are common, widespread and
highly damaging. The “I Love You” virus [27], Code Red [29]
and Slammer worms [28], to cite the most famous cases, have in-
fected hundreds of thousands of machines and caused, all together,
billions of dollars in damages. Underground markets for proces-
sor time on compromised end-systems are developing [17] thanks
to large population of home computers that can be easily comman-
deered by third-parties. The high ﬁnancial impact of security fail-
ures is explained by user surveys [6, 10], which show strong evi-
dence that comprehensive security precautions, be they patching,
spyware-removal tools, or even sound backup strategies, are miss-
ing from a vast majority of systems surveyed.

In other words, despite a self-professed interest in security, most
individuals do not implement effective security on their systems,
even though necessary technologies and methods are (by and large)
readily available. We propose to investigate the root causes of the
disconnect between users’ actions and their intentions.

In practice, there is a large variety of situations in which users
face security threats, and an equally large number of possible re-
sponses to threats. However, we postulate in this paper that one
can model most security interactions through a handful of “secu-
rity games,” and with a small number of decision parameters upon
which each user can act.

More precisely, building upon public goods literature [23, 43],
we consider the classical best shot, total effort, and weakest-link

209games, and will analyze them in a security context. We comple-
ment these three games with a novel model, called the “weakest-
target” game, which allows us to describe a whole class of attacks
ranging from insider threats to very aggressive worms. Further-
more, while most research on the economics of security focuses
on security investments as a problem with a single variable (e.g.,
amount of money spent on security), our analysis is the ﬁrst to de-
couple protection investments (e.g., setting up a ﬁrewall) from in-
surance coverage (e.g., archiving data as back up). This decoupling
allows us to explain a number of inefﬁciencies in the observed user
behaviors.

This paper is only a ﬁrst step toward a more comprehensive mod-
eling of user attitudes toward security issues. Indeed, the present
study relies on game theory, mostly using Nash equilibrium and
social optima concepts. As such, we primarily view this study as
a theoretical basis for follow up experimental work using labora-
tory experiments with human participants. We nevertheless show
that the models and results derived here provide for considerable
insights.

The rest of this paper is organized as follows. We elaborate in
Section 2 the relationship of our work with related research, before
introducing our game-theoretic models in Section 3. We present an
analysis of the Nash equilibria (Section 4) and social optima (Sec-
tion 5) for all of these games, and discuss our ﬁndings in Section 6.
We conclude in Section 7.

2. RELATED WORK

The economics of information security is a growing research
area with a diverse set of participating researchers from various
disciplines. Important common anchors are the observations that
misaligned incentives and positive and negative externalities play
signiﬁcant roles in the strategies used by each party in the battle
between attackers and potential victims [4, 5].

Economics as a tool for security analysis has gained in impor-
tance since the economy of attackers has become increasingly ra-
tional (e.g., motivated by greed), over the last years [17]. This
increasingly rational behavior stands in contrast to that exhibited
by the hacker communities of the 1980s and 1990s, who valued
reputation, intellectual achievement, and even entertainment above
ﬁnancial incentives [19, 20].

Most of the initial results obtained in security economics re-
search concern the analysis of optimal security investments. For
example, Gordon and Loeb [18] as well as Hausken [22] focus
on the impact of different security breach functions and degrees
of vulnerability on an entity’s investment strategy. More special-
ized models have been proposed to analyze a subset of important
security management problems. For instance, August and Tunca
scrutinize optimal system update strategies when patching a system
against security vulnerabilities is costly [7]. Rescorla investigates
the impact of code quality control on vulnerability of software [31].
Böhme and Kataria study individual security investment decisions
and market insurance offers when correlation of cyber-risks within
a single ﬁrm and across multiple ﬁrms differ [8].

From a policy standpoint, Bull et al. [11] argue that given the
state of heterogeneous networks no single security policy will be
applicable to all circumstances. They suggest that, for a system to
be viable from a security standpoint, individuals need to be empow-
ered to control their own resources and to make customized secu-
rity trade-offs. This stands in contrast to the traditional centralized
structure where all security decision are made by a central planner
(e.g., the IT department). Nevertheless, as Anderson suggests, or-
ganizational and structural dependencies have to be considered in
individual security decision making [3].

While many models prescribe behavior in individual choice sit-
uations, the focus of our work is to model and study strategic inter-
action with respect to security decisions in networked systems, in
an effort to understand the impact of individual choices on a larger
group. Such interaction usually involves common as well as con-
ﬂicting interests. (Pure conﬂict, in which the interests of the two
antagonists are completely opposed, is a special case.) This mu-
tual dependence as well as opposition guarantees for a much richer
scenario for analysis [35].

To better understand the implications of this mutual dependence,
Varian [43] conducts an analysis of system reliability within a pub-
lic goods game-theoretical framework. He discusses the best ef-
fort, weakest-link and total effort games, as originally analyzed by
Hirshleifer [23]. The main difference from classical public goods
theory is that within the framework of computer reliability “con-
siderations of costs, beneﬁts, and probability of failure become
paramount, with income effects being a secondary concern.” [43]
Varian focuses on two-player games with heterogeneous effort costs
and beneﬁts from reliability.1 He also adds an inquiry into the role
of taxes and ﬁnes, and differences between simultaneous and se-
quential moves.

Our work generalizes [43] in several aspects. First, instead of
considering security decisions to be determined by a single “secu-
rity” variable, we identify two key components of a security strat-
egy: self-protection (e.g., patching system vulnerabilities) and self-
insurance (e.g., having good backups). More precisely, we allow
agents to self-protect and/or self-insure their resources in N -player
games. We also contrast the three canonical games discussed by
Varian with two more complex “weakest-target” games that rep-
resent a more complicated incentive structure, which we believe
applies to a whole class of security issues.

Outside the information security context, the dual role of self-
protection and self-insurance was ﬁrst recognized by [15]. To pro-
vide a more precise deﬁnition, self-protection stands for the ability
to reduce the probability of a loss – for example, by installing a ﬁre-
wall application which limits the amount of trafﬁc allowed to com-
municate with one’s network. Self-insurance, on the other hand,
denotes a reduction in the magnitude of a loss, e.g., by performing
regular backups on existing data. Some technologies and practices
such as disconnecting a computer from a network do both. Ehrlich
and Becker [15] focus in their analysis on the comparison of self-
protection and self-insurance to market insurance. They ﬁnd that,
for rare loss events, there is less incentive to self-insure losses than
to use market insurance. This is due to their assumption, that the
price of self-insurance is independent of the probability of the loss.
An additional result is that the demand for self-insurance grows
with the base loss of a security threat. As an outcome of their
work, they characterize self-insurance and market insurance as sub-
stitutes, and self-protection and market insurance as complements.
Our analysis complements the work in [15] by extending the con-
cepts of self-protection and self-insurance to the public goods and
security context.

3. DESCRIPTION OF SECURITY GAMES
We deﬁne a security game as a game-theoretic model that cap-
tures essential characteristics of decision making to protect and
self-insure resources within a network. Varian [43] observed that
frequently the success of security (or reliability) decision making
depends on a joint protection level determined by all participants

1A distinction between reliability and security, in terms of conse-
quences, may exist [24]. In this study, we do not follow this dis-
tinction and consider reliability as a key component of security.

of a network. The computation of the protection level will often
take the form of a public goods contribution function with nonrival
and nonexcludable beneﬁts or consequences. A main observation
is that dependent on the contribution function individuals may be
able to freeride on others’ efforts. However, individuals may also
suffer from inadequate protection efforts by other members if those
have a decisive impact on the overall protection level.

Following Varian’s exposition, we analyze three canonical con-
tribution functions that determine a global protection level. Dif-
ferent from Varian’s work however, here network members have a
second action available: They can decide to self-insure themselves
from harm. The success of insurance decisions is completely in-
dependent of protection choices made by the individual and oth-
ers. Consequently, the games we consider share qualities of private
(on the insurance side) and public (on the protection side) goods.
We further add to the research literature by studying two additional
games with a more complex determination of protection levels.

Security games share the following key assumptions: (i) all en-
tities in the network share a single purely public protection output,
(ii) a single individual decides on protection efforts for each en-
tity (so we do not assume a second layer of organizational decision
making), (iii) protection costs per unit are identical for each entity,
and (iv) all decisions are made simultaneously. These assumptions
are commonly made also in models on decision making of partners
in military alliances [33]. We add to these main assumptions that
individuals are able to self-insure resources at a homogeneous cost
with self-insurance being a purely private good.

Formally, the basic model from which we develop the security
games has the following payoff structure. Each of N ∈ N players
receives an endowment M .
If she is attacked and compromised
successfully she faces a loss L. Attacks arrive with an exogenous
probability of p (0 ≤ p ≤ 1). Players have two security actions at
their disposition. Player i chooses an insurance level 0 ≤ si ≤ 1
and a protection level 0 ≤ ei ≤ 1. Finally, b ≥ 0 and c ≥ 0 denote
the unit cost of protection and insurance, respectively. The generic
utility function has the following structure:

Ui = M − pL(1 − si)(1 − H(ei, e−i)) − bei − csi ,

(1)

where following usual game-theoretic notation, e−i denotes the set
of protection levels chosen by players other than i. H is a “contri-
bution” function that characterizes the effect of ei on Ui, subject to
the protection levels chosen (contributed) by all other players. We
require that H be deﬁned for all values over (0, 1)N . However, we
do not place, for now, any further restrictions on the contribution
function (e.g., continuity). From Eqn. (1), the magnitude of a loss
depends on three factors: i) whether an attack takes place (p), ii)
whether the individual invested in self-insurance (1 − si), and iii)
the magnitude of the joint protection level (1 − H(ei, e−i)). Self-
insurance always lowers the loss that an individual incurs when
compromised by an attack. Protection probabilistically determines
whether an attack is successful. Eqn. (1) therefore yields an ex-
pected utility.

We introduce ﬁve games in the following discussion. In select-
ing and modeling these games we paid attention to comparability
of our security games to prior research (e.g., [23, 33, 43]). The ﬁrst
three speciﬁcations for H represent important baseline cases rec-
ognized in the public goods literature. To allow us to cover most
security dilemmas, we add two novel games, for which we could
not ﬁnd a formal representation in the literature. All games are easy
to interpret within and outside the online security context.

Total effort security game: The global protection level of the
network depends on the sum of contributions normalized over the
number of all participants. That is, we deﬁne H(ei, e−i) = 1
N

P

i ei,

so that Eqn. (1) becomes

Ui = M − pL(1 − si)(1 −

ek) − bei − csi .

(2)

1
N

X

k

Economists identiﬁed the sum of efforts (or total effort) contribu-
tion function long before the remaining cases included in this pa-
per [23]. We consider a slight variation of this game to normalize it
to the desired parameter range. A typical parable for the total sum
function is that the effectiveness of a dam or city wall depends on
its strength that is contributed to by all players. In terms of security
the average contributions matters if an attacker wants to success-
fully conquer the majority of machines in a network one-by-one.
For instance, consider a building plan for a new technology that is
spread across a company’s network and which is considerably more
valuable to an attacker, if obtained in its entirety.

As another example, maybe more related to Internet security,
consider parallelized ﬁle transfers, as in the BitTorrent peer-to-peer
service. It may be the case that an attacker wants to slow down
transfer of a given piece of information; but the transfer speed itself
is a function of the aggregate effort of the machines participating in
the transfer. Note that, the attacker in that case is merely trying to
slow down a transfer, and is not concerned with completely remov-
ing the piece of information from the network: censorship actually
results in a different, “best shot” game, as we discuss later.

Weakest-link security game: The overall protection level de-
pends on the minimum contribution offered over all entities. That
is, we have H(ei, e−i) = min(ei, e−i), and Eqn. (1) takes the
form:

Ui = M − pL(1 − si)(1 − min(ei, e−i)) − bei − csi .

(3)

This game describes the situation where a levee or city wall that is
too low at any point leads to a negative payoff to all players in the
event of a ﬂood or attack. The weakest link game is easily the most
recognized public goods problem in computer security by business
professionals and researchers alike.2 Once the perimeter of an or-
ganization is breached it is often possible for attackers to leverage
this advantage. This initial compromise can be the result of a weak
password, an inconsistent security policy, or some malicious code
inﬁltrating a single client computer.

Best shot security game:
In this game, the overall protection
level depends on the maximum contribution offered over all enti-
ties. Hence, we have H(ei, e−i) = max(ei, e−i), so that Eqn. (1)
becomes

Ui = M − pL(1 − si)(1 − max(ei, e−i)) − bei − csi .

(4)

As an example of a best shot game, consider a set of walls of
which the highest sets the effectiveness benchmark. Among infor-
mation systems, networks with built-in redundancy, such as peer-
to-peer, sensor networks, or even Internet backbone routes, share
resilience qualities with the best shot security game; for instance, to
completely take down communications between two (presumably
highly connected) backbone nodes on the Internet, one has to shut
down all possible routes between these two nodes. Censorship-
resistant networks are another example of best shot games. A piece
of information will remain available to the public domain as long

2See, for example, see a recent interview with a security company
CEO. New York Times (September 12, 2007), “Who needs hack-
ers,” available at http://www.nytimes.com/2007/09/
12/technology/techspecial/12threat.html.
Stat-
ing that: “As computer networks are cobbled together [...] the Law
of the Weakest Link always seems to prevail.”

as a single node serving that piece of information can remain un-
harmed [14].

Weakest-target security game (without mitigation): Here, an
attacker will always be able to compromise the entity (or enti-
ties) with the lowest protection level, but will leave other entities
unharmed. This game derives from the security game presented
in [12]. Formally, we can describe the game as follows:
 0 if ei = min(ei, e−i),

H(ei, e−i) =

1 otherwise,

(5)

which leads to

 M − pL(1 − si) − bei − csi

Ui =

M − bei − csi

if ei = min(ei, e−i),
otherwise.

(6)
The weakest-target game markedly differs from the weakest link.
There is still a decisive security level that sets the benchmark for
all individuals. It is determined by the individual(s) with the lowest
chosen effort level. However, in this game all entities with a protec-
tion effort strictly larger than the minimum will remain unharmed.
In information security, this game captures the situation in which
an attacker is interested in securing access to an arbitrary set of enti-
ties with the lowest possible effort. Accordingly, she will select the
machines with the lowest security level. An attacker might be in-
terested in such a strategy if the return on attack effort is relatively
low, for example, if the attacker uses a compromised machine to
distribute spam. Such a strategy is also relevant to an attacker with
limited skills, a case getting more and more frequent with the avail-
ability of automated attack toolboxes [41]; or, when the attacker’s
goal is to commandeer the largest number of machines using the
smallest investment possible [17]. Likewise, this game can be use-
ful in modeling insider attacks – a disgruntled employee may for in-
stance very easily determine how to maximize the amount of dam-
age to her corporate network while minimizing her effort.

Weakest-target security game (with mitigation): This game is a
variation on the above weakest-target game. The difference is that,
the probability that the attack on the weakest protected player(s) is
successful is now dependent on the security level min ei chosen.
That is,

H(ei, e−i) =

 1 − ei

1

if ei = min(ei, e−i),
otherwise,

so that

Ui =

 M − pL(1 − si)(1 − ei) − bei − csi

M − bei − csi

if ei = min(ei, e−i),
otherwise.

(8)
This game represents a nuanced version of the weakest-target game.
Here, an an attacker is not necessarily assured of success. In fact, if
all individuals invest in full protection, not a single machine will be
compromised. This variation allows us to capture scenarios where,
for instance, an attacker targets a speciﬁc vulnerability, for which
an easily deployable countermeasure exists.

Limitations: With the analysis in this paper we aim for a more
thorough understanding of the ecology of security threats and de-
fense functions an individual or organization faces and has to re-
spond to. We have generalized and newly developed models that
represent vastly different security scenarios and will call for dif-
ferent actions. As Hirshleifer observed [23] a security practitioner
will be presented with “all kinds of intermediate cases and combi-
nations,” e.g., social composition functions involving all of these
ﬁve rules as well as other not identiﬁed yet. Some minor variations
would be the ”location of the top decile, or the total of the best three

shots, or the average of the best and worst shots, or the variance or
skewness” etc. See also [7] and [26] for variations in which the
likelihood of a compromise depends on the number of unprotected
players.

4. NASH EQUILIBRIUM ANALYSIS

We next determine the equilibrium outcomes where each indi-
vidual chooses protection effort and self-insurance investments uni-
laterally, in an effort to maximize her own utility.
In Section 5,
we then compare these results to the protection efforts and self-
insurance levels chosen if coordinated by a social planner.

4.1 Total effort

Let us focus on player i, and consider ek for k 6= i as exogenous.
Then, Ui is a function of two variables, ei and si. From Eqn. (2),
Ui is twice differentiable in ei and si, with ∂2Ui/∂s2
i = 0 and
∂2Ui/∂e2
i = 0. Hence, according to the second derivative test,
only (ei, si) ∈ {(0, 0), (0, 1), (1, 0), (1, 1)} can be an extremum –
that is, possible Nash equilibria are limited to these four values (or
to strategies yielding a payoff constant regardless of ei and/or si).
As long as at least one of b or c is strictly positive, (ei, si) = (1, 1)
is always dominated by either (ei, si) = (1, 0) or (ei, si) = (0, 1)
and cannot deﬁne a Nash equilibrium. Let us analyze the three
other cases:

• (ei, si) = (0, 0). Replacing in Eqn. (2), we get
1

0

Ui = M − pL

@1 −

ek

A .

(9)

1
N

X

k6=i

• (ei, si) = (0, 1). Replacing in Eqn. (2), we get

Ui = M − c .

(10)

• (ei, si) = (1, 0). Replacing in Eqn. (2), we get

Ui = M − pL

@1 −

−

ek

A − b .

(11)

0

1

1
N

1
N

X

k6=i

(7)

Result 1: After investigating Eqs. (9–11) we can identify three
Nash equilibrium strategies.

• Full protection eq.: If pL > bN and c > b + pL N −1
N ,
meaning that protection is cheap, potential losses are high,
and insurance is extremely overpriced, then the (only) Nash
equilibrium is deﬁned by everybody protecting but not insur-
ing, that is, (ei, si) = (1, 0).

• Full self-insurance eq.: In the other cases where pL > bN ,
(ei, si) = (0, 1) is a Nash equilibrium. Also, if c < pL <
bN (expected losses above insurance costs), then (ei, si) =
(0, 1), is a Nash equilibrium.

• Passivity eq.: If pL < bN and pL < c, then the expected
losses are small enough so that complete passivity, deﬁned
by (ei, si) = (0, 0) for all players, is a Nash equilibrium.

Increasing number of players N: As the number of players in-
creases, protection equilibria become more and more unlikely to
occur. Indeed, in a total effort scenario, “revenues” yielded by a
player’s investment in security have to be shared with all of the
other participants, making it an increasingly uninteresting strategy
for the player as the network grows.

4.2 Weakest-link

Let e0 = mini(ei). From Eqn. (3), we have Ui = M − pL(1 −
= pL(1 − e0) − c, and, for

si)(1 − e0) − bei − csi, so that ∂Ui
∂si
all i,

Ui ≤ M − pL(1 − si)(1 − e0) − be0 − csi ,

which is reached for ei = e0. So, in a Nash equilibrium, everybody
picks the same ei = e0. It follows that Nash equilibria are of the
form (e0, 0) or (0, 1).

Result 2: In the weakest link security game, we can identify three
types of Nash equilibrium strategies. However, there exist multiple
pure protection equilibria.

Denote by ˆe0 the minimum of the protection levels initially cho-

sen by all players. We have

• Multiple protection equilibria: If pL > b and {(ˆe0 > (pL −
c)/(pL − b) for c < pL) ∪ (pL ≥ c)}, then (ei, si) =
(ˆe0, 0) for all i is a Nash equilibrium: everybody picks the
same minimal security level, but no one has any incentive
to lower it further down. This equilibrium can only exist
for b ≤ c, and may be inefﬁcient, as it could be in the best
interest of all parties to converge to ei = 1, as we discuss
later in Section 5.

• Full self-insurance eq.: If pL > c and {ˆe0 < (pL−c)/(pL−
b) ∪ b > pL}, then (ei, si) = (0, 1) for all i is a Nash equi-
librium: essentially, if the system is not initially secured well
enough (by having all parties above a ﬁxed level), players
prefer to self-insure.

• Passivity eq.: If pL < b and pL < c, then (ei, si) = (0, 0)
is the only Nash equilibrium – both insurance and protection
are too expensive.

Notice that if ˆe0 = (pL − c)/(pL − b), then both full self-
insurance ((ei, si) = (0, 1) for all i) and protection ((ei, si) =
(ˆe0, 0) for all i) form a Nash equilibrium. In particular, if b = c
(b < pL and c < pl) full protection (ei, si) = (1, 0) and full
self-insurance (ei, si) = (0, 1) are Nash strategies.

Increasing number of players N: The weakest link security game,
much like the tacit coordination game of [42] has highly volatile
protection equilibria when the number of players increase. In fact,
any protection equilibrium has to contend with the strategic cer-
tainty of a self-insurance equilibrium. To view this, consider the
cumulative distribution function F (ei) over the protection strate-
gies ei of a given player i. From what precedes, with pure strate-
gies, in the Pareto-optimum, F (1) = 1 and F (ei) = 0 for ei < 1.
Assuming all N players use the same c.d.f. F , then the c.d.f. of
e0 = mini{ei} is given by Fmin(e0) = 1 − (1 − F (e0))N [42].
So, Fmin(1) = 1 and Fmin(e0) = 0 for e0 < 1 as well. Now, as-
sume there is an arbitrarily small probability ε > 0 that one player
will defect, that is F (0) = ε. Then, Fmin(0) converges quickly
to 1 as N grows large. That is, it only takes the slightest rumor
that one player may defect for the whole game to collapse to the
(ei, si) = (0, 1) equilibrium.

4.3 Best shot

Let e∗ = maxi(ei). Eqn. (4) gives

Ui = M − pL(1 − si)(1 − e∗) − bei − csi .

Clearly, (ei, si) = (1, 1) is suboptimal, so that three strategies may
yield the highest payoff to user i.

• Selecting (ei, si) = (0, 0) yields Ui = M − pL(1 − e∗).

• Selecting (ei, si) = (1, 0) yields Ui = M − b.

• Selecting (ei, si) = (0, 1) yields Ui = M − c.

Result 3: From the above relationships, we can identify the follow-
ing pure Nash equilibrium strategies.

• Full self-insurance eq.: If b > c we ﬁnd that the self-insurance
equilibrium (∀i, (ei, si) = (0, 1)) is the only possible Nash
equilibrium.

• Passivity eq.: If pL < b and pL < c agents prefer to abstain

from security actions (∀i, (ei, si) = (0, 0)).

In particular, there is no protection equilibrium in this game. For
one protection equilibrium to exist, we would need b < c and pL >
b. But even assuming that this is the case, as long as the game is
synchronized, players endlessly oscillate between securing as much
as possible (ei = 1) and free-riding (ei = 0). This is due to the
fact that as soon as one player secures, all others have an incentive
to free-ride. Conversely, if everybody free-rides, all players have
an incentive to deviate and secure as much as possible.

Increasing number of players N: In the absence of coordination
between players, the outcome of this game is globally independent
of the number of players N , as there is no protection equilibrium,
and the insurance equilibrium is independent of the number of play-
ers. However, the game may be stabilized by using player coordi-
nation (e.g., side payments) for low values of N , something harder
to do as N grows.

4.4 Weakest-target (without mitigation)

Fix the strategy point and let ε < pL

2b . Let e0 be the minimum
effort level of any player. Then no player selects a higher effort than
e0 + ε because it dominates all higher effort levels. However, any
player at e0 would prefer to switch to e0 + 2ε. Then the change in
her payoff is greater than pL − 2 pL
2b b = 0. Because this deviation
is proﬁtable this strategy point is not an equilibrium.3

In the weakest-target game with an attacker of inﬁnite
Result 4:
strength we ﬁnd that pure Nash equilibria for non trivial values of
b, p, L and c do not exist.

Mixed strategy equilibria. While no pure Nash equilibria exist,
let us explore the existence of a mixed strategy equilibrium. We
use the shorthand notation ei = e, si = s here, and consider mixed
strategies for choosing e. There are two cases to consider.

Case c > pL: If c > pL then dominance arguments immediately
lead to s = 0 meaning that nobody buys any self-insurance.

An equilibrium strategy may be parametrized by e. For a given
player, the utility function U becomes a function of a single vari-
able e. Let f (e) be the probability distribution function of effort
in the weakest-target game and let F (e) be the cumulative distribu-
tion function of effort. Assuming only one player is at the minimum
protection level, shall an attack occur, the probability of being the
victim is then (1 − F (e))N −1. (All N players choose protection
levels greater than e.)

Then the utility is given by

U = M − pL(1 − F (e))N −1 − be .

(12)

3While this proof assumes the player is initially at (ei, si) =
(e0, 0), it can be trivially extended to the case (ei, si) = (e0, s)
with s > 0 by picking ε < pL

2b for any s in (0, 1].

2b (1 − s) + cs

In a Nash equilibrium, the ﬁrst-order condition dU /de = 0 must
hold, so that:

(N − 1)pLf (e)[1 − F (e)]N −2 − b = 0

If we substitute G = (1 − F (e)) and g = −f we can write
GN −2dG/de = −b/p(N − 1)L, which, by integration yields

Z G(0)

G(e)

GN −2dG =

Z 0

−b

p(N − 1)L

e

dˆe ,

GN −1˛
G(0)
˛
˛
G(e)

=

−b
pL

e .

(13)

that is

With G(0) = 1,

„

G(e) =

1 −

« 1
e

N −1

.

b
pL

Differentiating, we get

g(e) = −

1

N − 1

„

1 −

b
pL

b
pL

«− N −2
e

N −1

,

and, replacing g = −f we ﬁnd,

f (e) =

1

N − 1

„

1 −

b
pL

b
pL

«− N −2
e

N −1

,

(14)

as the probability distribution function of self-protection in a mixed
Nash equilibrium.

Case c ≤ pL: Now let us consider a game with insurance under
the more reasonable assumption c ≤ pL; that is, insurance is not
overpriced compared to expected losses. Dominance arguments in-
dicate that a Nash strategy must be of the form (e, s) ∈ {(e, 0), e ≥
0} ∪ {(0, 1)}.

Let q be the probability that a player chooses strategy (e, s) =
(0, 1). That is, F (0) = q. Because insurance is independent of
protection, we can reuse Eqn. (13) with the new boundary G(0) =
1 − q:

„

G(e) =

(1 − q)N −1 −

« 1
e

N −1

b
pL

(15)

However, since we are now including self-insurance, a second con-
dition must hold. The payoff for strategy (e, s) = (0, 1) must equal
the payoff for all other strategies.

Speciﬁcally, we may compare payoffs for strategies (e, s) =

(ε, 0) and (e, s) = (0, 1) which gives, by continuity as ε → 0,

pL(1 − q)N −1 = c .

(16)

Together Eqs. (15) and (16) yield:

F (e) = 1 − G(e) = 1 −

„ c − be

« 1

N −1

,

pL

which, differentiating, gives

f (e) =

1

N − 1

b
pL

pL

„ c − be

« 1

N −1 −1

.

(17)

This allows us to compute how often strategy (e, s) = (0, 1) is
played:

q = F (0) = 1 −

(18)

« 1

N −1

.

„ c
pL

Result 5:

In the weakest-target game with an attacker of inﬁnite

strength, a mixed Nash equilibrium strategy exists. The individual’s
strategy is given by Eqs. (17) and (18).

Also note that, per Eqn. (17) and continuity arguments, the upper
bound for protection effort is given by emax = c/b, which can be
less than 1 when protection costs dominate insurance costs b > c.

Increasing number of players N: From Eqn. (18), we can di-
rectly infer that an increase in the number of participating players
decreases the probability that a full self-insurance strategy is cho-
sen. When N grows large, q tends to zero, which means that play-
ers increasingly prefer to gamble in order to ﬁnd a protection level
that leaves them unharmed.

4.5 Weakest target (with mitigation)

Let us assume that there exists a Nash equilibrium where 0 <
K < N players who satisfy ei = e0 = min(ei, e−i), while
(N − K > 0) players satisfy ei > e0. We can show that such
an equilibrium does not exist and that players rather congregate at
the highest protection level if certain conditions are met. Due to
space constraints, we will only sketch the analysis of this equilib-
rium. By computing the partial derivatives ∂Ui/∂si and ∂Ui/∂ei,
and discriminating among values for ei and si, we get the following
results.

Result 6:
we ﬁnd that a pure Nash equilibrium may exist.

In contrast to the inﬁnite strength weakest-target game

• Full protection eq.: If b ≤ c we ﬁnd that the full protection
equilibrium (∀i, (ei, si) = (1, 0)) is the only possible pure
Nash equilibrium.

• For b > c we can show that no pure Nash equilibrium exists.

• There are no pure self-insurance equilibria.

Mixed strategy equilibrium To complement this analysis we also
present the mixed strategy equilibrium. The derivation is similar to
the one given by Eqs. (12–18), however, with an additional substi-
tution step. This gives the resulting distribution,

F (e) = 1 −

,

(19)

„ c − be

« 1

N −1

pL(1 − e)

so that

f (e) =

1

N − 1

„ (b − c)pL
pL2(1 − e)2

« „ c − be

«− N −2

N −1

.

pL(1 − e)

Interestingly, the probability of playing (e, s) = (0, 1) remains

q = F (0) = 1 −

(20)

« 1

N −1

„ c
pL

Note that if c < b there is a zero probability that e = 1 will be
chosen by any player. The upper bound for protection effort is
given by emax = c/b.

In the weakest-target game with an attacker of ﬁnite
Result 7:
strength we ﬁnd that a mixed Nash equilibrium strategy exists. The
relevant equations are given in Eqs. (19–20).

5.

IDENTIFICATION OF SOCIAL OPTIMA
Organizations and public policy actors frequently attempt to iden-
tify policies that provide the highest utility for the largest number of
people. This idea has been operationalized with the social optimum
analysis.
It states that a system has reached the optimum when
the sum of all players’ utilities is maximized. That is, the social
optimum is deﬁned by the set of strategies that maximize P
i Ui.
Consider N players, and denote by Φ(e1, s1, . . . , eN , sN ) the ag-
gregate utility, Φ(e1, s1, . . . , eN , sN ) = P
i Ui(ei, si). The social
optimum maximizes Φ(si, ei) over all possible (si, ei) ∈ [0, 1]2N .
Because enforcing a social optimum may at times be conﬂicting
with the optimal strategy for a given (set of) individual(s), to en-
force a social optimum in practice, we may need to assume the ex-
istence of a “social planner” who essentially decides, unopposed,
the strategy each player has to implement.

5.1 Total effort game

i ei and S = P

Summing the utility given by Eqn. (2) over i, we realize that
Φ((ei, si)i∈{1,...,N }) can be expressed as a function of two vari-
ables, E = P
i si. Φ is continuous and twice dif-
ferentiable in E and S, and the second derivative test tells us that
the only possible extrema of Φ are reached for the boundary values
of E and S, that is (E, S) ∈ {0, N }2. In other words, the only pos-
sible social optima are 1) passivity (for all i, (ei, si) = (0, 0)), 2)
full protection (for all i, (ei, si) = (1, 0)), or 3) full insurance (for
all i, (ei, si) = (0, 1)). As long as one of b or c is strictly positive,
a social planner will never advise agents to invest into protection
and self-insurance at the same time.

By comparing the values of Φ in all three cases, we ﬁnd that if
b < pL and b < c then all agents are required to exercise maximum
protection effort (ei, si) = (1, 0). With c < pL and c < b all
agents will self-insure at the maximum possible (ei, si) = (0, 1).
A social planner will not encourage players to invest in security
measures if they are too expensive (c > pL and b > pL).

In the total effort security game we observe that in the
Result 8:
Nash equilibrium there is almost always too little protection effort
exerted compared to the social optimum. In fact, for a wide range
of parameter settings no protection equilibria exist while the social
optimum prescribes protection at a very low threshold.

• Protection: Except for very unbalanced parameter settings
(i.e., pL > bN and c > b + pL N −1
N ) agents refrained from
full protection. Now full protection by all agents is a viable
alternative.

• Self-insurance: Full self-insurance now has to compete with

full protection effort under a wider range of parameters.

• Passivity: Agents remain passive if self-insurance is too ex-
pensive (c > pL). However, we ﬁnd a substantial difference
with respect to protection behavior. Agents would selﬁshly
refrain from protection efforts if pL < bN since they would
only be guaranteed the N -th part of their investments as re-
turns. Now the social planner can ensure that all agents pro-
tect equally so that it is beneﬁcial to protect up until b < pL.

5.2 Weakest link game

In the weakest link game agents are required to protect at a com-
mon effort level to be socially efﬁcient. We compute Φ by sum-
ming Eqn. (3) over i, and can express Φ as a function of ei, si
and e0 = mini(ei). In particular, for all i, we obtain ∂Φ/∂si =
pL(1 − e0) − c. Studying the sign of ∂Φ/∂si as a function of e0
tells us that, if b < c and b < pL the social planner requires all

agents to protect with maximum effort (ei, si) = (1, 0). If c < b
and c < pL the social planner requires all agents to self-insure
(ei, si) = (0, 1). Finally, the Nash equilibrium and social opti-
mum coincide when security costs are high. Agents do not invest
in protection or self-insurance if b > pL or c > pL.

Result 9: The availability of self-insurance lowers the risk of
below-optimal security in the Nash equilibrium since agents have
an alternative to the unstable Pareto-optimal protection equilib-
rium. From the analysis of the weakest link game with many agents
we know that deviation from the Pareto-optimal highest protection
level is very likely. A social planner can overcome these coordina-
tion problems.

• Protection: The Pareto-optimal Nash equilibrium coincides
with socially optimal protection. However, the protection
level would likely be lower in the Nash case due to coor-
dination problems.

• Self-insurance: The self-insurance equilibria are equivalent

for the Nash and social optimum analysis.

• Passivity: A social planner cannot expand the range of pa-
rameter values at which it would be socially beneﬁcial to
protect or self-insure while passivity would be prescribed in
the Nash equilibrium.

5.3 Best shot game

We compute the social optimum by summing Ui given in Eqn. (4)
over i, yielding that Φ can be expressed as a function of ei, si, and
e∗. It is immediate that, to maximize Φ, one should pick ei = 0
for all i, except for one participant j, where ej = e∗ ≥ 0. We
then get ∂P hi/∂si = pL(1 − e∗) − c, which tells us under which
conditions on e∗ (and consequently on b, c, and pL) self-insurance
is desirable.

We ﬁnd that if b/c < N (i.e., protection is not at a prohibitive
cost compared to insurance and/or there is a reasonably large num-
ber of players), the social optimum is to have one player protect as
much as possible, the others not protect at all, and no one insures.
In practice, this may describe a situation where all participants are
safely protected behind an extremely secure ﬁrewall.
If, on the
other hand b/c > N , which means there are either few players, in-
surance is very cheap compared to protection, then the best strategy
is to simply insure all players as much as possible.

Result 10: In the best shot security Nash outcome there is almost
always too little effort exerted compared to the social optimum. Ex-
ceptions are few points in which full self-insurance remains desir-
able for the social planner and all agents remain passive.

• Protection: Surprisingly, while protection is not even a Nash
strategy we ﬁnd that a social planner would elect an individ-
ual to exercise full protection effort.

• Self-insurance: Full self-insurance by every player is only
desirable if protection costs are large. Therefore, for most
cases the strategy of a social planner will not coincide with
the only Nash equilibrium strategy.

• Passivity: In the Nash equilibrium agents are also too inac-
tive. Passivity is highly undesirable from a social planner’s
perspective. Only if N pL < b no agent will be selected
to exercise maximum protection effort (while self-insurance
might remain an option).

It is important to note that the social optimum variation that re-
quires full protection by one individual results in the whole popula-
tion being unharmed, since one highly secure individual is enough
to thwart all attacks. Therefore, it is easy to see that protection is
extremely desirable from a planners perspective. Out of the three
classical public goods games with homogeneous agents the best
shot game can beneﬁt the most from a guiding hand.

5.4 Weakest-target security game (without mit-

igation)

We compute the social optimum by using Eqn. (8), assuming that
1 ≤ K ≤ N players pick e0 = mini(ei). By studying the varia-
tions on Φ as a function as ei, as a function of K, and as a function
of si (for both the K players picking e0 and the remainder of the
players), we ﬁnd that in the weakest-target game without mitiga-
tion a social planner would direct a single player to exacerbate no
protection effort.

Essentially, this player serves as a direct target for a potential
attacker. However, as long as c < pL the player would be directed
to maximize self-insurance (ei, si) = (0, 1). If insurance is too
expensive (c > pL) then the social planner would prefer to leave
the player uninsured (ei, si) = (0, 0). This strategy is independent
of the cost of protection. The remaining N − 1 players have to
select their protection effort as ei = ε > 0 (as small as possible).
These players will not be attacked, and therefore will set their self-
insurance to the possible minimum (ε, 0). Passivity by all players
is never an option in the social optimum.

Result 11: A social planner can easily devise a strategy to over-
come the coordination problems observed in the Nash analysis for
the weakest-target game with mitigation. We found that no pure
Nash strategy exists and, therefore, had to rely on the increased
rationality requirement for entities to play a mixed strategy.4 The
average payoff for each player in the social optimum is consider-
ably higher compared to the mixed Nash equilibrium.

Understandably, without side-payments the node with the lowest
protection effort is worse off compared to his peers. However, the
social planner could choose to devise a so-called “honeypot” sys-
tem with the sole goal of attracting the attacker while only suffering
a marginal loss. A honeypot is a computer system (or another de-
vice) that is explicitly designed to attract and to be compromised
by attackers. It serves usually a double purpose. First, it will de-
tract attention from more valuable targets on the same network.
Second, if carefully monitored it allows gathering of information
about attacker strategies and behaviors, e.g., early warnings about
new attack and exploitation trends [30].

An interesting aspect of the social optimum solution is the ques-
tion how the individual is selected (if a honeypot system cannot be
devised). Obviously, a social planner might be able to direct an in-
dividual to serve as a target (in particular, if c < pL). However,
if insurance costs are large being a target requires an almost cer-
tain sacriﬁce (dependent on the value of p). In anthropology and
economics there are several theories that relate to an individuals
willingness to serve as a sacriﬁcial lamb. Most prominently, altru-
ism and heroism come to mind. Simon also introduced the concept
of docility. This theory refers to an individual’s willingness to be
taught or to defer to the superior knowledge of others [39].

4Economists are generally cautious regarding the assumption that
individuals can detect and adequately respond to mixed strategy
play by opponents [36].

5.5 Weakest-target security game (with miti-

gation)

We adopt the same strategy for ﬁnding Φ’s maximum as in the
unmitigated case – that is, summing Eqn. (6) over i, and then study-
ing the variations of Φ over K, si and e0.

The ﬁrst observation is that the social planner might prescribe
the same strategy as in the case of the weakest-target game without
mitigation. However, now the planner has a second alternative.
Since an attacker will not be able to compromise players if they are
fully protected we ﬁnd that (ei, si) = (1, 0) for all N players is
a feasible strategy. The tipping point between the two strategies is
at N b < c. If this condition holds the social planner would elect
to protect all machines in favor of offering one node as honeypot
and investing in its self-insurance. Note that again we ﬁnd that if
protection and self-insurance are extremely costly the planner will
elect to sacriﬁce one entity without insurance. Passivity is not a
preferable option.

Result 12: Compared to the weakest-target game without mitiga-
tion the social planner is better off if protection is cheap. Otherwise
the planner has to sacriﬁce a node with or without self-insurance.
Interestingly, while compared to the pure Nash equilibrium out-
come the social planner can increase the overall utility in the net-
work we ﬁnd that security expenditures are lowered. In the Nash
equilibrium agents were willing to fully protect against threats as
long as (b ≤ c).

The last observation also holds for the mixed strategy case in
both weakest-target games (with or without mitigation). That is,
agents exert more effort in the Nash equilibrium (except when N b <
c for the game with mitigation).

6. DISCUSSION OF RESULTS

The results we obtained, and notably the disconnect between so-
cial optima and Nash equilibria we observed, lead to a number of
remarks that may prove relevant to organizational strategy. How-
ever, we want to preface this discussion by pointing out that our
analysis is a ﬁrst comparison of different security games with two
security options under common, but restrictive assumptions.

Most notably, we assume agents to be risk-neutral providers of
the public protection good. In our game formulation we also sim-
pliﬁed cost of protection (and insurance) to be linear.
Including
different risk preferences, as well as uncertainty and limited infor-
mation about important parameters of the game would be important
steps towards a sensitivity analysis of our results. Shogren found,
for example, that risk-averse agents will increase their contribu-
tions if information about other agents actions is suppressed [38].
Others, e.g., [34], have obtained more nuanced results. We defer
a more extensive analysis of such phenomena to future work, but
believe that the main trends and differentiating features between
security games we observed remain largely unchanged.

Security scenario identiﬁcation: We ﬁnd that security predic-
tions vary widely between the ﬁve different games. Similarly, poli-
cies set by a social planner do not only yield different contribution
levels but may also switch the recommended security action from
protection to self-insurance and vice versa. Chief Security Ofﬁcers’
tasks involve a careful assessment of threat models the company is
faced with.

We want to emphasize that an integral part of the threat model
should be an assessment of the organizational structure including
system resources and employees. Similarly important is a detailed
consideration whether resources are protected independently or by
an overarching system policy. For example, replication, redun-
dancy and failover systems (that automatically switch to a standby

database, server or network if the primary system fails or is tem-
porarily shut down for servicing) should most likely not be treated
as independent resources.

Managers should consider how the organizational structure of re-
sources matches potentially existing policies. For example, we can
see that a policy that requires full protection by every individual
is sub-optimal if the most likely threat and organizational structure
ﬁts the description of a best shot game. Contributions resources
are squandered and are likely to deteriorate. Not to mention that
employees may simply ignore the policy over time. See, for ex-
ample, recent survey results that highlight that 35% of white-collar
employees admit to violations of security policies [25].

Security scenario selection: A security professional might be
faced with a unidentiﬁable organization and system-policy struc-
ture. However, we want to highlight that our research allows a
more careful choice between security options if managers can re-
design organizations and policies. For example, the choice between
a system-wide ﬁrewall and intrusion detection system versus an in-
dividual alternative has important implications on how incentives
drive security-relevant behavior over time. Individual systems will
better preserve incentives, however, might have negative cost im-
plications. The same choice applies between the availability of
backup tools and protective measures.

Leveraging strategic uncertainty: The example of the weakest-
target game shows the importance of the degree of dependency be-
tween agents. We show that in larger organizations a much lower
average level of self-insurance investments will be achieved be-
cause the strategic dependence between actors is reduced. How-
ever, in turn more agents will elect to protect their resources (ei > 0
for more players). In contrast, agents in small groups will respond
to the increasing strategic uncertainty caused by the increased in-
terdependency by self-insuring their resources more often.

Introducing a social planner into the weakest-target game com-
pletely removes strategic uncertainty and leads to both reduced self-
insurance and protection investments. This apparent paradox em-
phasizes that higher security investments do not necessarily trans-
late in higher security – but instead that how the investments are
made are crucial to the returns.

7. CONCLUSIONS

We consider the problem of decision-making with respect to in-
formation security investments. To that effect, we model security
interactions through a careful selection of games, some established
(weakest-link, best-shot, and total effort) and some novel (weakest-
target, with or without mitigation). All of these games offer players
two independent decision parameters: a protection level, e, which
determines the level of security a player chooses for his resources;
and a self-insurance level, s, which mitigates losses, shall a suc-
cessful attack occur. We postulate that the ﬁve games considered
cover a vast majority of practical security situations, and study them
both from a rational agent’s perspective (Nash equilibrium analy-
sis) and from a central planner’s view (social optimum analysis).

Our main ﬁndings are that the effects of central planning com-
pared to laissez-faire considerably differ according to the game
considered. While in a number of traditional cases borrowed from
the public good literature, we observe that a central planner may
increase the average protection level of the network, we also note
that strategic decisions are highly impacted by the level of inter-
dependency between the actions of different players.

In particular, we found that the common wisdom that having a
central planner who decides upon security implementation always
yields higher protection contributions by individual players does

not hold. Indeed, it may at times be much more advantageous from
an economic standpoint to invest in self-insurance instead of pro-
tecting systems, or to select a few, unprotected, sacriﬁcial lambs in
order to divert the attention of potential attackers. This is particu-
larly the case in situations which exhibit a “strategic uncertainty”
due to a very strong correlation between the actions of different
agents, for instance, in our weakest-target game where the least se-
cure player is always the one attacked.

7.1 Future research directions

The work presented here opens a number of avenues for future
research. First, we have looked at homogeneous populations of
users, where all participants have the same utility function. In prac-
tice, this homogeneity assumption is reasonable in a number of im-
portant cases, particularly when dealing with very large systems
where a large majority of the population have the same aspirations.
For instance, most Internet home users are expected to have vastly
similar expectations and identical technological resources at their
disposal; likewise, modern distributed systems, e.g., peer-to-peer
or sensor networks generally treat their larger user base as equals.
It will be nevertheless prudent to study whether considering hetero-
geneous user populations impacts the results we obtained here and
in what way. Varian [43], for instance, evidences important asym-
metric user behaviors due to heterogeneity in his reliability games.
We also plan to extend our work to explore the impact of ﬁnes and
liability rules on security investments [9, 16].

Second, this paper assumes that execution of a player’s strategy
is always perfect, and that all players are perfectly rational. As
has been discussed elsewhere, e.g., [12], this assumption gener-
ally leads to idealized models, which deserve to be complemented
by empirical studies.
In that respect, we are currently develop-
ing a set of laboratory experiments to conduct user studies and at-
tempt to measure the differences between perfectly rational behav-
ior and actual strategies played. Our preliminary investigations in
the ﬁeld notably evidence that players often experiment with differ-
ent strategies to try to gain a better understanding of the game they
are playing.

Reconciling observed experimental behavior and theoretical anal-
ysis with the design of meaningful security policies is a very chal-
lenging goal. We do hope that the present paper will encourage
research in this area.

8. ACKNOWLEDGMENTS

We thank the anonymous reviewers for their valuable comments
and editorial guidance. Paul Laskowski greatly improved this manu-
script with his tremendously helpful feedback. This work is sup-
ported in part by the National Science Foundation under ITR award
ANI-0331659. Jens Grossklags’ research is also partially funded
by TRUST (Team for Research in Ubiquitous Secure Technology),
under support from the NSF (award CCF-0424422) and the fol-
lowing organizations: BT, Cisco, ESCHER, HP, IBM, iCAST, In-
tel, Microsoft, ORNL, Pirelli, Qualcomm, Sun, Symantec, Telecom
Italia, United Technologies, and AFOSR (#FA 9550-06-1-0244).

9. REFERENCES
[1] A. Acquisti and J. Grossklags. Privacy and rationality in

individual decision making. IEEE Security & Privacy,
3(1):26–33, January–February 2005.

[2] E. Adar and B. Huberman. Free riding on Gnutella. First

Monday, 5(10), Oct. 2000.

[3] R. Anderson. Why cryptosystems fail. In Proc. ACM

CCS’93, pages 215–227, Fairfax, VA, Nov. 1993.

[4] R. Anderson. Why information security is hard – an

economic perspective. In Proc. ACSAC’01, New Orleans,
LA, Dec. 2001.

[5] R. Anderson and T. Moore. The economics of information

security. Science, 314(5799):610–613, Oct. 1998.

[6] AOL/NSCA. Online safety study, Dec. 2005.

http://www.staysafeonline.org/pdf/
safety_study_2005.pdf.

[7] T. August and T. Tunca. Network software security and user

provision of public goods. Public Choice, 41(3):371–386,
Jan. 1983.

[24] P. Honeyman, G. Schwartz, and A. van Assche.

Interdependence of reliability and security. In Proc. (online)
WEIS’07, Pittsburgh, PA, June 2007.

[25] Information Systems Audit and Control Association.

Telephone survey conducted by MARC Research, Oct. 2007.
http://biz.yahoo.com/bw/071031/
20071031005079.html?.v=1.

incentives. Mgmt. Science, 52(11):1703–1720, Nov. 2006.

[26] H. Kunreuther and G. Heal. Interdependent security. J. Risk

[8] R. Böhme and G. Kataria. Models and measures for

correlation in cyber-insurance. In Proc. (online) WEIS’06,
Cambridge, UK, June 2006.

[9] J. Brown. Toward an economic theory of liability. Journal of

Legal Studies, 2(2):323–349, June 1973.

[10] Bruskin Research. Nearly one in four computer users have
lost content to blackouts, viruses and hackers according to
new national survey, 2001.
http://www.corporate-ir.net/ireye/ir_
site.zhtml?ticker=iom&script=410&layout=
-6&item_id=163653.

[11] J. Bull, L. Gong, and K. Sollins. Towards security in an open

and Uncertainty, 26(2–3):231–249, Mar. 2003.

[27] S. Malphrus. The “I Love You” computer virus and the

ﬁnancial services industry, May 2000.
http://www.federalreserve.gov/BoardDocs/
testimony/2000/20000518.htm.

[28] D. Moore, V. Paxson, S. Savage, C. Shannon, S. Staniford,

and N. Weaver. Inside the Slammer worm. IEEE Security
and Privacy, 1(4):33–39, July 2003.

[29] D. Moore, C. Shannon, and J. Brown. Code-Red: a case

study on the spread and victims of an internet worm. In Proc.
ACM/USENIX IMW’02, pages 273–284, Marseille, France,
Nov. 2002.

systems federation. In Proc. ESORICS’92, Springer LNCS
No. 648, pages 3–20, Toulouse, France, Nov. 1992.

[30] N. Provos. A virtual honeypot framework. In Proc. USENIX

Security’04, pages 1–14, San Diego, CA, Aug. 2004.

[12] N. Christin, J. Grossklags, and J. Chuang. Near rationality
and competitive equilibria in networked systems. In Proc.
ACM SIGCOMM’04 PINS Workshop, pages 213–219,
Portland, OR, Aug. 2004.

[13] D. Clark, J. Wroclawski, K. Sollins, and R. Braden. Tussle in

cyberspace: deﬁning tomorrow’s Internet. In Proc. ACM
SIGCOMM’02, pages 347–356, Pittsburgh, PA, Aug. 2002.

[14] G. Danezis and R. Anderson. The economics of resisting

censorship. IEEE Security & Privacy, 3(1):45–50,
January–February 2005.

[15] I. Ehrlich and G. Becker. Market insurance, self-insurance,

and self-protection. Journal of Political Economy,
80(4):623–648, July 1972.

[16] E. Fehr and S. Gaechter. Cooperation and punishment in
public goods experiments. American Economic Review,
90(4):980–994, Sept. 2000.

[17] J. Franklin, V. Paxson, A. Perrig, and S. Savage. An inquiry

into the nature and causes of the wealth of Internet
miscreants. In Proc. ACM CCS’07, Alexandria, VA,
Oct./Nov. 2007.

[18] L. Gordon and M. Loeb. The economics of information

security investment. ACM Transactions on Information and
System Security, 5(4):438–4572, Nov. 2002.

[19] S. Gordon. The generic virus writer. In Proc. Intl. Virus

Bulletin Conf., pages 121 – 138, Jersey, Channel Islands,
1994.

[20] S. Gordon. Virus writers - the end of the innocence? In 10th

Annual Virus Bulletin Conference (VB2000), Orlando, FL,
Sept. 2000. http://www.research.ibm.com/
antivirus/SciPapers/VB2000SG.htm.
[21] G. Hardin. The tragedy of the commons. Science,

162(3859):1243–1248, Dec. 1968.

[22] K. Hausken. Returns to information security investment: The
effect of alternative information security breach functions on
optimal investment and sensitivity to vulnerability.
Information Systems Frontiers, 8(5):338–349, Dec. 2006.

[23] J. Hirshleifer. From weakest-link to best-shot: the voluntary

[31] E. Rescorla. Security holes... who cares? In Proc. USENIX

Security’03, pages 75–90, Washington, DC, Aug. 2003.

[32] J. Saltzer, D. Reed, and D. Clark. End-to-end arguments in

system design. ACM Transactions on Computer Systems,
2(4):277–288, Nov. 1984.

[33] T. Sandler and K. Hartley. Economics of alliances: The

lessons for collective action. Journal of Economic Literature,
XXXIX(3):869–896, Sept. 2001.

[34] T. Sandler, F. Sterbenz, and J. Posnett. Free riding and

uncertainty. Economic Review, 31(8):1605–1617, Dec. 1987.

[35] T. Schelling. The Strategy of Conﬂict. Oxford University

Press, Oxford, UK, 1965.

[36] J. Shachat and J. Swarthout. Do we detect and exploit mixed

strategy play by opponents? Mathematical Methods of
Operations Research, 59(3):359–373, July 2004.

[37] S. Shenker. Making greed work in networks: A

game-theoretic analysis of switch service disciplines.
IEEE/ACM Trans. Networking, 3(6):819–831, Dec. 1995.

[38] J. Shogren. On increased risk and the voluntary provision of

public goods. Social Choice and Welfare, 7(3):221–229,
Sept. 1990.

[39] H. Simon. Altruism and economics. American Economic

Review, 83(2):156–161, May 1993.

[40] S. Spiekermann, J. Grossklags, and B. Berendt. E-privacy in

2nd generation e-commerce: privacy preferences versus
actual behavior. In Proc. ACM EC’01, pages 38–47, Tampa,
FL, Oct. 2001.

[41] The Honeynet Project. Know your enemy: the tools and

methodologies of the script-kiddie, July 2000. http:
//project.honeynet.org/papers/enemy/.

[42] J. Van Huyck, R. Battallio, and R. Beil. Tacit coordination

games, strategic uncertainty, and coordination failure.
American Economic Review, 80(1):234–248, 1990.

[43] H. Varian. System reliability and free riding. In L. Camp and
S. Lewis (ed.), Economics of Information Security (Advances
in Information Security, Volume 12), pages 1–15. Kluwer
Academic Publishers, Dordrecht, The Netherlands, 2004.

