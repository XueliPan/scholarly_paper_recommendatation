Service Availability: A New Approach to

Characterize IP Backbone Topologies

Ram Keralapura and Chen-Nee Chuah

Gianluca Iannaccone

Supratik Bhattacharyya

Univ. of California, Davis

{rkeralapura,chuah}@ucdavis.edu

Intel Labs., Cambridge, UK

Sprint ATL, CA

gianluca.iannaccone@intel.com supratik@sprintlabs.com

Abstract— Traditional SLAs, deﬁned by average delay or
packet loss, often camouﬂage the instantaneous performance
perceived by end-users. We deﬁne a set of metrics for service
availability to quantify the performance of IP backbone networks
and capture the impact of routing dynamics on packet forward-
ing. Given a network topology and its link weights, we propose a
novel technique to compute the associated service availability by
taking into account transient routing dynamics and operational
conditions, such as BGP table size and trafﬁc distributions.

Even though there are numerous models for characterizing
topologies, none of them provide insights on the expected perfor-
mance perceived by end customers. Our simulations show that
the amount of service disruption experienced by similar networks
(i.e., with similar intrinsic properties such as average out-degree
or network diameter) could be signiﬁcantly different, making it
imperative to use new metrics for characterizing networks. In the
second part of the paper, we derive goodness factors based on
service availability viewed from three perspectives: ingress node
(from one node to many destinations), link (trafﬁc traversing a
link), and network-wide (across all source-destination pairs). We
show how goodness factors can be used in various applications
and describe our numerical results.

I. INTRODUCTION

Service-Level Agreements (SLAs) offered by today’s Inter-
net Service Providers (ISPs) are based on four metrics: end-to-
end delay, packet loss, data delivery rate and port availability.
The ﬁrst three metrics are usually computed network-wide and
averaged over a relatively long period of time. For the fourth
metric, the term “port” refers to the point at which a customer’s
link attaches to the edge of an ISP’s network. Port availability
therefore refers to the fraction of time this port is operational
and measures a customer’s physical connectivity to the ISP’s
network. None of these SLA metrics capture the ability of the
network to carry customer trafﬁc to Internet destinations at
any point in time.

The main problem with the existing SLA speciﬁcations is
that they do not capture the effect of instantaneous network
conditions like failures and congestions. A recent study [1]
shows that failures occur on a daily basis due to a variety of
reasons (e.g., ﬁber cut, router hardware/software failures, and
human errors) and can impact the quality-of-service (QoS)
delivered to customers. When a link/node fails, all routers
will independently compute a new path around the failure. At
that time, routers may lack or have inconsistent forwarding

0This research project was supported by National Science Foundation
CAREER grant 238348 and University of California Micro with matching
funds from Fujitsu and Sprint

information, resulting in packet drops or transient routing
loops [2][3]. However, not all failures impact the network
equally. The failure of a critical backbone link carrying heavy
trafﬁc may be more detrimental than the failure of an access
link connecting a single customer. Yet, these service degrada-
tions are camouﬂaged by the average parameters reported in
current SLAs. Therefore, to measure IP network performance,
it is essential to consider the network routing conﬁguration
and trafﬁc pattern during link or node failures.
Reports from various tier-1 ISPs suggest

that IP back-
bone networks are usually over-provisioned where the link
utilization of backbone links is less than 50% of their total
capacity [4][5]. The reports also conﬁrm that congestion due
to link or router overload is a very rare event in backbone
networks. During a link failure event trafﬁc on the failed
link is rerouted and may congest links along alternate paths.
However, such congestions are usually not signiﬁcant for a
single failure event. Heavy congestions may occur when there
are multiple failures, but such events are relatively rare. Hence
in our current work we ignore the effect of congestions on
network performance and only consider failures.

In this paper, we deﬁne a set of metrics for service avail-
ability of IP backbone networks that capture the impact of
routing dynamics on packet forwarding. Instead of relying on
active or passive measurements, we propose a methodology to
estimate the service availability of a network in the presence
of independent link failures. Speciﬁcally, given a topology
(nodes, links and connectivity) and routing information (link
weights, link delays and BGP peering points), we are able to
compute the potential impact on service due to link failures.
To achieve this, we carefully model the factors identiﬁed in
the measurement based study by Iannaccone et al [6] that
contribute to routing convergence. Convergence refers to the
amount of time it
takes for trafﬁc forwarding to resume
correctly on the backup path after a link failure. We wish to
point out here that we focus mainly on single link failures for
IP backbone networks since these are the dominant class of
failures (i.e. over 70% of all failures in IP backbone networks)
as observed by Markopoulou et al [1].

We use the novel concept of service availability to evaluate
known topologies, such as full-mesh, ring and Tier-1 ISP
backbones. Our simulations show that the performance of a
network not only depends on routing dynamics, but also on
various other factors like IGP link weight assignment and BGP
preﬁx distribution. This brings out a necessity to identify new

0-7803-8277-3/04/$20.00 ©2004 IEEE.

232

metrics that customers can use to differentiate networks. There
have been many attempts to characterize the Internet topolo-
gies or to model their graph-theoretic properties [7][8][9].
The resulting models are useful for re-generating topologies
that best model real networks, such as GT-ITM [10] and
BRITE [11], but they do not provide any insights on the QoS
that a particular network can provide.

Using the concept of service availability, we derive good-
ness factors based on three different perspectives: ingress node
(from one node to many destinations), link (trafﬁc traversing a
link), and network-wide (across all source-destination pairs).
The goodness factors reveal how topologies with similar in-
trinsic graph properties, such as average out-degree or network
diameter, do not necessarily offer the same level of service
availability.

Finally, we describe several applications for the goodness
factors in network planning and provisioning. For example,
goodness from an ingress node perspective allows customers
to choose the best place to connect to a network (or to choose
among different providers), while link-based goodness helps
an ISP to identify the set of critical links to be upgraded.

The rest of the paper is organized as follows. Section II
identiﬁes the importance of routing dynamics in estimating the
end-to-end performance of a network and motivates this work.
In Section III, we describe the proposed metrics and introduce
the concept of service availability to characterize network
topologies. We also deﬁne a set of network goodness factors.
We discuss our numerical results in Section IV. We show the
various applications of the goodness factors in Section V and
conclude our paper in Section VI.

II. ANALYZING IMPACT OF ROUTING DYNAMICS

Intra-domain routing protocols, such as IS-IS [12] and
OSPF [13], deﬁne how each node in the network responds
to changes in the topology. Such protocols are also known as
link state protocols where each node has complete knowledge
of the network topology including all the links present in the
network.

Upon detection of a link/node failure or a conﬁguration
change in the network, each node is responsible for disseminat-
ing the new topology description to all its neighboring nodes
and recomputing the forwarding information in its own routing
table. From the time of the failure or conﬁguration change to
the time all nodes have been informed of the change and have
updated their routing tables, trafﬁc disruptions (like packet
drops and routing loops) are possible as the nodes may have
an inconsistent view of the network.

We deﬁne the “convergence time” (CT ) of a node due to a
failure event in the network as the time taken by the node to
update its routing and forwarding information in response to
the failure. A node that does not have to update its routing or
forwarding information has a convergence time of zero. The
convergence time for any node n (that has to update its routing
or forwarding information) due to a failure can be summarized
as a combination of 3 components:

• Detection time: This is the time taken by the adjacent
nodes to detect the failure. Today’s IP routers provide
several mechanisms to perform this function [14], but all
of them are based only on local information exchanged
between neighboring nodes. For example, hello messages
at the IP layer or alarms at the optical layer. Hence
detection time represents a ﬁxed price that is independent
of the network topology or conﬁguration.

• Notiﬁcation time: This represents the time taken by the
routing update to propagate through the network to reach
n. In link state protocols, messages are ﬂooded through-
out the network. Therefore, the notiﬁcation time strongly
depends on the hop distance between n and the adjacent
nodes. Each node along the forwarding route needs to
process the message update before forwarding it, thus
introducing a delay in the propagation of information.

• Route Computation and Update time: This is the time
spent by node n to compute the new shortest path routing
tree (that incorporates the failure information) and then
update its forwarding information based on the new
routing tree. The update procedure involves applying the
changes to all the network preﬁxes that have been learnt
via the BGP inter-domain protocol. The result of this
computation is a forwarding table where each preﬁx is
associated with a neighboring node as the next hop.
The route computation and update time at any node
heavily depends on the number of preﬁxes for which the
next hop information needs to be changed. In turn, the
number of preﬁxes to be updated not only depends on
the location of the failure, but also on the distribution of
preﬁxes to the next hops in the forwarding table. Indeed,
the closer the failure occurs to a node, the larger will
be the number of preﬁxes affected. Similarly, if a large
number of preﬁxes share the same next hop node, a
change in the topology close to that node will result in
long route update time for all nodes in the network.

Based on the per-node convergence time (CT ) we deﬁne
the “network convergence time” as the maximum value of
CT among all the nodes in the network. This indicates the
time at which all the nodes in the network have learned about
the failure, updated their routing tables and have a consistent
view of the network.

Service availability of a network depends on the conver-
gence time. Providers attempt to increase the overall availabil-
ity of their networks by reducing convergence time in order to
speed up the recovery after a failure. As we described above
convergence time depends on (i) router technology used for
failure detection, (ii) network topology, (iii) routing protocol
conﬁguration such as IGP weights and timers, and (iv) loca-
tion of peering points with other networks that determines the
distribution of network preﬁxes among egress nodes. Clearly,
it is not possible to look at a subset of the above mentioned
factors to derive the service availability of a network.

Consider the network illustrated in Figure 1. The number
on each link indicates the IGP link weight. Let node A be
the trafﬁc source and node D be the trafﬁc sink. Consider the

0-7803-8277-3/04/$20.00 ©2004 IEEE.

233

A

2

1

3

B

F

4

C

E

1

7

D

2

1

Fig. 1. Example of convergence time and service disruption

failure of link E – D. We assume that the nodes and the links
have similar characteristics with a detection time of 500ms, a
notiﬁcation time between neighboring nodes of 100ms, and a
node route computation and update time of 400ms (except for
node C for which we assume this to be 100ms due to the fact
that it does not have to change its route to reach D after the
failure of link E – D). Consider the disruption observed for the
trafﬁc sent from node A to node D following the failure event
at time t0 = 0. Table I shows the routing events, changes in
the trafﬁc forwarding path and service availability from node
A to node D. In this example, we assume that a node notiﬁes
neighboring nodes only after it has completed the update of
its own routing table. However, it is easy to verify that if the
updates are sent before updating the routing table, the example
yields similar results.

Interestingly, as the message update propagates across the
network, the forwarding path from node A to node D changes
four times. Some of these intermediate paths are valid thus
restoring service between A and D, while some are not,
causing packet drops (or trafﬁc black-hole) and routing loops.
For example, packets are dropped until node E has computed
a new forwarding path to reach D and routing loops occur
when nodes B and F have conﬂicting forwarding information.
Based on the example we can derive some initial observa-

tions:

are affected. Table II shows the impact of varying the
number of preﬁxes that have D as the next hop node on
convergence time and service availability. Even with the
same topology and identical failure scenario, increasing
route update time may lead to signiﬁcant differences in
service availability.

In the following section, we introduce a new topology met-
ric that exploits the knowledge of routing dynamics to deﬁne
and compare the goodness of topologies. It is based on the
algorithm to compute the cumulative time for which service
is not available (i.e. the service disruption time as presented in
Table I). The details of this algorithm are presented in Table
III.

III. SERVICE AVAILABILITY TO CAPTURE NETWORK

ROUTING DYNAMICS

A. Service Availability

We deﬁne service availability in three perspectives:
• Ingress node perspective: This is a measure of the net-
work performance as seen by a particular ingress node
where the trafﬁc enters the network. It provides an insight
about the level of service to expect when a customer
connects to different ingress nodes of a network. It also
helps an ISP to ensure that it can meet SLA speciﬁcations
for customers connecting to different ingress nodes.

• Link perspective: The performance of a network should
not heavily depend on the reliability of a few links in the
network. In other words, the network should not have
critical links whose failure results in serious performance
degradation. Service availability from a link perspective
evaluates the importance of various links for network
performance.

• Network perspective: This measures the performance of
the entire network from the perspective of an ISP. This is
useful in designing a network to achieve high end-to-end
performance.

• The network convergence time provides a rough upper
bound on service disruption time (i.e. the time for which
service is not available). The service is not available when
the packets cannot reach their destinations due to the lack
of forwarding information. Given that trafﬁc forwarding
may resume even if all the nodes in the network have not
updated their routing table, there may be a signiﬁcant lack
of correlation between network convergence and service
availability. For example, in our illustration above, the
cumulative time for which the service is not available
amounts to 1.1s while the network takes 1.9s to converge.
Therefore, network convergence time does not capture the
entire routing dynamics.

• The network topology by itself does not help in under-
standing the service availability of a network. Additional
information such as the location of peering points and
size of routing tables needs to be considered in char-
acterizing service availability. The time taken to update
the routing table depends on the number of preﬁxes that

We propose three metrics that capture routing dynamics in
a network due to single link failures for analyzing service
availability:

• Service Disruption Time (SD time): This represents the
time for which service between a particular source-
destination (O-D) pair is disrupted. From the point of
view of an ingress node, it indicates the loss in connec-
tivity with all/some parts of the Internet due to a link
failure.

• Trafﬁc Disruption (TD): This metric captures the total
trafﬁc disrupted between a particular source and desti-
nation node due to single link failures. It is the product
of trafﬁc rate between the O-D pair along the failed link
and the service disruption time. For an ISP, TD is more
important than SD time because of the fact that customers
are compensated for the amount of trafﬁc lost, irrespective
of the duration for which the service is disrupted.

• Number of Delay Parameter Violations (DV): In a well-
designed network, the end-to-end delay along alternate

0-7803-8277-3/04/$20.00 ©2004 IEEE.

234

SUMMARY OF ROUTING EVENTS (WITH A ROUTE UPDATE TIME OF 400MS). NETWORK CONVERGENCE TIME = 1.9S; SERVICE DISRUPTION TIME = 1.1S

Event

Forwarding Path (A – D)

Service from A to D

Notes

Time
0s
0.5s
0.9s
1.0s
1.1s
1.2s
1.4s
1.5s
1.6s
1.9s

Failure of link E – D
D,E: failure detection

D,E: route update

C,F: notiﬁed of failure

C: route update

B: notiﬁed of failure

F: route update

A: notiﬁed of failure

B: route update
A: route update

A-F-E-D
A-F-E-D
A-F-E-C-D
A-F-E-C-D
A-F-E-C-D
A-F-E-C-D
A-F-B-F-...
A-F-B-F-...
A-F-B-C-D
A-B-C-D

NO
NO
YES
YES
YES
YES
NO
NO
YES
YES

Forwarding is restored

Path C to D is not affected

Routing loop B – F
Routing loop B – F

Network Convergence

TABLE I

TABLE II

Routing update time
Convergence time
Time - service not available

100ms

200ms

1.0s
0.8s

1.3s
0.9s

300ms
1.6s
1.0s

400ms
1.9s
1.1s

500ms
2.2s
1.2s

1000ms

3.7s
1.7s

CONVERGENCE TIME DEPENDS ON ROUTING UPDATE TIME IN NODES

STEP 1: Initialize the Service Disruption Time, φl(x, y), for the path from x to y due to the failure of the link l to 0 i.e.
φl(x, y) = 0. If the original path from x to y does not contain link l then QUIT.

STEP 2: Find the convergence time (CT ) for each node and list the nodes in the increasing order of convergence time. Let
the convergence time of the ﬁrst node in the list be CT 1, second node be CT 2 and so on. In general, the convergence time
for the nth node in the list is CT n. Set the current node, k, (which is the node number on the sorted list) to 0.

STEP 3: Increment k to 1. Set φl(x, y) = CT 1. At the time instant CT 1 after the failure event, ﬁnd the path that a packet
from the source node, x, follows to reach the destination node, y, taking into account that the ﬁrst node in the list has
converged and others have not. If the path has a routing loop or black-hole, then set previousDisruption = true else set
previousDisruption = f alse

STEP 4: Increment k by 1. At the time instant CT k after the failure event, ﬁnd the path that a packet from the source node,
x, follows to reach the destination node, y, taking into account that the intermediate nodes might have converged or not.

STEP 5: If the path that the packet follows does not contain the failed link l and has no routing loop, then set
previousDisruption = f alse. Go to Step 7. Else go to Step 6.

STEP 6: If the path that the packet follows contains the failed link l or has a routing loop, then the path from
x to y is still disrupted. If previousDisruption = f alse, then do not update the Service Disruption Time but set
previousDisruption = true else if previousDisruption = true, then update the Service Disruption Time in the kth
iteration as, φl(x, y) = φl(x, y) + CT k − CT k−1

STEP 7: If there are more nodes in the list then go to STEP 4. Else QUIT.

ALGORITHM TO CALCULATE SERVICE DISRUPTION TIME FROM NODE x TO NODE y DUE TO A SINGLE LINK FAILURE

TABLE III

paths is generally higher than the original path. We
assume here that the delay parameter is representative
of the maximum end-to-end delay that can be tolerated
by delay sensitive trafﬁc in the network. If the end-to-end
delay along the alternate path exceeds the delay parameter
speciﬁcation, then there is a delay parameter violation.
DV measures the number of such delay parameter vio-
lations from the perspective of an ingress node due to
single link failures.

Note that SD time and TD capture the effect of a link failure
during service disruption, while DV is a post-convergence ef-
fect. However, given complete network topology speciﬁcations

(i.e, nodes, links, connectivity, link weights, link delays and
delay parameter), BGP preﬁx distribution and trafﬁc rate in
the network, all the metrics can be pre-computed and used to
characterize the end-to-end performance of a network.

B. Goodness Factors

it

To use the notion of service availability in characterizing
network topologies,
is necessary to capture it using a
quantitatively measure. Intuitively, this measure should yield a
numerical value that can estimate the end-to-end performance
of a network and help in differentiating various topologies. In
order to accomplish this, we deﬁne a set of goodness factors

0-7803-8277-3/04/$20.00 ©2004 IEEE.

235

based on different perspectives of service availability. These
factors can be deﬁned differently depending on the speciﬁc
scenario (for example, it could be driven by the cost involved,
SLA speciﬁcations etc.).

We ﬁrst introduce the notations that we use in the rest of
this section. Consider a network with N nodes and M links.
Let Γ and Λ represent the set of nodes and links respectively.
For any node, i, there are (N − 1) different destinations in
the network and hence (N − 1) different paths with node i as
the ingress node. For the failure of link j, all/some/none of
(N − 1) paths could be affected. Let Qihj and Tihj denote
the SD time and TD for the source-destination pair i − h, due
to the failure of link j. Similarly, let Sij denote the number
of delay parameter violations from ingress node i along all
(N − 1) paths due to the failure of link j.

1) Goodness from ingress node perspective: Typically,
many customers are connected to a network at an ingress node.
TD represents the total trafﬁc affected for all the customers
connected to the ingress node due to a link failure and does not
provide valuable information to individual customers. Instead
we use SD time and DV to measure the goodness of a network
from an ingress node perspective. We deﬁne,

GIi = f (Qi, Si)

(1)

where GIi is the goodness factor of the network from the
perspective of ingress node i. Qi is the average SD time for
node i across all (N − 1) paths and M possible single link
failures:

1

Qi =

M (N − 1)

(cid:1)

∀h∈Γ,h(cid:3)=i,∀j∈Λ

Qihj

(2)

Similarly, Si is the average of the number of delay param-
eter violations from node i, to all other nodes in the network
due to various single link failures:
(cid:1)

Si =

1
M

Sij

∀j∈Λ

GIi =

C

(Qi)q(Si)p

where C is a constant. The exponents q and p are SLA
dependent. In our simulations we assume the exponent values
to be 1.

2) Goodness from link perspective: The impact of a link
failure on the network performance directly depends on total
TD and total DV (i.e. sum of DV for all nodes) due to the
failure. High values of these metrics for a link implies that the
link is critical to network performance. We deﬁne the goodness
factor from a link perspective based on similar assumptions on
f as in Equation 4:

GLj =

C

(Tj)t(Sj)p

0-7803-8277-3/04/$20.00 ©2004 IEEE.

(3)

(4)

(5)

236

(6)

(7)

(8)

(9)

where GLj is the goodness from the perspective of link j. Tj
is the total TD due to the failure of link j:

Similarly, Sj is the total number of delay parameter violations
due to the failure of link j i.e.,

(cid:1)

Tj =

Tihj

∀i,h∈Γ,i(cid:3)=h

Sj =

Sij

(cid:1)

∀i∈Γ

To ﬁnd critical links in a network it is more useful to
compute the badness of a link rather than its goodness. Hence
we deﬁne the badness of link j in the network as the inverse
of it’s goodness:

BLj = C(Tj)t(Sj)p

3) Goodness from network perspective: We deﬁne the
goodness of the entire network as the sum of link goodness
factors for various links in the network:

GN =

(cid:1)

∀j∈Λ

C

(Tj)t(Sj)p

IV. RESULTS AND DISCUSSION

In this section, we use the metrics proposed in Section III to
quantify the impact of link failures on service availability of
a network. First, we illustrate how the algorithm presented
in Section II is used to compute SD time, TD time and
DV for different classes of network topologies. Then, we
examine how traditional graph properties such as out-degree,
network diameter, increase in tree depth, and disconnecting
sets correlate to QoS offered by the network. We will show
the effectiveness of goodness factors in differentiating various
network topologies by capturing the routing dynamics that
affect trafﬁc forwarding performance. Lastly, we will discuss
the implication of the graphs and how goodness factors can
be used in evaluating “quality” of connectivity and network
design applications.

We built a java-based simulator to emulate intra-domain
routing dynamics in the presence of link failures and to
implement the algorithm presented in Table III. The inputs to
the simulator are complete network topology speciﬁcations,
BGP preﬁx distribution, and trafﬁc load along different links
in the network. In our simulations, each node in the network
topology is mapped to a geographic location. The delays for
individual links are then calculated based on the geographical
distance between the nodes that the link connects. We cat-
egorize the nodes in a network as large, medium and small,
depending on the amount of trafﬁc they generate. We consider
20% of the nodes as large nodes, 30% as medium nodes and
the rest as small nodes. The classiﬁcation of nodes into large,
medium and small nodes in a network is based on PoP (Point
of Presence) level network topologies of various tier-1 ISPs.
The trafﬁc matrix for all the networks are generated using
this model. We distribute BGP preﬁxes proportional to the

The function f depends on SLA speciﬁcations between ISP
and customers. For simplicity, we assume that goodness is
inversely proportional to various metrics. Hence,

A. Simulation Setup

trafﬁc between nodes. Large trafﬁc ﬂow from a source node
to destination node implies that the source node reaches a large
number of preﬁxes in the Internet through the destination node.
Our results with different assumptions for preﬁx distributions
yielded interesting results. We will discuss this further in
Section V.

Based on these inputs, the simulator runs Dijkstra’s SPF
algorithm to ﬁnd the shortest path from every node to all other
nodes in the network. It then simulates single link failures
and executes Dijkstra’s SPF algorithm again,
to ﬁnd new
paths in the network. The SD time for various O-D pairs
are calculated based on the algorithm in Section II. TD is
then calculated using SD time and network trafﬁc distributions
between different source-destination pairs. DV is determined
using the link delay values and delay parameter speciﬁcation.
We assume that the maximum tolerable delay to be 100 ms in
our simulations.

Fig. 3.

ISP-B topology

We consider the following two sets of topologies.

Set A: The ﬁrst set of topologies represent standard topologies
which includes ring, full-mesh and two PoP-level tier-1 ISP
topologies (ISP-A and ISP-B in Figure 2 and Figure 3). We
use these topologies to show that the metric values calculated
from our simulations are intuitively correct. All the topologies
considered in the ﬁrst set have 20 nodes in their networks.
ISP-A and ISP-B have 44 links each while the ring and mesh
topologies have 20 and 190 links respectively.

0-7803-8277-3/04/$20.00 ©2004 IEEE.

237

Set B: For the second set of topologies, we consider 10
topologies which are “similar” in terms of their intrinsic
properties like average out-degree and network diameter. The
diameter refers to the maximum depth of all routing trees in
the network. One of the topologies considered here is ISP-A
(Figure 2) from Set A. The other 9 topologies were generated
by changing link connectivity in ISP-A. Each topology was
generated using the following procedure:

• Randomly delete x links from ISP-A topology. We con-

sider 10 ≤ x ≤ 15 in our simulations.

• Randomly add x links back into the topology while
honoring the following rules: (i) The minimum out-
degree of any node in the network is 2. (ii) The resulting
topology is connected i.e., a spanning tree for the result-
ing topology has 19 links.

All the topologies considered in Set B have 20 nodes and 44
links with an average out-degree of 4.4 per node. We found
that the network diameter for the topologies lie in the range
of 4-6.

In all cases, we assign equal weights to all the links in a
network, thus making it a minimum-hop routing scheme. In
reality, different IGP link weight assignment schemes yield
different metric values and we will further explore this in
Section V. To achieve a fair comparison, we consider the
same trafﬁc distributions in all the topologies. Trafﬁc rate
to/from large, medium and small nodes remain the same in all
the topologies. Finally, we want to point out that the values
of goodness factors are normalized to 1 in all the results
presented.

In this section, we use the metrics proposed for service
availability to study the performance of four known topologies
(Set A). Intuitively, a mesh topology should perform the best
among all the networks with the same number of nodes while
a ring topology should perform the worst. Figure 4 shows the
cumulative distribution (cdf) of the total TD in each of the
four networks for various single link failure scenarios. Every
link in the mesh topology carries trafﬁc between a single O-
D pair, thus resulting in small values of total TD for single
link failures while every link in a ring topology carries trafﬁc
between multiple O-D pairs resulting in very high values of
total TD. These two topologies are the extreme cases for any
topology with the same number of nodes. TD values for other
topologies, like ISP-A and ISP-B, lie in-between these extreme
values.

Figure 5 shows the cdf of total DV for single link failures.
There are no delay parameter violations for the mesh topology
after single link failures while ring topology is signiﬁcantly
worse compared to other topologies for obvious reasons.

C. Goodness Factors vs. Static Graph Properties

The following case studies illustrate the limitations of static
graph properties (like out-degree distributions or network
diameter) to evaluate the performance of a network. This
provides the motivation to characterize network graphs using

Fig. 2.

ISP-A topology

B. Service Availability for Known Topologies

1

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

f

d
c

0.5

f

d
c

0.5

Ring
ISP−A
ISP−B
Mesh

preﬁxes have a single exit point in the network (we refer
to this as extreme preﬁx distribution). None of the other
topology properties were altered. The graph in Figure 7 shows
that changing BGP preﬁx distribution in the network changes
the source goodness factor for various nodes. This implies
that out-degree is not an appropriate metric to measure the
performance of source nodes.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 

e
v
i
t
c
e
p
s
r
e
P
e
d
o
N
e
e
r
g
n

 

I
 

m
o
r
f
 
s
s
e
n
d
o
o
G

 

e
v
i
t
c
e
p
s
r
e
P
e
d
o
N
 
s
s
e
r
g
n

I
 

m
o
r
f
 
s
s
e
n
d
o
o
G

0

0

500

1000

1500

2000

2500

3000

Traffic Disruption (Traffic Units)

Fig. 4. CDF of Trafﬁc Disruption due to single link failures

Ring
ISP−A
ISP−B
Mesh

0

1

2

3

4

5

6

7

8

9

10

11

Out Degree

Fig. 6. Goodness factor from an ingress node perspective Vs out-degree in
various topologies with normal preﬁx distribution

0

0

10

20

30

40

50

60

Note: Mesh topology has no delay 
          parameter violations   

Number of delay parameter violations

Fig. 5. CDF of Number of Delay Parameter Violations due to single link
failures

goodness factors, which are directly derived based on esti-
mated performance, i.e., service availability. All the following
results are for topologies in Set B.

In the past, out-degree of a node has been used as a metric
for characterizing a source node [15]. Figure 6 shows the
maximum, minimum and average goodness factor values for
various source nodes as a function of their out-degree. The
goodness factors for source nodes with the same out-degree
are considerably different. Also, higher out-degree does not
always result
in higher goodness factor values. The main
problem in using out-degree to estimate the performance
of a source node is that it does not capture the effect of
various network characteristics such as BGP preﬁx distribution
and link weight assignment scheme which have a signiﬁcant
impact on source node performance. To show this, we repeat
the simulations by distributing BGP preﬁxes such that all the

0-7803-8277-3/04/$20.00 ©2004 IEEE.

238

0

1

2

3

4

5

6

7

8

9

10

11

Out Degree

Fig. 7. Goodness factor from an ingress node perspective Vs out-degree in
various topologies with extreme preﬁx distribution

Network diameter gives an estimate of the maximum con-
vergence time in the network. One would expect that a network
with a small diameter would exhibit small convergence time
and hence offer better service availability. The top graph
of Figure 8 shows the network goodness factor for various
topologies against network diameter. However our results
show that topologies with smaller diameter do not always
result in higher goodness factors. Also, topologies with same

4

5

6

7

V. APPLICATIONS OF GOODNESS FACTORS

Network Diameter

diameter exhibit different network goodness factor values.
Like out-degree, network diameter does not account for several
characteristics of a network and hence is not a good metric
for predicting its performance.

Another metric related to network diameter is the increase
in tree depth due to single link failures. In a well-designed
network, the end-to-end delay depends directly on the depth of
the routing tree from source to destination. Hence DV depends
on the increase in tree depth after a failure. Typically, lower
values of increase in tree depth due to single link failures
should result in better topologies. From our simulations we
ﬁnd that
true. The bottom graph in Figure 8
shows the average increase in tree depth for various topologies
against the network goodness factors. Even though increase in
tree depth can be used to roughly estimate DV, we ﬁnd that it
is not a good metric for predicting network goodness.

this is not

s
s
e
n
d
o
o
G
 
k
r
o
w
e
N

t

s
s
e
n
d
o
o
G
 
k
r
o
w
e
N

t

0

3

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

4

4.5

5

5.5

6

6.5

7

7.5

Average increase in Tree Depth 
 for single link failures

Top graph shows the variation of network goodness factor with
Fig. 8.
network diameter while the bottom graph shows the variation of network
goodness with average increase in tree depth due to single link failures

Another important traditional metric used to estimate net-
work performance is the number of disconnecting sets in a
network [7]. Disconnecting set of a topology is deﬁned as a
set of links, whose cardinality is less than the minimum node
out-degree in the network and whose removal disconnects
the topology into two or more smaller topologies [16]. The
topologies that we consider here, have nodes with out-degree
of two. Hence, by the deﬁnition above, the cardinality of
the disconnecting set should not be more than 1. Since this
deﬁnition yields a null set for all the topologies, we deﬁne a
disconnecting set, D, with the following modiﬁcations:

• D partitions a topology into 2 parts with each part having

at least 2 nodes.

• Exclude Supersets: If D1, D2, ..., Dn are n disconnecting

sets for a network, then Di (cid:2)⊆ Dj when i (cid:2)= j.

of D is less than 3, i.e, sets with at most 3 links removed.
Disconnecting sets determine links that have a big impact
on network performance when they fail. The failure of links
in the disconnecting set could potentially result in high TD
and DV. As the cardinality of the disconnecting set increases,
the criticality of the links in the set decrease. For example,
a single link connecting two sub-topologies is more critical
than two links connecting two sub-topologies. The bottom
graph of Figure 9 shows the number of disconnecting sets with
different cardinalities for various topologies. The top graph in
the same ﬁgure shows the network goodness factor for various
topologies. Even though topology-4 and topology-8 have no
disconnecting sets, we see that they do not result in the best
network goodness factor values. Hence, similar to the other
traditional metrics, disconnecting sets are not good metrics to
capture service availability of a network.

The top graph in Figure 9 shows that even though the
topologies are similar, the end-to-end performance based on
service availability is signiﬁcantly different. ISPs can take ad-
vantage of this network differentiation to design their networks
to provide high service availability to customers. It is also
helpful in estimating the cost of compensating the customers
for SLA violations of the network.

This section explores the various applications of the pro-
posed goodness factors, and present our initial numerical
results using topologies in Set B as case studies.

A. Goodness Factors from Ingress-node and Link perspectives

Given an ISP network, the top graph in Figure 10 shows
the performance that a customer can expect when connected
to different ingress nodes. This helps a customer to choose an
ideal location to get connected to the network. It also helps
the ISP to ensure that it can meet the SLA speciﬁcations for
customers at a given source node.

The bottom graph in Figure 10 shows badness factors of
the links (inverse of goodness, Eq. 9) for one speciﬁc network
topology (Topology-6). The graph shows that
there are a
handful of critical links (e.g., Link-4 and Link-14) that have a
big impact on network performance when they fail. The rest of
the link failures only result in minor service degradation. The
ability of the badness factors to clearly distinguish the critical
links in the network is extremely useful for capacity planning
and trafﬁc engineering purposes of an ISP. For example, this
information allows the ISP to make an informed decision about
bringing down a link for maintenance to minimize the impact
on network performance. ISPs can also re-negotiate peering
relationships to divert trafﬁc away from critical links. With a
better picture of how the failure of individual links can impact
performance, the ISP can better estimate whether certain SLAs
can be met.

B. Network-wide Goodness Factors

There are numerous solutions to the above deﬁnition of
D, but we consider only those sets for which the cardinality

Our results from the previous sections have established the
importance of characterizing topologies based on meaningful

0-7803-8277-3/04/$20.00 ©2004 IEEE.

239

r
o

t
c
a
F
 
s
s
e
n
d
o
o
G
 
k
r
o
w
e
N

t

1

0.8

0.6

0.4

0.2

0

s
t

e
s
 

g
n

i
t
c
e
n
n
o
c
s
d

i

 
f

o

 
r
e
b
m
u
N

5

4

3

2

1

0

1

0.8

0.6

0.4

0.2

 
s
s
e
r
g
n

I
 

m
o
r
f
 
s
s
e
n
d
o
o
G

e
v
i
t
c
e
p
s
r
e
p

 

e
d
o
n

 

0

0

1

0.8

0.6

0.4

0.2

r
o
t
c
a
F
 
s
s
e
n
d
a
B

0

0

1

2

3

4

7

8

9

10

6
5
Topology

Cardinality = 1
Cardinality = 2
Cardinality = 3

1

2

3

4

7

8

9

10

6
5
Topology

Fig. 9.
graph shows the disconnecting sets for different topologies

Top graph shows the network goodness factors while the bottom

2

4

6

8

10

12

14

16

18

20

Ingress node

same topology can behave quite differently when different link
weights are used. As part of our future work, we will explore
how goodness factors can be used as an optimization metric for
selecting link weights to provide the best service availability.
BGP peering points: As discussed in Section IV-C, the loca-
tion of BGP peering points determine the preﬁx distribution
size at different nodes, and hence also inﬂuences service
availability. In Figure 12, we compare network goodness for
topologies with three ways of distributing BGP preﬁxes across
different nodes: (i) equal distribution, (ii) extreme distribution
where all the BGP preﬁxes are located at one exit point,
and (iii) typical preﬁx distribution observed in a tier-1 ISP.
Depending on the network topology, different locations of
BGP peering points result in different network goodness in
terms of overall service availability. This illustration shows
that goodness factors can be a useful metric in determining
“ideal” BGP peering points that result in the most desired
network performance.
Network upgrade: To cope with customer demands and meet
SLAs, an ISP may have to schedule network upgrade to intro-
duce a new node or link into its network. Deciding where in
the existing network to connect this new node/link to, becomes
a design challenge. In this case study, we consider adding a
new node with 3 links into ISP-A network (Figure 2). Figure
13 shows ﬁve possible solutions and compares the resulting
network-wide goodness of the new network. Solution-2 clearly
shows the best network goodness in terms of the offered
service availability in the presence of failures.

r
o

t
c
a
F
 
s
s
e
n
d
o
o
G

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

5

10

15

30

35

40

45

25
20
Link Number

Fig. 10.
Top graph shows the goodness factors from the perspective of
various ingress nodes in topology-6 while the bottom graph shows the link
badness factor for various links in topology-6

performance metrics such as service availability. The proposed
network-wide goodness aims to capture the impact of routing
dynamics on service degradation measured in terms of TD and
DV across all source-destination pairs. This goodness metric
forms the basis for optimizing network design decisions. We
explore three such applications in the following discussion.
IGP Link Weight Assignment: IGP link weights determine
the routing trees and hence can signiﬁcantly inﬂuence the
service availability of a network. We explore how good-
ness factors can be used to “evaluate” different link weight
assignment schemes in Figure 11. We consider the same
ten topologies in Set B but with three different link weight
assignments: (i) weights proportional to link delays, (ii) equal
weights, and (iii) random weights. Figure 11 shows that the

0-7803-8277-3/04/$20.00 ©2004 IEEE.

240

Delay
Equal
Random

1

2

3

4

7

8

9

10

5
6
Topology

Fig. 11.
assignment scheme

Network Goodness for topologies with different

link weight

VI. CONCLUSIONS AND FUTURE DIRECTIONS

In this paper, we examined the importance of incorporating
network dynamics in characterizing topologies. Our simula-
tions show that traditional metrics like out-degree, network
diameter and disconnecting sets that disregard network dynam-
ics do not effectively capture the performance of a network.

REFERENCES

[1] A. Markopoulou, G. Iannaccone, S. Bhattacharrya, C. N. Chuah, and
C. Diot, “Characterization of Failures in an IP Backbone Network,” in
Proceedings of IEEE Infocom, 2004.

[2] C. Boutermans, G. Iannaccone, and C. Diot, “Impact of Link Failures

on VoIP Performance,” in Proceedings of NOSSDAV, May 2002.

[3] A. Sridharan, S. Moon, and C. Diot, “On the Causes of Routing Loops,”
in Proceedings of ACM Sigcomm Internet Measurement Conference, Oct.
2003.

[4] C. Filsﬁls, “Deploying Tight-SLA services on an IP Backbone,” June

2002. [Online]. Available: http://www.nanog.org/mtg-0206/ppt/ﬁlsﬁls

[5] C. Fraleigh, S. Moon, B. Lyles, C. Cotton, M. Khan, D. Moll, R. Rockell,
T. Seely, and C. Diot, “Packet-Level Trafﬁc Measurements from the
Sprint IP Backbone,” IEEE Network, Nov. 2003.

[6] G. Iannaccone, C. Chuah, R. Mortier, S. Bhattacharyya, and C. Diot,
“Analysis of Link Failures in an IP Backbone,” in Proceedings of ACM
Sigcomm Internet Measurement Workshop, Nov. 2002.

[7] P. Radoslavov, H. Tangmunarunkit, H. Yu, R. Govindan, S. Shenker, and
D. Estrin, “On Characterizing Network Topologies and Analyzing Their
Impact on Protocol Design, Tech. Rep. USC-CS-TR-00-731, Mar. 2000.
[8] C. Faloutsos, P. Faloutsos, and M. Faloutsos, “On Power-Law Relation-
ships of the Internet Topology,” in Proceedings of ACM Sigcomm, Sept.
1999.

[9] R. Govindan and H. Tangmunarunkit, “Heuristics for Internet Map

Discovery,” in Proceedings of IEEE Infocom, 2000.

[10] K. Calvert, M. Doar, and E. W. Zegura, “Modelling Internet Topology,”

in IEEE Communications Magazine, 1997.

[11] A. Medina, A. Lakhina, I. Matta, and J. Byers, “BRITE: An Approach
to Universal Topology Generation,” in Proceedings of the International
Workshop on Modeling, Analysis and Simulation of Computer and
Telecommunications Systems (MASCOTS), Aug. 2001.

[12] D. Oran, “OSI IS-IS Intra-domain Routing Protocol,” RFC 1142, Feb.

1990.

[13] J. Moy, “OSPF Version 2,” RFC 2328, Apr. 1998.
[14] G. Iannaccone, C.-N. Chuah, S. Bhattacharyya, and C. Diot, “Feasibility

of IP Restoration in a Tier-1 Backbone,” IEEE Network, Mar. 2004.

[15] B. Waxman, “Routing of Multipoint Connections,” IEEE Journal on
Selected Areas in Communication, vol. 6, no. 9, pp. 1617–1622, Dec.
1988.

[16] R. Ahuja, T. Magnanti, and J. Orlin, Network Flows : Theory, Algorithms

and Applications. Prentice Hall, 1993.

Equal Dist
Extreme Dist
Unequal Dist

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

r
o

t
c
a
F
 
s
s
e
n
d
o
o
G

r
o

t
c
a
F
 
s
s
e
n
d
o
o
G
 
k
r
o
w
e
N

t

1

2

3

4

7

8

9

10

6
5
Topology

Fig. 12. Network Goodness for topologies with different preﬁx distribution
schemes

1

2

4

5

3

Topology

Fig. 13. Adding a new node into the network - Network Goodness for various
solutions

Hence it calls for a new approach to characterize topologies.
To ﬁll this void, we proposed a novel methodology based
on the concept of service availability and demonstrated its
effectiveness using simulations.

To the best of our knowledge, this is the ﬁrst work to
consider network dynamics in characterizing topologies. The
approach is the ﬁrst step in the right direction and is appealing
to both ISPs and customers alike. We have identiﬁed numerous
applications for goodness factors in capacity planning and net-
work design, but their detailed analysis is a part of our future
work. In addition, much work remains in extending this work
to incorporate realistic failure patterns that includes multiple
simultaneous link failures. As a complement to simulations we
plan to validate our results through experiments on a test-bed
or measurements in real world networks.

0-7803-8277-3/04/$20.00 ©2004 IEEE.

241

