Fast Compiler Optimisation Evaluation Using
Code-Feature Based Performance Prediction

Christophe Dubach, John Cavazos,

Björn Franke, Michael O’Boyle

Member of HiPEAC

Institute for Computing Systems Architecture,

University of Edinburgh, UK

Grigori Fursin and Olivier Temam

INRIA Futurs and LRI, Paris-Sud University,

Member of HiPEAC
ALCHEMY Group

France

ABSTRACT
Performance tuning is an important and time consuming task which
may have to be repeated for each new application and platform.
Although iterative optimisation can automate this process, it still
requires many executions of different versions of the program. As
execution time is frequently the limiting factor in the number of
versions or transformed programs that can be considered, what is
needed is a mechanism that can automatically predict the perfor-
mance of a modiﬁed program without actually having to run it.
This paper presents a new machine learning based technique to au-
tomatically predict the speedup of a modiﬁed program using a per-
formance model based on the code features of the tuned programs.
Unlike previous approaches it does not require any prior learning
over a benchmark suite. Furthermore, it can be used to predict the
performance of any tuning and is not restricted to a prior seen trans-
formation space. We show that it can deliver predictions with a high
correlation coefﬁcient and can be used to dramatically reduce the
cost of search.

Categories and Subject Descriptors
D.3 [Software]: Programming languages; D.3.4 [Programming
languages]: Processors—Compilers, Optimization ; I.2.6 [Artiﬁ-
cial intelligence]: Learning—Induction

General Terms
Performance, Experimentation, Languages

Keywords
Performance Modelling, Compiler optimisation, Architecture, Ma-
chine learning, Artiﬁcial Neural Networks

1.

INTRODUCTION

Tuning applications to improve performance is an important but
tedious and time consuming task. For performance critical appli-
cations such as those found in embedded devices, it has to be per-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CF’07, May 7–9, 2007, Ischia, Italy.
Copyright 2007 ACM 978-1-59593-683-7/07/0005 ...$5.00.

formed for each new application and each new platform. The pro-
grammer has to, ﬁrst of all, make a potentially beneﬁcial program
modiﬁcation, then compile the transformed program, before ﬁnally
executing this new program and recording its execution time. This
modify-compile-execute cycle must be repeatedly performed until
a sufﬁcient performance gain is achieved or the programmer has
run out out time.

There has been much work in the area of iterative optimisation
aimed at automating this process [7, 9, 10, 11, 16, 19, 22]. Such ap-
proaches focus on choosing good program modiﬁcations or trans-
formations so that the number of modify-compile-execute cycles
is reduced. Although it is possible to ﬁnd good performance im-
provement automatically, iterative optimisation still requires many
executions of different versions of the program. As execution time
is frequently the limiting factor in the number of versions or trans-
formed programs that can be considered, what is needed is a mech-
anism that can automatically predict the performance of a modiﬁed
program without actually having to run it. Ideally, such a predictor
would be independent of platform, program and most importantly
not be restricted to certain classes of program modiﬁcations. Such
a scheme would allow many different versions to be rapidly eval-
uated, dramatically reducing the time to produce a tuned applica-
tion. Alternatively, for the same amount of time, many more pro-
gram versions could be considered and increased in performance
achieved.

This paper presents a new technique to automatically predict the
speedup of a modiﬁed program using a performance model based
on the code features of the tuned programs. To build such a model,
we ﬁrst randomly transform the program to be tuned and run it
on the target platform a number of times. The code features of the
modiﬁed programs plus their execution time are then used for train-
ing a machine learning based model. This learnt model is then able
to predict the speedup of any new modiﬁed version of the program
without executing it. Unlike previous work [4] it does not need to
run an extensive training suite of benchmarks to build a predictor,
it only needs a few runs of the program to be tuned. Nor is it lim-
ited to a ﬁxed pre-examined transformation space as is the case of
the reactions-based model [4]. Instead, because it uses features of
the code, it can predict the performance of any modiﬁcation of the
program.

In this paper we show that we are able to effectively predict the
performance of a large number of tuned programs with few sam-
ples. Using just 16 samples we can predict the performance of
88000 modiﬁcations with a correlation coefﬁcient of 0.65. With
128 samples, this rises to 0.8 - a highly accurate predictor. Further-
more, we show that this technique can be used to guide an iterative
optimisation process to help selecting those transformations likely
to give good performance four times faster than random search.

combinations of the 13 different program transformations, listed in
Table 3. We restricted our attention to sequences of these transfor-
mations of up to length 5 giving 88000 different program versions
(see Section 5 for further details).

We applied each of these transformation sequences to the UTDSP
[17] compress program and ran the corresponding transformed
programs on an embedded platform, the Texas Instrument C6713
clustered VLIW processor. The results of applying each of the
transformation sequence is shown in Figure 1 where the curve la-
belled Real speedups is the actual speedup achieved when var-
ious transformations or tunings are applied to compress. The y-axis
is the speedup obtained after applying a transformation sequence,
and the x-axis is simply the transformation sequences sorted by
increasing actual speedup. Most versions of the program give a
speedup with the best achieving 1.66.

We want to build a predictor that can predict this behaviour based
on a small number of program executions or samples. In this ex-
ample, we randomly generated 64 transformed programs and exe-
cuted them in order to build a predictor that was able to predict the
remaining points in the space. The performance prediction of our
scheme is shown by the line labelled Features predictor.
As it is visually apparent, our model is able to fairly accurately
predict the performance of this program when applied to different
transformation sequences. It requires runs of just 64 randomly gen-
erated transformation sequences of the program to learn a model.
This represents less than 0.1% of the whole space. This model can
then predict the performance of the remaining 88000 − 64 trans-
formation sequences applied to that program. At ﬁrst, it may be
surprising that such a small training set size is sufﬁcient to cap-
ture such a huge space. However, transformed programs have large
areas of similar behaviour which can be captured in a few trials.
Furthermore, as we use code features, the model automatically de-
termines which “performance region” a transformed code belongs
to.

2.1 Mean predictor and mean absolute error
Although our scheme appears to perform well, it is important to
have a fair comparison with a default scheme in order to evaluate
the predictions. The simplest naive scheme is one which always
predicts the same speedup for all transformations based on the av-
erage value of any transformed program. If we perform such an
experiment frequently enough, such a naive predictor will always
predict the mean value of the space and thus is called the mean
predictor. Its predictions are shown with the line labelled Mean
predictor on Figure 1.

The role of the mean predictor becomes apparent once we con-
sider metrics to quantify the quality of our predictions. A com-
monly accepted metric is the mean absolute error deﬁned as :

PN

mae =

i=1 |predicted valuei − real valuei|

,

N

where N represents the total number of observed values. In Fig-
ure 2(a) the mae of our predictor is compared with that of the mean
predictor and plotted as a function of the number of runs used to
build the model. While our model improves with the number of
runs, both the predictors exhibit similar mae. However, we know
from Figure 1 that the mean predictor is poor at performance pre-
diction and does not distinguish between different versions of the
program.

In fact, the mae of the mean predictor gives information about
the variance of the space. Formally, the variance of the space is
equal to the mean squared error of the mean predictor, which is
strongly related to its mae.

Figure 1: Optimisation space of the compress program. The
x-axis represents the 88000 transformed versions of the pro-
gram sorted in order of increasing speedup. The y-axis de-
notes the speedup value of the corresponding transformed pro-
gram over the original program. The curve labelled Real
speedup denoted the actual performance while the curve la-
belled Features predictor is the predicted value of our
feature-based scheme using 64 evaluations of randomly trans-
formed programs.

While the focus of this paper is in using prediction to avoid exe-
cution on a new platform, it has much wider potential application.
The ability to automatically predict program performance is par-
ticularly worthwhile in the early stages of processor design. Typi-
cally, simulators are used as they allow easy exploration of different
conﬁgurations at the cost of drastically increased execution time.
The overhead of simulation makes program tuning prohibitively
expensive until the actual hardware is available. If we were able
to build an accurate performance predictor, it would overcome the
cost of simulation allowing programs and processor architecture
co-design.

The paper is structured as follows: a motivation example is ﬁrst
provided in Section 2 illustrating the beneﬁts of using a perfor-
mance predictor.
It shows that a simple error metric is a poor
measure for performance prediction and shows how the correla-
tion coefﬁcient is a better ﬁt. Section 3 brieﬂy describes the vari-
ous models and predictors evaluated in this paper. This is followed
by Section 4 which describes in some detail, the program features
that are used to build our models. Section 5 describes the exper-
imental setting used in Section 6 to evaluate the different models
and predictors. As one of the main beneﬁts of this approach is it
ability to predict the performance on unseen transformations, Sec-
tion 7 shows how the predictor can be used to select good versions
of a program in unseen transformation spaces. This is followed by
a brief review of related work in Section 8 and ﬁnally, Section 9
concludes this paper.

2. MOTIVATION

This section illustrates how our predictor can be used to estimate
the speedup of different versions of a program and describes ways
in which its accuracy may be evaluated.

We want to compare the predicted performance of different ver-
sions of a program against their actual values. In order to give a re-
alistic evaluation, rather than evaluating a few hand-tuned versions,
we automatically generated many different program versions using

 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7speedupsequencescompressReal speedupsFeatures predictorMean predictor(a) Mean absolute error.

(b) Correlation coefﬁcient.

Figure 2: Evaluation of the performance of the predictors for
the compress program. The sample size is a logarithmic scale
(21, . . . , 29) and represents the number of executions of dif-
ferent versions of the compress program, or training data,
needed to train each model. Both models have been trained
50 times using different random training samples. The values
shown are thus averaged.

2.2 Correlation coefﬁcient

Although mae gives some insight into the accuracy of a predic-
tor, it is not good at distinguishing between good and poor ver-
sions of a program. To evaluate the quality of the predictor, we
therefore chose to use the correlation coefﬁcient. This metric, ex-
plained in Section 6.1, takes a value between 0 and 1 (neglecting
the sign). The closer to 1 this value is, the better the predictor is.
Figure 2(b) shows the correlation coefﬁcient for our feature-based
predictor and for the mean predictor. The mean predictor has a cor-
relation of 0, meaning that it is unable to predict the shape of the
curve, while our predictor is able to improve its accuracy as the
number of training samples is increased. The scale is logarithmic,
so point 6 represents 26=64 sample runs with a correlation coefﬁ-
cient of 0.85 - a very accurate predictor. The detailed performance
of this particular version of the predictor has already been shown
in Figure 1. The correlation coefﬁcient is therefore a good met-
ric since it allows us to quantify in a single number how well the
predicted values follow the real speedups.

Figure 3: Searching the large space of the compress program.
The x-axis represents the number of evaluations or executions
of transformed versions of the program. The y-axis shows the
amount of performance improvement achieved. Pred 8 is a
search using a features-based model which has 8 prior training
evaluations. Random denotes a simple random search.

2.3 Using the predictor to ﬁnd good points

Another way of evaluating the quality of our predictor is to use
it to search for good optimisation points in a previously unseen
transformation space. In other words, how well does it predict the
performance of programs transformed in a manner it has never seen
before, and how can that be used to guide the search for good opti-
misations.

Figure 3 shows how random search and our predictor perform
when searching a new optimisation space. This new space con-
tains transformation sequences of length 20 selected from up to 54
transformations, which we refer to as the large space (see Table 4
for further details). The random search is performed by randomly
selecting a transformation sequence in the space and by running the
resulting program. This is repeated 100 times and the best transfor-
mation sequence found so far is kept.

The curve labelled pred 8 shows the performance of our pre-
dictor built using just 8 samples from the small 88000 element
space shown in Figure 1. It is used as follows: initially 500 ran-
dom points are selected but not executed from the new space (1034
different sequences; described in Section 7) and their code features
extracted. We refer to this space as the large space. The predic-
tor then ranks those samples based on their predicted performance.
Figure 1 shows that the predictor is not accurate in determining the
absolute best performing option, but is good at identifying good
candidates. So the prediction based search pred 8 starts execut-
ing the version with the best predicted execution time, then the sec-
ond best, and so on, up to 100 times.

It is interesting to notice that using this scheme with the mean
predictor results in random search; the mean predictor assumes all
versions are equally good (same prediction) and hence randomly
selects. As it can be seen in Figure 3, our features-based predictor
dramatically outperforms random search.

The predictor used to search the new space has been trained us-
ing only 8 samples from the small and different space, but it is still
useful to search a space of transformation sequences never seen be-
fore unlike previous approaches [4]. This example illustrates one
way in which the model can be used for performance tuning.

 0 0.05 0.1 0.15 0.2 0.25 1 2 3 4 5 6 7 8 9maesample size (power of 2)features predictormean predictor 0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 6 7 8 9correlationsample size (power of 2)features predictormean predictor 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 0 10 20 30 40 50 60 70 80 90 100Speedupevaluationspred 8randomArtiﬁcial neural network model.

The second model is our preferred model and is an ANN (Ar-
Its main advantage over the linear
tiﬁcial Neural Network [3]).
regressor is that it can model a non-linear space. Our ANN is a
multi-layer perceptron which has one hidden layer that contains 1
to 5 hidden neurons (weights). The standard back-propagation al-
gorithm is used to train the ANN.

3.2 Non-feature-based alternative predictors
We compare our model against the mean predictor, the sequence
encoding-based predictor and the reactions-based approach, neither
of which use code features.

Mean predictor.

Introduced in Section 2, the mean predictor acts as our baseline
predictor. It does not use any features at all and always predicts
the mean of the points that it has sampled. This value, computed
from the training set, tends towards the mean of the space when
enough samples are taken from the space. The mean of the space is
the constant value that minimises the mean absolute error mae and,
therefore, makes a good base case predictor.

Sequence encoding-based predictor.

This approach, similar to our code feature-based predictor uses a
vector of bits that encodes the sequence of transformations instead
of program features as an input to the model. The chosen encoding
does not take into account the ordering of the transformations, but
simply the presence of a given transformation in the sequence.

Reaction-based predictor.

The last predictor considered is the reaction-based scheme de-
scribed in [4]. Like the previous predictor, it directly uses the se-
quence of transformations applied to a program as an input to a
learnt model [4]. In addition to this, it uses a signature of the pro-
gram as an input. This signature, referred as reactions, corresponds
to the speedups obtained when some predeﬁned sequences of trans-
formations have been applied to the program. This characterises the
behaviour of the program. The major difference with all the other
predictors presented in this paper is that it requires extensive prior
training on a benchmark suite. In effect, it builds a model correlat-
ing transformation sequences with speedup. It has been shown to
work well and is straightforward to use.

4. SOURCE CODE FEATURES

The main distinctive characteristic of our methodology is the use
of program features as a mean of building accurate performance
models. This section describes how the essential program charac-
teristics or features are extracted from a transformed program in
order to build the features-based models. Since these features are
extracted at the source level, they are platform independent, unlike
other approaches. Having the right features is critical for perfor-
mance accuracy. The following sections describe and provide jus-
tiﬁcation for our selection.

4.1 Description of the features

The feature categories that summarise and characterise a pro-
gram are derived from high-level information describing the oper-
ations within a program. Those operations can be classiﬁed into
three categories, as shown in Table 1, which roughly correspond to
computation, address and control-ﬂow operations. Since our fea-
tures are extracted using SUIF [12], the operations considered are
simply based on the intermediate representation used within SUIF.

Figure 4: Training and using the features-based predictor. Dur-
ing the training phase, a set of transformation sequences is ap-
plied to the original C program. Then pairs of features/speedup
values are gathered; the features are extracted and the trans-
formed programs are executed. Those pairs of values compose
the training set and are used to train the model using an itera-
tive process. Once the model is trained, it can be used to predict
the speedup of a new transformation; features are extracted
from the new transformed program and used as an input to the
model.

3. LEARNING A PREDICTIVE MODEL

In this section we describe our approach to building an accurate
predictor using a small number of runs of the transformed program
and their associated code features. Other approaches are also con-
sidered and later used for comparative evaluation in Section 6.

3.1 Features-based predictor

Figure 4 shows how the features based predictor is trained and
used. During training, a set of randomly selected transformation
sequences (T1, . . . , TN ) is applied to the target C program. From
the resulting transformed programs, the features are extracted and
the speedup values are collected after running those programs. This
creates a set of pairs < codef eatures, speedup > that are used to
train the model.

Once the model has been trained, any new transformation se-
quence (Tnew) can be applied to the program. The features of the
new program are then extracted and fed into the model which pro-
vides a prediction of its speedup. Unlike the reaction based ap-
proach techniques [4], we do not need to train on a prior bench-
mark suite, instead we simply use code features of the transformed
program to predict performance. The type of code features used, is
described in detail in Section 4.

The above description gives a summary of the overall approach.
However, given the < codef eatures, speedup > pairs, there are
a number of modelling approaches that can be considered for build-
ing a predictor. We consider two of the simplest schemes.

Linear model.

Linear regression is one of the basic models that is often uses
It makes prediction by computing a weighted sum
in statistic.
of the input variables. The weights are determined by minimis-
ing the mean squared error from the training set. The advantage
of this method resides in it’s simplicity; the problem of ﬁnding
the optimal weights is simply reduced to a few matrix operations
and once the optimal weights (ω1, . . . , ωN ) are computed, the pre-
diction can be made with a simple weighted sum of the inputs
(f eat1, . . . , f eatN ) : speedup =
i=1 ωi · f eati. Its perfor-
mance is evaluated in Section 6.

PN

computation operations
Load a constant value
Conversion between ﬂoat/int
Load from memory
Store to memory
Multi-way branch
Comparison using int/ﬂoat
Unconditional/Conditional jump
Copy
Shift
Rotation
Arithmetic operation on int/ﬂoat
Multiplication on int/ﬂoat
Division on int/ﬂoat
Logical operation
Function call
Array accesses on int/ﬂoat
Memory operation
Array reference
Return from function
address computation operations
load a constant value
conversion between ﬂoat/int
...
control-ﬂow operations

Label
LDC
CVT
LOD
STR
MBR
CMPI/CMPF
UJMP/CJMP
CPY
SFT
ROT
ARII/ARIF
MULI/MULF
DIVI/DIVF
LOG
CAL
ARYI/ARYF
MEM
AREF
RET
Label
ARYLDC
ARYCVT
ARY...
Label
LOOP_BODY While loop body
LOOP_TEST
FOR
IF

While loop test
For loop
If-then-else statement

Table 1: The 3 categories of operations considered for features
extraction; computation operations, memory access computa-
tions and control-ﬂow operations.

Figure 5: The four class of features that represent the factors
that inﬂuence performance.

Classes of features.

We use four separate classes of features based on the three dif-
ferent categories of high-level operations to characterise a program
as shown in Figure 5. These features are considered to be good
predictors of a program’s performance. The feature class that char-
acterises code size is determined by simply counting the number of
occurrence of each operation in the program. The second class, is
derived by counting the number of operations executed using once-
only proﬁling information of the original non-transformed program
(see the next section for further detail). To characterise the level of
parallelism available (the third feature class) we assume an ideal
machine that can execute each operation in one cycle with unlim-
ited resources. Given the proﬁling information, the total number
of cycles required to execute the program is quickly approximated.
Finally, we estimate the number of distinct memory accesses stat-
ically from the source code. These 4 feature classes are easy to
determine and provide a signature of the program’s behaviour.

Relative features.

As we are interested in speedup relative to the original program,

Figure 6: Proﬁling information extraction.

we need to record the difference between the features of the base-
line and transformed program. Therefore the feature vector is ex-
tracted by taking the relative difference between the features of the
baseline and the transformed program. the elements of the feature
vector are thus normalised. Elements that contain null values mean
that the corresponding features of the original code and the trans-
formed code are the same.

4.2 Extracting execution frequency

Certain features cannot be determined statically. For instance,
the number of iterations of a loop might be unknown hence pro-
ﬁling information is used to determine the execution frequency of
each basic block. The extraction of this information is summarised
in Figure 6.

Counters are inserted into the original C source code for each
dynamic control-ﬂow structure and proﬁle information is collected
when the original program is ﬁrst run; it has negligible overhead.
This is in fact very similar to the micro-proﬁler developed in [15].
The original program is then annotated with this information, so
that the information is available to any subsequent transformations.
When a transformation is applied, the proﬁling information is up-
dated deterministically. This way we are able to extract accurate
features from any subsequent modiﬁed version, without affecting
program behaviour.

4.3 Reduction of dimensionality

The total number of features extracted per C program is 118,
which means that our model should have 118 inputs. Unfortunately,
in an ANN, every input corresponds to a neuron. Since the number
of free parameters increase with the number of neurons, we need to
keep this number small.

The ﬁrst step consists in removing redundant features. For in-
stance ﬂoating point operations can be dropped if the program does
not perform any ﬂoating point operation. This ﬁltering is done au-
tomatically when the model is trained, leaving on average 35 fea-
tures. The resulting features will thus be speciﬁc for each program
(but stay the same accross the different versions of the same pro-
gram).

To further reduce the number of inputs, we apply a well known
technique called PCA (Principal Components Analysis). PCA [3]
is a linear transformation that transforms the data into a new coor-
dinate system such that the greatest variance by any projection of
the data comes to lie on the ﬁrst coordinate (called ﬁrst principal
component), the second greatest on the second coordinate, and so
on. In our setup, we keep only the main components that account
for 95% of the total variance. In our case, the number of inputs is
typically reduced to 5 using this technique.

Table 2: Programs used in our experiment and the correspond-
ing maximum speedup available in the small space.

Program
adpcm
compress
edge detect
fft
ﬁr
histogram
iir
lmsﬁr
lpc
spectral estimation

maximum speedup
1.31643
1.64141
1.29729
1.82053
1.84127
1.00001
2.04555
1.00396
1.12012
1.09078

Transformation
Loop unrolling (factor 1,2,3,4)
FOR loop normalisation
Non-perfectly nested loop conversion
Break load constant instructions
Common subexpression elimination
Dead code elimination
Hoisting of loop invariants
IF hoisting
Move loop-invariant conditionals
Copy propagation

Table 3: The 13 transformations used to generate the 88000
versions of each program.

5. EXPERIMENTAL SETUP

This section provides a brief description of the programs, trans-

formations and platforms used in our evaluation.

Benchmarks.

The UTDSP [17, 20] benchmark suite contains small, but com-
pute-intensive DSP kernels as well as larger applications composed
of more complex algorithms. The size of programs ranges from 20
to 500 lines of code. The programs used, listed in Table 2, repre-
sent widely used compute-intensive kernels from embedded appli-
cations.

Despite the fact that those programs are relatively small compare
to other benchmark suites, our approach can still be used on bigger
programs. Bigger programs can be optimised locally, for instance
on a per-function basis, and a predictor built for each individual
function.

Transformations.

We consider source-to-source transformations available in the re-
structuring compiler SUIF 1 [12]. We have selected the transforma-
tions described in Table 3. As we (arbitrarily) consider four loop
unroll factors, this increases the number of transformations con-
sidered to 13. We then exhaustively evaluated all transformations
sequences of length 5 selected from these 13 options. There are
154440 possible transformation sequences since no transformation
can appear twice in the sequence. However, since unrolling can
only appear once in any sequence (only one possible unroll factor),
it decreases the total number of possible sequences we evaluated to
88000 per benchmark.

Platform.

The Texas Instrument C6713 is a high-end ﬂoating point DSP,
running at 300MHz. This wide-clustered VLIW processor contains
256KB of internal memory. The programs were compiled using
the TI’s Code Composer Studio Tools Version 2.21 compiler with
the highest -O3 optimisation level and -ml3 ﬂag (generates large
memory model code).

Figure 7: The correlation coefﬁcient of the Features ANN pre-
dictor with 5 hidden neurons. Correlation is plotted as a func-
tion of the training set size (logarithmic scale) on a per program
basis. The mean predictor has constant 0 correlation.

Statistical signiﬁcance.

Training involves randomly selecting samples from the 88000
possible transformation sequences.
In order to get a statistically
signiﬁcant behaviour, we repeat this sampling 50 times. Thus, for
each sample size, we show the average result over the 50 trials and,
where appropriate, record the standard deviation.
In addition to
the training set, we need one execution of the baseline program to
compute the relative speedups of subsequent transformations.

6. EXPERIMENTAL RESULTS

In this section we compare the quality of our scheme using the
different models proposed in Section 3.1 and against the different
regressors described in Section 3.2.

6.1 Correlation coefﬁcient

As shown in the motivation section, mae, though an important
metric, is not a good measure at predicting the right shape or trend
of the space. As we want to use the predictor to discriminate be-
tween good and bad transformed programs, we need a metric that
captures the modelling accuracy of the shape of the space.

σX ·σY

To analyse the quality of our models, we therefore use the corre-
lation coefﬁcient. The correlation between two variables is deﬁned
as ρX,Y = cov(X,Y )
, where σX and σY represent the standard
deviation of variable X and Y respectively, and cov(X, Y ) is the
covariance of variable X and Y . The correlation coefﬁcient only
takes values between -1 and 1. The larger this value is, the stronger
the relation between the two variables is (ignoring the sign). At the
extreme, a correlation of 1 means that both variables are perfectly
positively correlated; one variable can be expressed as the product
of the other one (linear relation). A correlation of 0 means that
there is no linear relationship between the two variables.

Figure 7 shows how this coefﬁcient varies with the number of
runs used to train the model for each program. The model used here
is the features-based model using 5 hidden neurons, since it leads
to the best average performance as we will see in the next section.
Each line corresponds to a particular program and each point on
that line corresponds to the correlation coefﬁcient for a given train-
ing set size. The x-axis is a logarithmic scale and given sufﬁcient
(29 = 512) training data, our predictor performs extremely well in
all cases except adpcm. Even with smaller training sets (26 = 64)

-0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 4 5 6 7 8 9sample size (power of 2)Coefficient of correlationMean predictoradpcmcompressedge detectfftfirhistogramiirlmsfirlpcspectral est.(a) Correlation coefﬁcient.

(b) Standard deviation of the correlation coefﬁcient.

Figure 8: Correlation coefﬁcient and its standard deviation for
the different features-based models, averaged across all bench-
marks (50 runs per program), as a function of the training set
size (logarithmic scale).

our scheme still performs well, with an average correlation coefﬁ-
cient of .75. As expected the mean predictor performs badly across
all benchmarks and is unsuitable as a means of distinguishing be-
tween fast and slow versions of the program.

6.2 Comparison of models

The previous section shows that our scheme performs well re-
quiring only a few program runs plus the associated program fea-
tures to accurately predict the performance of a large number of
program variants. In this section we evaluate the different models
proposed in Section 3.1.

Figure 8(a) shows the correlation coefﬁcient for each of the dif-
ferent models. Each line represents the performance of a particular
model averaged across the entire benchmark suite. As before, each
point describes the correlation coefﬁcient for a particular training
set size. The Features ANN based approaches outperform linear re-
gression for small sample sizes, while linear regression improves in
performance when it has greater than 25 = 32 training runs. How-
ever, the best feature based scheme across the entire training set
size is the one using the ANN with 5 hidden neurons. With enough
samples in the training set, the correlation coefﬁcient is greater than
0.8 showing that this model is working extremely well.

Figure 9: Comparison of the different predictor averaged
across all benchmarks. The correlation coefﬁcient as a func-
tion of the training set size (logarithmic scale).

Given that each of these models is based on random samples
from the transformation space (to build the training set), it is useful
to know the standard deviation of each of the predictors. Clearly
having a volatile predictor is not very useful. Figure 8(b) shows
this standard deviation for varying training sample sizes (logarith-
mic scale). All schemes show decreasing standard deviation with
increased sample size as expected. Again the ANN model that uses
5 hidden neurons outperforms all the other models with the small-
est standard deviation.

6.3 Comparison of predictors

Figure 9 shows the comparison of our features-based predic-
tor against the other predictors. As expected, the mean predic-
tor has the worst correlation coefﬁcient. The predictor based on
the encoding of the sequence as an input, needs approximatively
4 times as many samples or executions as our best scheme. The
reactions-based approach [4] Reactions ANN 5 performs sim-
ilarly (trained with 128 samples per program from the benchmark),
independently of the training size. This behaviour can be explained
by the fact that the training of the reactions-based model is done
ofﬂine. The runs required from the program of interested are used
to characterise it and not to train the model. Thus only a small
number of runs is necessary. But even with the knownledge gains
during the ofﬂine training phase on other programs, the reactions-
based regressor perform worst than our features-based approach for
more than 2 runs.

This section shows that using program features allows the con-
struction of good performance predictors. The next section evalu-
ates their use in selecting good performance improving transforma-
tions.

7. PREDICTING NEW SEQUENCES

One of the main advantages of using code features is the ability
to predict the performance of new transformation sequences and
hand-tuned code. In order to evaluate its use on combination of
unseen transformations, we randomly generated 500 different pro-
gram versions using compositions of 54 different transformations,
listed in Table 4, of up to length 20. This new space, refered as
large space in the motivation section, leads to approximately 1034
unique sequences.

-0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2 3 4 5 6 7 8 9sample size (power of 2)CorrelationFeatures ANN h5Features ANN h3Features ANN h1Features Linear 0 0.1 0.2 0.3 0.4 0.5 1 2 3 4 5 6 7 8 9sample size (power of 2)Standard deviation of the correlationFeatures ANN h5Features ANN h3Features ANN h1Features Linear-0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2 3 4 5 6 7 8 9sample size (power of 2)CorrelationFeatures ANN h5Sequence ANN h5Reactions ANN h5Mean predictorTransformation
Loop unrolling (factor 1,2,3,4)
Loop tiling (tile size 0,1,...,9)
Splitting of deep FOR loops (0,1,...,9)
Loop ﬂattening
For loop normalisation
Turn imperfectly nested loops into perfectly nested loops
Hoisting of loop invariants
Move loop-invariant conditionals
Guard FORs
Induction variable detection
Array padding (padding 0,1,...,9)
Extract array upper bounds
Improve array bound information
Reconstruct explicit array reference
Scalarise constant array references
Aggressively scalarise constant array references
Expression tree breakup (0,1,...,9)
Reassociation
Control simpliﬁcation
Forward propagation
Copy propagation
Constant propagation
Constant folding
Bounds comparison substitution
Common subexpression elimination
Replace constant variables
Reduction detection
Privatisation
Secularisation
Dead code elimination
Bit packing
If hoisting
Unstructured control ﬂow optimisation
Replace call-by-reference
Array Deliberation
Form arrays
Chain multiple array references
Dismantle TREE_FORs
Dismantle TREE_LOOPs
Dismantle TREE_BLOCKs
Dismantle array instructions
Dismantle multi-way branches
Dismantle non-constant FORs
Dismantle TREE_FORs with spilled index variable
Dismantle TREE_FORs with modiﬁed index variable
Dismantle empty TREE_FORs
Dismantle TREE_BLOCKs with empty symbol table
Lift call expressions
Eliminate struct copies
Eliminate sub-variables
Globalise local static variables
Global variable privatisation
Put in explicit load/stores for non-local variables
Eliminate enumeration types

Table 4: Transformations used in the large space.

In order to ﬁnd good tranformation sequences in the large space,
we perform a search by using our features-based predictor (ANN 5
hidden neurons). The predictor is trained with a few samples from
the small space described earlier. Using this predictor, we evaluate
how it can be used to ﬁnd good transformation sequences in the
new large space.

7.1 Searching the space

Figure 10 shows the result obtained when our features-based pre-
dictor is used to search this large space. Predictors, built from dif-
ferent amount of training data (8, 64, 512) from the small space, are
shown in this diagram and compared against random search. The
predictor is used to predict the speedups of all 500 program ver-
sions and order them based on their predicted speedup in decreas-
ing order. Then, when the search occurs, the program version with
the highest predicted speedup is executed, then the second best, and
so on. As we see on this graph, the more samples we use during the
training phase (8 vs 64 vs 512), the better the search results are. In
the case where we have a model trained on 512 prior runs, we can
achieve 70% of the available speedup in 10 runs, which takes over
40 random evaluations to achieve.

7.2 Threshold-based search

An alternative way of using the predictor to search the space
consists of randomly searching within those points predicted to be

Figure 10: Searching the large space for good sequences. The
predictor uses ANN with 5 hidden neurons and the training size
has been varied between 8 and 512 samples.

within x% of the maximum. For instance, if we chose x = 1%,
all the sequences whose prediction is within 1% of the maximum
predicted value are candidates for search. The reason for doing
this is apparent on reexamination of Figure 1 which shows several
predicted maxima. Ordering the predictions by decreasing order
and starting with the best one can cause the predictor to get stuck
in one of those local maxima.

By keeping only the points within a given percentage of the
maximum prediction, we ﬁlter away poor predictions without get-
ting stuck in local maxima. This assumes that the real maximum
speedup value of the space lies within a few percent of the max-
imum prediction; Figure 11 shows this assumption holds. Again,
three models with varying amounts of training data (8, 64, 512) are
used to search the large space. For each model, the ﬁrst approach
denoted by the line pred 100st is shown. It performs well ini-
tially but fails to provide substantial improvement later on. The
threshold-based schemes denoted by pred x% where x represents
the threshold applied, show slightly worse initial performance but
are able to sustain performance gains throughout the search.

It seems that having this threshold set to 5% leads to the best
trade off and allows substantial improvement over the ﬁrst method
after only 20 runs. Obviously the choice of the strategy depends
on the number of evaluations available. The more evaluations are
allowed, the less tight the ﬁlter needs to be. Overall, our schemes
consistently deliver a good performance level 5× faster than the
number of evaluations required by random search. It is interesting
to note that a threshold of 100% corresponds to random search.

This section shows that not only is our features-based predic-
tive model good at predicting the performance of transformed pro-
grams, but it can also be used to ﬁnd good new transformed pro-
grams when employed in a iterative search on a new transformation
space.

8. RELATED WORK

Most prior work has focused on predicting good optimisations
rather than predicting optimisation performance some of which re-
lies on program features-based characterisation of the programs.
For instance, Monsifrot et al. [18], Stephenson et al. [21] and Aga-
kov et al. [1] all use static loop nest features. Features may capture
those characteristics of the static program that are best at predicting
program transformations to apply. Cavazos et al. [5] describe using

 10 20 30 40 50 60 70 80 0 10 20 30 40 50 60 70 80 90 100% of available improvement foundevaluationspred 512pred 64pred 8random(a) 64 training samples.

(b) 8 training samples.

(c) 512 training samples.

Figure 11: Each graph shows the predictor trained with a different amount of training data. Each line corresponds to the perfor-
mance of the corresponding search technique.

supervised learning to control whether or not to apply instruction
scheduling. Monsifrot et al. [18] use a classiﬁer based on deci-
sion tree learning to determine which loops to unroll: they looked
at the performance of compiling Fortran programs from the SPEC
benchmark suite using g77 for two different architectures, an Ultra-
SPARC and an IA64. Stephenson et al. [21] use machine learning
to characterise the best unroll loop factor for a given loop nest, and
improve over the ORC compiler heuristic. All of these approaches
are successful in automatically generating compiler heuristics for
code segments rather than in predicting the eventual performance
of the selected optimisations for whole programs.

Rather than predicting the impact of a single transformation,
others have looked at searching [2, 6, 9, 10, 11, 16, 19, 22] for
the best set or sequence of optimisations for a particular program.
Cooper et al. [6] propose a number of algorithms to solve the com-
pilation phase ordering problem. Their technique searches for the
best phase order of a particular program. Such an approach gives
impressive performance improvements, but has to be performed
each time a new application is compiled. While our models also
need to be constructed for each new application, it can be used to

accurately predict the quality of unseen transformations, thus re-
quirering fewer training samples.

Kulkarni et al. [16] introduce techniques to allow exhaustive
enumeration of all distinct function instances that would be pro-
duced from the different phase-orderings of 15 optimisations. This
exhaustive enumeration allowed them to construct probabilities of
enabling/disabling interactions between the different optimisation
passes. Using these probabilities, they constructed a probabilis-
tic batch compiler that dynamically determined which optimisation
should be applied next depending on which one had the highest
probability of being enabled.

Fursin et al. [10] developed a technique to speed up program it-
erative optimisations using static multi-versioning of the most time
consuming code sections, and low-overhead run-time phase detec-
tion scheme. This technique can speed up iterative search by sev-
eral orders of magnitude and can be beneﬁcial during the training
data generation stage of our models.

Pan et al. [19] partitioned a program into tuning sections and then
developed fast techniques to ﬁnd the best combination of optimisa-
tions for each of these tuning section. They are able to reduce the

 10 20 30 40 50 60 70 80 90 0 10 20 30 40 50 60 70 80 90 100% of available improvement foundevaluationspred 100stpred 1%pred 5%pred 10%random 10 20 30 40 50 60 70 80 90 0 10 20 30 40 50 60 70 80 90 100% of available improvement foundevaluationspred 100stpred 1%pred 5%pred 10%random 10 20 30 40 50 60 70 80 90 0 10 20 30 40 50 60 70 80 90 100% of available improvement foundevaluationspred 100stpred 1%pred 5%pred 10%randomtime to ﬁnd good optimisation settings from hours to minutes. This
approach can typically be used in conjunction with our technique
in order to deal with bigger programs.

Agakov et al. [1] build models of good transformation sequences
from training data on a per program basis. This is then used to guide
iterative search on a new program. Unlike this paper, they only
attempt to predict good transformations to apply rather than pre-
dicting the performance impact of any particular transformation.
Predicting performance is a signiﬁcantly more difﬁcult problem as
it requires the precise capture of architecture behaviour.

Although there has been little work in predicting performance
of programs in an arbitrary transformation space, there has been
related work performed in architecture design space exploration.
Karkhanis et al. [14] propose an analytical model for hardware ex-
ploration that captures the key performance features of superscalar
processors. This model can potentially be used for software explo-
ration, but the construction of the model is ad hoc and a complex
process, which makes it difﬁcult to generalise and replicate. Eeck-
hout et al. [8] and the SMARTS [23, 24] framework, two indepen-
dent approaches, use both statistical simulation to similarly cap-
ture processor characteristics, and generate synthetic traces that are
later run on a simpliﬁed superscalar simulator. After any program
transformation, a new trace (requiring a full functional simulation)
needs to be generated if this approach were to be used for software
exploration. Thus, this approach is not suitable for software explo-
ration.

Recently Ipek [13] has proposed a distinct method for both con-
siderably speeding up and automating the hardware design-space
exploration process. The idea is to train an ANN to predict the im-
pact of hardware parameter variations (e.g., cache size, memory
latency, etc) on the performance behaviour of a target architecture.
After training on less than 5% of the design space, the model can
accurately predict performance variations with less than 2% error.
Though as we noted earlier this does not mean it is discriminat-
ing and should be compared against a mean predictor. Also, any
modiﬁcation of the program binary, such as applying a program
transformation, requires training a new model using several thou-
sands simulations. As a result, this approach is also not suitable
for software exploration. Our approach similarly relies on machine
learning to build a performance model, but it can accommodate any
new program transformation without retraining.

9. CONCLUSION AND FUTURE WORK

This paper has shown that it is possible to automatically derive a
performance predictor for tuning programs. By using program fea-
tures we have shown that such predictors can be constructed using
machine learning based approaches. Unlike previous approaches,
we require only a few training runs per program and no prior train-
ing on a benchmark suite. In addition, the predictor is not restricted
to previously seen transformations. By incorporating our features-
based approach with a single hidden layer ANN we show that high
level of predictive accuracy is achievable. Furthermore, we show
that such a predictor can be used to ﬁnd good transformation se-
quences in an unseen transformation space.

Future work will combine our technique with architectural per-
formance prediction allowing automatic performance prediction of
the compiler/architecture co-design space.

10. REFERENCES
[1] AGAKOV, F., BONILLA, E., CAVAZOS, J., FRANKE, B.,

FURSIN, G., O’BOYLE, M. F. P., THOMSON, J.,
TOUSSAINT, M., AND WILLIAMS, C. K. I. Using machine

learning to focus iterative optimization. In CGO ’06:
Proceedings of the International Symposium on Code
Generation and Optimization (Washington, DC, USA,
March 2006), IEEE Computer Society, pp. 295–305.

[2] ALMAGOR, L., COOPER, K. D., GROSUL, A., HARVEY,

T. J., REEVES, S. W., SUBRAMANIAN, D., TORCZON, L.,
AND WATERMAN, T. Finding effective compilation
sequences. In LCTES ’04: Proceedings of the 2004 ACM
SIGPLAN/SIGBED conference on Languages, compilers,
and tools for embedded systems (New York, NY, USA,
2004), ACM Press, pp. 231–239.

[3] BISHOP, C. Neural Networks for Pattern Recognition.

Oxford University Press, 2005.

[4] CAVAZOS, J., DUBACH, C., AGAKOV, F., BONILLA, E.,

O’BOYLE, M. F. P., FURSIN, G., AND TEMAM, O.
Automatic performance model construction for the fast
software exploration of new hardware designs. In CASES
’06: Proceedings of the 2006 international conference on
Compilers, architecture and synthesis for embedded systems
(New York, NY, USA, October 2006), ACM Press,
pp. 24–34.

[5] CAVAZOS, J., ELIOT, J., AND MOSS, B. Inducing heuristics

to decide whether to schedule. In PLDI ’04: Proceedings of
the ACM SIGPLAN 2004 conference on Programming
language design and implementation (New York, NY, USA,
2004), ACM Press, pp. 183–194.

[6] COOPER, K., GROSUL, A., HARVEY, T., REEVES, S.,

SUBRAMANIAN, D., TORCZON, L., AND WATERMAN, T.
Searching for compilation sequences. Tech. rep., Rice
University, 2005.

[7] COOPER, K. D., GROSUL, A., HARVEY, T. J., REEVES,

S., SUBRAMANIAN, D., TORCZON, L., AND WATERMAN,
T. Acme: adaptive compilation made efﬁcient. In LCTES
’05: Proceedings of the 2005 ACM SIGPLAN/SIGBED
conference on Languages, compilers, and tools for
embedded systems (New York, NY, USA, 2005), ACM Press,
pp. 69–77.

[8] EECKHOUT, L., JR., R. H. B., STOUGIE, B., BOSSCHERE,
K. D., AND JOHN, L. K. Control ﬂow modeling in statistical
simulation for accurate and efﬁcient processor design
studies. SIGARCH Comput. Archit. News 32, 2 (2004), 350.
[9] FRANKE, B., O’BOYLE, M., THOMSON, J., AND FURSIN,

G. Probabilistic source-level optimisation of embedded
programs. In LCTES ’05: Proceedings of the 2005 ACM
SIGPLAN/SIGBED conference on Languages, compilers,
and tools for embedded systems (New York, NY, USA,
2005), ACM Press, pp. 78–86.

[10] FURSIN, G., COHEN, A., O’BOYLE, M., AND TEMAM, O.

A practical method for quickly evaluating program
optimizations. In Proceedings of the 1st International
Conference on High Performance Embedded Architectures &
Compilers (HiPEAC) (2005), pp. 29–46.

[11] FURSIN, G., O’BOYLE, M., AND KNIJNENBURG, P.

Evaluating iterative compilation. In Proceedings of the 15th
Workshop on Languages and Compilers for Parallel
Computers (LCPC) (2002), pp. 305–315.

[12] HALL, M. W., ANDERSON, J. M., AMARASINGHE, S. P.,
MURPHY, B. R., LIAO, S.-W., BUGNION, E., AND LAM,
M. S. Maximizing multiprocessor performance with the suif
compiler. Computer 29, 12 (1996), 84–89.

[13] IPEK, E., MCKEE, S. A., CARUANA, R., DE SUPINSKI,

B. R., AND SCHULZ, M. Efﬁciently exploring architectural

design spaces via predictive modeling. Proceedings of the
12th International Conference on Architectural Support for
Programming Languages and Operating Systems
(ASPLOS-XII) 34, 5 (October 2006), 195–206.

[14] KARKHANIS, T. S., AND SMITH, J. E. A ﬁrst-order

superscalar processor model. In Proceedings of the 1st
Annual International Symposium on Computer Architecture
(ISCA’04) (2004), p. 338.

[15] KARURI, K., FARUQUE, M. A. A., KRAEMER, S.,

LEUPERS, R., ASCHEID, G., AND MEYR, H. Fine-grained
application source code proﬁling for asip design. In DAC
’05: Proceedings of the 42nd annual conference on Design
automation (New York, NY, USA, 2005), ACM Press,
pp. 329–334.

[16] KULKARNI, P., ZHAO, W., MOON, H., CHO, K.,

WHALLEY, D., DAVIDSON, J., BAILEY, M., PAEK, Y.,
AND GALLIVAN, K. Finding effective optimization phase
sequences. In Proceedings of the Conference on Languages,
Compilers, and Tools for Embedded Systems (LCTES)
(2003), pp. 12–23.

[17] LEE, C. Utdsp benchmark suite. In

http://www.eecg.toronto.edu/~corinna/
DSP/infrastructure/UTDSP.html (1998).

[18] MONSIFROT, A., BODIN, F., AND QUINIOU, R. A machine

learning approach to automatic production of compiler
heuristics. In AIMSA ’02: Proceedings of the 10th
International Conference on Artiﬁcial Intelligence:
Methodology, Systems, and Applications (London, UK,
2002), Springer-Verlag, pp. 41–50.

[19] PAN, Z., AND EIGENMANN, R. Fast, automatic,

procedure-level performance tuning. In PACT ’06:
Proceedings of the 15th international conference on Parallel
architectures and compilation techniques (New York, NY,
USA, 2006), ACM Press, pp. 173–181.

[20] SAGHIR, M., CHOW, P., AND LEE, C. A comparison of

traditional and vliw dsp architecture for compiled dsp
applications. In Proceedings of the International Workshop
on Compiler and Architecture Support for Embedded
Systems (CASES) (1998).

[21] STEPHENSON, M., AND AMARASINGHE, S. Predicting

unroll factors using supervised classiﬁcation. In CGO ’05:
Proceedings of the international symposium on Code
generation and optimization (Washington, DC, USA, 2005),
IEEE Computer Society, pp. 123–134.

[22] TRIANTAFYLLIS, S., VACHHARAJANI, M.,

VACHHARAJANI, N., AND AUGUST, D. I. Compiler
optimization-space exploration. In CGO ’03: Proceedings of
the international symposium on Code generation and
optimization (Washington, DC, USA, 2003), IEEE Computer
Society, pp. 204–215.

[23] WENISCH, T. F., WUNDERLICH, R. E., FALSAFI, B., AND

HOE, J. C. Turbosmarts: accurate microarchitecture
simulation sampling in minutes. SIGMETRICS Perform.
Eval. Rev. 33, 1 (2005), 408–409.

[24] WUNDERLICH, R. E., WENISCH, T. F., FALSAFI, B., AND

HOE, J. C. Smarts: accelerating microarchitecture
simulation via rigorous statistical sampling. In ISCA ’03:
Proceedings of the 30th annual international symposium on
Computer architecture (New York, NY, USA, 2003), ACM
Press, pp. 84–97.

