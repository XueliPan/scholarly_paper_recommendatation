Turning Down the Noise in the Blogosphere

Khalid El-Arini

Carnegie Mellon University

kbe@cs.cmu.edu

Dafna Shahaf

Carnegie Mellon University
dshahaf@cs.cmu.edu

Gaurav Veda

Carnegie Mellon University

gveda@cs.cmu.edu

Carlos Guestrin

Carnegie Mellon University
guestrin@cs.cmu.edu

ABSTRACT
In recent years, the blogosphere has experienced a substantial in-
crease in the number of posts published daily, forcing users to cope
with information overload. The task of guiding users through this
ﬂood of information has thus become critical. To address this issue,
we present a principled approach for picking a set of posts that best
covers the important stories in the blogosphere.

We deﬁne a simple and elegant notion of coverage and formalize
it as a submodular optimization problem, for which we can efﬁ-
ciently compute a near-optimal solution. In addition, since people
have varied interests, the ideal coverage algorithm should incorpo-
rate user preferences in order to tailor the selected posts to individ-
ual tastes. We deﬁne the problem of learning a personalized cov-
erage function by providing an appropriate user-interaction model
and formalizing an online learning framework for this task. We
then provide a no-regret algorithm which can quickly learn a user’s
preferences from limited feedback.

We evaluate our coverage and personalization algorithms exten-
sively over real blog data. Results from a user study show that our
simple coverage algorithm does as well as most popular blog ag-
gregation sites, including Google Blog Search, Yahoo! Buzz, and
Digg. Furthermore, we demonstrate empirically that our algorithm
can successfully adapt to user preferences. We believe that our
technique, especially with personalization, can dramatically reduce
information overload.

Categories and Subject Descriptors
I.2.6 [Artiﬁcial Intelligence]: Learning; G.3 [Probability and Statis-
tics]

General Terms
Algorithms, Experimentation

1.

INTRODUCTION

“How many blogs does the world need?” asked TIME Magazine
in 2008 [21], claiming that there are already too many. Indeed, the
blogosphere has experienced a substantial increase in the number

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
KDD’09, June 28–July 1, 2009, Paris, France.
Copyright 2009 ACM 978-1-60558-495-9/09/06 ...$5.00.

of posts published daily. One immediate consequence is that many
readers now suffer from information overload.

While the vast majority of blogs are not worth reading for the
average user, even the good ones are too many to keep up with.
Moreover, there is often signiﬁcant overlap in content among mul-
tiple blogs. To further complicate matters, many stories seem to
resonate in the blogosphere to an extent that is largely uncorrelated
with their true importance. For example, in the spring of 2007,
Politico broke a story about John Edwards’ $400 haircut in a blog
post [26], which was almost instantly seized upon by the rest of the
blogosphere. Over the next two weeks, the haircut story sparked
several major online debates. Avoiding this story was difﬁcult for
most Web users, and nearly impossible for those interested in poli-
tics but not in this particular line of debate.

The goal of this paper is to turn down the noise in the blogo-
sphere. We assume that users have very limited time for reading
blog posts, and thus our goal is to show them a small set of posts
covering the important stories currently being discussed. Further-
more, we allow users to personalize the process; after all, one man’s
noise may be another man’s music.

In this paper, we formally deﬁne what it means for a set of posts
to cover the blogosphere. One desired property of this notion of
coverage is that it must be an efﬁciently computable function. For
instance, due to the large size of our data sets, we cannot use most
clustering algorithms, as they require quadratic computation.
In
addition, the coverage function must be expressive enough so that
it can recognize the important stories in the blogosphere while at
the same time identify the important features of a particular docu-
ment. Finally, the notion should be soft, allowing partial (or prob-
abilistic) coverage, as posts rarely offer complete coverage of their
stories. We propose a simple and elegant notion that addresses
these requirements and formalize a corresponding objective func-
tion, which exhibits a natural diminishing returns property known
as submodularity. We present a near-optimal efﬁcient algorithm for
optimizing this function.

We then extend our notion of coverage to personalized coverage.
Posts that cover the blogosphere for the average population may not
be optimal for a particular user, given her personal preferences. For
example, a user may like stories about badminton, irrespective of
their prevalence. Learning a personalized coverage function allows
us to show the users posts that are better suited to their tastes.

We formalize and address the problem of learning a personal-
ized coverage function. First, we deﬁne an interaction model for
user feedback that takes into account the order in which the posts
are read. Using this model, we then deﬁne an online learning set-
ting for coverage functions and provide a simple no-regret algo-
rithm that guarantees we can quickly adapt to a user’s preferences.
We evaluate our algorithm, Turning Down the Noise (TDN), on
real blog data collected over a two week period in January 2009.
We compare TDN to popular blog aggregation sites (Google Blog

289Search [4], Yahoo! Buzz [7], Digg [3], and BlogPulse [1]), measur-
ing topicality and redundancy. Results from a user study show that
our simple, fully-automated coverage algorithm performs as well
as, or better than, most of these sites, including those based on user
voting or human editing.

Perhaps most importantly, we demonstrate TDN’s ability to suc-
cessfully adapt to user preferences. Personalization not only im-
proves user satisfaction, but is also able to simulate users with
different interests. We believe that our algorithm, especially with
personalization, can dramatically improve the information overload
situation.

In summary, our main contributions are:
• We deﬁne the notion of covering the blogosphere and for-
malize it as a submodular optimization problem, for which
we provide a near-optimal solution.

• We deﬁne and formalize the problem of learning a person-
alized coverage function, and provide a no-regret algorithm
for learning user preferences from limited feedback.

• We evaluate our algorithm on real blog data using both user
studies and simulations, and compare it to popular blog ag-
gregation sites.

2. COVERAGE

Figure 1(a) shows a typical day in the blogosphere (January 17,
2009). The size of a word is proportional to its frequency across
the blogosphere. Examining the picture, we can spot some of the
popular stories for that day: the inauguration of Barack Obama and
the Israel-Gaza conﬂict.

Many posts cover the same story, e.g., the inauguration. More-
over, stories may have a certain degree of overlap. Intuitively, our
goal is to select a small set of blog posts that captures the important
stories of the day. At the same time, we wish to avoid redundancy.
In the following section we formally state the problem of coverage
and present an efﬁcient optimization algorithm.

2.1 Documents and Features

We characterize the posts in the blogosphere by features. Fea-
tures can be any arbitrary collection of objects, high- or low-level,
for example: signiﬁcant words (such as named entities and noun
phrases), topics extracted from the corpus, or even higher-level se-
mantic relations. As an example, refer again to Figure 1(a). Here,
our features are common named entities. Each document will be
about one or more of these features. More formally:

DEFINITION 2.1

(BLOGOSPHERE). A blogosphere is a triplet
(cid:2)U, Posts, cover·(·)(cid:3). U = {u1, u2, ...} is a ﬁnite set of features,
and Posts is a ﬁnite set of posts. The relation between posts and
features is captured by the covering function. coverj(i) : U → R+
quantiﬁes the amount postj ∈ Posts covers feature ui.

In the simplest case, cover·(·) is a binary indicator function, turn-
ing posts into subsets of features. Later, we explore other softer
notions of coverage functions, e.g., ones with probabilistic inter-
pretations.

2.2 Covering Features

Given our model (cid:2)U, Posts, cover·(·)(cid:3), we wish to determine how
effectively a given small set of posts can cover the important sto-
ries in the blogosphere. More formally, our goal is to pick a set of k
posts A ⊆ Posts, in order to maximize some coverage objective. In
this section we deﬁne desired properties of this objective function,
and propose a solution that addresses these requirements.

Perhaps the most natural idea is to ﬁrst cluster the posts, where
posts in the same cluster cover the same features. Then, given clus-
ters, we can pick a representative post from each of the k largest

clusters. Such clustering approaches are common in the litera-
ture [28]. However, most clustering methods require us to compute
the distance between every pair of posts, which amounts to O(n2)
comparisons for n posts. Due to the sizable amount of posts pub-
lished daily, methods that require O(n2) computation are practi-
cally infeasible. Our ﬁrst desirable property for a coverage function
is scalability, i.e., we should be able to evaluate coverage in time
linear in the number of posts.

Another solution, which does not require quadratic complexity,

would be to formulate coverage as maximizing the function,

F (A) =

coverA(i),

(1)

X

i∈U

where the coverA(i) function measures the degree to which posts
A cover feature ui. If posts correspond to a collection of features,
and cover·(·) are binary indicator functions, then Eq. 1 reduces to
the Budgeted Maximum Coverage problem:

DEFINITION 2.2

(BUDGETED MAXIMUM COVERAGE).

Given a set of ground elements U, a collection S = {S1, ...Sm} of
subsets of U, and a budget k ≥ 0, select A ⊆ S of size at most k which
maximizes the number of covered elements, |

S

Sj ∈A Sj|.

In our setting, this coverage can be formalized as maximizing:

F (A) =

1(∃aj ∈ A : coverj (i) = 1).

X

i∈U

Although max-coverage is an NP-hard problem, there are several
efﬁcient and effective approximation algorithms for this task. How-
ever, this naïve approach suffers from some serious drawbacks:

• Feature signiﬁcance in corpus: All features in a corpus are
treated equally, and thus we cannot emphasize the impor-
tance of certain features. For example, covering “Cathedral
High School” should not be as valuable as covering “Obama.”
• Featuresigniﬁcanceinpost: This objective function does not
characterize how relevant a post is to a particular feature, e.g.,
a post about Obama’s speech covers Obama just as much
as a post that barely mentions him. As a side effect, this
objective rewards “name-dropping” posts (posts that include
many features, without being about any of them).

• Incremental coverage: This coverage notion is too strong,
since after seeing one post that covers a certain feature, we
will never gain anything from another post that covers the
same feature. This does not correspond to our intuitive no-
tion of coverage, which should be subject to the law of di-
minishing returns: each additional time we see a feature we
get an additional reward, which decreases with the number
of occurrences. For example, suppose we show the user a
post about Obama’s inauguration. The second post we con-
sider showing her is about the effect of Obama’s presidency
on China. Figure 1(b) shows the raw coverage of the sec-
ond post, and “Obama” is the top-covered feature. However,
if we take into account the fact that we have already cov-
ered the feature “Obama” to some extent by the ﬁrst post, the
coverage by the second post changes. Figure 1(c) shows the
incremental coverage by the second post. As illustrated, the
signiﬁcance of this post towards “Obama” is diminished, and
most of our reward would come from covering “China.”

We now address each of these three issues. To address Feature
signiﬁcance in corpus, we can simply assign weights wi to each
feature ui:

F (A) =

wi 1(∃aj ∈ A : coverj (i) = 1).

X

i∈U

If features are words, the weights can correspond to their frequency
in the data set.

290(a)

(b)

(c)

Figure 1: (a) Global word frequency across the blogosphere (January 17, 2009). The size of a word is proportional to its frequency.
(b) Coverage vs. (c) incremental coverage of a post about Obama and China, given that we already saw a post about Obama. The
incremental coverage of Obama is much smaller than the regular coverage.

We now turn our attention to Feature signiﬁcance in post. Each
post should exhibit different degrees of coverage for the features
it contains, which can be achieved by softening the notion of cov-
erage, coverj(i). One approach is to use a generative model to
estimate the probability of a feature given a post, P (ui | postj ). If,
for example, our features are topics discovered by a topic model,
then this term is simply the probability that document j is about
topic i. More generally, any generative model for the particular set
of features can be used to deﬁne this probability.

Given such a probabilistic model, we can deﬁne the notion of soft
coverage more formally. If our features are sufﬁciently high-level,
e.g., topics in a topic model, then a post can be thought of as being
about a single feature, in which case coverj(i) = P (ui | postj).
Alternatively, for lower-level features, such as named entities, we
could assume that each post is about (cid:2) features. If these features
are picked, for example, at random with replacement from P (ui |
postj), then our coverage will become coverj (i) = 1 − (1 − P (ui |
postj))(cid:2). By requiring that all posts cover the same number of fea-
tures, we alleviate the problem of “name-dropping,” since a post
cannot cover a large number of features well.

P

P

i∈U wi

The probabilistic approach allows us to deﬁne feature impor-
tance in individual posts as well as in the whole corpus. However,
if we deﬁne coverage as F (A) =
aj ∈A coverj(i),
then the Incrementalcoverage problem would persist, as this func-
tion does not possess the diminishing returns property.
Instead,
extending the probabilistic interpretation further, we can view set-
coverage as a sampling procedure: each post tries to cover feature
i with probability coverj (i), and the feature is covered if at least
one of the posts in A succeeded. Thus, as A grows, adding a post
provides less and less additional coverage. Formally, we can deﬁne
the probabilistic coverage of a feature by a set of posts A as:

coverA(i) = 1 −

(1 − coverj(i)).

(2)

Y

aj ∈A

Finally, we propose the following objective function for the prob-
lem of probabilistic coverage of the blogosphere:

Our task is to ﬁnd k posts maximizing the above objective function:

F (A) =

wicoverA(i).

X

i∈U

A∗

= argmax

A⊆Posts:|A|≤k

F (A).

(3)

(4)

2.3 Optimizing Coverage of the Blogosphere
Using the notion of coverage in Eq. 2, our goal now is to ﬁnd
the set of posts A that maximizes our objective function in Eq. 3.
Unfortunately, we can show by reduction from max-coverage that

this objective is NP-complete, suggesting that the exact maximiza-
tion of this function is intractable. However, our objective function
satisﬁes an intuitive diminishing returns property, submodularity,
which allows us to ﬁnd good approximations very efﬁciently:

DEFINITION 2.3

(SUBMODULARITY). A set function F is sub-

modular if, ∀A ⊆ B ⊆ V, ∀s ∈ V \ B, F (A ∪ {s}) − F (A) ≥
F (B ∪ {s}) − F (B).

CLAIM 2.4. The probabilistic coverage function for the blogo-

sphere in Eq. 3 is submodular [15].

Intuitively, submodularity characterizes the notion that reading a
post s after reading a small set of posts A provides more coverage
than reading s after having already read the larger set B ⊇ A.

Although maximizing submodular functions is NP-hard [20], by
discovering this property in our problem, we can take advantage of
several efﬁcient approximation algorithms with theoretical guaran-
tees. For example, the classic result of Nemhauser et al. [24] shows
that by simply applying a greedy algorithm to maximize our objec-
tive function in Eq. 3, we can obtain a (1 − 1
e ) approximation of
the optimal value. Thus, a simple greedy optimization can provide
us with a near-optimal solution. However, since our set of posts is
very large, a naïve greedy approach can be too costly. Therefore,
we use CELF [22], which provides the same approximation guaran-
tees, but uses lazy evaluations, often leading to dramatic speedups.

3. PERSONALIZATION

Thus far, we have deﬁned a global notion of coverage for the
blogosphere. However, each user has different interests, and the
selected posts that cover the prevalent stories may contain many
topics that do not interest him. Instead, our goal in this section is
to utilize user feedback in order to learn a personalized notion of
coverage for each user.

Recall that, in the previous section, F (A) assigns a ﬁxed weight
wi to every feature, representing its importance. In practice, fea-
ture importance varies among different users. One user might care
about a feature “NASCAR,” while others may be indifferent to it.
To address this issue, we augment the ﬁxed weights wi with per-
sonalized preferences πi for each feature i. In the following, we
assume that a user’s coverage function is of the form:

Fπ∗ (A) =

∗
i wi coverA(i),

π

(5)

X

i∈U

for some unknown set of weights {π∗
i }. Our goal now is to learn
a user’s coverage function Fπ∗ (A) by learning this optimal set of
preferences {π∗

i }.

2913.1 Interaction Models

In order to receive personalized results, users need to communi-
cate their preferences. Since Fπ is a set function, the most natu-
ral notion of feedback from a machine learning perspective would
be for users to provide a single label for the set of posts that they
are presented, indicating whether they like or dislike the entire set.
However, this approach suffers from two limitations. First, from
the point of view of the user, it is not very natural to provide feed-
back on an entire set of posts. Second, since there are exponentially
many such sets, we are likely to need an extensive amount of user
feedback (in terms of sets of posts) before we could learn this func-
tion. Instead, we assume that users go through a list of posts A in
order, submitting feedback fj (“liked”= +1, “indifferent” = 0, “dis-
liked” = -1) for each post aj ∈ A. We take no feedback on a post
to mean “indifferent.”

3.2 Personalization by Minimizing Regret

Our objective function is deﬁned in terms of sets, but our feed-
back is in terms of individual posts. How should we provide an
appropriate credit assignment?

One possible solution would be to assume that the feedback that
a user provides for a particular post is independent of the other
posts presented in the same set. In this case, one can view the user
feedback as being labeled data on which we can train a classiﬁer
to determine which posts the user likes. However, this assumption
does not ﬁt with our interaction model, as a user might not like a
post either because of its content or because previous posts have
already covered the story.

To address this issue, we consider the incremental coverage of
a post, i.e., the advantage it provides over the previous posts. The
incremental coverage we receive by adding post aj to the set A is:

inc-coverj(A, i) = coverA∪aj (i) − coverA(i).

Note that if coverA(i) is deﬁned as in Eq. 2, then the incremental
coverage is the probability that aj is the ﬁrst post to cover feature
ui. Furthermore, if we view the set of documents A as an ordered
set A = {a1, . . . , ak}1, the sum of incremental coverages is a tele-
scoping sum that yields the coverage of a set of documents A:
X

X

inc-coverj (a1:j−1, i) =

covera1:j (i) − covera1:j−1 (i)

aj ∈A

aj ∈A

= coverA(i),

where a1:j−1 is shorthand for the set of documents {a1, . . . , aj−1}.
Using incremental coverages, we can now deﬁne the reward we
receive after presenting A to a user with preferences π and obtain-
ing feedback f:

Rew(π, A, f) =

πi wi

fj inc-coverj(a1:j−1, i).

X

i∈U

X

aj ∈A

If the user liked all of the documents in A (i.e., ∀j, fj = 1), this
reward becomes exactly the coverage function we are seeking to
maximize, Fπ (A) =

i∈U πi wi coverA(i), as in Eq. 5.

P

Our algorithm maintains an estimate of the user’s preferences at
each time step t, π(t). Given this estimate, we optimize Fπ(t) (A)
and pick a set of documents A(t) to show the user. After receiving
feedback f (t), we gain a reward of Rew(π(t), A(t), f (t)). After T
time steps, our average reward is therefore:

AvgRew(T ) =

Rew(π

(t)

, A(t)

, f

(t)

).

1
T

TX

t=1

Since our decisions at time t can only take into account the feed-
back we have received up to time t − 1, the decisions we made
may have been suboptimal. For comparison, consider the reward
we would have received if we had made an informed choice for
the user’s preferences π considering all of the feedback from the T
time steps:

BestAvgRew(T ) = max

π

Rew(π, A(t)

, f

(t)

).

(6)

1
T

TX

t=1

That is, after seeing all the user feedback, what would have been the
right choice for user preference weights π? The difference between
our reward and this best choice in retrospect is called the regret:

DEFINITION 3.1

(REGRET). Our average regret after T time

steps is the difference BestAvgRew(T ) − AvgRew(T ).

Positive regret means that we would have preferred to use the weights
π that maximize Eq. 6 instead of our actual choice of weights π(t).
A no-regret learning algorithm, such as the one we describe in the
next section, will allow us to learn π(t) such that, as T goes to
inﬁnity, the regret will go to zero at a rapid rate. Intuitively, this
no-regret guarantee means that we learn a sequence π(t) that does
as well as any ﬁxed π–including the true user preferences, π∗–on
the sets of posts that the user is presented. By learning the person-
alized coverage function for a particular user in this manner, the
posts we provide will be tailored to his tastes.

A stronger guarantee would be to show that the weights π(t) not
only do well on the sets of posts from which they were learned, but
also on the posts that would have been selected had we used the
true π∗ as the user preference weights for each day. For example,
consider a user who is interested in politics and sports, but is also
passionate about bagpiping. We may never show him any bagpip-
ing posts, since they are not likely to be common. Thus, we may
never receive feedback that would allow us to accurately model this
portion of the user’s true preferences. We intend to address this is-
sue in future work.

3.3 Learning a User’s Preferences

We now describe our algorithm for learning π∗ from repeated
user feedback sessions. Like many online algorithms [12], our ap-
proach updates our estimated π(t) using a multiplicative update
rule. In particular, our approach can be viewed as a special case
of Freund and Schapire’s multiplicative weights algorithm [18].

The algorithm starts by choosing an initial set of weights π(1).
(WLOG, we assume weights are normalized to sum to 1, since the
coverage function is insensitive to scaling.) In the absence of prior
knowledge about the user, we can choose the uniform distribution:

(1)
i =

π

1
|U| .

If we have prior knowledge about the user, we can start from the
corresponding set of weights.

At every round t, we use our current distribution π(t) to pick k
posts, A(t), to show the user. After receiving feedback f (t), we
would like to increase the weight of features covered by posts the
user liked, and decrease the weight of features covered by posts the
user disliked. These updates can be achieved by a simple multi-
plicative update rule:

(t+1)
i

π

=

(t)
i β

π

−M(i,f (t))

,

1
Z

(7)

1This ordering could be deﬁned by the order the posts are presented
to the user, e.g., the one picked by the greedy algorithm.

where Z is the normalization constant, β ∈ (0, 1) is the learning
rate, and, intuitively, M(i, f (t)) measures the contribution (posi-

292tive or negative) that feature i had on our reward:

P

wi

(t)
aj ∈A(t) f
j

M(i, f

(t)

) :=

inc-coverj (a1:j−1, i)

,

(8)

2 maxi wi

where the normalization by 2 maxi wi is simply used to keep this
term in the range [−0.5, 0.5].

If the learning rate β is small, we make large moves based on
the user feedback. As the learning rate tends to 1, these updates
become less signiﬁcant. Thus, intuitively, we will start with a small
value of β and slowly increase it.

CLAIM 3.2. If, for number of personalization epochs T , we use

a learning rate βT given by:

βT :=

1
q

,

1 +

2 ln |U |

T

(9)

then our preference learning procedure will have regret bounded
by:

BestAvgRew(T ) − AvgRew(T ) ≤ O

 r

!

.

ln |U|

T

Since our regret goes to zero as T goes to inﬁnity, our approach
is called a no-regret algorithm. The proof follows from Freund
and Schapire [18], by formalizing our learning process as a two-
player repeated matrix game involving our algorithm and the user
(cf. extended version of this paper for details [15]).

4. EVALUATION

We evaluate our algorithm on real blog data collected over a two
week period in January 2009. These posts come from a diverse set
of blogs, including personal blogs, blogs from mainstream news
sites, commercial blogs, and many others.

We obtain the data from Spinn3r, which indexes and crawls 12
million blogs at the rate of approximately 500,000 posts per day
[5]. After performing some simple data cleaning steps, such as
duplicate post removal, we reduce this number to about 200,000
posts per day in our data set. However, as this is real Web data, it is
still invariably noisy even after cleaning. Thus, our algorithm must
be robust to content extraction problems.

For each post, we extract named entities and noun phrases using
the Stanford Named Entity Recognizer [17] and the LBJ Part of
Speech Tagger [25], respectively. We remove infrequent named
entities and uninformative noun phrases (e.g., common nouns such
as “year”), leaving us with a total collection size of nearly 3,000.
(More details can be found in the extended version [15].)

We evaluate an instantiation of our algorithm with high level
topic model-based features, which we refer to as TDN+LDA. We
deﬁne our set of features as topics from a latent Dirichlet allocation
(LDA) [9] topic model learned on the noun phrases and named en-
tities described above. We take the weight of each feature to be the
fraction of words in the corpus assigned to that topic. As described
in Section 2.2, we can directly deﬁne coverj(i) = P (ui | postj),
which in the setting of topic models is the probability that postj is
about topic i. We use a Gibbs sampling implementation of LDA
[19] with 100 topics and the default parameter settings.

Once we have extracted the named entities and noun phrases,
LDA is the slowest part of running TDN+LDA. After a 300 itera-
tion burn-in period, we run 2,500 iterations of Gibbs sampling and
select 500 samples from them. On a single 3GHz processor, this
process takes less than 2GB of RAM and between 1-2 hours to run
for an eight hour corpus of blog posts. The submodular function
optimization needed to generate posts takes under a minute.

We also evaluate a variant of our algorithm with features con-
sisting of the named entities and noun phrases directly, which we
refer to as TDN+NE. As this variant uses a lower-level feature set,
it assumes a post can cover multiple features, and thus uses the cov-
erage function for covering (cid:2) features described in Section 2.2. The
value of (cid:2) is set to be the average number of occurrences of named
entities and nouns per document in our corpus, which is approxi-
mately 16. In this setting, post selection takes about ﬁve minutes.

4.1 Evaluating Coverage

As detailed in Section 2, the main objective of our algorithm is to
select a set of posts that best covers the important and prevalent sto-
ries currently being discussed in the blogosphere. The major world
events that took place during the time corresponding to our data
set included the Israel-Gaza conﬂict, the inauguration of Barack
Obama, the gas dispute between Russia and Ukraine, as well as the
global ﬁnancial crisis. As an example, here is the set of posts that
our algorithm selects for an eight hour period on January 18, if our
budget k is set to ﬁve:

1. Israel unilaterally halts ﬁre as rockets persist
2. Downed jet lifted from ice-laden Hudson River
3. Israeli-trained Gaza doctor loses three daughters and niece to

IDF tank shell

4. EU wary as Russia and Ukraine reach gas deal
5. Obama’s ﬁrst day as president: prayers, war council, economists,

White House reception

The selected ﬁve posts all cover important stories from this par-
ticular day. The Israel-Gaza conﬂict appears twice in this set, due
to its extensive presence in the blogosphere at the time. It is impor-
tant to note, however, that these two posts present different aspects
of the conﬂict, each being a prevalent story in its own right. By ex-
panding the budget to ﬁfteen posts, the algorithm makes additional
selections related to other major stories of the day (e.g., George
W. Bush’s legacy), but also selects “lifestyle” posts on religion and
cooking, since these represent the large portion of the blogosphere
that is not directly related to news and current events.

As another example, here are the top ﬁve selected posts from the
morning of January 23, the day after the Academy Award nomina-
tions were announced:

1. Button is top Oscar nominee
2. Israel rules out opening Gaza border if Hamas gains
3. Paterson chooses Gillibrand for U.S. Senate
4. Fearless Kitchen: Recipe: Medieval Lamb Wrap
5. How Obama avoided a misguided policy blunder

A post describing the Oscar-nominated movie The Curious Case
of Benjamin Button supplants the Israel-Gaza conﬂict at the top of
the list, while a cooking post makes it up to the fourth position.

We wish to quantitatively evaluate how well a particular post
selection technique achieves the notion of coverage we describe
above on real blog data. However, the standard information re-
trieval metrics of precision and recall are not directly applicable in
our case, since we do not have labels identifying all the prevalent
stories in the blogosphere on a given day and assigning them to spe-
ciﬁc posts. Rather, we measure the topicality of individual posts as
well as the redundancy of a set of posts. We say a post is topical
with respect to a given time period if its content is related to a major
news event from that period. A post r is redundant with respect to
a previous post p if it contains little or no additional information to
post p. An ideal set of posts that covers the major stories discussed
in the blogosphere would have high topicality and low redundancy.
We conducted a study on 27 users to obtain labels for topical-
ity and redundancy on our data. We compared TDN+LDA and

293Figure 2: Topic representing the peanut butter recall from Jan-
uary 18, 2009, with the size of a word proportional to its impor-
tance in the topic.

TDN+NE to four popular blog aggregation sites: the front page of
Digg, Google Blog Search, Nielsen BuzzMetrics’ BlogPulse, and
Yahoo! Buzz. We intended on evaluating Technorati as well, but
their RSS feed was unavailable for most days in our evaluation pe-
riod. Additionally, we also examine the performance of simpler
objective functions on the post selection task.

4.1.1 Measuring Topicality

In order for users to measure the topicality of a blog post, they
need an idea of what the major news stories are from the same time
period. We express this information to our study participants by
providing them with headlines gathered from major news sources
in ﬁve different categories: world news, politics, business, sports,
and entertainment. The headlines for each category are aggregated
from three different news sources to provide a wider selection for
the users and to avoid naming a single source as the deﬁnitive news
outlet for a category. For instance, for politics we present headlines
from Reuters, USA Today, and The Washington Post. This collec-
tion of headlines is akin to a condensed newspaper, and we refer to
these stories as reference stories.

We present the participants with reference stories gathered at a
particular time, e.g., January 18, 2009, 2:00pm EST, which we call
the reference time. We then show each participant a set of ten posts
that was chosen by one of the six post selection techniques, and
ask them to mark whether each post is “related” to the reference
stories. Each post is presented as a title along with a short descrip-
tion. The users are not made aware of which technique the posts
come from, so as not to bias their ratings. The posts selected by
TDN+LDA and TDN+NE were chosen from an eight hour window
of data ending at the reference time, while the posts selected by
the popular blog aggregation sites were retrieved from these sites
within ﬁfteen minutes of the reference time.

Figure 3(left) shows the results of the topicality user ratings on
the six techniques. On average, the sets of ten posts selected by
Google Blog Search, TDN+LDA and Yahoo! Buzz each contain
ﬁve topical posts out of ten presented. The topicality of these tech-
niques is signiﬁcantly better than that of TDN+NE, Digg and Blog-
Pulse. BlogPulse selects the most linked-to posts of the day, which
does not seem to be a good heuristic for covering the important
stories. Many of these posts are technology how-to pages, such
as “Help With Social Bookmarking Sites,” the highest ranked post
from January 18. Digg selects its top posts by user voting, and thus
the top selected posts consist of a few prevalent stories and many
entertaining or shocking posts, such as “Teen Stabbed But Makes
It To Job Interview,” the top post from February 6.

TDN+LDA outperforms TDN+NE because high-level features,
such as LDA topics, capture stories in a better way than low-level
features do. For example, for one eight hour period in our data
set, there is a coherent LDA topic about the EU-Russia gas crisis.
Therefore, when we cover this topic, we will present a story that is
about the crisis. However, the named entity “Russia” may be cov-
ered by multiple stories. TDN+NE selects a post about Russia’s
plan to go ahead with the opening of a pediatric medical center in

Moscow despite the current ﬁnancial crisis, since it contains impor-
tant named entities and nouns like “Russia,” “Putin,” “crisis,” etc.
Hence, if we only cover low-level features, we might select a post
that is not topical, yet contains multiple important features.

While topicality captures a major aspect of our notion of cov-
erage, in that important current events are covered by the selected
posts, one drawback of this evaluation method is that lifestyle blog
posts are not adequately represented. It is difﬁcult to deﬁne a set
of reference sites that summarize the day’s most important recipes
or most prevalent do-it-yourself tips, for instance. Furthermore, in
our case, we did not want to show our study participants more than
ﬁve categories of reference stories, so as not to overwhelm them.
As a result, a post related to an important technology story would
likely not be considered topical, as we left this category out.

4.1.2 Measuring Redundancy

The user study described in the previous section allowed us to
measure whether posts were topical or not. However, topicality
is not enough to judge the goodness of a set of posts, since they
may all be about the same story, and hence not interesting. Instead,
we want the posts to be diverse, so that they capture all of the im-
portant stories in the blogosphere, as well as appeal to everyone’s
interests. As part of our user study, we asked users to look at a set
of ﬁfteen posts selected by one of the six previously described post
selection techniques, and mark any occurrences they thought were
redundant. Each of 27 participants was presented with either two or
three sets of posts generated by different algorithms over the same
time period. The users were not aware of the sources of the posts.
Figure 3(right) shows that both variants of our algorithm outper-
form Digg, BlogPulse and Google Blog Search on the redundancy
metric. In other words, our algorithm selects diverse sets of posts.
This diversity is primarily due to the diminishing returns property
of our objective function. If we have covered the important fea-
tures of a story once, covering it again yields only a small reward.
Google Blog Search has the highest number of redundant results,
and has high variance, suggesting that on some days many of the
posts on its front page are similar. In fact, on average, the posts
selected by Google Blog Search are nearly six times as redundant
as those selected by TDN+LDA.

However, it should be noted that performing well on the redun-
dancy metric alone is not sufﬁcient. For example, it may turn out
that all the posts picked by an algorithm are non-redundant, but
meaningless, and hence of no interest to a user. Thus, an algorithm
needs to perform well on both the topicality and the redundancy
metric in order for it to be useful.

TDN+LDA and Yahoo! Buzz were the two techniques that per-
formed well in both metrics. However, while Yahoo! Buzz uses
Web search trends, user voting and other features to select its posts,
TDN+LDA achieves the same topicality and redundancy perfor-
mance by selecting posts only using simple text features. Fur-
thermore, TDN+LDA adapts its results to user preferences, as de-
scribed in Section 4.2.

4.1.3 Alternative Objective Functions

As an alternative to the submodular objective function deﬁned

in Eq. 3, we consider two simpler objective functions.

LDA-based Modular Function. A modular function is an additive
set function where each element is associated with a ﬁxed score,
and the value for a set A is the sum of the scores of the elements
of A. Since the score of a post does not depend on the other ele-
ments in the set, there is no incentive to select a diverse set of posts.
The naïve way of selecting posts using LDA ﬁts under this modular
framework. We ﬁrst pick the top k topics based on their weight in
the corpus. For each one, we pick the post that covers it the most.

294e
c
n
e
r
e

f

e
R
o

 

t
 

l

t

d
e
a
e
R
 
s
t
s
o
P

 
f

o
 
r
e
b
m
u
N
g
v
A

 

6

5

4

3

2

1

0

t

e
S

 
t
l

u
s
e
R

 
r
e
p

 
s
t
s
o
P

 
t

n
a
d
n
u
d
e
R

 
f

o

 
r
e
b
m
u
N
g
v
A

 

3.5

2.5

1.5

3

2

1

0

0.5

Google TDN+LDAY! Buzz TDN+NE Digg BlogPulse

Google TDN+LDAY! Buzz TDN+NE Digg BlogPulse

Figure 3: Left: Results from user study measuring topicality. The bars show the average number of posts (out of 10) that users
found to be topical with respect to the reference stories. Right: Results of the redundancy user study. Users report the number of
redundant posts for each post selection technique they are presented with. Error bars on all plots indicate standard error.

In addition to the potential for redundancy mentioned above, this
technique suffers from the fact that it commits to a topic irrespec-
tive of the quality of the posts covering it. Furthermore, even if a
post covers multiple topics well, it might not be selected as there
may be some posts that better cover each individual topic. Using a
strictly submodular objective function alleviates these problems.

For example, if we deﬁne our features based on a 50-topic LDA
model trained on an eight hour data set from January 18, the topic
with the lowest weight is about the peanut butter recall, a major
news story at this time (cf. Figure 2). Thus, if we select ﬁfteen
posts following the naïve LDA approach, we do not pick a post
from this topic. However, the weight of this topic (0.019) is not
much lower than the mean topic weight (0.020). Moreover, since
this topic closely corresponds to a prevalent news story, many posts
cover it with high probability. TDN selects such a post because,
unlike the naïve LDA approach, it simultaneously considers both
the topic weights and the post coverage probabilities.

Budgeted Maximum Coverage. Another simple objective func-
tion we consider is budgeted maximum coverage, introduced in
Deﬁnition 2.2, but with each feature (in this case, noun phrases
and named entities) weighted by its corpus frequency. Optimizing
this objective leads to the aforementioned “name-dropping” posts.
For example, on an eight hour data set from January 20, the second
post selected announces the schedule of a rock band’s upcoming
world tour, and thus completely covers the features, “Washington,”
“Boston,” “New York,” “London,” “Rome,” and a few dozen more
cities and countries. Once this post has been selected, there is no
further incentive to cover these features.

4.2 Personalization

There are two methods by which we evaluate how well our algo-
rithm personalizes the posts it selects in response to user feedback.
In one setting, we conduct a user study to directly measure how
many of the presented posts a study participant would like to read.
In the second setting, we simulate user preferences on a targeted
set of blog posts and observe how our objective function F (A)
changes with respect to the unpersonalized case.

4.2.1 Preferences of Real Users

short summary. The user is instructed to read down the list of posts
and, one by one, mark each post as “would like to read,” “would
not like to read,” or “indifferent.” The user is told to make each
decision with respect to the previous posts displayed in that set, so
as to capture the notion of incremental coverage. For example, a
user might be excited to read a post about Obama’s inauguration
appearing at the top slot in a particular result set, and thus would
mark it as “like to read.” However, if four other very similar posts
appear below it, by the time he gets to rating the ﬁfth inauguration
post in a row, he will likely label it as “not like to read.”

After each set of ten posts, our personalization algorithm uses the
user ratings to update the weights π(t), and selects a personalized
set of posts for the next epoch2. We also ask the user to mark his
preferences on unpersonalized posts presented for the same epochs.
The order in which these two conditions are presented is random-
ized. We repeat this process for a total of ﬁve epochs. As this is not
a longitudinal study, and we do not wish it to be overly tedious for
our participants, we accelerate the personalization process by us-
ing a learning rate β of 0.5, corresponding to a short-term learning
horizon (i.e., T ≈ 9 from Eq. 9).

Figure 4(a) shows the result of this study on twenty users. The
vertical axis of the plot shows the average number of posts liked by
a user in a single epoch. As one would expect, at epoch 0, when
the posts are always unpersonalized, the number of liked posts is
approximately the same between the personalized and unpersonal-
ized runs. However, in just two epochs, the users already show a
preference towards the personalized results.

If a user only prefers sports posts, personalization is easy, as the
user’s interests are narrow. In our study, however, the participants
were simply instructed to rate posts with their own personal pref-
erences. As people are often eclectic and have varied interests, this
task is harder, but more realistic. Thus, it is notable that we are
still able to successfully adjust to user tastes in very few epochs,
showing a signiﬁcant improvement over the unpersonalized case.

If instead of asking users to rate posts according to their personal
tastes, we ask them to pretend that they only want to read posts on
a speciﬁc subject (e.g., India), we observe interesting qualitative
behavior. Initially, the top posts selected are about the main stories
of the day, including the Israel-Gaza conﬂict and the Obama inau-

We divide our blog data into 33 eight hour segments (epochs),
and pick a starting segment at random for a particular user. We
present our user with a set of ten posts from his starting segment,
selected using TDN+LDA. The posts are displayed as a title and

2As topics tend to change from one epoch to the next, we employ a
simple bipartite matching algorithm to map personalization weights
across epochs. Alternatively, one could use more recent topic mod-
els that are designed to work on streaming data ([10]).

295guration. After a few epochs of marking any India-related posts
as “like” and all others as “dislike,” the makeup of the selected
posts changes to include more posts about the Indian subcontinent
(e.g., “Pakistan ﬂaunts its all-weather ties with China”). This is
particularly notable given that these posts appear relatively infre-
quently in our data set, and thus without personalization, are rarely
selected. Also, while after enough epochs, stories about India even-
tually supplant the other major news stories at the top of the result
set, the Israel-Gaza stories do not disappear from the list, due to
their high prevalence. We believe this is precisely the behavior one
would want from such a personalization setting.

4.2.2 Simulating Preferences

We consider the case of a hypothetical sports fan, who always
loves to read any sports-related post. In particular, every day, he
is presented with a set of posts from the popular sports blog Fan-
House.com, and he marks that he likes all of them. We simulate
such a user in order to empirically examine the effect of personal-
ization on the objective function.

Speciﬁcally, we simulate this sports fan by marking all Fan-
House.com posts as “liked” over a speciﬁed number of personal-
ization epochs, updating the personalization weights π(t)at each
epoch. On the next epoch, which we call the evaluation epoch,
we compute our objective function F (A) on three different sets of
posts. First, we compute F (A) on the FanHouse.com posts from
this epoch, hypothesizing that the more epochs we spend person-
alizing prior to the evaluation epoch, the higher this value will be.
Second, we compute F (A) on all the posts from DeadSpin.com,
another popular sports blog. We also expect to see a higher value
of our objective in this case. Finally, we compute F (A) on all the
posts from the HufﬁngtonPost.com Blog, a popular politics blog.
The expectation is that by personalizing on sports posts for several
days, F (A) for a set A of politics posts will decrease with respect
to the unpersonalized case.

Figure 4(b) shows the results of this experiment with a β value
of 0.5, and we observe precisely the hypothesized behavior. The
vertical axis of this plot shows the ratio of F (A) computed with
the learned personalization weights to that of F (A) with the un-
personalized uniform weights, allowing us to compare across the
three blogs. Thus, points on the plot that appear higher along the
vertical axis than 1 indicate an improvement over the unpersonal-
ized case, while any value below 1 indicates a decline with respect
to the unpersonalized case.

Figure 4(c) shows the same simulation but with β = 0.1. This is
an aggressive setting of the learning rate, and thus, as expected, the
plot shows the objective function changing in the same direction but
more rapidly when compared to Figure 4(b). These ﬁgures capture
an important trade off for a deployed system, in that by varying the
learning rate β, we trade off the speed of personalization with the
variety of selected posts.

5. RELATED WORK

Recently, there has been an increase in the number of websites
that index blogs and display a list of the most popular stories. Some
examples of such websites are Google Blog Search [4], Yahoo!
Buzz [7], Digg [3], Technorati [6], and Blogpulse [1]. Some of
these websites display posts without any manual intervention, e.g.,
Google Blog Search and Blogpulse. However, most of these web-
sites display posts which have either been handpicked by editors
or have been voted for by users of the website. Most websites that
pick posts automatically use a combination of features such as link
structure [2], trends in search engine queries [7], and the number
of times a post is emailed or shared. Currently, we are only using
features derived from the text of the posts, although in the future
we hope to incorporate the link structure between posts into our al-

gorithm. Another key difference is that most of these websites lack
the personalization functionality we provide.

In a recent paper [8], Agarwal et. al address a problem similar
to ours. Their task is to select four out of a set of sixteen stories
to be displayed on the Yahoo! homepage. The sixteen stories are
manually picked by human editors; hence, all are of high quality.
The authors use click-through rate to learn online models for each
article. Their setting differs signiﬁcantly from ours, since we tackle
the problem of selecting ten out of roughly 60,000 posts for each
eight hour segment. Moreover, as described in section 4, our data
is very noisy, and we do not have access to click-through rates.

Another line of related research is the area of subtopic retrieval
[27, 13, 11]. In subtopic retrieval, the task is to retrieve documents
that cover many subtopics of the given query.
In the traditional
information retrieval setting, it is assumed that the relevance of
each document is independent of the other documents. However,
in subtopic retrieval the utility of a document is contingent on the
other retrieved documents.
In particular, a newly retrieved doc-
ument is relevant only if it covers subtopics other than the ones
covered by previous documents. Thus, the concept of relevance
in subtopic retrieval is similar to our notion of “coverage,” which
has a diminishing returns characteristic. However, while subtopic
retrieval is query-based, we intend to cover all the popular stories
being discussed in the blogosphere.

Two common approaches to personalization are collaborative ﬁl-
tering [23, 14] and content-based ﬁltering. In collaborative ﬁlter-
ing, user preferences are learned in a content-agnostic manner by
correlating the user’s past activity with data from the entire user
community. In a content-based approach, documents are recom-
mended to a user if they are similar to documents that the user
previously liked, where similarity is based on document content.
Using a content-based approach, we provide theoretical guarantees
for personalization. Moreover, we currently do not have the kind of
user base that is needed for collaborative ﬁltering to be effective.

Leskovec et al. propose a solution to the problem of selecting
which blogs to read in order to come across all the important sto-
ries quickly [22]. Although related to our problem, a fundamental
difference is that instead of trying to select which blogs to read,
we present the user with a selection of posts from various blogs.
Moreover our approach is completely content based, whereas the
approach of Leskovec et al. is based only on the links between
blogs.
In addition, we also incorporate personalization into our
algorithm, which they do not.

There has also been extensive work on building models and an-
alyzing the structure of the blogosphere. For example, Finin et
al. [16] present a model of information ﬂow in the blogosphere.
We could potentially leverage such analysis in the future in order
to extract better features for our algorithms. Blogscope [2] is in-
tended to be an analysis and visualization tool for the blogosphere.
Unlike us, they are not trying to cover the blogosphere. Instead,
Blogscope presents the user with a search interface, and suggests
some related words based on the search query. They give a pref-
erence to words whose frequency increases by a large amount in
the past 24 hours (e.g., words with a high “burstiness”). Moreover,
they do not employ any personalization.

6. CONCLUSIONS

In this paper we describe the problem of turning down the noise
in the blogosphere. While the vast majority of blog posts are not
interesting for the average user, their quantity is truly remarkable.
For this reason, many readers suffer from information overload.
Our goal is to show them a small set of posts covering only the
important stories currently being discussed.

We start by exploring different desired properties of coverage
functions. We then formalize the notion of coverage as a submod-

296 

1.1

 

 

F(FanHouse.com)

1.08

F(Deadspin.com)

F(HuffingtonPost.com)

F(FanHouse.com)

F(Deadspin.com)

F(HuffingtonPost.com)

s
t
s
o
p
d
e
k

 

i
l
 
f

o
 
r
e
b
m
u
n

6.5

5.5

6

5

4

4.5

3.5

3

 

d
e
z

i
l

a
n
o
s
r
e
p
n
u

d
e
z

i
l

a
n
o
s
r
e
p

1.06

F

/

1.04

1.02

F

1

0.98
0

 

1.5

1.4

1.3

1.1

d
e
z

i
l

a
n
o
s
r
e
p
n
u

d
e
z

i
l

a
n
o
s
r
e
p

F

/

1.2

F

1

 

0.9
0

personalized
unpersonalized

4

5

0

1

2
epochs

3

(a)

5

10

15

Number of epochs of personalization

5

10

15

Number of epochs of personalization

(b)

(c)

Figure 4: (a) Results of the personalization user study, measuring how many posts each user liked, out of the ten presented by
TDN+LDA in each epoch. The personalized line corresponds to a learning rate β = 0.5.
(b,c) Effect of number epochs spent
personalizing to the simulated preferences of a sports fan on the objective function F, with respect to no personalization. F is
evaluated on two sports blogs and one politics blog. Learning rate β = 0.5 (b), 0.1 (c)

ular optimization problem, and present an efﬁcient algorithm to se-
lect the top stories in the blogosphere.

Next, we generalize the coverage notion to the personalized case,
where we assume that each user has his own coverage function
based on his personal preferences. We introduce the problem of
learning these coverage functions from limited user feedback. We
formalize the notion of feedback, and illustrate a simple online per-
sonalization method based on multiplicative updates of weights.
This method achieves no-regret personalization.

We derive two different algorithms based on our general frame-
work, each using different feature instantiations. Both algorithms
are efﬁcient enough that they can be run on large, real-world blog
feeds. We compare both algorithms against popular blog aggrega-
tion websites like Google Blog Search, Yahoo! Buzz, Digg, and
BlogPulse. In addition to post content, most of these websites use
richer features such as click-through rate, trends in search queries
and link structure between posts, or use human intervention to pick
posts. We present results based on simulations and a user study.
Our TDN algorithm outperforms all others except for Yahoo! Buzz
(with which it is comparable), despite having access to text-based
features only. Furthermore, our experiments demonstrate that our
algorithm can adapt to individual users’ preferences.

Our results emphasize that the simple notion of coverage we in-
troduced successfully captures the salient stories of the day. We
believe that this combination of coverage and personalization will
prove to be a useful tool in the battle against information overload.

Acknowledgments. We thank Spinn3r for providing us access to
their data. We are grateful to Geoff Gordon for helpful discus-
sions and to the reviewers for their useful comments. Tag clouds
in this paper were generated using wordle.net. This work was
partially supported by the ARO under MURI W911NF0710287
and W911NF0810242, by NSF Career IIS-0644225, and by NSF
NeTS-NOSS CNS-0625518.

7. REFERENCES
[1] Blogpulse, http://blogpulse.com.
[2] Blogscope, http://www.blogscope.net/.
[3] Digg, http://digg.com.
[4] Google Blog Search, http://blogsearch.google.com.
[5] Spinn3r, http://spinn3r.com/.
[6] Technorati, http://technorati.com.
[7] Yahoo! Buzz, http://buzz.yahoo.com.
[8] D. Agarwal, B.-C. Chen, P. Elango, R. Ramakrishnan, N. Motgi,

S. Roy, and J. Zachariah. Online models for content optimization. In
NIPS, 2008.

[9] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation.

JMLR, 2003.

[10] K. R. Canini, L. Shi, and T. L. Grifﬁths. Online inference of topics

with latent Dirichlet allocation. In AISTATS, 2009.

[11] J. Carbonell and J. Goldstein. The use of MMR, diversity-based

re-ranking for reordering documents and producing summaries. In
SIGIR, 1998.

[12] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games.

Cambridge University Press, 2006.

[13] H. Chen and D. Karger. Less is more. In SIGIR, 2006.
[14] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google News

personalization: scalable online collaborative ﬁltering. In WWW,
2007.

[15] K. El-Arini, G. Veda, D. Shahaf, and C. Guestrin. Turning down the

noise in the blogosphere. Tech. Report CMU-ML-09-103, CMU,
2009.

[16] T. Finin, A. Joshi, P. Kolari, A. Java, A. Kale, and A. Karandikar. The

information ecology of social media and online communities. AI
Magazine, 2008.

[17] J. R. Finkel, T. Grenager, and C. Manning. Incorporating non-local
information into information extraction systems by Gibbs sampling.
In ACL, 2005.

[18] Y. Freund and R. E. Schapire. Adaptive game playing using

multiplicative weights. Games and Economic Behavior, 2000.

[19] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. PNAS, 2004.
[20] S. Khuller, A. Moss, and J. Naor. The budgeted maximum coverage

problem. Information Processing Letters, 1999.

[21] M. Kinsley. How many blogs does the world need? TIME Magazine,

172(22), December 2008.

[22] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and

N. Glance. Cost-effective outbreak detection in networks. In KDD,
2007.

[23] G. Linden, B. Smith, and J. York. Amazon.com recommendations:

Item-to-item collaborative ﬁltering. IEEE Internet Computing, 7,
2003.

[24] G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of the

approximations for maximizing submodular set functions.
Mathematical Programming, 14:265–294, 1978.

[25] N. Rizzolo and D. Roth. Modeling discriminative global inference. In

ICSC, 2007.

[26] B. Smith. The hair’s still perfect. Politico, April 16, 2007.
[27] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent

relevance: methods and evaluation metrics for subtopic retrieval. In
SIGIR, 2003.

[28] B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen, and W.-Y. Ma.

Improving web search results using afﬁnity graph. In SIGIR, 2005.

297