Experiments Toward Reverse Linking on the Web

Yeliz Yesilada, Darren Lunn, and Simon Harper

Information Management Group

School of Computer Science, University of Manchester

Manchester UK

[yeliz.yesilada | darren.lunn | simon.harper]@manchester.ac.uk

ABSTRACT
Multi-headed reverse linking (incoming links) is a fundamental con-
cept of Open Hypermedia Systems. However, this bi-directionality
has been lost in the move to the World Wide Web (Web). Here,
we suggest a Web based solution for rediscovering these reverse
links, and develop a series of experiments to demonstrate our ap-
proach. Simply our algorithm involves parsing a Web server’s log
ﬁle, identifying each Web page viewed and saving an ordered list
of referrers within a ‘name–matched’ XML ﬁle. This ﬁle is then
used as a link point within a standard XHTML Web–page using a
freely available Javascript library. While we have not performed
any comprehensive user evaluation initial qualitative results sug-
gest users are positive regarding our additions and that widespread
adoption would increase user satisfaction due to constancy of the
browsing experience.

Categories and Subject Descriptors
I.7.2 [Document Preparation]: [Hypertext / hypermedia];
H.5.2 [User Interfaces]: [Interaction styles];
H.5.4 [Hypertext/Hypermedia]: [Navigation]

General Terms
Algorithms, Experimentation, Human Factors

Keywords
Hypertext, Bi-directional Linking, Inbound Links, World Wide Web

1.

INTRODUCTION

Bi-directional link traversal is a fundamental component of the
Open Hypermedia Systems (OHS) as it was ﬁrst suggested [18]
and implemented [24]. Indeed, this bi-directionality is further en-
hanced when each anchor point can also be multi–headed. These
twin concepts have both been lost in the move to the World Wide
Web (Web) interaction model so that only outbound uni-directional

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
HT’07, September 10–12, 2007, Manchester, United Kingdom.
Copyright 2007 ACM 978-1-59593-820-6/07/0009 ...$5.00.

links remain [4, 25]. However with the birth of Web 2.01 tech-
nologies, in particular Asynchronous JavaScript and XML (AJAX)
technologies [14], we suggest that we can now join together these
previously disjoint threads.

Conventional OHS propose the concept of the Linkbase as the
method to facilitate these bi-directional multi–headed links how-
ever on the Web this is impractical due to its distributed and closed
model of hypertext linkage. In this case we suggest a pragmatic
approach which aims to rediscover these inbound2 links by an anal-
ysis of Web server logs.

Problem — Browsing is a fundamental concept of hypermedia,
indeed, it is recognised that users often do not fully read Web pages
but browse/scan them looking for possible links destinations and
important terms [20]. Links are therefore important elements of
Web pages and facilitate both scanning and browsing. However, us-
ing the Web interaction model links can only be followed to pages
that the authors have actively encoded within the page (outbound
links). However, what about pages that link to the current page but
are not known by the author (incoming links)? We can see that
these links, and therefore the options for serendipitous discovery,
are lost and the users browsing experiences are limited. The litera-
ture on scanning and browsing activities [6, 19, 9] suggest that both
kinds of linking are not only involved in the process, but are also
necessary for its successful completion. Therefore, our work aims
to address the problem of the missing incoming link.

Solution — We propose that by parsing a Web server’s log ﬁle
we can identify each Web page viewed in the ﬁle store of that
server, along with the page that referred the user to it. This in-
formation can then be saved to an ordered list of referrers based on
the frequency of page hits derived from this Web trafﬁc analysis.
This list is saved within an eXtensible Mark-up Language (XML)
ﬁle which name matches the target (not the referrers) page name.
This ﬁle is then used as a link point within a standard eXtensible
Hypertext Mark-up Language (XHTML) Web–page using a freely
available Javascript library (e.g., OverLib3 or Overlibmws DHTML
Library4). Here we develop a series of experiments to demonstrate
our approach.

Contribution — By understanding the Web Hypermedia Model
(WHM) we propose the concept of the reverse–link which we de-
ﬁne as an incoming link previously traversed by one or many users.

1A phrase coined by O’Reilly Media,
referring to a sup-
posed second-generation of Internet-based services, http://
tinyurl.com/743r5.
2Inbound links, also called incoming links, backlinks, inlinks, and
inward links, are any links received by a Web page (Web node,
directory, Website, or top level domain) from another Web page.
3OverLib, http://www.bosrup.com/web/overlib/
4Overlibmws, http://www.macridesweb.com/oltest/

We then build a set of algorithms which when captured as a soft-
ware tool enable reverse–linking to be captured on the individual
pages of all Websites, so enhancing the browsing experience.

Synopsis — Prior–art in the area of incoming linking is scant
however we consider bi-directional linking and its importance for
browsing within our background section (Sec. 2). Next we present
our approach (Sec. 3) as an overview for, and pre-cursor to, the
more important section detailing our initial experiments (Sec. 4).
While we have not performed any comprehensive user evaluations
(Sec. 5) initial qualitative results suggest users are positive regard-
ing our additions and that widespread adoption would increase user
satisfaction due to constancy of the browsing experience. Our work
has also raised a number of interesting questions (Sec. 6), which we
draw together before our conclusions (Sec. 7), helping to motivate
our plans for future work.

2. BACKGROUND

Multi–headed bi-directional links are fundamental to OHS how-
ever moving them to the Web has been difﬁcult due to its closed and
distributed nature. While we do not propose true bi-directionality
for each link anchor we do see a use at the page level for the con-
cept of reverse–linking to inbound links. However, to understand
the signiﬁcance of our work we must ﬁrst look at prior–art and the
concepts of linking and browsing in the user experience.

2.1 Open Hypermedia System

The Web can be considered as a closed hypermedia system since
the links are embedded into the pages, they are uni-directional and
single-headed [15]. Whereas an open system in hypermedia simply
is the one where the reader is given the same access as the author
– links are ﬁrst class citizens so they are stored and managed sep-
arately, can be multi-headed and bi-directional [10]. Open Hyper-
media Systems are well researched by the hypermedia community
where several systems have been developed including MicroCosm,
Chimera and Devise Hypermedia [23]. Several systems have also
been introduced to provide an open hypermedia system on the Web
including DLS, DHM/WWW, Webvise and the Arakne Environ-
ment [5]. Using these systems, readers can create links and other
hypermedia structures on top of arbitrary Web pages, and can share
these links with others through the use of external linkbases.

2.2 Bi-Directional Linking

Bi-directional links are links along which it is possible to travel
in both directions [15]. This is different from the common Web
model because in this case it is only possible to travel one way
- with the browser simulating a bi-directional link traversal using
the ‘Back’ button. However, if the user has not already visited
the page the ‘Back’ button option is not available. When the Hy-
pertext Mark-up Language (HTML) was ﬁrst designed, there was
no explicit mechanism in the design to keep track of bi-directional
linking within software, as this carried additional logistical and net-
work overhead.

Bi-directional linking was originally created in OHS systems by
storing the link as two neutral points within a Linkbase. As the
linkage was independent of the page anchor points, incoming and
outgoing links could be generated on each page by dynamically
‘mashing–up’5 the Page with the Linkbase on each page request.
The Web does not have the luxury of the Linkbase and so until
recently the concept of the reverse link had not been possible to
implement. However, the Web 2.0 has given us the concept of the
backlink or the trackback. These backlinks enable the user to keep

5To use a Web 2.0 term.

track of other pages on the Web that link to a target page. In Web
2.0 terms the number of backlinks is an indication of the popularity
or importance of that Website or page, indeed, search engines often
use the number of backlinks that a Website has as one of the factors
for determining that Website’s search engine ranking (For example,
Google’s PageRank algorithm uses backlinks to help determine a
site’s rank [12]). In fact, Chakrabarti et al [7] propose using search
engines to retrieve backlinks and augment browsers to display a list
of URLs referring to a speciﬁc page. This work was conducted in
1999 and in summary, an applet was created as an extension to the
Netscape browser that talked to search engines including AltaVista
and Hotbot to retrieve backlinks. Although this work shows how
search engines could be used to create bi-directional links, the work
itself is obsolete now. Furthermore, while other meta sites also keep
track of some HTML backlinks, the data can be delayed by hours
or months, and backlink data is not kept for pages that these sites
do not watch, such as password-protected areas or dynamic pages.
Alternatives to bi-directional links are sometimes generated intra-
site by speciﬁc Website software which internally keeps track of
backlinks (Examples of this include most wiki and CMS software).
Other mechanisms have been developed to track backlinks between
disparate Web–pages controlled by organizations that are not asso-
ciated with each other; the most notable being the TrackBack be-
tween blogs6. A Trackback is simply an acknowledgement sent via
a network signal (ping) from the originating site to the receiving
site. The receptor often publishes a link back to the originator in-
dicating its worthiness. However, a Trackback requires both sites
to be Trackback enabled in order to establish this communication.
Some individuals and companies have abused the TrackBack fea-
ture to insert spam links on some blogs forcing many blogs to stop
using TrackBacks because dealing with spam has become too bur-
densome. Alternative to this approach is the usage of server log
ﬁles by particularly using the referrer request-header of the HTTP
protocol7. Chakrabarti et al [7] introduce a module extension to
Apache Web server that can be queried by browsers to retrieve a list
of referrer links of a Web page. Although this approach is similar to
ours as it is also suggests using server log ﬁles, the two approaches
are fundamentally different. Chakrabarti et al’s approach requires
changes on the browser and expects the client and the server to
negotiate on retrieving referrer links. However, in our approach by
using AJAX, backlinks are served with original pages which means
that the client does not need to be extended or changed and also
there is no need for further negotiation between the server and the
browser. Our approach provides a more integrated interface com-
pared to Chakrabarti et al [7]’s approach.

There are also some emerging technologies that are introduced
to overcome the limitations of the embedded, uni-directional links
on the Web such as XLink [26]. Although XLink is a powerful lan-
guage for representing links that offers signiﬁcant improvements, it
is not widely used or supported by existing browsers.

2.3 Browsing

Browsing is an activity that is difﬁcult to deﬁne [6], but there is
general agreement that “we all browse in various context to make
sense of the world around us” [8]. Some researchers also describe
it as a process of “picking out bits and pieces· · · selecting worth-
while information need or interest” [9]. Different disciplines look
at browsing from different perspectives [8]. Various reviews sug-
gest that browsing is a kind of searching, in which initial search
criteria or goals are only partly deﬁned or known in advance.

6See http://en.wikipedia.org/wiki/Trackback.
7HTTP/1.1, http://www.w3.org/Protocols/

Browsing involves scanning, which has been described as look-
ing, examining, or sampling, during which the person’s body or
eyes move smoothly at will [20]. Browsing also involves distinct [8]
consumer shopping behaviour that is related but not equated with
buying behaviour. Methodologically, eye movement can be a useful
indicator of browsing and has been used to test the effect of differ-
ent page layout or catalogues on browsers’ attention [8]. Browsing
is fundamentally scanning and has been related to environmental
perception and cognition. For example, sightseeing is environmen-
tal browsing as perceptual experience [8].

Although all of these views of browsing have various approaches
and provide different deﬁnitions, there seems to be an agreement on
the essential characteristic of browsing which is movement. Brows-
ing can be thought of as travel in information space, and in fact
many users refer to real world metaphors to describe browsing [17].
Indeed research suggests [19, 13] that browsing is made up of Search-
ing, the task of looking for a known target. Inquiry, the task of look-
ing to see what is available in the world. Querying, using a search
engine to submit a description of the object being sought and re-
ceiving relevant content of information. And Navigation, moving
oneself sequentially around an environment, deciding at which step
where to go.

In investigating the nature of browsing, several researchers have
attempted to establish different types of browsing [13]. Fundamen-
tally, these types have been established by considering the goal,
purpose or the information need. Search strategies have been de-
ﬁned as “a set of ordered tactics or behavioral moves that are con-
sciously selected, applied and monitored to solve the problem” [19].
Analytical search strategies are formal, discrete and deterministic,
in contrast, browsing search strategies are informal, continuous and
heuristic.
Indeed the four distinguishable browsing strategies of
scanning, observing, monitoring, and navigating have been identi-
ﬁed [19].

In summary, browsing is movement in the information space and
the user is in control of what to read or examine. While chance or
synchronicity may have some part to play in browsing behaviour
the user is still in control of the ﬁltering of the information pre-
sented. Many studies have addressed different browsing types and
strategies [17] however movement is the essential characteristic of
browsing [3]. We all browse in various contexts picking out bits and
pieces of information and selecting worthwhile information [16];
and we accomplish this by using a searching and scanning be-
haviour over organisations of the material [22, 20], interfaces to
that material [2], and feedback about the material [19].

3. APPROACH

Our solution is produced from a series of experiments performed
in the ﬁrst half of 2007, and are detailed below. Most Web servers
provide the option to store log ﬁles. These ﬁles are typically created
using the common log ﬁle format, which is supported by the ma-
jority of analysis tools [11]. Log ﬁles are typically plain text ﬁles
that include a time stamp, the page that was accessed and the query
string if the page being accessed required one. To allow ﬂexibility,
Web servers generally allow administrators to determine the values
that are logged by the server. This server logging is at the centre of
our approach, which is summarised in Fig 1 and described in the
associated list along with the experiments described in Section 4.

1. A Web server Log contains a list of remote page referrers for

a speciﬁc local Web page (see Fig 1 at 1);

3. Creation of an XML ﬁle with the same name as the matching
local Web–page (references to home/about.shtml cre-
ates home/about.xml) (see Fig 1 at 3);

4. When a client requests a page, since speciﬁc AJAX com-
ponents are encoded into that page, the client gets both the
requested page and the matching XML reverse linkbase (see
Fig 1 at 4);

5. With an open-source Javascript library, a link box is dynami-
cally generated by using the XML reverse linkbase (see Fig 1
at 5).

4. EXPERIMENTS

To realise our objective we performed a series of experiments to
determine if our approach was possible. These experiments took
the form of a number of proofs of concepts tied to our overall
approach, and involved a number of technical and manual inter-
ventions that were formative as opposed to a full implementation.
However, we feel these early proofs of concept are compelling and
are therefore of interest to the research community.

As a case study we used the Hypertext 2007 (HT07)8 Website
which is hosted on an Apache server. As it is the case with the other
Websites, the original HT07 Website only included uni-directional,
single-headed links. In the following sections, we explain the method-
ology used to realise reverse links on the HT07 pages.

4.1 Web Server Log Analysis

As described in Section 3, most Web servers provide the option
to store log ﬁles. As an initial experiment, the log ﬁle parameters
of a Web server9 running within the SIGWEB ACM10 servers were
modiﬁed to establish what results could be achieved.

Methodology — When a user selects a link within a Web page,
(Fig 1 at 1) the Web server hosting the ﬁle at the destination of
the link requires speciﬁc details from the client. As a minimum,
the server requires the ﬁle that has been requested and the destina-
tion (client IP Address) to which the requested ﬁle should be trans-
ferred. In addition to these basic facts, servers can also log more
detailed information. For example, the Apache server used dur-
ing our initial experiments allows administrators to log “common”
server information and “combined” server information within the
log ﬁle. The “common” log ﬁle pattern records the remote host
name; the remote logical username; the remote user that was au-
thenticated; the date and time; the ﬁrst line of the request that the
client made; the HTTP status code; and the bytes returned to the
client.

The “combined” log ﬁle pattern records the same information as
the “common” log ﬁle but with two additional components. These
are the HTTP request header providing the site that the client re-
ports having been referred from; and the User-Agent HTTP request
header, which provides information about the client. In our experi-
ments, we used “combined” logging format.

Outcomes — Fig 2 shows a small section of the log ﬁle retrieved
from the Apache Web Server. The log ﬁle is indicated at (Fig 1 at 1)
in our overall approach. The ﬁle has been modiﬁed by replacing the
tab delimitation (\t) with newlines (\n) in order to improve clarity.
The line numbers have also been added by the text editor.

Line 01 of the log from Fig 2 provides us with the IP address of
the client, in this case 130.88.199.206. The remote user name and
authentication was not obtained and has been represented by “-” on

2. Processing the local Web server Log enables the creation of
an ordered list of remote page references by frequency of the
reference (see Fig 1 at 2);

8HT07, http://www.sigweb.org/ht07/
9Apache Tomcat, http://tomcat.apache.org/
10SIGWEB, http://www.sigweb.org/.

Figure 1: Our Overall Approach

01: 130.88.199.206
02: -
03: -
04: [08/Mar/2007:18:30:39 +0000]
05: "GET /ht07/ HTTP/1.1"
06: 200
07: 3811
08: "http://markbernstein.org/"
09: "Mozilla/5.0 (Windows NT 5.1; en-GB;)

Gecko/20061204 Firefox/2.0.0.1"

Figure 2: Sample Fragment of a Web Sever Log File.

Lines 02 and 03. Line 04 shows that the access time was 18:30 with
Line 05 indicating that the client requested a Get method on the
page /ht07/index.php. Line 06 provides the status code initi-
ated by the server. In this example, Status Code 200 was returned,
which suggests that the request was successful. This is conﬁrmed
by Line 07 which shows that 3811 bytes of data was transferred
from the server to the client. Line 08 denotes the page that re-
ferred the client to the current page. From this we can see that the
client had previously visited http://markbernstein.org/
revealing that a link exists within the page that points to the ﬁle
/ht07/index.php on the SIGWEB Web server. Finally, Line
09 tells us that the client device was Mozilla Firefox using an En-
glish version of Microsoft Windows NT.

For the purposes of our experimentation, we were interested in
Lines 05 and 08 just to create URL pairs of referrer to Web page.
This was so that we could establish the Web pages that contained
links to pages on our server. It is these lines that we parsed to create
the XML ﬁles containing the reverse links for the Web pages on our
server.

4.2 XML File Creation and Manipulation

Methodology — The main task of the server log processor is to
parse a given log ﬁle to extract URL pairs – a pair consists of a
local page URL and a referrer page URL. For example, the HT07
server log ﬁle was processed to identify pages that were requested
from the HT07 server and the pages that these requests were made
from. For instance, from the example log ﬁle in Fig 2, the pair
“/ht07/index.php - http://markbernstein.org/” was extracted. These

extracted pairs were grouped by the page URLs based on the fre-
quency of the requests made. These groups were then used to create
an XML document for each page on the HT07 site. These XML
ﬁles were given same name as the original Web pages (e.g., “in-
dex.php” - “index.xml”). They can be considered as individual
linkbases. We chose to store reverse links in separate ﬁles because
of several reasons:

(cid:129) It is easy to maintain and update reverse linkbases regularly

instead of updating the original documents all the time;

(cid:129) These linkbases can be stored, transported, shared and searched

separately from the documents themselves;

(cid:129) These individual linkbases can potentially be used in other
applications (e.g., for identifying the most popular page of a
Website for personalisation11 purposes);

(cid:129) This approach means a clean separation between the process
of adding links into the document and getting the referrer
links form a server log ﬁle.

Although we create individual XML linkbases, we could also
create a single XML linkbase for the entire Website. However,
since we process the XML documents dynamically on the client-
side (i.e., when the user requests a page), having a single XML
document, could mean long processing delays. This would directly
effect the user’s experience with loading the Web page.

Currently, in these XML documents, we only store referrer page
URLs and their titles. In our experiments, we manually retrieved
page titles, however this can easily be automated by using differ-
ent techniques. For example, we can use Web services such as
Google12 and Amazon13, or we can retrieve and parse the refer-
rer page to get its title. Of course the latter technique can be time
consuming. But since the processing is done on the server-side in
advance, the processing speed is not an important issue that will
directly affect the user’s experience. By automating the title re-
trieval process we can also get more information about the referrer,
for example the summary or thumbnail image of the page can be
retrieved.

11Personalisation is tailoring a product to a user based on personal
details or characteristics he provides.
12Google Web Service, code.google.com/
13Amazon Web Service, aws.amazon.com/

...
<linkbase>

...
<link>

</link>
<link>

</link>
<link>

</link>
...

</linkbase>

<title>Home page of Mark Bernstein
</title>
<url>http://markbernstein.org
</url>

<title>HCI Conference and Workshops
</title>
<url>http://degraaff.org/hci/
conference.html</url>

<title>D-Lib Workshops and Conferences: 2007
</title>

<url>http://dlib.org/groups.html</url>

Figure 3: Part of an XML document that is created as a link
base to store reverse links for a particular Web page.

In our experiments, when we generated the XML reverse linkbases,

we did not do extensive quality checks for the extracted referrer
URLs. For example, we did not check whether these URLs point
to good quality, well-respected pages or to spam pages. However,
one can imagine adding a number of heuristics for quality checks
into the server log processor (see Section 6).

Outcomes — Fig 3 shows part of the XML reverse linkbase cre-
ated for the HT07 index page. This XML document was served to-
gether with the original index page on the server. It was processed
on the client-side to create a list of referrer links for that particular
page (having a simple structure meant it was very easy and quick
to process it on the client-side). Once additional information is re-
trieved about the referrer page, this XML document can easily be
extended.

4.3 Reverse–Link Placement

For placing reverse links stored in the XML linkbases to Web
pages, we chose to use AJAX, a name given to a combination of
readily-available Web technologies, and an open source Javascript
library (OverLib). AJAX is a family of technologies based on
Javascript that allows the Web page to retrieve small amounts of
data from the server without reloading the entire page. It allows
Web pages to be more interactive and behave like local applica-
tions, which are also known as “rich client” applications. Javascript
can be used from within a Web page to make a call asynchronously
to a server and request an XML document. The XML document can
then be used to augment the Document Object Model (DOM) 14) of
the originating HTML page. The main component of AJAX is the
XMLHttpRequest object of Javascript that is now supported by the
most mainstream browsers across all platforms. Using AJAX has
several advantages:

(cid:129) AJAX is based on open standards and supported by many
14The Document Object Model (DOM) is a platform- and language-
neutral interface that will allow programs and scripts to dynami-
cally access and update the content, structure and style of docu-
ments.

browsers and platforms, so AJAX provides a cross-browser
and platform solution;

(cid:129) With popular sites using AJAX (e.g., Google, Amazon, etc.),
users are becoming familiar with the AJAX interaction model;

(cid:129) Users are not required to install any software or plugins;

(cid:129) Using AJAX can also mean that we can open up the XML

reverse-linkbases to other Web 2.0 applications.

As part of our experiments, pages on the HT07 Website were
augmented with the additional AJAX components. Therefore, when
a page was requested from the HT07 site, the corresponding XML
reverse linkbase is also requested from the server. The returned
XML document is then parsed on the client-side and the list of
links are stored within the local DOM representation of the page.
After the list of links are transferred to the local DOM, they can
be displayed in different ways (e.g., as a navigation menu, a pop
up box or a ﬂuid interline15) at different parts of the page. We
decided that reverse links provide similar secondary navigation to
breadcrumbs16 so we presented reverse links in the style of bread-
crumbs [21]. We also decided to give the user option to display
or not to display these reverse links. Therefore, we added a link
next to the breadcrumbs, and when the user clicks on this link, a
multi-headed link with the list of referrers is displayed.

Outcomes — Fig 4 shows a screenshot of the home page of the
HT 2007 site. This ﬁgure shows how we placed the link to referrers
list next to the breadcrumb (“you are here” link). When the user
clicks on the link labelled as “Referred by:
these pages”’, with
the help of the OverLib library, a link box is displayed including a
list of pages referring to this particular page (see Fig 5). Although
we hide reverse links and display them on-demand, we can also
add them to the main content of the page. In our experiments, we
wanted to change the layout and the original content of the page
as little as possible, not to distract the users with these extra links.
Especially, when the page is popular, the list of referrers can get
quite big. We actually observed with the HT07 Website that the
number of referrers of the home page increased gradually since the
Website was released. Potentially there could be an over-linking
problem in the page. But since we are hiding the links in the page
and revealing them to the user on-demand, this eliminates the over-
linking problem.

5. EVALUATION

As already stated, we have not performed any comprehensive
user evaluation yet. But Chakrabarti et al. [7] present a small user
study that compare pages served with and without reverse-links,
and show that reverse-links improve the overall user experience.
Even though their application is different (i.e., it was a browser ex-
tension) from ours (i.e., backlinks are served as part of the page),
their study shows that incorporation of backlink navigation into a
browser produces some measured improvement in the quality of
the information discovered on the Web. Our informal initial qual-
itative results also support this study and suggest that users see
reverse–linking as a positive addition to the browsing experience.
Indeed, some comment that widespread adoption would change
their browsing behaviour as their serendipitous discovery of Web
based resources would be enhanced.

15Fluid interline technique displays the information about a link di-
rectly below the anchor [27].
16Breadcrumbs use a single line of text to show a page’s location in
the site hierarchy.

Figure 4: The home page of the Hypertext 2007 conference.

Figure 5: The home page of the Hypertext 2007 conference with reverse links.

Reliability — While this informal evaluation provided interest-
ing feedback it is by no-means scientiﬁcally reliable and so we in-
tend to carry out more rigourous evaluations to support our contri-
bution. In this case we plan to iteratively evaluate the effects of the
application on a small set of subjects while at the same time feed-
ing back qualitative results into our experimental prototype. We are
speciﬁcally planning to use UsaProxy [1] which is an application
that provides website usage tracking functionality using an HTTP
proxy approach. This proxy will help us to better understand the
usage of reverse-links as it provides detailed information about the
user’s interaction with the page. With this proxy, we will be able

to identify the context in which these reverse links are used and
more speciﬁcally we will be able to understand the tasks that the
additional reverse links support. All evaluation, including the ﬁnal
evaluation, will use a randomised controlled trial methodology in
an attempt to eliminate subjective bias on the part of both experi-
mental subjects, experimenters, and statisticians.

6. UNANSWERED QUESTIONS

Our experiments conclude that the server logs can be used to en-
hance Web pages with reverse links. These experiments also raise a
number of interesting research questions. Further research needs to

be conducted to better understand how to create and re-use reverse
links, and their effects on enhancing user experience.

How often do we parse the log ﬁle? In our experiments, we pro-
cessed the main log ﬁle monthly. This seems to work sat-
isfactorily but further investigations need to be conducted to
identify the most appropriate time period. If we do it too of-
ten that would adversely affect the speed and responsiveness
of the system. But if we do it infrequent then the reverse
links may be out of date.

How do we update the list of reverse links? When we re-parsed
the server log ﬁle of the HT07 site, we noticed that the num-
ber of referrers were increasing gradually. This meant that
the list presented to the user was getting bigger. To address
this we did a frequency analysis of the requests made from
the extracted referrers. However, we could also check the
timing of the requests made and create a list of referrers that
are recently visited. Here we cannot conclude that one ap-
proach is better than the other; we need to conduct further
technical evaluations to compare the usefulness of the reverse
links created based on different metrics.

How do we address the over-linking problem? If a link exists within

a menu of a Website, then we will log every page from the
server that people have clicked the link in. For example
http://www.xyz.com/page1.html, http://www.
xyz.com/page2.html ... http://www.xyz.com/
pageN.html, In this case it would be sufﬁcient to only pro-
vide a reverse link to the index page rather than overloading
the user with links. However, aggregation may loose some
links. For example http://www.cs.manchester.ac.
uk/~lunnd and http://www.cs.manchester.ac.
uk/~yesilady both contain links to the HT07 index page.
They have the same domain (i.e., http://www.cs.man
chester.ac.uk) but in reality they are separate personal
Websites.

How do we ensure the quality of suggested links? Regarding the
quality of the reverse links there are several issues that need
to be addressed. For example, in our HT07 experiments, we
observed the following cases that need special attention:

(cid:129) Some of the referrers that we suggested were actually
password-protected. In that case, not many users could
actually follow and access these suggested links;

(cid:129) Some of the referrers were actually spam pages point-

ing to the HT07 site;

(cid:129) Some pages that point to the HT07 index page were
actually re-located. Therefore the suggested URLs did
not exist anymore. For example, news articles on the
SIGWEB website are maintained by Blogger17 and they
are regularly archived. This means the URLs of the ar-
ticles change.

This list gives some examples of the kinds of quality checks
that need to be conducted. To address these, a set of heuris-
tics can be developed and encoded into the log processor to
ensure that the extracted referrer links are good quality.

Can we achieve ﬁne-grain reverse linking? In our experiments,
we only created reverse links between Web pages, but the

17Blogger, http://www.blogger.com.

question is can we achieve ﬁner-granularity? Can we ac-
tually link page fragments to page fragments. This can be
useful for example when the target page is quite long and
dense (e.g., portal pages). In the case of HT07, when a user
follows a reverse link from the index page, and if the target
page is very long and talks about HT07 at the bottom of the
page, then it might be confusing to the user why this page is
suggested as a reverse link.

Are there alternative ways of reverse linking? In this paper we
have proposed processing a log ﬁle and creating individual
XML linkbases. But with the advances in Web services, this
can be achieved in different ways. For example, a Web ser-
vice can be created that dynamically takes the log ﬁle and re-
turns a list of the reverse links. However, with this approach
there can be performance and processing delay issues. But
it would be good to conduct further experiments to compare
the pros and cons of the different methods.

7. CONCLUSIONS

With access to Web Server Data logs, we assert that it is pos-
sible to parse the data log ﬁle and extract the Websites that refer
to each individual page held on the server of interest. Clients typ-
ically do not have URLs, and so their address will be logged as
an Internet Protocol (IP) number such as 130.88.199.206. There-
fore it is relatively straight forward to distinguish client addresses
from Website addresses with the use of pattern matching. Not only
can we discover the Web pages that link to a Website, but there is
enough information held within the logs to identify intra–Website
links too. By implementing our analysis techniques we can achieve
a form of bi-directional linking while addressing the problems of
link spamming associated with the trackbacking; as the user must
traverse the link for a Web server log entry to be created.

However, we ﬁnd that problems with our experimental proto-
types do exist. Firstly the OverLib library is built using AJAX and
Javascript and this means that it is currently inaccessible to assistive
technologies of the type used by visually disabled users. Secondly,
our evaluation is not rigourous enough to come to any ﬁrm con-
clusions regarding the usefulness of our techniques. In the future
we will be looking to address both these problems by: (1) Creating
an accessible solution to OverLib; along with (2) The creation of a
user study by using the UsaProxy [1].

8. REFERENCES
[1] R. Atterer, M. Wnuk, and A. Schmidt. Knowing the user’s

every move - user activity tracking for website usability
evaluation and implicit interaction. In Proceedings of the
15th International Conference on World Wide Web, 2006.

[2] R. Barrett, P. P. Maglio, and D. C. Kellem. How to

personalize the web. In CHI ’97: Proceedings of the SIGCHI
conference on Human factors in computing systems, pages
75–82, 1997.

[3] M. Bates. The design of browsing and berrypicking

techniques for the online search interface. Online Review,
13(5):407–424, 1989.

[4] M. Bieber, F. Vitali, H. Ashman, V. Balasubramanian, and
H. Oinas-Kukkonen. Fourth generation hypermedia: some
missing links for the world wide web. Int. J. Hum.-Comput.
Stud., 47(1):31–65, 1997.

[5] N. O. Bouvin. Unifying strategies for web augmentation. In

Proceedings of the 10th International Conference on
Hypertext, pages 91–100, Germany, 1999. ACM.

[6] E. Carmel, S. Crawford, and H. Chen. Browsing in

[18] K. C. Malcolm, S. E. Poltrock, and S. Schuler. Industrial

hypertext: a cognitive study. IEEE Transactions on Systems,
Man, and Cybernetics, 22(5):865–883, 1992.

[7] S. Chakrabarti, D. A. Gibson, and K. S. McCurley. Surﬁng

the web backwards. In WWW ’99: Proceedings of the eighth
international conference on World Wide Web, pages
1679–1693, New York, NY, USA, 1999. Elsevier
North-Holland, Inc.

[8] S. Chang and R. Rice. Browsing: a multidimensional

framework. Annual Review of Information Science and
Technology, 28:231–276, 1993.

[9] J. Cove and B. Walsh. Online text retrieval via browsing.

Information Processing & Management, 24(1):31–37, 1998.
[10] H. C. Davis, W. Hall, I. Heath, G. J. Hill, and R. J. Wilkins.

Towards an integrated information environment with open
hypermedia systems. In Proceedings of the Fourth ACM
Conference on Hypertext, pages 181–190. ACM, 1992.

[11] P. M. Hallam-Baker and B. Behlendorf. Extended log ﬁle

format, w3c working draft wd-logﬁle-960323. W3C
Working Draft.

[12] M. R. Henzinger. Link analysis in web information retrieval.

IEEE Data Engineering Bulletin, 23(3):3–8, 2000.

[13] S. Jul and G. Furnas. Navigation in electronic worlds: a CHI

strength hypermedia: requirements for a large engineering
enterprise. In HYPERTEXT ’91: Proceedings of the third
annual ACM conference on Hypertext, pages 13–24, 1991.

[19] G. Marchionini. Information Seeking in Electronic

Environments. Cambridge Series on Human-Computer
Interaction. Cambridge University Press, 1995.

[20] J. Morkes and J. Nielsen. Concise, scannable, and objective:

How to write for the web, 1997. http://www.useit.
com/papers/webwriting/writing.html.

[21] J. Nielsen. Breadcrumb navigation increasingly useful.
Alertbox, April 2007. http://www.useit.com/
alertbox/breadcrumbs.html.

[22] H. Obendorf and H. Weinreich. Comparing link marker

visualization techniques: changes in reading behavior. In
Proceedings of the 12th international conference on World
Wide Web, pages 736–745. ACM Press, 2003.

[23] K. Osterbye and U. K. Wiil. The ﬂag taxonomy of open
hypermedia systems. In Proceeding of the Seventh ACM
Conference on Hypertext, pages 129–139, Washington, 1996.

[24] A. Pearl. Sun’s link service: a protocol for open linking. In
HYPERTEXT ’89: Proceedings of the second annual ACM
conference on Hypertext, pages 137–146, 1989.

97 workshop. ACM SIGCHI Bulletin, 29(4):44–49, 1997.

[25] F. Vitali and M. Bieber. Hypermedia on the web: what will it

[14] R. M. Lerner. At the forge: Beginning AJAX. Linux J.,

take? ACM Computing Surveys, 31(4es):31, 1999.

2006(151):10, 2006.

[15] D. Lowe and W. Hall. Hypermedia and the Web: An

Engineering Approach. John Wiley and Sons Ltd, 1998.

[16] P. Maglio and R. Barrett. Intermediaries personalize
information streams. Communications of the ACM,
43(8):96–101, 2000.

[17] P. Maglio and T. Matlock. Metaphors we surf the web by. In

Workshop on Personalized and Social Navigation in
Information Space, 1998.

[26] E. Wilde and D. Lowe. XPath, XLink, XPointer, and XML: A

Practical Guide to Web Hyperlinking and Transclusion.
Addison Wesley, 2002.

[27] P. Zellweger, S. Regli, J. Mackinlay, and B. Chang. The
impact of ﬂuid documents on reading and browsing: an
observational study. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, pages
249–256. ACM, 2000.

