User Behavior Driven Ranking without Editorial Judgments

Taesup Moon,
Georges Dupret

Yahoo! Labs, 701 First Ave,

Sunnyvale, CA 94089
{taesup, gdupret}
@yahoo-inc.com

Shihao Ji, Ciya Liao

Microsoft

∗

WA 98052

{shihaoji, cliao}
@microsoft.com

Zhaohui Zheng

Yahoo! Labs

94089

zhaohui@yahoo-inc.com

One Microsoft Way, Redmond,

701 First Ave, Sunnyvale, CA

ABSTRACT
We explore the potential of using users click-through logs
where no editorial judgment is available to improve the rank-
ing function of a vertical search engine. We base our anal-
ysis on the Cumulated Relevance Model, a user behavior
model recently proposed as a way to extract relevance sig-
nal from click-through logs. We propose a novel way of
directly learning the ranking function, eﬀectively by-passing
the need to have explicit editorial relevance label for each
query-document pair. This approach potentially adjusts
more closely the ranking function to a variety of user be-
haviors both at the individual and at the aggregate levels.
We investigate two ways of using behavioral model; First,
we consider the parametric approach where we learn the es-
timates of document relevance and use them as targets for
the machine learned ranking schemes. In the second, func-
tional approach, we learn a function that maximizes the be-
havioral model likelihood, eﬀectively by-passing the need to
estimate a substitute for document labels. Experiments us-
ing user session data collected from a commercial vertical
search engine demonstrate the potential of our approach.
While in terms of DCG the editorial model out-performs the
behavioral one, online experiments show that the behavioral
model is on par –if not superior– to the editorial model. To
our knowledge, this is the ﬁrst report in the Literature of a
competitive behavioral model in a commercial setting.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval; I.2.6 [Articial Intelligence]: Learn-
ing

General Terms
Algorithms, Experimentation

∗

Labs.

This work was done while the authors were with Yahoo!

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CIKM’10, October 26–30, 2010, Toronto, Ontario, Canada.
Copyright 2010 ACM 978-1-4503-0099-5/10/10 ...$10.00.

Keywords
Click-Through Data, User Behavior, Search Engines, Prob-
abilistic Model

1.

INTRODUCTION

It is quite natural to assume that users’ interactions with
the result pages of a search engine convey information about
the degree of relevance of the web pages. This idea has
inspired many researchers to use click-through logs to im-
prove user search experience. The main approach was to
show how useful an appropriate interpretation of user in-
teractions –clicks and query reformulations, in particular–
can help in constructing more performant relevance func-
tions. On the other hand, little work has been published
on the problem of learning a ranking solely based on user
interactions, entirely by-passing editorial judgment of edi-
tors. More precisely, we formulate the problem as follows;
Start with a reasonable Information Retrieval ranking func-
tion like BM25 or a good language model based approach,
collect clicks and use this information to bootstrap a full
ﬂedge, state-of-the-art function able to compete with the
best learning to rank algorithms based on tens of thousands
of query-document judgment pairs.

State of the art search engines use various algorithms that
extend the supervised machine learning framework to rank-
ing. These have become popular approaches because they
have both reasonable complexities and good scalability for
very large size of Web data. One caveat of learning to rank
methods, however, is that they require expensive manual la-
beling of documents to generate training data. The problem
is multifold: one is that the labels needs to be updated and
expanded regularly in order to maintain a large size of train-
ing data and cope with the time-varying, dynamic nature of
the Web. The other is that those labor intensive relevance
judgments may contain errors and be diﬀerent from what
the real users on the Web require. The problem is made
worse, as the internet expands, by the need for ever more
specialized search engines –called verticals for which it is
impractical, time consuming, expensive and even infeasible
to gather enough editorial judgment. For example, in this
work we carry the numerical experiments on the data of a
local search engine where it is hard for an editor to judge if
such business is a good recommendation for a user living in
a remote neighborhood he is unfamiliar with.

Contribution. In this paper, we explore two related ways
of exploiting the record of user interactions with the search

1473engine. Namely, the methods we propose attempt to learn
a ranking function directly from the user behavior data, in-
stead of editorial judgment data. The basic assumption is
that the user behavior is inﬂuenced by the overall relevance
of the search result sets, and such behaviors can be captured
by probabilistic models, treating the relevance of the search
result sets as hidden variables. Once a probabilistic model
has been decided upon, we have two possibilities:

• We can use the click-through data to estimate the doc-
ument relevance and use these as labels for a learning
to rank algorithm,

• We can relate those relevance-based hidden variables
with functions deﬁned on a set of features and learn
the relevance functions by ﬁnding the function that
maximizes the likelihood of the probabilistic model.

The second approach is more novel and we describe the gen-
eral functional learning framework it belongs to in Section 3.
The rest of the paper is organized as follows. In Section 2,
we review some prior work. Then, in Section 4, we summa-
rize a particular user behavior model, the Cumulated Rele-
vance model, which was recently introduced in [3] and that
serves as a basis for further experiments. Section 4.1 will
explain the exact procedure for using the Cumulated Rel-
evance model and functional gradient boosting method to
obtain user behavior driven ranking functions. Section 5 re-
ports experimental results, and Section 6 concludes with a
discussion and future research directions.

2. RELATED WORK

Over the past decade, a large number of learning to rank
algorithms, which require human editorial judgment labels,
have been proposed, see [7] and references therein. They
mainly focus on devising sensible loss functions as surrogates
to the true ranking loss so that minimizing such loss func-
tions would lead to a good ranking performance. The user
behavior modeling has been primarily centered around mod-
eling click-through logs of search engines. We can divide in
essentially four categories according to how click information
has been used in the literature: gaining insight into typical
user behaviors [6], estimating the relevance feature or target
values for documents from the user clicks [9, 2], deriving a
metric based on clicks to compare ranking functions [1], and
directly re-ranking the top documents retrieved from search
engine [5]. By no means, these lists are exhaustive.

3. USER BEHAVIOR DRIVEN RANKING

In this section, we describe the general framework for
learning a ranking function both in parameter and functional
space. We use user sessions as the basic data structure to
record user behavior as well as the content of the result set.
To this end, we use S to denote a user session, and it con-
sists of three components: a sequence of queries Q = {qi},
a set of documents D = {dij} presented in response to the
queries, and user interactions with those documents in the
form of clicks C = {cij }, where cij is 1 if the j-th document
of query i was clicked and 0 otherwise. Namely, we can ex-
press the session S as S (cid:2) {Q, D, C}. Once a user session
S is given, we can associate the collection of events E that
are generated from the session to describe the content of the
result sets and user behaviors.
In addition to associating
the observable events with the session S, we also assume a
set of hidden variables H that capture aspects that are not

directly observable.
In this work, our set of hidden vari-
ables only includes document relevance, but the framework
we propose is by no means limited to this choice.

The user behavior models (UBM) can then be derived
from the session data S by imposing a joint probabilistic
model on the event E and the hidden variables H, param-
eterized by the parameter set U , i.e., P (E, H|U ). In exist-
ing work, the main usage of such models was to take ad-
vantage of abundant user session data SN = {S1, . . . , SN }
and estimate the parameters of the model by maximizing
the likelihood of the observations E N = {E1, . . . , EN }. This
takes the form of ﬁnding the set of parameters U that maxi-
mize the following expression with respect to the parameters
p(En, Hn|U ), and such estimated values of
in U :
U were used as relevance features for ranking functions.

QN

n=1

P

Hn

We can go one step further in this line of reasoning and
instead of learning the values of the parameters in U , we
consider these as functions of a set of relevant features.
To ﬁx ideas, we postulate for a query and document pair
(qi, dij) the existence of a corresponding parameter uij ∈ U
that represents the degree of relevance of document dij to
query qi. We further assume that this relevance can be
estimated as a function of features that characterizes the
query document pair: uij = f (xij), where xij ∈ Rd rep-
resents a d-dimensional feature vector for (qi, dij ), and f
is a function that maps Rd into R. Then, we can directly
learn the relevance function by ﬁnding f that maximizes
QN
, and sorting documents ac-
cording to the values of f for a given query. We try to show
the eﬀectiveness of such obtained ranking function via ex-
periments.

`
En, Hn|{f (x(m)

´
ij )}

n=1

P

Hn

p

In the next sections, we illustrate the framework both in
parametric and functional space on the Cumulated Relevance
Model, a particularly simple model of the inﬂuence of docu-
ment relevance on user behavior.

4. CUMULATED RELEVANCE MODEL

Here, we brieﬂy summarize the original cumulated rele-
vance model. For simplicity, we only consider the case where
each session consists of a single query q. Then, without loss
of generality, we can express D = {di} and C = {ci}, and
the size of both sets n. We then denote I(C) (cid:2) {i : ci ∈
C, ci = 1} as the set of clicked document indices in C and,
for all i ∈ I(C), denote I(C)i (cid:2) {j : cj ∈ C, cj = 1, j ≤
i} ⊆ I(C) as the set of clicked document indices up to i-
th document. Given these notations, for given N sessions,
the cumulated relevance model tries to predict the session
success and estimates the document utilities by minimizing
L(u0, u1, . . . , un) =
u0 +
P
with (cid:2)i = 1 − 2si. For more details, re-
fer to [3]. They also partially solve the sparsity problem by
adding a Gaussian priors with mean μu and variance σ2
u to
the utilities; they introduce regularization terms and modi-
ﬁes the original loss function as

`
1 + exp

j∈I(Cm)i uj

i∈I(Cm) log

`
(cid:2)i

PN

´´´

m=1

P

`

„

minimize
u1,...,un

L(u0, u1, . . . , un) +

nX

i=1

(ui − μu)2

2σ2u

«

.

(1)

which becomes a maximum a posteriori estimation.

4.1 Functional learning

Once the loss function is deﬁned as in the previous section,
we can use the step described in Section 3 to express it in a

1474functional form. That is, by associating the utility variable
as ui = f (xi), where xi is the feature vector for i-th example,
we convert the loss function into L(u0, f ), which equals to

NX

X

“

log

1 + exp

(cid:2)i

“

`
u0 +

X

´””

f (xj )

,

(2)

m=1

i∈I(Cm)

j∈I(Cm)i

and try to minimize it in a functional space to obtain the
ranking function. We can also add the regularization term
as in (1). Since the loss function (2) takes into account the
whole document list to compute the loss, it can be seen as
a listwise loss function. The optimization of the loss func-
tions can be carried out by using functional gradient de-
scent method [4] with using regression trees as weak learners,
which results in the ranking function.

5. EXPERIMENTS ON LOCAL SEARCH

We now present the experimental results on a commer-
cial local search engine.
, i.e. a specialized search engine
dedicated to ﬁnd business of diﬀerent kinds like restaurant,
hardware, or ﬂower shops, etc.
in the neighborhood of the
user. First, we describe how we collected data, then describe
the performance metric we use. Finally, we present some re-
sults to demonstrate the eﬀectiveness of the user behavior
driven ranking functions.

5.1 Data and performance metric
Click Session Data: We collected 1.8 million sessions with
at least on click from the local search engine, which resulted
in 3 million query-document pairs. Each query-document
pair (q, d) is represented with a feature vector x, of which
dimension is more than 500.
Editorial Judgment Data: To evaluate the performance
of ranking functions derived from the diﬀerent approaches,
we also gathered a set of editorial judgment data. We ran-
domly sampled 13,916 queries from the query log, and for
each query up to the top 15 documents are retrieved. This
results in 170,431 query-document pairs. We then requested
human editors to label each query-document pair to be {Per-
fect, Excellent, Good, Fair, Bad} according to the degree of
relevance of the document with respect to the query. Since
we need to train the editorial models, the data is further
divided randomly into training set and test set, with 11,583
queries (or 141,238 query-document pairs) in training set
and 2,333 queries (or 29,193 query-document pairs) in the
test set.

P

We adopt Discounted Cumulative Gain (DCG) as our ed-
itorial metric because of its popularity and its general ac-
ceptance. The DCG at position 5 (DCG@5) of a ranked list
of documents with respect to a query q is deﬁned as a
weighted average of the scores of the documents in the rank-
2gr −1
log(1+r) , where gr ∈ {0, 1, 2, 3, 4} is
ing: DCG@5 :=
the relevance score of the r-th document in the ranked re-
sult set. gr = 4 corresponds to a “perfect” label, and gr = 0
corresponds to a “bad” label. Note that this metric would be
naturally biased toward the editorial judgment based rank-
ing function. However, we still use this metric for our oﬄine
experiments as a sanity check.

5
r=1

5.2 Results

We now move on to demonstrate the performances of the
user behavior driven learning. We compare various behav-
ioral and editorial model implementations.

5.2.1 Ofﬂine experimental result

We compared the following six diﬀerent ranking functions:
Scheme 1: Baseline. This is a hand-tuned ranking function
deployed in the local search engine for several years.
The click session data used in the experiments was
collected from the users in response to the rankings
generated from this ranking function;

Scheme 2: State-of-the-art learning to rank algorithms that
are based on editorial judgment data, such as GBDT [4]
and GBRank [9]. They are collectively refered to as
Editorial Machine Learned Ranking or “Editorial MLR”;

Scheme 3: 1-step implementation of user behavior driven
learning, i.e., directly optimizing (2) via functional gra-
dient boosting trees;

Scheme 4: 2-step implementation of user behavior driven
learning, i.e., ﬁrstly, estimate the document utilities
by optimizing (1) in parametric space, and then treat
the derived utilities as targets and apply GBDT or
GBRank to train a ranking function;

Scheme 5: Pair-wise learning based on “Skip Above” and
“Skip Next” pairs [8]. We extract pairwise preference
between documents from the click-through data. These
pairs then serve as labels for the GBRank algorithm;
Figure 1 provides an overall performance comparison in
terms of DCG. The diﬀerent curves denote the best perfor-
mances we achieve with diﬀerent parameter sets in the gradi-
ent boosting trees method and in the model. All algorithms
are limited to a maximum of 800 trees, which is enough for
all of them to converge. The editorial models training and
test sets are the product of randomly sampling the queries
with editorial judgments. Training sample size is varied and
the eﬀect on performance on the test set is shown in Figure 1
where the x-axis reports the proportion of the total set used
for training.

It is demonstrated in Figure 1 that
• As the size of training set increases, the editorial model
shows an increase in DCG@5 values on the test set.
Compared to the baseline, there is about 1.8% DCG@5
at one end, and 7.9% DCG5@5 gain at the other end.

• Scheme 3 yields a model with 1.9% DCG@5 gain over
the baseline, almost on par with the “Editorial MLR”
with 1% of training data.

• Surprisingly, the 2-step implementation of user behav-
ior learning, Scheme 4, with GBDT is worse by 1%
than the baseline in DCG terms.

• On the other hand, the 2-step implementation, Scheme
4, with GBRank has 4.9% DCG@5 gain over the base-
line. This is about 3% DCG@5 gap to the best “Editorial
MLR” trained on the full data set.

• Skip Above & Skip Next is slightly below production.
It is probably possible to optimize further this method,
but it is not too promising when compared with the
other methods. It serves essentially as an extra sanity
check.

Overall, both the 1 and 2-step methods improve over the
baseline, which shows their viability. The 1-step imple-
mentation and 2-step GBDT optimize similar log-likelihood
functions (2) and (1), respectively, we would expect simi-
lar performance. An important diﬀerence between the two
is that the utilities of the documents are learned indepen-
dently for each query in the 2-step method while they are
learned together in the 1-step one. This entails that they are
constrained to be on the same scale in the 1-step method,

1475 

 

Editorial MLR
2−step GBRank

5
@
G
C
D

10.6

10.5

10.4

10.3

10.2

10.1

10

9.9

9.8

9.7

 
0

R
T
C
d
e
z

 

i
l

a
m
r
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

 
1

Editorial MLR (GBDT)
2−step GBrank
1−step
Baseline
SkipAbove/Next
2−step GBDT

20

40

60

80

100

Percentage of Training Data (%)

2

3

4

7

8

9

10

5
6
Position

Figure 1: Performance comparison among diﬀerent
algorithms.

Figure 2: Click-through rate for Editorial and Be-
havioral models in online bucket tests. Values are
normalized by the largest CTR.

but not in parametric space. When learning the paramet-
ric utilities, GBDT, being a point-wise method, enforce the
scores to share a common scale, while GBRank, being a pair-
wise method, is sensitive only to the score diﬀerences. This
means that GBRank is more ﬂexible and this might explain
why it performs better. This also suggests that we should
revise the Cumulated Relevance model to work on diﬀerence
in utility rather than absolute values.

5.2.2 Online experimental result

Ranking functions can be compared in terms of DCG, but
they can also be compared on how users interact with them.
To gain more insight, we select the editorial model and the
behavioral model with highest DCG (2-step GBRank) and di-
verted part of the live traﬃc from the production function
towards the two functions we want to test. Figure 2 reports
the click-through rate (CTR) of both models. We observe
that the behavioral model experiences a slightly larger, sta-
tistically signiﬁcant, CTR at all positions. The average click
rank of the behavioral model is 3.85 against 3.94 for the ed-
itorial model. The average CTR at positions 1 and 2 are
larger by 3.4% and 1.8% respectively. Although simply com-
paring CTR is a tricky comparison, we believe improving
clicks on all positions clearly is a positive signal for ranking
quality. As far as we know, this is the ﬁrst record in the
Literature of a purely user behavior model showing perfor-
mance on par or better than an editorial model in comercial
settings. This also demonstrates the limitations of DCG.

6. CONCLUSION AND FUTURE WORK

We approached the problem of using the click-through
logs of user interactions with a search engine to construct
a new ranking function that adapts to users. The goal is
ambitious as we want this new function to show comparable
performance to a state-of-the-art ranking algorithm trained
on tens of thousands query-document of editorial judgments.
We setup a general framework and used the cumulated rele-
vance model as a concrete example to carry out experiments.
Although the DCG of the editorial based model was higher
than the user behavior model in the oﬄine experiment, we
observed that in the online experiments, users seem to prefer
it. To our knowledge, this is the ﬁrst report in the Literature
of a ranking function trained exclusively on click data that
outperforms or is at least on par with the state-of-the-art,

heavily trained editor based ranking function. As for future
work, we plan to relax some assumptions of the cumulated
relevance models to further improve the performance of the
user behavior driven ranking models.

7. REFERENCES
[1] B. Carterette and R. Jones. Evaluating search engines

by modeling the relationship between relevance and
clicks. Advances in Neural Information Processing
Systems, 20:217–224, 2008.

[2] O. Chapelle and Y. Zhang. A dynamic bayesian

network click model for web search ranking. In WWW
’09: Proceedings of the 18th international conference on
World wide web, pages 1–10, New York, NY, USA,
2009. ACM.

[3] G. Dupret and C. Liao. Cumulated relevance: A model

to estimate document relevance from the clickthrough
logs of a web search engine. In Proceedings of the third
International ACM Conference on Web Search and
Data Mining (WSDM), 2010.

[4] J. Friedman. Greedy function approximation: a

gradient boosting machine. Technical report, Stanford
University, 1999.

[5] S. Ji, K. Zhao, C. Liao, Z. Zheng, G. Xue, O. Chapelle,
G. Sun, and H. Zha. Global ranking by exploiting user
clicks. In Proceedings of the 28th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 35–42, 2009.

[6] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. In Proceedings of ACM SIGIR 2005,
pages 154–161, New York, NY, USA, 2005. ACM Press.
[7] T. Y. Liu. Learning to Rank for Information Retrieval.

Now Publishers, 2009.

[8] F. Radlinski and T. Joachims. Evaluating the

robustness of learning from implicit feedback. In ICML
Workshop on Learning In Web Search, 2005.

[9] Z. Zheng, H. Zha, K. Chen, and G. Sun. A regression

framework for learning ranking functions using relative
relevance judgments. In Proccedings of Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval, 2007.

1476