K-Nearest Neighbor Search for Fuzzy Objects

Kai Zheng#

Pui Cheong Fung#

Xiaofang Zhou#∗

#School of Information Technology and Electrical Engineering

The University of Queensland

Brisbane 4072, Australia

∗NICTA Queensland Research Laboratory

{kevinz, g.fung, zxf}@itee.uq.edu.au

ABSTRACT
The K-Nearest Neighbor search (kNN) problem has been investi-
gated extensively in the past due to its broad range of applications.
In this paper we study this problem in the context of fuzzy objects
that have indeterministic boundaries. Fuzzy objects play an impor-
tant role in many areas, such as biomedical image databases and
GIS. Existing research on fuzzy objects mainly focuses on mod-
elling basic fuzzy object types and operations, leaving the process-
ing of more advanced queries such as kNN query untouched. In
this paper, we propose two new kinds of kNN queries for fuzzy ob-
jects, Ad-hoc kNN query (AKNN) and Range kNN query (RKNN),
to ﬁnd the k nearest objects qualifying at a probability threshold
or within a probability range. For efﬁcient AKNN query process-
ing, we optimize the basic best-ﬁrst search algorithm by deriving
more accurate approximations for the distance function between
fuzzy objects and the query object. To improve the performance
of RKNN search, effective pruning rules are developed to signiﬁ-
cantly reduce the search space and further speed up the candidate
reﬁnement process. The efﬁciency of our proposed algorithms as
well as the optimization techniques are veriﬁed with an extensive
set of experiments using both synthetic and real datasets.

Categories and Subject Descriptors
H.2.8 [Database Applications]: Spatial databases and GIS

General Terms
Algorithms

Keywords
nearest neighbor query, fuzzy database, probabilistic database

1.

INTRODUCTION

K-Nearest Neighbor (kNN) search is one of the most important
operations in spatial DBMS, due to its broad range of applications

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGMOD’10, June 6–11, 2010, Indianapolis, Indiana, USA.
Copyright 2010 ACM 978-1-4503-0032-2/10/06 ...$10.00.

including molecular biology [4], medical imaging [13], multime-
dia databases [26] and so on. After an extensive study on this
topic during the last decades, numerous indexing and searching
strategies have been proposed [19, 11, 13, 27, 32, 33]. Recently,
with the proliferation of mobile computing, kNN queries have also
been extended to moving object databases [5, 30] and uncertain
databases [8]. These work has collectively made signiﬁcant ad-
vances in improving the efﬁciency of search algorithms and en-
riching the queries to support more complex data types. To the best
of our knowledge, all these work assume the underlying data to be
crisp objects, which means their compositions and boundaries are
deterministic. However, in some real applications such as biomed-
ical image analysis and geographical information systems, the data
may not satisfy this assumption.

Microscope images are typical sources of such kind of non-crisp
data. Nowadays, as the high-throughput microscopes are producing
images much faster than before, the huge size of the datasets rules
out the traditional approach of identifying objects and relationships
manually. Instead, we must rely on automatic techniques. How-
ever, it is often impossible to interpret the objects in microscope
images unequivocally due to the limitation of image resolution and
interference of electronic noises. In order to reﬂect the uncertainty
embedded in images and offer subsequent analysis more informa-
tion to work with, probabilistic mask is produced on the extent of
identiﬁed cells by probabilistic segmentation [14]. For example,
Figure 1 shows a typical cell image in biomedical analysis. The
boundary of the cell cannot be identiﬁed easily, i.e. it is not crisp.
Under the model of probabilistic mask, different pixels in the image
will be assigned different probabilities to indicate the likelihood
that the pixel belongs to the cell. By this means, each object is
transformed into a collection of points with probabilities. As such,
uncertainty lies in their compositions, i.e., a point may or may not
belong to the object. Therefore, they are essentially different from
the uncertain databases in which the objects are assumed to have
probabilistic locations at query time.

The concept of probabilistic mask essentially represents the cells
in images as fuzzy objects. Although fuzzy objects have long been
studied in GIS community [25, 28, 2, 22], common spatial queries
such as range and kNN queries still remain uninvestigated at large.
In this paper we will address the problem of searching the k nearest
objects in Euclidean space over large fuzzy dataset. This type of
query has many applications in the biomedical ﬁeld such as brain
aging study [21], Alzheimer’s disease analysis [17] and so on.

Before stepping further to propose our own model, we have to
raise the question: does the fuzziness of objects change the nature
of traditional kNN problem? Or can we adopt existing solutions

699shrinking property of fuzzy objects and monotonicity of α-
distance.

• Effective heuristic rules, including pruning most disqualify-
ing objects and accelerating candidate reﬁnement, are devel-
oped to improve the efﬁciency of RKNN query processing.

• Extensive experiments are conducted on both synthetic and
real datasets to verify that the proposed algorithms as well
as their optimization mechanisms achieve satisfactory per-
formance.

The remainder of this paper is organized as follows. In Section
2, the fuzzy object model and deﬁnitions of kNN queries are given.
Proposed algorithms for answering AKNN and RKNN query are
presented in Section 3 and 4, followed by a brief analysis to esti-
mate the average number of object access in Section 5. We show
the experiment results in Section 6 and discuss the related work in
Section 7. Finally we conclude the paper in Section 8.

2. MODELS AND QUERIES

In this section, we ﬁrstly introduce a fuzzy object model based
on fuzzy set theory. Then we deﬁne a distance function to measure
spatial closeness between two fuzzy objects in Euclidean space. Fi-
nally, two new kNN queries are proposed in order to meet different
user purposes. Table 1 summarizes the notations we use through
out the paper.

Notation
D
α
As
Ak
Aα
||a − b||
dα(A, B)
d+(−)
α
MA
MA(α)
MA(α)∗
UA
Ω(A)

(A, B)

Deﬁnition
fuzzy object dataset
probability threshold
the support set of fuzzy object A
the kernel set of fuzzy object A
the α-cut of fuzzy object A
Euclidean distance between point a and b
α-distance between A and B
upper (lower) bound of dα(A, B)
the MBR of As
the MBR of Aα
the approximated MBR of Aα
the distinct membership value set of A
the critical probability set of A

Table 1: Summarization of notations

2.1 Fuzzy Object Model

Fuzzy objects are usually modeled by fuzzy sets [34], which are
characterized by their membership function µ : Rd → [0, 1], map-
ping any element in the object space to a real value µ(x) within the
interval [0, 1]. An element mapped to zero means that the mem-
ber is not included in the fuzzy set, while one describes a fully
included member. Values strictly in between characterize the fuzzy
members. Fuzzy objects can be deﬁned in continuous space if a
continuous membership function can be given. But in real applica-
tions, such a membership function is often not explicitly available
due to the diversity of fuzzy objects. Besides, fuzzy objects iden-
tiﬁed from raster images are normally represented by pixels which
are actually discrete points. For these reasons, we adopt a very
general discrete form to model fuzzy objects.

Figure 1: A typical cell image in biomedical analysis. Darker pixels
have higher probability of belonging to the cell.

to support this type of data? The most common type of nearest
neighbor query is the point-kNN query that ﬁnds the k points from
a dataset which are closest to a query point. If we want to plug the
fuzzy type into existing point-kNN algorithms, an object should be
represented by single point. Then a problem arises: how to ﬁnd
this point? Usually it is not easy to choose a suitable represen-
tative point because the underlying object (e.g. neuron cells) has
complex shape, and more importantly, not all parts of the object
are equally important (kernel vs. boundary) in terms of their con-
ﬁdence. Hence choosing arbitrary points will cause considerable
information loss and even produce misleading results.

To address this problem, we identify the key issue as how to de-
ﬁne a meaningful query in which the fuzzy information is taken
into account along with spatial proximity.
In this work, instead
of mixing the probability and distance into some uniﬁed similarity
function, we offer the users freedom to choose the conﬁdence level
on which the kNN set is required. Speciﬁcally, we introduce proba-
bility threshold as a user-deﬁned parameter in the kNN query. Once
the threshold is given, we can know which parts of objects should
be counted in and then distances between objects and queries are
also adjusted accordingly. Users can beneﬁt from this kind of query
by tuning the probability threshold to compare outcomes on differ-
ent fuzzy levels. Considering the biomedical image analysis again
as an example, if the nearest cells are needed merely based on the
clearest region (e.g. kernel), one can specify a high threshold. On
the other hand, if he/she wants to perform the search by considering
fuzzier region, a query with low threshold can be issued. The ma-
jor difference of our new queries with the traditional kNN queries
is that the fraction of objects which will be taken into consideration
is unknown until the query is given. Consequently, while this novel
query offers more ﬂexibility to users, it also brings difﬁculties to
our indexing and search algorithms, which will be investigated and
overcome in the rest of the paper.

To sum up, we make the following major contributions:

• We deﬁne a distance function, α-distance, which measures
the spatial proximity between two fuzzy objects. The key
difference from other distance functions like Lp-norms is that
it takes a probability value, α, as the parameter so that users
can control the conﬁdence level based on which the distance
is evaluated.

• Two new kinds of kNN queries are presented, which returns
the k nearest fuzzy objects, at single probability threshold
and a range of probability thresholds, respectively.

• We propose efﬁcient algorithm to answer AKNN query by
utilizing R-tree index. Tightness of lower and upper bounds
of distance are further improved by taking advantage of the

700Similar as the fuzzy sets, we can also deﬁne the following terms

Figure 2: α-distance of fuzzy objects

DEFINITION 1. A fuzzy object in d-dimensional space is rep-

resented by a set of probabilistic spatial point

A = {(cid:104)a, µA(a)(cid:105)|µA(a) > 0}

where a is a d-dimensional point and µA(a) is the membership
value of a which indicates the probability of a belonging to A.

for convenience of use.

DEFINITION 2. Given a fuzzy object A, the set As = {a ∈
A|µA(a) > 0}, Ak = {a ∈ A|µA(a) = 1} and Aα = {a ∈
A|µA(a) ≥ α} is called the support set, the kernel set and the
α-cut of A, respectively.

In this paper, we assume the kernel of any fuzzy object is not
empty. This is reasonable since that once an object can be identi-
ﬁed, there must be some certain information belonging to it; other-
wise the object itself may not exist.

In order to perform the nearest neighbor search, we need a scor-
ing function to measure spatial closeness between two fuzzy ob-
jects in space. There exists some work [6, 7] in mathematics and
image processing ﬁeld which proposed to measure the distance be-
tween fuzzy sets. The basic idea is to calculate the distance for each
α-cut and then do integration over the entire interval [0,1]. The ﬁ-
nal score calculated in this way is actually the expected distance
by treating the probability as the weight of each distance. Based
on this deﬁnition, a fuzzy object with low probability region may
never be regarded as the nearest neighbor even it is very close to
the query object in terms of spatial locations. In other words, users
cannot explore the different possibilities of outcomes inherently ex-
isting in reality, because the information with low probability can
be easily dominated and ignored. In the light of this observation,
we in this paper deﬁne the distance between fuzzy objects as fol-
lows.

DEFINITION 3. For two fuzzy objects A and B, their α-distance

is given by the function:

dα(A, B) =

min

||a − b||

(cid:104)a,b(cid:105)∈Aα×Bα

where α is a user-speciﬁed probability threshold.

The merit of this distance deﬁnition is that we do not mix proba-
bilities into the ﬁnal score computation. On the contrary, we leave
it as a parameter for users to set. Once the probability threshold is
given, objects are defuzzied to their α-cuts. Then we adopt the min-
imum distance as their distance measurement. Minimum distance
is commonly used in the cases that an object cannot be abstracted
by single point due to its relatively large size. For example, when
we say a coffee shop is beside (near to) a football stadium, their
minimum distance is what we really refer to. From the computa-
tional aspect, calculating minimum distance is equivalent to ﬁnding
the closest pair (CP) between two point sets, which is well studied
in both computational geometry [18] and database area [9].

The advantage of this deﬁnition is that users can explore the
possible distances between two fuzzy objects by setting different
It is easy to verify that the α-distance is
probability thresholds.
a monotonically non-decreasing function of α, i.e., ∀α1 < α2,
dα1 (A, B) ≤ dα2 (A, B). Figure 2 exampliﬁes the α-cuts of two
fuzzy objects and their corresponding α-distances. We would like
to clarify that there is no assumption for the probability distribution
of fuzzy objects in this paper. The monotonicity of the α-distance
directly comes from the shrinking property of α-cut.

2.2 KNN Queries

In this subsection, we propose two new types of kNN queries for

fuzzy objects, i.e., ad-hoc kNN query and range kNN query.

DEFINITION 4. Given a fuzzy object dataset D, natural number
k, probability threshold α and query (fuzzy) object Q, ad-hoc kNN
(AKNN) query retrieves k objects from D which have smallest α-
distances with respect to Q.

The intuition behind this query deﬁnition is to let users choose
the conﬁdence level of information in the objects based on which
the nearest neighbors will be derived. Furthermore, one can even
try different probability thresholds to examine the differences of the
results, to make sure no valuable information is discarded. Moti-
vated by this application, we generalize AKNN query by replacing
single probability threshold with a range of probability thresholds,
resulting in our second query deﬁnition.

DEFINITION 5. Given a fuzzy object dataset D, a natural num-
ber k, a probability range I = [αs, αe] and query (fuzzy) object Q,
range kNN (RKNN) query returns a set of objects {(cid:104)A, IA(cid:105)|IA ∈
[αs, αe]}, where A belongs to the k nearest neighbor set at any
probability α ∈ IA. IA is called the qualifying range of A.

RKNN query is more powerful (yet more computationally chal-
lenging) since it allows the user to specify a range of probability
thresholds and returns all the possible nearest neighbors along with
their qualifying ranges. In the above query deﬁnitions, the order of
k nearest neighbors is not important (order insensitive).

Consider Figure 3 where the α-distances of four objects with re-
spect to the query Q are shown.
If we set probability threshold
to 0.4, ad-hoc 2-NN query will return the set {A, B} as the re-
sult; but the set {A, C} turns to be result if α is changed to 0.5.
Furthermore, if the user issues a RKNN query with k = 2 and I =
[0.3, 0.6], the result set should be {(cid:104)A, [0.3, 0.6](cid:105), (cid:104)B, [0.3, 0.45] ∪
(0.55, 0.6](cid:105), (cid:104)C, (0.45, 0.55](cid:105)}.

Figure 3: KNN queries

AB0.30.50.30.5d0.3(A,B)d0.5(A,B)01ABCD0.60.30.450.55αdα701The new kNN queries for fuzzy objects are quite different from
the traditional ones such as point-NN queries or continuous kNN
queries, in the sense that each object may be fully, partially or not
involved at all in the query evaluation, depending on the given prob-
ability threshold. In other words, objects themselves are not deter-
mined until the query is issued, which poses great challenges on the
efﬁciency of search algorithms. In the following two sections, we
will try to overcome these difﬁculties and design efﬁcient solutions
to answer these queries.

3. AKNN QUERY PROCESSING

3.1 Basic AKNN Search

The most straightforward approach for answering AKNN query
is to linearly scan the whole dataset. For each fuzzy object, we
calculate its α-distance with the query object and keep the top-k
results in a heap. However, this method could be both CPU and IO
intensive. First, the evaluation of α-distance is quadratic with the
number of points in fuzzy objects. In addition, the whole dataset
usually cannot ﬁt into the memory due to its high space cost. There-
fore, to improve the efﬁciency, we employ the branch-and-bound
technique to prune the search space with the help of R-tree index.
But we make some modiﬁcations to suit the characteristic of fuzzy
objects. In particular, each leaf node of the R-tree corresponds to
a fuzzy object. Instead of storing all the points, we just keep the
MBR of its fuzzy region in the main memory along with a pointer
which refers to the actual location on hard disk. Then the R-tree
is traversed in best-ﬁrst paradigm using the following principles:
a) when an intermediate entry is encountered, we visit its subtree
only if it may contain any qualifying objects; b) for a leaf node, we
retrieve its corresponding fuzzy object from external storage only
if it may belong to the k nearest neighbors.

Algorithm 1 sketches the main structure of this process. Similar
to the original BFS method, we maintain a priority queue which
stores intermediate or leaf index entries. In the traversal process,
the algorithm will encounter three kinds of elements popped from
the priority queue, which must be treated differently.

• If the popped element is an intermediate node, we insert each
child node back to the queue together with its minimum dis-
tance to MQ(α), i.e., the MBR of Qα.

• The second possibility is that we encounter an object which
has been retrieved from hard disk. Because the associated
value is already the actual α-distance to the query instead of
a lower bound, this object is guaranteed to be closer than any
other object left in the queue. So we add it to the result set.

• If a leaf node is encountered, we retrieve the corresponding
object from hard disk, evaluate its α-distance with Q and
insert it into the queue.

In the above process, we use the minimum distance between the
MBRs of fuzzy objects to serve as a simple lower bound for their α-
distance. Compared to the α-distance, the evaluation of minimum
distance is much more efﬁcient and does not require the retrieval
of the object. In d-dimensional space, we denote a MBR by M
= (M 1+, M 1−, . . ., M d+, M d−), where M i+ (M i−) is the up-
per (lower) bound of its i-th dimension. The minimum distance
between MA and MB can be evaluated by:

M inDist(MA, MB) =

(1)

v
u
u
t

d

X

i=1

l2
i

Algorithm 1: Basic AKNN Search
Input: root, Q, α, k
Output: N N : the result set
initialize priority queue H;
enqueue (cid:104)root, M inDist(MQ(α), Mroot)(cid:105) into H;
while |N N | < k and H is not empty do

E ← dequeue H;
if E is an intermediate entry then
for each child entry V of E do

enqueue (cid:104)V, M inDist(MQ(α), MV )(cid:105);

else if E is an object then

add E to N N ;

else

probe E and enqueue (cid:104)E, dα(E, Q)(cid:105);

1
2
3
4
5
6
7

8
9

10
11

12

return N N ;

where

li =

8
<

:

A − M i+
B ,
B − M i+
A ,

M i−
M i−
0,

A > M i+
B > M i+

if M i−
if M i−
otherwise

B

A

3.2 Improving the Lower Bound

The AKNN search algorithm adopts M inDist between the MBRs

of fuzzy objects as the lower bound of their α-distance. Though
computationally simple, this lower bound is relatively loose, espe-
cially in the cases that α is set high. This is due to the fact that
the α-cut of a fuzzy object shrinks as α increases. Accordingly, the
size of MBR of its α-cut reduces as well. However, we approxi-
mate the object by the MBR of its support all the time, making the
lower bound much smaller than the actual distance. In order to im-
prove it, the key is to use more accurate MBR to represent the α-cut
whenever α changes. Of course it is not realistic to pre-compute the
MBR for every possible α-cut since it is extremely space costly. Is
there any way to capture the shrinking trend of fuzzy objects, in the
mean time cost little extra space?

Given a fuzzy object A in d-dimensional space, we denote the

MBR of its α-cut by
MA(α) = (M 1+
A (α) (M i−

A (α))

A (α), M d−

A (α), . . . , M d+

A (α), M 1−
where M i+
A (α)) represents the upper (lower) bound of
the i-th dimension of Aα. Without loss of generality, we just use
the M i+
A (α) with arbitrary i to illustrate our idea, since the same
techniques apply for other dimensions as well as the lower bounds.
By the deﬁnition of fuzzy object, Aα will gradually shrink from
In
A (1). Let δ(α)
A (1), i.e., δ(α) =
A (1)|, then we can obtain the boundary function
A by calculating all pairs (cid:104)α, δ(α)(cid:105) for each α ∈ UA,

the support As to the kernel Ak as α increases from 0 to 1.
the mean time, M i+
denote the difference between M i+
|M i+
(bf) for M i+
i.e.,

A (α) will also approach M i+

A (α) and M i+

A (α) − M i+

bf = {(cid:104)α, δ(α)(cid:105)|α ∈ UA}

where UA is the set of all distinct membership values of A, i.e.,
UA = {r ∈ (0, 1]|∃a ∈ A, µA(a) = r}. Naturally, from the
shrinking property of α-cut, we have ∀αi < αj, δ(αi) ≥ δ(αj).
If we treat bf as a series of 2d points, it should be plotted as a de-
creasing curve, as shown by the dashed line in Figure 4. We would
like to approximate this bf so that with any give α, we can have
a better estimation for the Aα. Ideally, bf can be approximated

702by arbitrary function. But computing and storing a linear function
need considerably less overhead than a higher order function. For
this reason, we focus on using a linear function to approximate bf
and leave other functions as future work.

There are many applications where it is necessary to determine a
classical regression line without any constraint conditions. A con-
ventional regression line is to ﬁnd the parameters (m, t) of the lin-
ear function y = m · x + t which minimizes the least square error.
This line, however, is not a conservative approximation of the bf
and hence cannot satisfy the lower bounding property for the MBR.
In order to guarantee no false dismissals, we need to ﬁnd a line
which minimizes the above condition while meeting the constraint
that the estimated y values are more than or equal to the actual δ
values, i.e., (m · α + t) ≥ δ(α). We formally deﬁne this line as
follows.

DEFINITION 6. The optimal conservative approximation of

the boundary function is a line

Lopt : y = mopt · x + topt

with the following constraints:

1. Lopt is a conservative approximation of δ, i.e.,

∀α ∈ UA, δ(α) ≤ mopt · α + topt

2. Lopt minimizes the mean square error, i.e.,

((mopt · α + topt) − δ(α))2 = min

X

α∈UA

Then the only problem left is how to construct this optimal line
when the boundary function is given. We can use the algorithm
proposed by Achtert et al. [1] where they try to conservatively ap-
proximate the k-nearest neighbor distances for every k by a linear
function. We brieﬂy summarize this algorithm below.

• First, this optimal line must interpolate at least one point,
called anchor point, of the upper convex null (UCH) of the
bf . The U CH is a sequence extracted from bf ,

U CH = ((cid:104)α1, δ(α1)(cid:105), . . . , (cid:104)αu, δ(αu)(cid:105)

where α1 = 0, αu = 1 and ∀i < j, αi < αj. The most
important property of U CH is that the line segments com-
posed by connecting adjacent points form a “right turn”, i.e.,
the slopes of the line segments are monotonically decreas-
ing. Given the bf , its U CH can be constructed efﬁciently in
linear time [3].

• Next, a bisection search is performed to ﬁnd the correct an-
chor point. The algorithm selects the median point p of the
U CH as the ﬁrst anchor point and computes its anchor opti-
mal line (AOL), which is a line interpolating the anchor point
while minimizing the objective function. (a) If both the di-
rect predecessor and successor of p are not above AOL, the
global optimal line is found. (b) If successor (predecessor)
of p is above AOL, the algorithm proceeds recursively with
the right (left) half of the U CH.

Figure 4 illustrates the U CH as well as the Lopt. Once the Lopt
has been derived, it can be used to estimate the MBR of α-cut of a
fuzzy object A, by the following formula,
A (1) + (mi+
A (0)
A (1) − (mi−
A (0)

 M i+
M i+
 M i−
M i−

opt(A) · α + ti−

opt(A) · α + ti+

A (α)∗ = max

A (α)∗ = min

opt(A)),

opt(A)),

M i−

M i+

(2)

Figure 4: Approximating boundary function

The purpose of using min (max) operator is to assure MA(α)∗
not worse than the MBR of As. Moreover, the conservative prop-
erty of Lopt guarantees that MA(α) is always enclosed by MA(α)∗.
Then we can safely use the M inDist between MA(α)∗ and MQ(α)
as a tighter lower bound for their α-distance, i.e.,

d−
α (A, Q) = M inDist(MA(α)∗, MQ(α))

Figure 5 demonstrates the improvement of the lower bound, where
d−
α is closer to the actual α-distance compared to the original M inDist.
To optimize the basic AKNN search by d−
α , we just need to addi-
tionally store the Lopt (i.e., mopt and topt) as well as the MBR
of the kernel for each fuzzy object in the leaf node. Whenever a
leaf node A is about to be inserted into the queue, we compute the
tighter MBR, MA(α)∗, using the additional information and eval-
uate d−

α (A, Q) as the associated value of this entry.

Figure 5: Improvement of lower bound

3.3 Lazy Probe

The basic algorithm probes the object and evaluates its actual
distance with query object, whenever it encounters a leaf node. For
the point object, it is suitable since the distance evaluation is very
fast. But for the fuzzy object, we may want to delay this costly
evaluation as long as possible. This motivates us to propose a lazy
probe optimization.

The main idea is to maintain another priority queue G which can
accommodate at most k elements as a buffer. The search principle
is still the same with the basic method. But when a leaf node E
is dequeued, we do not immediately probe the object it refers to.
Instead, we compare its distance lower bound against the upper
bound of every element U already in the buffer G and re-insert E
into G. If there exists some element U in G satisfying d+
α (U, Q) <
d−
α (E, Q), we can be sure that U is better than all objects left in H.
Even though we cannot decide whether U is better than the objects
in G, since there are at most k elements in G, U is guaranteed to be
in the top-k.

This algorithm will not retrieve the object until it has to do so,
i.e., the buffer G is overﬂow. In other words, the lazy probe makes
all the object retrieval mandatory. Algorithm 2 illustrates the main

01LoptUCHδαM  (α)QM  (0)AM  (α)*AMinDistdα(A,Q)dα(A,Q)-703structure of this optimization. We use the maximum distance be-
tween MA(α)∗ and MQ(α) as the upper bound of their α-distance,
which can be computed by the following equation given two MBR,
MA and MB in d-dimensional space.

M axDist(MA, MB) =
v
u
u
t

max{|M i+

X

d

i=1

A − M i−

B |, |M i−

A − M i+

B |}2

(3)

Figure 6 shows how this optimization works. Suppose a new
AKNN query is issued from Q. At some stage of the search, the pri-
ority queue H contains leaf nodes A, B, C and queue G is empty.
Then A is popped from the queue ﬁrst since it has smallest d−
and re-inserted into G. The next element dequeued is B. Since
d−
α (B, Q) < d+
α (A, Q), we insert B into G as well. Afterwards, C
is popped and found that d−
α (A, Q), then A can be re-
moved from G and directly added to the result set without probing
on hard disk.

α (C, Q) > d+

α (A, Q),

Algorithm 2: Lazy Probe
Input: root, Q, α, k
Output: N N : the result set
initialize priority queue H, G;
enqueue (cid:104)root, M inDist(MQ(α), Mroot)(cid:105) into H;
while |N N | < k and H, G is not empty do

probe E and enqueue (cid:104)E, dα(E, Q)(cid:105) into G;

if |G| > k − |N N | then

E ← dequeue G;
if E is a leaf node then

else

add E to N N ;

else

E ← dequeue H;
if E is a leaf node then

add the element U of G which satisﬁes
d+
α (U, Q) < d−
from G;
add E into G;

α (E, Q) into N N and remove U

else

for each sub-entry V of E do

if V is a leaf node then

enqueue (cid:104)V, d−

α (V, Q)(cid:105) into H;

else

enqueue (cid:104)V, M inDist(V, Q)(cid:105) into H;

1
2
3
4
5
6
7

8
9

10
11
12
13

14

15

16

17

18

19

20

Figure 6: Lazy probe optimization

21

return N N ;

3.4

Improving the Upper Bound

According to the deﬁnition of α-distance, for any point a ∈
A, b ∈ B, if µA(a) ≥ α, µB(b) ≥ α, then the distance between a
and b naturally upper bounds the α-distance of A, B. In the light
of this observation, we propose to improve the upper bound of α-
distance by the following ofﬂine processing.

• We randomly choose a point from the kernel of each fuzzy
object A as its representative point, denoted by rep(A), and
store it in the leaf node of the R-tree.

• For the query object Q, we randomly sample n points from

its α-cut and form the sample set Q(cid:48)

α, where n (cid:28) |Qα|.

LEMMA 1. Given a fuzzy object A and a query object Q, their
α-distance is upper bounded by the minimum distance between
rep(A) and Q(cid:48)

α, i.e.,

d+
α (A, Q) = min
q∈Q(cid:48)
α

||rep(A) − q|| ≥ dα(A, Q)

The proof is omitted due to its trivialness. Since n is quite small
compared to |Qα|, the evaluation of d+
α can be very efﬁcient. Be-
sides, it does not require the retrieval of the fuzzy object, hence will
not incur extra I/O cost. The effect of the improved upper bound is
shown in Figure 7, where q1, q2, q3, q4 are the sampled points from
Qα.

4. RKNN QUERY PROCESSING

The AKNN query restricts the user to get the kNN set on a single
probability threshold. But sometimes, they may want to to see how
the kNN set varies on different thresholds. In such a case, they can

Figure 7: Improvement of upper bound

issue an RKNN query where a probability range instead of a single
probability threshold is accepted.

To process the RKNN query, the most straightforward approach
is to issue an AKNN query at every α within the range. Even
though the number of probabilities is inﬁnite in the continuous
range, we can always get the exact results by enumerating all val-
ues in UD which is the universe of all membership values of the
object set, since the α-distances of all objects with respect to Q do
not change at α /∈ UD. However, due to the huge cardinality of
UD, the cost of this method could become prohibitive. We call this
method naive approach.

In the sequel, we will introduce a basic RKNN search approach
which is more efﬁcient than the naive approach. Moreover, we
develop several pruning rules to further improve the performance.

4.1 Basic RKNN Search

Before discussing the algorithm, we ﬁrst introduce the concept

of critical probability set.

DEFINITION 7. Given a fuzzy object A and a query object Q,

MQ(α)MA(α)*MC(α)*MB(α)*rep(A)M  (0)Aq1q2q3q4M  (α)QM  (α)*AMaxDistdα(A,Q)dα(A,Q)+704Algorithm 3: Basic RKNN Search
Input: root, Q, k, [αs, αe]
Output: N N : the result set
α ← αs;
while α ≤ αe do

N Nα ← AKNN search at threshold α;
for each A ∈ N Nα do

βA ← minα(cid:48)∈ΩA {α(cid:48) ≥ α} ;

α∗ ← minA∈N Nα βA;
add (cid:104)N Nα, [α, α∗](cid:105) into N N ;
α ← α∗ + (cid:15);

return N N

1
2
3
4
5

6
7
8

9

the critical probability set of A with respect to Q, denoted by ΩQ(A),
is deﬁned as

ΩQ(A) = {α ∈ (0, 1]|(cid:64)β > α, dβ(A, Q) = dα(A, Q)}

Intuitively, ΩQ(A) refers to all the end points of the horizontal
line segments on the curve of dα(A, Q), as shown in Figure 8.
The semantics of its each element is that the α-distance is about to
change (increase) beyond this probability.

4.2 Reducing Search Space

Although the size of critical probability set is considerably smaller
than UD in the naive approach, the basic algorithm still requires
many AKNN search against the whole dataset, which will cause a
number of R-tree traversal and hence great IO overhead. To en-
hance its efﬁciency, our ﬁrst goal is to reduce the search space by
pruning most disqualifying objects.

LEMMA 3. Given a RKNN query (Q, k, [αs, αe]), B is the k-
th nearest neighbor at αe, then object A cannot be a result of this
RKNN query if dαs (A, Q) > dαe (B, Q).

PROOF. Let N Nαe be the k nearest neighbor set at αe. For any

α ∈ [αs, αe] and P ∈ N Nαe , we have

dα(P, Q) ≤ dαe (P, Q) ≤ dαe (B, Q)
< dαs (A, Q) ≤ dα(A, Q)

(4)

So all the objects in N Nαe have smaller α-distance than A to Q.
In other words, there are at least k objects closer than A at any
α ∈ [αs, αe], which means A cannot be a result in this range.

Consider an RKNN query with k = 2 and I = [0.3, 0.6] is is-
sued against the four objects in Figure 9. According to Lemma
3, object D cannot belong to the result set within the range I,
since B is the 2-th nearest neighbor at α = 0.6 and d0.3(D, Q) >
d0.6(B, Q).

Figure 8: Critical probability set

LEMMA 2. Suppose object A is one of the k nearest neighbors
at some probability α. Let α(cid:48) denote the minimum critical proba-
bility not less than α, i.e., α(cid:48) = min{β ∈ ΩQ(A)|β ≥ α}. Then
A remains in the kNN set within the range [α, α(cid:48)].

PROOF. By deﬁnition, the α-distance between A and Q does
not change within the range [α, α(cid:48)]. On the other hand, though the
distances of other objects with Q may vary, they are only possible
to increase according to the monotonicity of α-distance. So A is
guaranteed to remain in the kNN set within this range.

Motivated by Lemma 2, we can design a basic RKNN search

algorithm as follows.

Given an RKNN query with [αs, αe] as the range, we issue an
AKNN query at αs and ﬁnd the k nearest neighbor set N Nαs . For
each object A ∈ N Nα, we ﬁnd the next smallest critical prob-
ability after αs, and select their minimum value α(cid:48). According to
Lemma 2, the distance of all objects in N Nα will not change within
the range [αs, α(cid:48)]. Then, another AKNN query is issued again to
ﬁnd the new kNN set at probability α(cid:48)+(cid:15), where (cid:15) is a small enough
real value (e.g. the precision of ﬂoating number). The above steps
will repeat until it reaches αe. This method is more efﬁcient than
the naive method since it only considers the critical probability val-
ues which is only a small fraction of the original membership set.

Figure 9: Reducing search space

Algorithm 4 shows the new search strategy optimized by Lemma
3. Speciﬁcally, given an RKNN query with I = [αs, αe], we ﬁrst
ﬁnd the k nearest neighbor set by issuing an AKNN query with
α = αe. Then we use the k-th nearest neighbor distance as the
radius to perform a range search at α = αs. Only the objects
included within the radius are the candidates which are possible to
be the k nearest neighbors at some probabilities within I. So we
only need to search for the ﬁnal results from candidate set C instead
of the whole dataset at each critical probability. The size of set C is
usually small and can ﬁt into main memory, thus a large amount of
IO operations can be avoided.

4.3 Improving Candidate Reﬁnement

With the help of Lemma 3, the search space is signiﬁcantly re-
duced. However, high computation cost still exists for the candidate
reﬁnement, where we have to check lots of critical probability val-
ues within the range. So our next goal is to accelerate the candidate
reﬁnement.

LEMMA 4. Suppose an object A belongs to the kNN set at some
probability α, and object B is the (k + 1)-th nearest neighbor at α.
Then A is guaranteed to be in the kNN set within the range [α, α(cid:48)]
as long as dα(cid:48) (A, Q) < dα(B, Q).

αdα(A,Q)01ΩQ(A)01ABCD0.60.3αdαd0.3(D,Q)d0.6(B,Q)705Algorithm 4: Reducing Search Space
Input: root, Q, k, [αs, αe]
Output: N N : the result set
N Nαe ← AKNN search at αe;
r ← the k-th nearest neighbor distance at αe from N Nαe ;
candidate set C ← perform a range search with radius r at
probability αs to get the candidate set;
N N ← reﬁne candidate set C using the method of basic
RKNN search;
return N N

1
2
3

4

5

PROOF. Let P be any object that does not belong to the k near-
est neighbors of Q at α, i.e., P ∈ D\N Nα. Naturally, dα(P, Q) ≥
dα(B, Q). Then, for any probability β ∈ [α, α(cid:48)], we have

dβ(A, Q) ≤ dα(cid:48) (A, Q) < dα(B, Q)

≤ dα(P, Q) ≤ dβ(P, Q)

(5)

return N N

That means all the objects in D\N Nα have larger distance than A
at any probability within [α, α(cid:48)]. In other words, there are at most k
objects which could be closer than A with respect to Q. Therefore
A must be one of the k nearest neighbors within [α, α(cid:48)].

As a special case of Lemma 4, if we set α to αs and α(cid:48) to αe,
then it is safe to conclude that object A is a qualifying result across
the entire probability range.

Figure 10 illustrates how Lemma 4 can help improve the can-
didate reﬁnement. Consider an RKNN query with k = 2 and
I = [0.3, 0.6]. Since object C is the 3-th nearest neighbor at
α = 0.3 and d0.5(A, Q) < d0.3(C, Q), by Lemma 4 object A
is guaranteed to be in the 2NNs within [0.3, 0.5]. The reﬁnement
continues until it reaches α = 0.5. Again, as object B is the 3-th
nearest neighbor at α = 0.5 and d0.6(A, Q) < d0.5(B, Q), A is
still a result in [0.5, 0.6]. Thus, by checking only two probability
thresholds, we can already conclude A is a result within the range
[0.3, 0.6].

Algorithm 5: Improving Candidate Reﬁnement
Input: C, Q, k, [αs, αe]
Output: N N : the result set
α ← αs;
initialize C(cid:48) to be empty;
while α ≤ αe do

C(cid:48) ← ﬁnd the objects in N N which qualify at α;
N Nα ← search C\C(cid:48) for the k − |C(cid:48)| nearest neighbors at
α;
dk+1 ← the k + 1-th nearest neighbor distance at α;
for each A ∈ N Nα do

β ← maxα∈ΩQ(A){dα(A, Q) < dk+1}
add (cid:104)A, [α, β](cid:105) into N N ;
ΩQ(A) ← ΩQ(A)\{α ∈ ΩQ(A)|dα(A, Q) <
dk+1};

α∗ ← minA∈N Nα∪C(cid:48) ΩQ(A);
α ← α∗ + (cid:15);

1
2
3
4
5

6
7

8

9
10

11
12

13

the elements α1 < α2 < . . . < αn of ΩQ(A) at which its dis-
tance is less than dk+1. Since A is guaranteed to be a result within
[α, αn], we can immediately add A along with this range into the
result set N N . At the next round, we ﬁrst obtain the objects which
are still in the safe range into a set C(cid:48). Then we only need to search
the set C\C(cid:48) for the top k − |C(cid:48)| objects. This algorithm is more
efﬁcient than the basic RKNN search, since it removes lots of criti-
cal probability values during the reﬁnement process, which should
have been checked by the basic method.

5. COMPLEXITY ANALYSIS

In this section, we try to estimate the number of object access
during the AKNN search process since retrieving objects from hard
disk and computing their α-distances to query are the most costly
parts in the whole algorithm. In order to make the analysis feasible,
we assume the dataset is formed by ideal fuzzy objects.

DEFINITION 8. An ideal fuzzy object A is circle (or sphere),
and the radius of its α-cut is characterized by a function R : α →
radius.

By assuming the data space is composed of a set of ideal fuzzy
objects in 2d space, our problem is, given a query object Q and
a probability threshold α, to estimate the number of object access
during the AKNN search in the data space. For the basic AKNN
search, the objects to be accessed is the ones whose M inDist with
Q is smaller than dknn(α), where dknn(α) is the α-distance be-
tween Q and its k-th nearest neighbor. So we need to estimate
dknn(α) ﬁrst. By representing each fuzzy object with its center,
the dataset becomes a point set. Then we can borrow some for-
mulas from existing work [16] to estimate the average number of
neighbors nb((cid:15), ‘shape(cid:48)) of a point Q within distance (cid:15) from Q,
using the concept of correlation fractal dimension of the point set:

nb((cid:15), ‘shape(cid:48)) =

· (N − 1) · (2(cid:15))D2

„ vol((cid:15), ‘shape(cid:48))
vol((cid:15), ‘rect(cid:48))

« D2

E

where N is the number of points, D2 is the correlation fractal di-
mension and vol((cid:15), ‘shape(cid:48)) indicates the volume of a shape of
radius (cid:15). In a 2-d dimensional space (E = 2), we want to estimate
the minimum (cid:15) that encloses k points. As vol((cid:15), ‘circle(cid:48)) = π(cid:15)2

Figure 10: Improving candidate reﬁnement

Motivated by Lemma 4, whenever we have obtained the kNN set
at some α, it is often helpful to look for the “safe range” for each
object in the kNN set. By this means, a large number of probabil-
ity checking can be avoided. Based on this, we modify the basic
RKNN search to improve candidate reﬁnement, which is shown in
Algorithm 5. In each iteration of the algorithm, we search the can-
didate set C to get the k nearest neighbors at current threshold α. In
addition, we also obtain the k + 1-th nearest neighbor distance at
α, denoted by dk+1. Then for each object A ∈ N Nα, we remove

0.30.60.5ABCαdαd0.3(C,Q)d0.5(A,Q)d0.5(B,Q)d0.6(A,Q)706Parameter
Default value
Number of objects N
50000
Number of results k
20
Probability threshold α 0.5
Probability range L
0.2

Table 2: Parameter settings

(6)

6.2 AKNN Performance Evaluation

In this subsection, we compare the performance of the basic
AKNN search algorithm against its competitors, namely, AKNN
search with improved lower bound (LB), LB with lazy probe (LB-
LP), and LB-LP with improved upper bound (LB-LP-UB).

dknn(α) = (cid:15) − 2 · R(α)

6.2.1 Effect of dataset

and vol((cid:15), ‘rect(cid:48)) = (2(cid:15))2, the above equation can be simpliﬁed
to:

nb((cid:15), ‘circle(cid:48)) = (

D2
2 · (N − 1) · (2(cid:15))D2

π(cid:15)2
4(cid:15)2 )

= (N − 1) · ((cid:15)

√

π)D2

By substituting nb((cid:15), ‘circle(cid:48)) with k, and D2 = 2 for a uniform
dataset, we get:

(cid:15) =

1
√
π

r k

N − 1

The (cid:15) derived from Equation (6) can be treated as distance from
the center of query to the center of its k-th nearest neighbor. So
their α-distance is:

Then the problem turns into estimating the number of leaf node
accessed by a range query, for which a formula has been given in
[16]:

L =

N − 1
Cavg

· ((

Cavg

N

)1/D0 + 2d)D2

(7)

Cavg = Cmax · Uavg

where d is the search range, D0 is the Hausdorff fractal dimen-
sion of the dataset (≈ 2 for uniform set), Cmax is the maximum
node capacity and Uavg is the average space utilization of the R-
tree nodes. By substituting d with dknn(α) + R(α), we can esti-
mate the average number of object accessed as the function of α:

L =

r

N − 1
Cavg

· (

Cavg

N

+ 2(

1
√
π

r k

N − 1

− R(α)))2

(8)

From Equation (8) we can see that, more objects need to be ac-

cessed as N , k or α increases independently.

6. EXPERIMENTS

In this section, we perform extensive experiments to verify the
efﬁciency of the proposed methods and optimizations on both syn-
thetic and real datasets. All the algorithms are implemented in Java
and run on a normal PC with Pentium IV 2.4 GHz CPU and 1GB
memory.

6.1 Dataset

The datasets we use for experiments are setup as follows.

• For the synthetic dataset, each object is a circle with radius
of 0.5 and containing 1K uniformly distributed points, whose
membership values follow the two dimensional Gaussian dis-
tribution with mean at the center of circle and σx = σy =
0.5. In order to assure the kernel set is not empty, we nor-
malize the probability values across 0 to 1.

• For the real dataset, each object is formed by 1K points ran-
domly sampled from the horizontal cell identiﬁed by proba-
bilistic segmentation [14],[15]. Then we normalize the posi-
tions of all points to restrict them into a 1 × 1 square. Similar
with the synthetic dataset, we also normalize the probability
values across 0 to 1.

For both datasets, we generate N objects and randomly distribute
them into 100 × 100 space. All the actual points are stored in ﬁles
and we index the fuzzy regions by R-tree. In the following experi-
ments, we measure the number of object access from hard disk and
running time of the algorithms under different parameter settings.
Table 2 summarizes the parameters and their default values.

(a) No. of object access

(b) Running time

Figure 15: Effect of dataset

First we run all algorithms on both datasets to see the impact
of different fuzzy objects by using the default parameter settings.
The results are shown in Figure 15a and 15b. As expected, the ba-
sic method exposes the worst performance on both datasets. More
speciﬁcally, LB can avoid quite a few unnecessary object access
and distance calculations by applying a tighter approximation for
the α-cut of fuzzy objects. The beneﬁt of lazy probe, however, is
not signiﬁcant since the upper bound it uses is rather loose. By fur-
ther applying the tighter upper bound, LB-LP-UB method achieves
the best performance. As each algorithm shows similar perfor-
mance on both datasets, we will just show the results on real dataset
to keep our presentation concise.

6.2.2 Effect of N

We then study the impact of dataset size on the performance of
AKNN search algorithms by varying the number of objects from
1K to 50K. As shown in Figure 11a, all the algorithms need to
access more objects on hard disk in order to determine the k nearest
neighbors with a larger dataset. This is because, when the number
of objects grows, the density of whole data space becomes higher,
which makes it more difﬁcult to prune objects by checking distance
lower bound. From Figure 12a, their running time also increases
because more IO operation is invoked and more α-distances need
to evaluated. We also notice that the performance of all algorithms
are still comparable when the dataset is relatively small, since the
simple lower bound serves as a tolerable approximation when the
data space is sparse. The advantages of the proposed optimizations
become more obvious in larger dataset.

6.2.3 Effect of k

Next we compare the performance of all methods by varying k
from 5 to 50. According to Figure 11b and 12b, the performance
of all algorithms deteriorate as k increases. This is due to the fact
that the best-ﬁrst search strategy has to verify all the objects A sat-
isfying d−
α (A, Q) ≤ dknn where dknn is the α-distance of k-th

 65 70 75 80 85 90SyntheticReal# of object accessBasic AKNNLBLB-LPLB-LP-UB 0 2 4 6 8 10 12 14SyntheticRealRunning time [s]Basic AKNNLBLB-LPLB-LP-UB707(a) Diff. N

(b) Diff. k

(c) Diff. α

Figure 11: Object access of AKNN search

(a) Diff. N

(b) Diff. k

(c) Diff. α

Figure 12: Running time of AKNN search

nearest neighbor. As k increases, dknn increases as well, which
means more objects need to be retrieved. On the other hand, the
optimized algorithms are less sensitive to the variation of k, due to
the more accurate estimation of α-distance.

6.2.4 Effect of α

We also study the impact of the probability threshold by varying
α from 0.3 to 0.9. From Figure 11c and 12c we can observe that the
number of object access as well as the running time increases for
the basic search method. This is due to the greater α-distance of the
k-th nearest neighbor when α is higher. As a consequence, more
candidates are retrieved for veriﬁcation. Although the optimized
algorithms face the same situation, their performance are heading
for the opposite direction, i.e., fewer objects accessed and less CPU
cost. This is because, as α increases, the improved lower and upper
bound can reﬂect the actual α-distance more accurately, making the
best-ﬁrst search more efﬁcient.

6.3 RKNN Performance Evaluation

To verify the efﬁciency of our proposed methods for answering
RKNN query, we compare the performance of the basic algorithm
with the optimized algorithms, i.e., basic RKNN method with re-
duced search space (RSS) and RSS with improved candidate re-
ﬁnement procedure (RSS-ICR).

6.3.1 Effect of N

The ﬁrst set of experiments tests the scalability of all RKNN
schemes by varying the dataset size from 1K to 50K. Results are
shown in Figure 13a and 14a, from which we can see that perfor-
mance of all algorithms degrade as the dataset grows, but the opti-
mized algorithms are constantly superior than the basic method by
at least one order of magnitude. Since the basic method relies on
invoking lots of AKNN search procedure, it needs to traverse the

R-tree many times and incurs high IO and computation cost. With
the AKNN performance getting worse in larger dataset, the basic
RKNN search deteriorates even faster. On the other hand, RSS
only issues an AKNN query at the end of the probability range,
and then ﬁnds all the candidates by a range search at the beginning
of the probability range. Other than that, no extra IO operation is
invoked. That explains why the number of object access by RSS
is within the same magnitude as the corresponding AKNN search.
RSS-ICR adopts the same strategy to obtain the candidate set, thus
having the same performance in terms of object probe. But its CPU
cost is lower than RSS since it avoids a lot of critical probability
checking in the candidate reﬁnement process.

6.3.2 Effect of k

Then we study performance trend of all algorithms when differ-
ent numbers of results are required. Figure 13b and 14b clearly
demonstrates the remarkable advantage of our optimizations, the
cause of which is similar with the last set of experiments. For all
algorithms, their running time increases are also caused by more
critical probability values to be checked as k increases.

6.3.3 Effect of L

Finally we examine the impact of different lengths of probabil-
ity range on the performance of all algorithms by varying L from
0.05 to 0.5. As shown in Figure 13c and 14c, with L increasing,
the performance of the basic algorithm deteriorates very fast. This
is because the number of AKNN queries issued by the basic al-
gorithm increases rapidly. On the contrary, the number of object
access for the optimized algorithms is not sensitive to the variation
of L. Their running time increase is mainly caused by the longer
candidate reﬁnement process. By avoiding exhaustively checking
all critical probability values, the advantage of RSS-ICR becomes
more signiﬁcant when the probability range is broader.

 40 50 60 70 80 90100050001000050000# of object accessBasic AKNNLBLB-LPLB-LP-UB 0 50 100 150 200 2505102050# of object accessBasic AKNNLBLB-LPLB-LP-UB 50 55 60 65 70 75 80 85 90 95 1000.30.50.70.9# of object accessBasic AKNNLBLB-LPLB-LP-UB 0 2 4 6 8 10 12 14100050001000050000Running time [s]Basic AKNNLBLB-LPLB-LP-UB 0 5 10 15 20 25 30 35 405102050Running time [s]Basic AKNNLBLB-LPLB-LP-UB 5 6 7 8 9 10 11 12 13 14 150.30.50.70.9Running time [s]Basic AKNNLBLB-LPLB-LP-UB708(a) Diff. N

(b) Diff. k

(c) Diff. L

Figure 13: Object access of RKNN search

(a) Diff. N

(b) Diff. k

(c) Diff. L

Figure 14: Running time of RKNN search

7. RELATED WORK

There is a large body of research work on nearest neighbor search
in spatial databases. The most common type of nearest neighbor
search is the point-kNN query that ﬁnds the k objects from a dataset
that are closest to a query point q. Roussopoulos et al [19] proposed
a depth ﬁrst method that, starting from the root of R-tree, visits the
entry with the minimum distance from q. The process is repeated
recursively until the leaf level. During the backtracking to the upper
level, the algorithm only visits entries whose minimum distance is
smaller than the distance of the nearest neighbor already found.
Hjaltason and Samet [11] implemented a best-ﬁrst traversal that
follows the entry with the smallest distance among all those visited.
In order to achieve this, the algorithm keeps a priority queue with
the candidate entries and their minimum distances from the query
point. Furthermore, conventional NN search and its variations in
low and high dimensional spaces have also received considerable
attention due to their applicability in domains such as content based
retrieval and multimedia database [13, 27, 32, 33]. But since it is
difﬁcult to map a fuzzy object to a single point while keeping the
original information, the proposed algorithms for point-NN search
cannot be directly applied to our problem.

With the proliferation of location-based e-commerce and mobile
computing, a lot of attention has been received for moving-object
databases, where the locations of data objects or queries (or both)
are changing. Assuming object movement trajectories are known a
priori, Saltenis et al [20] proposed the Time-Parameterized R-tree
(TPR-tree) for indexing moving objects, in which the location of
a moving object is represented by a linear function. Benetis et al
[5] developed query evaluation algorithms for NN and reverse NN
search based on TPR-tree. Tao et al [31] optimized the performance
of the TPR-tree and extended it to the TPR∗-tree. Continuous kNN
monitoring for moving queries has also been investigated on sta-

tionary objects [30] and linearly moving objects [12]. The work in
this category shares some features in common with our proposed
kNN queries for fuzzy objects. In continuous kNN queries, the re-
sult set is only valid within certain time span. Correspondingly, in
our work each k nearest neighbor is associated with its qualifying
probability range. Yet essential differences still exist. In moving
object databases, what varies is the location of an object; while in
fuzzy object databases, the dynamic part is the composition of an
object, which alters with the probability threshold.

As data uncertainty attracts more research interests, KNN query
has also been extended to uncertain databases. Cheng et al. [8]
proposed the probabilistic nearest-neighbor query (PNNQ) in un-
certain environments, which ﬁnds a set of data objects that have
non-zero probability to be the nearest neighbor of the query point.
Tao et al [29] developed U-tree to index multi-dimensional uncer-
tain data with arbitrary probability density functions. Although an
uncertain object is often represented by a pdf region, which looks
similar with the fuzzy object model in this paper, essential differ-
ence exists. The accumulation of probabilities within a pdf region
is equal to one, which is the basic assumption for most of their
techniques to work correctly. Obviously, the fuzzy object model
disobeys this assumption. That is why we cannot adopt their index
method to support the kNN queries in this paper.

On the other hand, the notion of fuzzy objects has not been in-
troduced to database ﬁeld until Altman [2] adopted fuzzy set the-
oretic approach for handling imprecision in spatial databases. Af-
terwards, fuzzy data types such as fuzzy points, fuzzy lines, and
fuzzy regions were deﬁned in [22, 10]. Based upon that, simple
metric operations such as the area of a fuzzy region and the length
of a fuzzy line were further developed in [23], in which only unary
functions were considered. On relationship between fuzzy objects,
different models of fuzzy topological predicates, which character-

 10 100 1000 10000100050001000050000# of object accessBasic RKNNRSSRSS-ICR 10 100 1000 100005102050# of object accessBasic RKNNRSSRSS-ICR 100 1000 100000.050.10.20.5# of object accessBasic RKNNRSSRSS-ICR 1 10 100 1000100050001000050000Running time [s]Basic RKNNRSSRSS-ICR 10 100 1000 100005102050Running time [s]Basic RKNNRSSRSS-ICR 10 100 1000 100000.050.10.20.5Running time [s]Basic RKNNRSSRSS-ICR709ize the relative position of two fuzzy objects toward each other were
discussed in [25, 28, 24]. However, it remains largely untouched to
answer more advanced spatial queries such as the nearest neighbor
query, which is addressed in this paper.

8. CONCLUSION

Although kNN query is one of the most useful spatial queries and
fuzzy objects can often be found in many important applications,
considering both has received rather limited attention. In this pa-
per we have studied this problem in depth, deﬁning two new types
of kNN queries for fuzzy objects. We have developed efﬁcient al-
gorithms to answer these queries by extending the R-tree indexing
structure and deriving several highly effective heuristic rules. Ex-
tensive experiments on both synthetic and real datasets have shown
that our optimized algorithms achieve superior performance than
the baseline approaches constantly. Given the relevance of fuzzy
objects to a wide range of applications, we expect this research
to trigger further work in this area, opening a way for other ad-
vanced queries such as spatial join queries, reverse nearest neighbor
queries and skyline queries.

Acknowledgement: The research reported in this paper has been
partially supported by ARC Centre of Excellence in Bioinformat-
ics.

9. REFERENCES
[1] E. Achtert, C. Bohm, P. Kroger, P. Kunath, A. Pryakhin, and

M. Renz. Efﬁcient reverse k-nearest neighbor search in
arbitrary metric spaces. In SIGMOD, pages 515 – 526, 2006.

[2] D. Altman. Fuzzy set theoretic approaches for handling
imprecision in spatial analysis. International Journal of
Geographical Information Science, 8(3):271–289, 1994.

[3] A. M. Andrew. Another efﬁcient algorithm for convex hulls
in two dimensions. Information Processing Letters, 9, 1979.
[4] A. Badel, J. Mornon, and S. Hazout. Searching for geometric

molecular shape complementarity using bidimensional
surface proﬁles. Journal of Molecular Graphics,
10(4):205–211, 1992.

[5] R. Benetis, C. Jensen, and G. Simonas. Nearest neighbor and

reverse nearest neighbor queries for moving objects. In
IDEAS, pages 44–53, 2002.

[6] I. Bloch. On fuzzy distances and their use in image
processing under imprecision. Pattern Recognition,
32(11):1873–1895, 1999.

[7] B. Chaudhuri and A. Rosenfeld. On a metric distance

between fuzzy sets. Pattern Recognition Letters,
17(11):1157–1160, 1996.

[8] R. Cheng, D. Kalashnikov, and S. Prabhakar. Evaluating

probabilistic queries over imprecise data. In SIGMOD, pages
551–562, 2003.

[9] A. Corral, Y. Manolopoulos, Y. Theodoridis, and

M. Vassilakopoulos. Closest pair queries in spatial databases.
SIGMOD Record, 29(2):189–200, 2000.

[10] A. Dilo, A. Rolf, and A. Stein. A system of types and

operators for handling vague spatial objects. International
Journal of Geographical Information Science,
21(4):397–426, 2007.

[11] G. Hjaltason and H. Samet. Distance browsing in spatial

databases. TODS, 24(2):265–318, 1999.

[12] G. Iwerks, H. Samet, and K. Smith. Continuous k-nearest

neighbor queries for continuously moving points with
updates. In VLDB, pages 512–523, 2003.

[13] F. Korn, N. Sidiropoulos, C. Faloutsos, E. Siegel, and

Z. Protopapas. Fast nearest neighbor search in medical image
databases. In VLDB, pages 215–226, 1996.

[14] V. Ljosa and A. Singh. Probabilistic segmentation and

analysis of horizontal cells. In ICDM, pages 980–985, 2006.

[15] V. Ljosa and A. Singh. Top-k spatial joins of probabilistic

objects. In ICDE, pages 566–575, 2008.

[16] A. Papadopoulos and Y. Manolopoulos. Performance of

nearest neighbor queries in r-trees. In ICDT, pages 394–408,
1997.

[17] S. Peng, B. Urbanc, L. Cruz, B. Hyman, and H. Stanley.

Neuron recognition by parallel potts segmentation. National
Academy of Sciences, 100(7):3847–3852, 2003.

[18] F. Preparata and M. Shamos. Computational geometry: an

introduction. Springer, 1985.

[19] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest
neighbor queries. In SIGMOD, pages 71–79, 1995.

[20] S. Saltenis, C. Jensen, S. Leutenegger, and M. Lopez.

Indexing the positions of continuously moving objects. ACM
Sigmod Record, 29(2):331–342, 2000.

[21] C. Schmitz, N. Grolms, P. Hof, R. Boehringer, J. Glaser, and

H. Korr. A stereological study using a novel
three-dimensional analysis method to estimate the nearest
neighbor distance distributions of cells in thick sections.
Cerebral Cortex, 12(9):954–960, 2002.

[22] M. Schneider. Uncertainty management for spatial data in

databases: fuzzy spatial data types. In SSD, pages 330–354,
1999.

[23] M. Schneider. Metric operations on fuzzy spatial objects in

databases. In ACMGIS, pages 21–26, 2000.

[24] M. Schneider. A design of topological predicates for complex

crisp and fuzzy regions. In ICCM, pages 103–116, 2001.

[25] M. Schneider. Fuzzy topological predicates, their properties,

and their integration into query languages. In ACMGIS,
pages 9–14, 2001.

[26] T. Seidl and H. Kriegel. Efﬁcient user-adaptable similarity

search in large multimedia databases. In VLDB, pages
506–515, 1997.

[27] T. Seidl and H. Kriegel. Optimal multi-step k-nearest
neighbor search. In SIGMOD, pages 154–165, 1998.

[28] X. Tang and W. Kainz. Analysis of topological relations

between fuzzy regions in a general fuzzy topological space.
In Symposium on Geospatial Theory, Processing and
Applications, 2002.

[29] Y. Tao, R. Cheng, X. Xiao, W. Ngai, B. Kao, and

S. Prabhakar. Indexing multi-dimensional uncertain data
with arbitrary probability density functions. In VLDB, pages
922–933, 2005.

[30] Y. Tao, D. Papadias, and Q. Shen. Continuous nearest

neighbor search. In VLDB, pages 287–298, 2002.

[31] Y. Tao, D. Papadias, and J. Sun. The tpr*-tree: An optimized

spatio-temporal access method for predictive queries. In
VLDB, pages 790–801, 2003.

[32] R. Weber, H. Schek, and S. Blott. A quantitative analysis and

performance study for similarity-search methods in
high-dimensional spaces. In VLDB, pages 194–205, 1998.

[33] C. Yu, B. Ooi, K. Tan, and H. Jagadish. Indexing the

distance: An efﬁcient method to knn processing. In VLDB,
pages 421–430, 2001.

[34] L. Zadeh. Fuzzy sets. Information and Control, 8:338–353,

1965.

710