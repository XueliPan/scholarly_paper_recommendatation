Session Based Click Features for Recency Ranking

Yoshiyuki Inagaki and Narayanan Sadagopan and Georges Dupret and Ciya Liao

Anlei Dong and Yi Chang and Zhaohui Zheng

Yahoo Labs

701 First Avenue

Sunnyvale, CA 94089

inagakiy,narayans,gdupret,ciyaliao,anlei,yichang,zhaohui@yahoo-inc.com

Abstract

Recency ranking refers to the ranking of web results by ac-
counting for both relevance and freshness. This is particu-
larly important for “recency sensitive” queries such as break-
ing news queries.
In this study, we propose a set of novel
click features to improve machine learned recency ranking.
Rather than computing simple aggregate click through rates,
we derive these features using the temporal click through data
and query reformulation chains. One of the features that we
use is click buzz that captures the spiking interest of a url for
a query. We also propose time weighted click through rates
which treat recent observations as being exponentially more
important. The promotion of fresh content is typically deter-
mined by the query intent which can change dynamically over
time. Quite often users query reformulations convey clues
about the query’s intent. Hence we enrich our click features
by following query reformulations which typically beneﬁt the
ﬁrst query in the chain of reformulations. Our experiments
show these novel features can improve the NDCG5 of a major
online search engine’s ranking for “recency sensitive” queries
by up to 1.57%. This is one of the very few studies that ex-
ploits temporal click through data and query reformulations
for recency ranking.

Introduction

Web search engines are expected to return relevant search
results for a query. A user who issues a query can have
varying information needs depending on the context. For
example, a user querying for “circus” could be interested in
local circus shows in an area or may be looking for informa-
tion about Britney Spears’ album “circus”. Such queries that
have temporally varying intent are called “recency sensitive”
queries. Ranking of search results for these queries is ex-
tremely challenging. For example in the case of query “cir-
cus”, users might prefer to see a result about Britney Spears’
album closer to the day the album was released. However if
the album was released in the distant past the user may prefer
results about the local circus shows in his geographical area.
The problem of ranking documents by relevance while tak-
ing freshness into account is called recency ranking (Dong
et al. 2009). Recency ranking on web-scale offers several
unique and challenging research problems. First, ranking

algorithms for recency sensitive queries need to satisfy both
relevance and freshness requirements. In order to ensure ro-
bustness, the system needs to promote recent content only
when appropriate, so as to avoid degrading relevance for
other queries classes.

Quite often a machine learned approach is used for gen-
eral web search ranking (Burges et al. 2005) (Zheng et
al. 2007). This approach usually employs several features
to predict a relevance score. The number of features can
range in the order of a few thousands and can be obtained
from link analysis, semantic matching of query to document,
etc. Several studies have also demonstrated the utility of
user click feedback in improving the relevance of machine
learned ranking (Joachims 2002a) (Agichtein, Brill, and Du-
mais 2006). Since most of these studies focus on improv-
ing ranking for general web search, they compute aggregate
click through rate features.
In this work we focus on the
design of several novel click features that can be used to im-
prove machine learned recency ranking. Due to the temporal
aspect of recency ranking these features are derived from the
time series of click data. One of the features is click buzz that
captures the spiking interest of a url for a query. We also pro-
pose a simple exponential weighted scheme for computing
click through rates that over emphasizes recent data.

Typically users issue one or more queries to satisfy their
information needs (Jones and Klinkner 2008). These query
sequences (or chains) contain a wealth of information that
can be further exploited for computing click features. For
example, in the case of the query “circus”, if the user is not
satisﬁed with the current search results, he/she might refor-
mulate the query to “britney spears circus” to get the desired
search result. In this case it may be desirable to smooth the
clicks for “britney spears circus” and attribute it to the orig-
inal query “circus”.
Intuitively this mechanism which we
call “smoothing” would help in improving the ranking of
queries that have temporally varying intents. Our experi-
ments show that the proposed set of click features along with
smoothing across query chains does indeed produce substan-
tial relevance gains (up to 1.57%) over a major online search
engine’s recency ranking.

Related Work

Copyright c(cid:13) 2010, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Recency Ranking:
There has been a lot of studies on
improving general Web search ranking. State of the art

1334Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10)work in this area employs machine learning to automati-
cally construct a ranking function which optimizes retrieval
performance metrics (Burges et al. 2005; Joachims 2002a;
Zheng et al.
2007; Y. Cao and Hon 2006). Machine-
learned rankers have demonstrated signiﬁcant and consis-
tent gains over many manually tuned rankers for a variety
of data sets. Optimization usually is formulated as learning
a ranking function from human labeled preference data in
order to minimize a loss function (e.g., the number of in-
correctly ordered documents pairs in the training data). Dif-
ferent algorithm cast the preference learning problem from
different points of view. For example, RankSVM (Joachims
2002b) used support vector machines; RankBoost (Freund
et al. 1998) applied the idea of boosting from weak learn-
ers; GBrank (Zheng et al. 2007) used gradient boosting with
decision trees; RankNet (Burges et al. 2005) used gradient
boosting with a neural network. Improving recency rank-
ing has also attracted a lot of interest. There are two main
aspects to this ranking problem: detecting if the query is re-
cency sensitive (i.e.
the user prefers more recent content)
and appropriately ranking recent content along with other
results. Diaz proposed a solution to integrate search results
from a news vertical search into web search results, where
the news intent is detected by either inspecting query tempo-
ral dynamics or using click feedback (Diaz 2009). The main
focus of their work is to detect if a query is “newsworthy”.
Zhang et al. proposed a ranking score adjustment method
on year-qualiﬁed-queries, in which a few simple but effec-
tive adjustment rules are applied to the ranking results based
on the timestamps extracted from the documents(Zhang et
al. 2009). Dong et al. studied the impact of different ma-
chine learning strategies and algorithms on recency ranking
(Dong et al. 2009). In this work we propose a set of novel
temporal click features that can be used to improve the ma-
chine learned ranking for all time-sensitive queries.
Query Chain and Query Segmentation: Segmenting the
query stream of a user into sets of related queries has many
applications, such as query recommendation, user proﬁling
and personalization. He and G¨oker (He and G¨oker 2000)
studied various timeouts to segment user sessions, and later
extended their work (He, G¨oker, and Harper 2002) to take
features like the overlap between terms in two consecutive
queries into consideration. Radlinski and Joachims (Radlin-
ski and Joachims 2005) observed that users often issue a
series of queries with a similar information need. They re-
ferred to this sequence of related, often reformulated queries
as query chain. They presented a simple method to au-
tomatically detect query chains in query and click through
logs and showed how clicks inferred query chains could be
used to train better ranking functions. Jones and Klinkner
(Jones and Klinkner 2008) proposed an automated method
for segmenting sessions based on a machine learning ap-
proach. More recently, Boldi et al.
(Boldi et al. 2008)
proposed a novel query segmentation method by building
query-ﬂow graph and using a machine learning approach to
identify query chains. Our intuition is that these user refor-
mulation chains can help in identifying temporal changes in
the query intent. In this work we leverage query chains to
smooth our proposed click features.

Recency Data

This section brieﬂy describes the recency training data, in-
cluding evaluation guideline, recency features, recency data
collection.

Collecting training data collection for recency ranking
is different from regular web search ranking. For regular
web search ranking, the relevance judgement for a query-
url pair is usually static over time because document fresh-
ness does not affect the user satisfaction. The judgement
for a recency-sensitive query-url pair, however, should in-
corporate the freshness of the document. Thus for re-
cency ranking, we need editorial labels for tuples of the
form (cid:104)query, url, tquery(cid:105) instead of only (cid:104)query, url(cid:105), where
tquery is the time at which the query is issued. If the judging
time tj of (cid:104)query, url(cid:105) is far behind query issue time tquery,
it is impractical for the editors to make a reliable judgement.
Therefore, we collect sets of recency data periodically in-
stead of collecting them all at once. Each time we collect
a set of (cid:104)query, url, tquery(cid:105) tuples, we ask editors to judge
the query-url pairs immediately, so that the query issue time
and judging time are as close as possible. Collecting recency
data periodically can also prevent the data distribution from
being too biased towards a short period of time. For exam-
ple, within one day, there are many similar queries related to
the same breaking news, which are very different from the
query distribution over a longer time span.

We apply ﬁve judgement grades on query-url pairs: per-
fect, excellent, good, fair and bad. For human editors to
judge a query-url pair, we ask them to ﬁrst grade it by non-
temporal relevance, such as intent, usefulness, content, user
interface design, domain authority, etc. Subsequently the
grade is adjusted solely based on the recency of the result.
More speciﬁcally, a result should receive a demotion if the
date of the page, or age of the content, makes the result less
relevant in comparison to more recent material or changes
in time which alter the context of the query. This demotion
should be reﬂected in the following judgment options:
• shallow demotion (1-grade demotion):

if the result is
somewhat outdated, it should be demoted by one grade
(e.g., from excellent to good);

• deep demotion (2-grade demotion): if the result is totally
outdated or totally useless, it should be demoted by two
grades (e.g., from excellent to bad).
The advantages of this recency-demotion grading method

include:
• Recency is incorporated into the modeling of the the rank-

ing problem

• Recency can also be decoupled from overall relevance so
that recency and non-recency relevance can be analyzed
separately during learning.
To best represent page recency with available informa-
tion, Dong et al. used four categories of recency-related fea-
tures besides traditional relevance ranking features: times-
tamp features, linktime features, webbuzz features and page
classiﬁcation features. For timestamp features and linktime
features, time values were extracted from web page content
and page discovery log respectively; these two categories

1335of features were used as an attempt to represent page fresh-
ness. Webbuzz features are similar to page and link activity
used in the T-Rank algorithm and represent the update rates
of pages and links, reﬂecting the recent popularity of the
pages (Berberich, Vazirgiannis, and Weikum 2005). They
also used page classiﬁcation features that are closely related
to page recency, and are useful in ranking model learning
to help us appropriately use recency features for different
classes of pages. They did not use any information from the
user query log data. In this study we build upon their work
by proposing a set of novel features derived from user click
logs that can be used to improve recency ranking.

Click Features for Recency Ranking

In this study we demonstrate the utility of user clicks for re-
cency ranking. Since modeling time is essential for recency
ranking, we design click features that are derived from the
time series of click data. We also use query reformulation
smoothing to exploit user feedback for these time-sensitive
queries.

The most common click feature used in several studies is

click through rate that is deﬁned as follows:

Deﬁnition 1 Click Through Rate CTR(query, url) is deﬁned
as the ratio of the number of sessions with clicks on the given
url for the given query and the total number of sessions in
which the url is viewed for the query 1.

More formally, CTR(query,url) is computed as:

CTR =

(cid:80)T

(cid:80)T

i=1 ci
i=1 vi

where T is period of data collection, ci and vi are the number
of clicks and views respectively for (query,url) on day i.

Apart from CTR, in this study, we also propose new click
features such as Only Click Through Rate and Click Attrac-
tivity which are deﬁned below:

Deﬁnition 2 Only Click Through Rate CTRo(query,url) is
the ratio of the number of sessions in which the user clicked
only on the given url (and nothing else) for the given query
and the total number of sessions in which the url is viewed
for the query.

Deﬁnition 3 Click Attractivity ATTR(query,url) is the ratio
of the number of sessions with clicks on the given url for the
given query and the total number of sessions in which the
url was either clicked or examined 2

Along the same lines, we deﬁne similar features for the
query, host pair. These features are called CTRH, CTRHo and
ATTRH respectively.

Speciﬁcally for recency ranking, we propose a time vary-
ing CTR with a simple weighting strategy. Click and view
counts for a day are exponentially weighted to emphasize

1Every time a url is displayed on the ﬁrst page, it is counted as

a view

2A url is considered examined if its position is above the maxi-

mum click position in a session.

recent observations. The exact form of weighting we use is
the following:

CTRw(query, url, tquery) =

(cid:80)tquery

i=1,vi>0(ci · (1 + x)i−tquery )
i=1,vi>0(vi · (1 + x)i−tquery )

(cid:80)tquery

where x > 0.

If x = 0, CTRw = CTR,

the traditional unweighed
CTR. We can vary x based on how steep we want to over-
emphasize recent observations.

In our experiments we use weighted versions of all ear-
o , ATTRw, CTRHw,

lier mentioned CTRs such as CTRw, CTRw
CTRHw

o and ATTRHw.

We also deﬁne the click buzz feature to identify whether a
page, a site or a host that receives an unusual level of atten-
tion from the users compared to the past. Measure of atten-
tion can be the number of clicks on the document, number
of clicks on the query document pair, on the host, etc. Take
the particular example of clicks on a query url pair ct, the
number of clicks during the time interval indexed by t. We
ﬁrst compute the average and the variance of the statistic of
interest over T periods of time:

cT =

ct

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

VAR(c) = σ2

T (c) =

(ct − cT )2

The buzz of a feature is deﬁned as the deviation from the
empirical average of the feature value over a period of time,
normalized by the standard deviation over the same period.
It is therefore a measure of how atypical the feature value is
on a given day, compared with its value on average.

The buzz feature is computed as follows

b(query, url, tquery) =

ctquery − ctquery

σtquery (c)

Naturally, we can compute the buzz of any feature varying
over time, for example the number of host clicks, number of
times a query is issued, etc. (see Figure 1).

It is important to note that the proposed click features for
a given (query, url, tquery) triplet is computed using only
the user click data till tquery. These click features favor urls
that have been of recent interest for a query. One of the chal-
lenges in recency ranking is to model changes in the query
intent. We believe that looking at query reformulations can
be a valuable source of information for this purpose. We
discuss them in the next section.

Click Smoothing using Query Chains

In most analysis of web search relevance, a single query is
the unit of user interaction with search engines. However,
users tend to issue multiple queries in order to complete
their task (Jones and Klinkner 2008). Jones et al. deﬁned
2 classes of query chains, namely goals and missions (Jones
and Klinkner 2008). A goal is an atomic information need
that results in one or more queries while a mission is a re-
lated set of information needs that results in one or more

1336Figure 1: The buzz of CTR for a given query. The dotted
lines represent one standard deviation around the mean. The
right most vertical segment represents the deviation of a par-
ticular observation from the average. The largest the devi-
ation counted in standard deviation units, the most unusual
the click behavior.

Table 1: URLs that had been clicked for the query, “Circus”.

u1=www.ringling.com
u2=www.youtube.com/watch?v=1zeR3NSYcHk
u3=en.wikipedia.org/wiki/Circus_(Britney_Spears_album)
u4=www.metrolyrics.com/circus-lyrics-britney-spears.html
u5=www.youtube.com/
u6=en.wikipedia.org/wiki/Circus
u7=britneyspearscircus.net/

goals. For instance, the mission of a user may be to organize
a trip to Hawaii. Any one user mission can (but does not
need to) be made up of several goals, and goals in the mis-
sion may be to ﬁnd a hotel in Hilo, look at Kona restaurant
reviews and check the NOAA Hawaii weather site.

Once we successfully identify task boundaries and seg-
ment a long sequence of queries into query chains, all
queries in a query chain are presumably issued to satisfy a
single information need by the user. Therefore, clicks made
on any URLs in search results for any query in the query
chain can be used to enrich the clickthrough data for the ﬁrst
query in the chain.

Consider the following example: On November 28th,
2008, Britney Spears released her new album called “Cir-
cus”. Our query logs shows that the query “Circus” got pop-
ular a few days later (see Figure 2).

About the same time, URLs about Britney Spears’ “Cir-
cus” started to be viewed and clicked more (see Figure 3 and
4). Since users could not get URLs about her album with the
query “Circus”, they issued a series of queries by reﬁning
the previous query until they got URLs about the album and
clicked those URLs.

This series of queries to retrieve URLs about the album
constitute one query chain. In this case, the goal of the user
is to obtain results for Britney Spears’ circus album. We
can take the URLs that were clicked for “Circus Album” or

Figure 2: Users got interested in “Circus” a few days after
the release of “Circus”album.

Figure 3: URLs (u2, u3, u4 and u7 in Table 1) about “Cir-
cus” album get more views.

Figure 4: URLs (u2, u3, u4 and u7 in Table 1) about “Cir-
cus” album get more clicks.

1337(Jarvelin and Kekalainen 2002). NDCG is deﬁned as

NDCGn = Zn

n
(cid:88)

i=1

Gi

log2(i + 1)

where i is the position in the document list, Gi is the func-
tion of relevance grade. Zn is a normalization factor, which
is used to make the NDCG of the ideal ranked list to be 1.
We use NDCG5 to evaluate the ranking results.

We used the state of the art recency ranking model de-
veloped by Dong et al. (Dong et al. 2009) as the baseline
ranking function for comparison, but we did not use click
buzz features. We evaluated our proposed set of features
across several combinations of x (for exponential weighting)
and click smoothing mechanisms. We ﬁrst ﬁx a smoothing
mechanism and generate all features using the same mecha-
nism.

Our ﬁrst set of experiments were designed to evaluate the
impact of exponential weighting of the click features. Hence
we augmented the baseline ranking function with click fea-
tures that did not have exponential weighting (i.e. x = 0)
and did not use any smoothing or Click buzz features were
excluded from these experiments. In these experiments, we
varied the value of x in the range of 0 − 2 in steps of 0.2.
We also generated our click features by smoothing clicks us-
ing different query segmentations such as timeouts, missions
and goals using the research of (Jones and Klinkner 2008)
(Boldi et al. 2008). Table 2 shows that non zero values of
x does indeed help recency ranking. In some cases, the gain
in NDCG5 can be as high as 1.08% (x = 0.4 and using
mission click smoothing). Our evaluation shows that expo-
nential weighting helps in improving NDCG5 for recency
ranking across all the mechanisms for smoothing clicks con-
sidered in this study. In particular goal based smoothing al-
most always provides statistically signiﬁcant gains over the
baseline function.

Table 2: Effect of x on percentage improvement in NDCG5.
All models were trained without click buzz features. The
baseline is x = 0, i.e., with CTR features that are not time
weighted or smoothed. The numbers in parenthesis are the
p-values. Bold entries are signiﬁcant beyond 0.05 level of
signiﬁcance.

x
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
3.0
4.0

Timeouts
0.67 (0.019)
0.74 (0.026)
0.67 (0.027)
0.95 (0.005)
0.65 (0.117)
0.55 (0.073)
0.59 (0.062)
0.75 (0.021)
0.63 (0.039)
0.65 (0.045)
0.71 (0.036)
0.76 (0.039)

Missions

0.92 (0.009)
1.08 (0.001)
0.79 (0.021)

0.95 (0)

0.75 (0.027)
0.62 (0.059)
0.59 (0.119)
0.84 (0.042)
0.67 (0.047)
0.64 (0.022)
0.89 (0.019)
0.78 (0.051)

Goals

0.71 (0.043)
0.80 (0.018)
0.90 (0.010)
1.03 (0.003)
1.00 (0.003)
0.85 (0.010)
0.93 (0.002)
0.81 (0.029)
0.96 (0.003)
1.08 (0.002)
0.86 (0.004)
0.64 (0.084)

We also evaluated the effect of the click buzz features
across different values of x (for exponential weighting) and

Figure 5: Smoothing Clicks for “Circus Album” or “Britney
Spears Album” for “Circus”.

“Britney Spears Album” in the same query chain as those
clicked for “Circus” (see Figure 5). We call this process
of attributing clicks from one query in the query chain to
another query (preferably the ﬁrst one) as smoothing.

We evaluate 3 different smoothing mechanisms: the sim-
plest 30 minute timeouts and the more complex ones, goals
and missions based on (Jones and Klinkner 2008) and (Boldi
et al. 2008).

Experiments

The training and testing data sets for recency consists of
query-URL pairs that were judged in a time-sensitive man-
ner. The training data set consists of 53,775 query-url pairs
and 3291 queries that were made unique with their judged
dates. The testing data set consists of 29,460 query-url pairs
and 1771 queries that were made unique with judged dates.
We use the GBrank algorithm for the ranking model learn-
ing, as GBrank is one of the most effective learning-to-rank
algorithms (Zheng et al. 2007). For the purpose of better
readability, we brieﬂy introduce the basic idea of GBrank.
For each preference pair < x, y > in the available preference
set S = {< xi, yi > |xi (cid:31) yi, i = 1, 2, ..., N }, x should be
ranked higher than y. In GBrank algorithm, the problem of
learning ranking functions is to compute a ranking function
h , so that h matches the set of preference, i.e, h(xi) ≥ h(yi)
, if x (cid:31) y, i = 1, 2, ..., N as many as possible. The fol-
lowing loss function is used to measure the risk of a given
ranking function h

R(h) =

(max{0, h(yi) − h(xi) + τ }2)

1
2

N
(cid:88)

i=1

where τ is the margin between the two documents in the
pair. To minimize the loss function, h(x) has to be larger
than h(y) with the margin τ , which can be chosen to be
a constant value, or as a value varying with the document
pairs. When pair-wise judgments are extracted from editors’
labels with different grades, pair-wise judgments can include
grade difference, which can further be used as the margin τ .
We use normalized discounted cumulative gain (NDCG)
which is a variant of the popular discounted cumulative gain
(DCG) metric to evaluate the quality of the ranking model

1338different smoothing strategies. Table 3 shows that the click
buzz features provide additional improvements (with better
statistical signiﬁcance) over the same baseline function. The
best gains in NDCG5 (1.57%) are observed for x = 0.8 and
goal based click smoothing.

Table 3: Effect of x on percentage improvement in NDCG5
together with click buzz features. Baseline is the same as
used in Table 2.

x
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
3.0
4.0

Timeouts
0.56 (0.178)
0.72 (0.021)
0.87 (0.007)
0.83 (0.003)
0.67 (0.016)
0.63 (0.053)
0.85 (0.003)
0.86 (0.006)
0.75 (0.014)
0.70 (0.027)

1.04 (0)

0.91 (0.003)

Missions

0.99 (0.005)
1.02 (0.002)
1.05 (0.003)

1.23 (0)
1.18 (0)

0.85 (0.014)
1.07 (0.001)
1.30 (0.0002)
1.03 (0.006)

1.13 (0)

0.97 (0.004)
1.06 (0.002)

Goals

1.13 (0.001)
0.90 (0.008)

1.12 (0)
1.57 (0)

1.08 (0.003)
1.00 (0.002)
0.98 (0.001)
0.92 (0.019)
0.94 (0.001)
0.90 (0.028)
1.00 (0.010)
1.12 (0.002)

Discussions and Conclusions

In this study we propose novel temporal click features 3 for
recency ranking. Rather than computing simple aggregate
click through rates, features are derived using the tempo-
ral click through data and query reformulations. One of the
features that we use is click buzz that captures the spiking
interest of a url for a query. We also propose time weighted
click through rate which treats recent observations as being
exponentially more important. We enrich our click features
by following query reformulations which typically beneﬁt
the ﬁrst query in the chain of reformulations. Our experi-
ments demonstrate that these features can improve NDCG5
of a major online search engine’s recency ranking by up to
1.57%.

As part of future work, we would like to evaluate sev-
eral other time series properties such as auto-correlation as
features for recency ranking. Another interesting problem
would be to automatically learn the exponential weighting
scheme based on query or query categories. It would be in-
teresting to evaluate these ideas for general web search rank-
ing as well.

References

Agichtein, E.; Brill, E.; and Dumais, S. 2006. Improving
web search ranking by incorporating user behavior informa-
tion. Proc. of ACM SIGIR Conference.

3Because the proposed click features are all time-sensitive, how
fast and often they can be generated seems to be an issue. However,
a large-scaled commercial search engine can update the click-based
features every hour. Therefore, they can reﬂect the latest popularity
of the documents.

Berberich, K.; Vazirgiannis, M.; and Weikum, G. 2005.
Internet Math 2(3):301–
Time-aware authority rankings.
332.
Boldi, P.; Bonchi, F.; Castillo, C.; Donato, D.; Gionis, A.;
and Vigna, S. 2008. The query-ﬂow graph: model and ap-
plications. In Proceedings of the ACM Conference on Infor-
mation and Knowledge Management (CIKM).
Burges, C.; Shaked, T.; Renshaw, E.; Lazier, A.; Deeds, M.;
Hamilton, N.; and Hullender, G. 2005. Learning to rank us-
ing gradient descent. Proc. of Intl. Conf. on Machine Learn-
ing.
Diaz, F. 2009. Integration of news content into web results.
Proceedings of the Second ACM International Conference
on Web Search and Data Mining (WSDM) 182–191.
Dong, A.; Chang, Y.; Zheng, Z.; and Gilad Mishne, Jing Bai
Karolina Buchner, R. Z. C. L. G. L. 2009. Towards recency
ranking in web search. WSDM.
Freund, Y.; Iyer, R. D.; Schapire, R. E.; and Singer, Y. 1998.
An efﬁcient boosting algorithm for combining preferences.
Proceedings of International Conference on Machine Learn-
ing.
He, D., and G¨oker, A. 2000. Detecting session boundaries
from web user logs. In Proceedings of the BCS-IRSG 22nd
annual colloquium on information retrieval research.
He, D.; G¨oker, A.; and Harper, D. 2002. Combining evi-
dence for automatic web session identiﬁcation. Inf. Process.
Manage. 38(5):727–742.
Jarvelin, K., and Kekalainen, J. 2002. Cumulated gain-based
evaluation of ir techniques. ACM Transactions on Informa-
tion Systems 20:422–446.
Joachims, T. 2002a. Optimizing search engines using click-
through data. Proc. of ACM SIGKDD Conference.
Joachims, T. 2002b. Optimizing search engines using click-
In Proceedings of the ACM Conference on
through data.
Knowledge Discovery and Data Mining (KDD).
Jones, R., and Klinkner, K. 2008. Beyond the session time-
out: Automatic hierarchical segmentation of search topics
In Proceedings of the ACM Conference on
in query logs.
Information and Knowledge Management (CIKM).
Radlinski, F., and Joachims, T. 2005. Query chains: Learn-
In ACM SIGKDD In-
ing to rank from implicit feedback.
ternational Conference On Knowledge Discovery and Data
Mining (KDD).
Y. Cao, J. Xu, T.-Y. L. H. L. Y. H., and Hon, H.-W. 2006.
Adapting ranking svm to document retrieval. Proceedings
of ACM SIGIR conference.
Zhang, R.; Chang, Y.; Zheng, Z.; Metzler, D.; and Nie,
J. 2009. Search result re-ranking by feedback control ad-
justment for time-sensitive query. North American Chapter
of the Association for Computational Linguistics - Human
Language Technologies (NAACL HLT).
Zheng, Z.; Zhang, H.; Zhang, T.; Chapelle, O.; Chen, K.;
and Sun, G. 2007. A general boosting method and its appli-
cation to learning ranking functions for web search. NIPS.

1339