Generalized Task Markets for Human and Machine Computation

Dafna Shahaf∗

dshahaf@cs.cmu.edu
Carnegie Mellon University

Eric Horvitz

horvitz@microsoft.com

Microsoft Research

5000 Forbes Avenue, Pittsburgh, PA 15213

One Microsoft Way, Redmond, WA 98052

Horvitz 2007; Kapoor et al. 2008). Work in this area has
focused on methods that explicitly consider the competen-
cies and availabilities of both computational and human re-
sources to solve problems at hand.

Abstract

We discuss challenges and opportunities for developing gen-
eralized task markets where human and machine intelligence
are enlisted to solve problems, based on a consideration of the
competencies, availabilities, and pricing of different problem-
solving resources. The approach couples human computation
with machine learning and planning, and is aimed at opti-
mizing the ﬂow of subtasks to people and to computational
problem solvers. We illustrate key ideas in the context of Lin-
gua Mechanica, a project focused on harnessing human and
machine translation skills to perform translation among lan-
guages. We present infrastructure and methods for enlisting
and guiding human and machine computation for language
translation, including details about the hardness of generat-
ing plans for assigning tasks to solvers. Finally, we discuss
studies performed with machine and human solvers, focusing
on components of a Lingua Mechanica prototype.

1

Introduction

Connectivity, programmatic access, and the density of hu-
man resources on the Web has enabled human computation
systems (von Ahn 2008), centering on the online recruitment
of people to perform tasks. Human computation includes
methods for acquiring human effort as an implicit output of
peoples’ engagements with online games. The enlistment
of people to solve tasks and subtasks is also central in task
markets, such as Amazon’s Mechanical Turk (mTurk.com),
which provides an online platform for specifying, recruiting,
and reimbursing people for their efforts.

We focus in this paper on opportunities for broadening
methods used in human computation to generalized task
markets (GTM) which admit both human and machine prob-
lem solvers, and that employ learning and planning to coor-
dinate and optimize the ﬂow of tasks and subtasks to peo-
ple and computational solvers. The decomposition and as-
signment of problems within generalized task markets can
rely on distributed processes or on centrally coordinated
monitoring and optimization. Research on generalized task
markets is related to prior efforts on complementary com-
puting on coupling human and machine solvers in an ideal
manner via learning and planning (Horvitz and Paek 2007;

∗Research performed during an internship at Microsoft Re-

search.
Copyright c(cid:13) 2010, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Schematic view of components of a generalized task
market platform.

We foresee the rise of human-computer problem-solving
ecosystems that make available pools of candidate percep-
tual, inferential, and motor competencies that can be har-
nessed and coordinated by generalized task market systems
to create custom-tailored solutions to problems. On the way
to such a potential future, several challenges must be ad-
dressed. In particular, learning, reasoning, and optimization
will play a critical role in making such generalized task mar-
kets a reality.

GTM challenges include the development of components
that interpret problem instances, decompose these posed
problems into subproblems, federate these subtasks to hu-
man and computational solvers via computing ideal feder-
ation and sequencing plans, and ﬁnally composing answers
via a combination of the results into overall solutions. Ef-
fective distribution and recombination of subproblems re-
quires the system to identify, recruit, and reimburse human
and machine solvers. As such, task market platforms may
often be enhanced via the use of machinery for learning
about competencies, availabilities, and the pricing of differ-

986Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10)information) of agents that can provide problem-solving ef-
fort in return for some reimbursement that provides incen-
tives solve task subproblems. Execution is monitored and
data is collected about multiple aspects of problem solv-
ing. A case library of interactions is used to build predictive
models about the abilities, availabilities, and preferences of
agents. These predictive models are harnessed in the feder-
ation planner.

The task federation problem lies at the heart of GTM op-
eration. For this planning challenge, we consider a set of n
agents, A = {A1, ..., An}. Each agent, Ai is associated with
a set of abilities, {abilityj}. An ability is a pair (cid:104)bk, vk(cid:105) for
bk ability identiﬁer and vk ∈ R+ represents the skill level of
the agent for that ability. Agents can represent any source of
problem solving drawing upon human or computational re-
sources. Abilities can span a spectrum of competency from
such computational tasks as “Factor number N ” to “Write a
review for movie M .” We shall focus on the planning prob-
lem for harnessing people and computation to perform trans-
lation among multiple languages.

Translation is an illustrative domain for human-machine
collaboration within a generalized task market. Translat-
ing long documents is often a cumbersome task, requiring
skilled bilingual people. Although machine translation (MT)
has made signiﬁcant progress in recent years, human re-
ﬁnements are still essential for natural-sounding results (see
(Callison-Burch, Bannard, and Schroeder 2004)). Figure 2
displays an example of an online French-to-English machine
translation service. The translation provided is far from per-
fect, but provides understandable output to English speakers.
Moreover, providing ﬁxes for MT output is easier, requiring
less skill and can be performed by monolingual people.

In addition to agents, the planning problem is deﬁned as
a set of m independent high-level tasks H = {h1, ..., hm}
and a set T of low-level tasks. Each high-level task hi can be
solved via one or more solution (scenarios). Scenarios are
represented as directed acyclic graphs (DAGs), where ver-
tices are low-level tasks, and edges specify a partial prece-
dence order: there is an edge from ti to tj if task tj cannot
be performed unless task ti is already satisﬁed (e.g., because
the output of ti is needed as an input to tj). A low-level task
tj is associated with a set demandj of abilities.

Figure 3 displays a representation of a sample high-level
task for translation. In this case, we wish to translate a para-
graph from English to French. The ﬁgure shows three dif-
ferent scenarios for accomplishing this task: (a) acquire a
bilingual person who can translate the paragraph directly,
(b) acquire two bilingual people, one of whom can trans-
late from English to an intermediate language (in this case,
Italian), and the other from the intermediate language into
French, or (c) use machine translation procedures to trans-
late the paragraph from English to French and then engage
a French-speaking person to correct the paragraph. Sce-
nario b might be assumed a priori to require more compe-
tent language skills from both of the human translators, as
the pipeline with two translations is likely to create more
errors than single-stage translations. However, more gener-
ally, the errors incurred with each scenario will depend on
the competencies of the human and computational agents.

More formally, we say that a task market coalition C ⊂

Figure 2: Sample case of machine translation. Errors are marked
with circles and sample partial ﬁxes are noted in text balloons.

ent problem-solving resources.

We shall illustrate key aspects of the challenge of devel-
oping a GTM platform for the task of language translation.
We have constructed components and an overall prototype as
part of the Lingua Mechanica project on methods for using
a generalized task market for translation. The prototype for-
mulates plans for acquiring translations via machine trans-
lation and from people with translation skills, and performs
translation among multiple languages. The core approach
is that people with different competencies are recruited to
reﬁne rough translations generated by a machine translation
(MT) system or by other human translators. The system con-
tinues to optimize the ﬂow of task assignments based on the
expected utility of engaging human or machine assistance,
and continues to collect data to reﬁne its understanding of
competencies and availabilities.

After an overview of key components of a GTM platform
and service, we discuss the hardness of the GTM planning
problem for language translation. We provide polynomial
approximations to several classes of task federation mod-
els. Then, we present the use of games to provide incentives
for people to reﬁne and conﬁrm translations. We describe
results from studies of the behavior of people working with
components of the Lingua Mechanica prototype. We discuss
the distribution of language competencies among a popula-
tion of contributors in a Lingua Mechanica study engaged to
provide self-assessments of translation competency and to
ﬁx the output of machine translation. We explore the opera-
tion of the task federation planner of the prototype on plan-
ning problems deﬁned by this user base. Then, we report
on the experience with the use of a gaming component for
acquiring implicit signals about the accuracy of translations
via a game, and discuss the construction of a classiﬁer for
learning how to assign accuracy to translations being reﬁned
in the Lingua Mechanica pipeline.

2 GTM Planning Problem

As displayed in the schematic in Figure 1, a GTM platform
includes components for accepting, interpreting, and decom-
posing a task into subproblems and for generating task fed-
eration plans. A GTM planner accesses information about
the availabilities, abilities, and costs (and other preference

987Simple Tasks
Let us start with a simple problem: A high-level task hi is
composed of a single scenario, which is composed of a sin-
gle low-level task ti.

Hardness The problem is N P-hard. The decision prob-
lem is in N P (given a solution, it can be veriﬁed in poly-
time). We show a reduction from weighted exact set cover.
Deﬁnition 3.1. Given a set X and a set S of subsets of X
with associated rewards R(S) for each S ∈ S, an exact set
cover S ∗ is a subset of S such that every element in X is
contained in exactly one set in S ∗. The goal is to ﬁnd the
exact cover with the maximal reward.

Weighted exact set-cover is NP-hard. Given a weighted

exact set-cover instance, we construct a coalition instance:
Let A = X. Each Sj ∈ S deﬁnes a high-level task hj.
hj is composed of a single scenario and a single low-level
task tj, requiring (cid:104)bj, |Sj|(cid:105) (|Sj| units of ability bj). Each
Ai ∈ Sj has 1 unit of ability bj ( (cid:104)bj, 1(cid:105) ). Therefore, the
only coalition able to perform tj (and thus, hj) is Sj. Deﬁne
the evaluation function such that (cid:80) U (Sj, tj) = R(Sj).

A solution to the coalition problem corresponds exactly to
a solution to weighted exact set cover. Thus, the problem is
N P-hard.

Tractable Approximation via Limiting Coalition Size
The number of possible coalitions considered in GTM plan
generation is exponential in n. Thus, a natural way to reduce
the search space is to restrict the maximal size of a coalition
to k, thus reducing the number of coalitions to O(nk). Such
a restriction is reasonable for language translation as most
tasks do not require more than a few participants.

We consider a graph, in which the agents are vertices and
tasks are a collection of weighted hyperedges. A hyperedge
corresponds to assigning these agents to the task; its weight
is the expected reward. This is also a useful framework for
the case of extra constrains (e.g., that status of the availabil-
ity of an agent that is assigned to a coalition): in this case,
hyperedges correspond to the possible coalitions.

Every task is identiﬁed with a color, and all of its edges
are of this color. We want to ﬁnd a maximum value matching
in the graph such that only one edge of each color is used.
Claim 3.2. A greedy strategy for selecting agents is a
constant-factor approximation to the coalition problem (k
is constant).

The proof is based on a standard greedy analysis. Any
coalition chosen for the solution contains at most k agents,
and we compare their assignment to the optimal solution’s
assignment.

Tractable Approximations via Special Utility Functions
We now relax the assumption of limited coalition size. The
number of coalitions we need to consider is exponential in
n again. We now seek tractability by introducing constraints
on the utility functions to restrict our search space. We can
assume submodularity (diminishing returns, ui(S ∩ S(cid:48)) +
ui(S ∪ S(cid:48)) ≤ ui(S) + ui(S(cid:48))), subadditivity (ui(S ∪· S(cid:48)) ≤
ui(S) + ui(S(cid:48))), or superadditivity.

As an example, suppose that the utility functions are

monotone and submodular.

(a)

(b)

(c)

Figure 3: Three translation scenarios: (a) direct human transla-
tion, (b) human translation in two steps via an intermediate lan-
guage, and (c) machine translation followed by repair by a person.
Numbers refer to target quality.

A is deﬁned as a group of agents which cooperate in order
to achieve a common task. We assume that a coalition can
work on a single (low-level) task at a time. We consider
situations where agents may or may not be members of more
than one coalition. The utility achieved by performing the
task tj by coalition C is U (C, tj). The utility depends on
tasks demands and the coalition’s composite abilities. The
utility of a high-level task hi depends on the utilities of the
low-level tasks that comprise the high-level task.

We now consider approximations to the task federation
planning problem. We ﬁrst assume modularity of effort,
which asserts that the combined abilities of a coalition of
agents is the sum of the abilities of the participating agents.
If agents participate in more than one coalition, they can dis-
tribute their abilities among the coalitions. In the context of
a binary utility model, where a task is either completed for
full utility or not completed generating no value, modular-
ity of effort means that tj can be satisﬁed by a coalition of
agents C only if their combined abilities are higher than the
task’s demands. Otherwise, U (C, tj) = 0. A high-level task
hi is satisﬁed if all of the low-level tasks tj of at least one
scenario are satisﬁed. Later, we shall consider other types of
utility functions.
Deﬁnition 2.1 (Coalition Problem). Given (cid:104)A, H, T (cid:105), the
coalition problem is to assign tasks t ∈ T to coalitions of
agents C ⊆ A such that the total utility is maximized and
the precedence order is respected.

In general, agents and tasks are both associated with sets
of constraints (e.g., dollar value, availability, time critical-
ity).
In this formulation, the problem is to maximize the
utility under all of the constraints.

3 Hardness and Approximations for the

GTM Planning Problem

We shall now review the hardness of the GTM federation
planning problem and provide polynomial approximations
for models of different expressiveness.

988Claim 3.3. A greedy algorithm yields a 1
2 -approximation. In
special cases where the submodular function is of a special
type (discussed later), a (1 − 1
e )- approximation has been
achieved; this is optimal for these problems.

Those results follow from efﬁcient approximation al-
gorithms (Nemhauser, Wolsey, and Fisher 1978; Vondrak
2008) that exist for the Submodular Welfare Problem. The
coalition problem can be easily formalized as a Welfare
In this problem, m items are to
Maximization Problem.
be distributed among n players with utility functions ui :
2[m] → R+. Assuming that player i receives a set of items
Si, we wish to maximize the total utility (cid:80)
i ui(Si). In our
case, an agent corresponds to an item and tasks correspond
to players. For task ti, utility function ui(C) is U (C, ti).

Tractable solutions can also be formulated for subadditive
utility functions. The problem has been transformed into
linear programming problem (Feige 2006), with the round-
ing of fractional solutions, achieving an approximation ratio
of 1
2 . For the superadditive case, an approximation ratio of
√
log m
m can be achieved (Mirrokni, Schapira, and Vondrak

2008).

Tasks Consisting of a Single Scenario
We now move to the more complex case where a high-level
task hi takes the form of a single scenario. The scenario
DAG corresponds to a partial ordering on low-level tasks tj.
The hardness of the problem follows from the previous
section. We build on the algorithms described by Shehory
and Kraus, to provide a constant-factor approximation to the
case of limited-size coalitions. The algorithm can be viewed
as a centralized version of the methods described in (She-
hory and Kraus 1995).

For each task t, we denote its set of predecessors, includ-
ing t, by P (t). The choice of t for coalition formation will
depend on the costs and beneﬁts of the formation of coali-
tions that perform all of the tasks in P (t).

At each iteration, we choose in a greedy manner to per-
form the task t associated with the maximal u(P (t)) to-
gether with all of its predecessors, and form the required
coalitions. We remove t, all of its predecessors, and the as-
signed agents from the list, and iterate. We note that only a
small portion of the calculations needs to be updated.

Tasks with Multiple Scenarios
We now consider the most expressive case where there are
multiple scenarios for solving tasks. As before, we assume
that the size of coalitions is limited. We also limit the max-
imal number of low-level tasks executed within each sce-
nario.

The algorithm we propose is a combination of two algo-
rithms we have already discussed. We again construct a hy-
pergraph, where a high-level task is a set of hyperedges, cor-
responding to coalitions that can carry out one of the scenar-
ios. When a coalition is chosen, one can also optimize over
the scenarios it can carry. The analysis of this case is similar.

Case of Transitive Tasks An interesting case of multi-
ple scenarios for GTM solvers is where the tasks are transi-
tive. Transitive tasks allow for a compact representation of
many possible scenarios by encoding them as graph paths.

Such transitivity is especially valuable in GTM for language
translation. The task of translating from French to English is
the same as translating from French to any intermediate lan-
guage, and then to English. The path might even involve a
chain of translations among several intermediate languages.
A natural way to solve the case of transitive tasks is
with multi-commodity ﬂow. We have a weighted multigraph
G = (V, E, w). Each s ∈ V represents a language. Each
problem-solving resource, whether a human or machine rea-
soner, corresponds to a set of directed edges. An edge
weight represents the user’s degree of competence at trans-
lating between the two languages represented by the nodes
that it connects. As an example, in Figure 4 we characterize
with numeric scores the competencies of different transla-
tors. We have one participant who speaks Hindi and Tel-
ugu well. Another participant is a Hindi speaker who knows
some English and Kannada (dashed edges correspond to low
weights on competency). Finally, we have several speakers
of European languages.

Figure 4: Translation abilities graph. Each resource is a collection
of edges (in the same color). Dashed/thin edges correspond to low
skills, thick edges correspond to high skills.

In addition, we have k tasks (commodities), where each
one has source language si ∈ V , sink (target language) ti ∈
V , and a starting ﬂow di ∈ R (in the basic scenario, di = 1).
The goal of a GTM federation planner is to maximize the
total throughput.

Note that unlike our earlier binary utility notion (a task is
either completed for full utility, or generates no value), the
linear program allows for a softer notion of task quality.
As an example, the GTM planning problem displayed in
Figure 4 shows two candidate paths for English-to-French
translation for the solvers available: direct translation or
translating from English to French through a translation by
one person from English into the intermediate Italian lan-
guage and then by another person from Italian to French.
The low weight on the direct edge indicates a low compe-
tency for the direct translation, but the other two edges are
strong. However, as errors accrue over chains, the use of
intermediate translations would tend to reduce the quality.
How should we compare the output qualities?

In GTM for translation, edge weights represent the proba-
bility that the translation at each transition will reach a spec-
iﬁed threshold of quality. Thus, the quality of a path is the
product of its edge probabilities. We need to modify the tra-
ditional multi-commodity ﬂow solution to handle this case.
Each directed edge e has f in
, and a capacity ce. The
key concept is that when ﬂow goes through an edge, it suf-

, f out

i

i

989fers some loss in quality. We seek some threshold quality at
the sink. (Note that the product model is simplistic. In the
future, we will need to either learn or model the way quality
depends on competencies along the sequence.)

We shall ﬁrst formalize this optimization problem as a lin-
ear program (LP), and then introduce rounding techniques to
achieve a feasible solution. We can formalize the optimiza-
tion problem as follows:

max

f out
i

(e) s.t.

i e: edges to ti

f in
i (e) ≤ ce

f in
i (e) = di

e: edges from si
f out
i

i

w

(e) = w(e) · f in

i (e) (*Quality reduction*)

f out
i

(w, u) =

f in
i (u, v) u (cid:54)= si, ti

(*Flow*)

v

The GTM plan seeks to maximize the quality of commodity
i reaching its sink. The ﬂow on every edge (workload) is no
more than the capacity of every edge. Alternatively, we can
restrict total capacity per contributor.

Another interesting feature of transitive tasks is the feasi-
bility of reusing intermediate results. Consider the sample
goal of translating an English article into both Kannada and
Hindi. The previous algorithm would treat them as two sep-
arate problems, translating directly from English into both.
However, given the resources and competencies available,
it may be more beneﬁcial to translate English to Hindi, and
then Hindi to Kannada. Alternatively, it may be better travel
via an intermediate node for both.

A similar ﬂow formulation can be applied for re-using
intermediate results. We deﬁne a variable for every
(cid:104)language,task(cid:105) combination (the quality of translation for
a speciﬁc task in that language). A task would have one
source and possibly many sinks. The goal now is to mini-
mize the number of intermediate translations, while enforc-
ing minimal translation quality in all sinks. We note that this
minimization is a constraint, not the objective in this case.
The objective should be interpreted as a routing constraint.
Longer paths cost more and are prone to the accrual of er-
rors. While quality is acceptable, we prefer shorter paths.

Given a set of directions for tractable implementation of
GTM federation, we will now move to describe studies of
components of a Lingua Mechanica prototype.

4 Lingua Mechanica Prototype

We implemented a Lingua Mechanica prototype as a set of
components for experimental studies of federation planner
and of computational and human performance on the trans-
lation domain. In this section, we shall describe the proto-
type.

The Lingua Mechanica prototype includes components
that interact with people, including a method for displaying
sentences (including prior machine or human translations of
sentences) and seeking direct translations, and tools for au-
thoring and ﬁelding of games for engaging people to provide
human computation as part of gaming. We will later present

Figure 5: Screenshot of Lingua Mechanica interface for creating
scenarios. The three blocks on the left correspond to inputs. The
blocks on the right represent problem solving effort.

more detail on several sample games and other interactive
capabilities for capturing translation expertise.

The prototype also provides functionalities for deﬁning
tasks. Common tasks are input via task templates. As an ex-
ample, the prototype provides a “Translate a page” option.
The author of the task provides the system with several task
parameters, including the content that needs to be translated
and the source and destination languages. In addition, the
author of the task can change the metadata associated with
the task, including parameters describing the desired qual-
ity, deadlines, and how much (if any) they are willing to pay
for human contributions. The price and deadline affects the
result: the planner can route the request through a profes-
sional translator (high-quality, slow, expensive) or through
MT (low-quality, quick, cheap). Translation challenges can
also be deﬁned as custom-tailored high-level tasks with as-
sociated parameters, and details about the ﬂow of machine
and human effort on tasks can be speciﬁed. We use a Popﬂy-
like1 interface for such authoring. Figure 5 displays an ex-
ample graphical speciﬁcation of tasks and execution. Task
authors deﬁne and connect blocks together into a DAG, such
that the output of one block can be used as inputs to oth-
ers. The speciﬁcations correspond to scenarios described
in Section 2. The sample speciﬁcation in Figure 5 corre-
sponds Scenario c in Figure 3, where the output of the “Ma-
chine Translation” block is the input of “Fix Sentence.” We
implemented the GTM planning algorithm for task federa-
tion that executes the ﬂow procedure described in Section 3.
Rather than having a single node representing a language,
the planner considers two nodes for each language (before
improvement, after improvement), extending the plans to in-
clude monolingual people who can improve the quality of a
sentence that has been translated into their language by a
person or machine. A monolingual person corresponds to
a directed edge between those two nodes. We set the ’after
improvement’ node as our sink node for tasks. Finally, we
add an edge of no cost (and no quality loss associated) be-
tween each such pair of nodes, so the sink is reachable even
if no monolingual people are involved.

Several other changes were made with the planning algo-
rithms. First, to speed up the assignment process, we deﬁned
equivalence classes of abilities. Instead of using the ﬁne-
grained (cid:104)bi, vi(cid:105) abilities, we mapped them to equivalence
classes (thus mapping agents and tasks). For example, we
deﬁned three levels of English proﬁciency. We refer to this

1Popﬂy.com is a website allowing users to create mashups.

990Figure 6: Games for translation. From left to right: Moon Climb, WordTetris, Hangman, Dictionary Builder

primary method for task federation as the Flow procedure.

When the greedy algorithm picked a coalition to perform
a task, equivalent tasks and coalitions are identiﬁed and we
assign as many of those as possible. The approximation
guarantee still holds, and computation is signiﬁcantly faster.
We also integrated logging functions to capture traces of
the detailed activity of people making contributions and ma-
chine components, as well as information about the avail-
ability of people. Such data can allow the system to learn
from data so as to better optimize task federation. There
are multiple places for learning about machine reasoners, as
well as people and their competencies and availabilities in
a GTM system. As an example, the online nature of the
system makes learning about the availability of people an
attractive functionality. A planner might not try to execute
a translation to Chinese now if it knows some highly-skilled
contributors will log in soon. We will describe one of the
learning tasks in more detail in Section 6.

5 Acquiring Human Computation

We have explored several methods for engaging people to
provide human computation for translation, including the
use of games with a purpose and providing monetary re-
wards or lotteries for rewards.

Accessing Human Computation via Games
Incentives for recruiting human computation include provid-
ing monetary rewards, receiving points, achieving skill lev-
els, increased rankings or reputation in a community, altru-
ism, patriotism, and playing or competing in games. There
has been growing interest in engaging people with participa-
tion in fun or challenging online games with a purpose. This
idea was ﬁrst explored by (von Ahn and Dabbish 2004), who
used a matching game to seek labels about images from peo-
ple. We designed and implemented several different games
for enlisting human computation for translation, drawing as-
sistance from MT experts. Screenshots from these games
are displayed in Figure 6. The game of MOON CLIMB chal-
lenges bilingual players to match a sentence with its best
translation. With DICTIONARY BUILDER, players translate
speciﬁc words (e.g., words that are not available in a bilin-
gual dictionary or words assigned low conﬁdence by a trans-
lation system). The players are shown translated search re-
sults for this word (images and snippets), and try to guess

the missing word. We shall provide more detail on two
additional games, HANGMAN and WORDTETRIS. Addi-
tional information on these games is provided in (Shahaf and
Horvitz 2009).

The goal of HANGMAN is to ﬁx a machine-translated
paragraph. Players switch between serving as a ﬁxer and
as a guesser. The ﬁxer receives content translated by MT
and improves it; the guesser sees the translated version and
the ﬁxer’s sentence, with the changes displayed as empty
spaces. The guesser tries to ﬁll in the missing parts, one let-
ter at a time, in a Wheel-of-Fortune manner. Both players
receive points for each word correctly identiﬁed.

WORDTETRIS also centers on the ﬁxing of machine
translations. Two players who speak the target language see
blocks attached to chunks of a sentence falling from the sky.
Each block may have several alternatives corresponding to
MT uncertainty (e.g., “il” is either “he” or “it,” as displayed
in Figure 2). The initial choice is random per user. Users
choose the relative location of the block (effectively choos-
ing word ordering), and its phrase, applying wildcards and
blanks when needed. Their goal is to match the sentence
with their partner. A complete match makes a whole row
disappear, while partial matches result in partial disappear-
ances. The game ends when the stack of blocks reaches the
top of the playing ﬁeld and no new blocks are able to enter.
In a validation study, we found that participants over-
all found WORDTETRIS to be the most engaging game out
of the four tested. Most players became comfortable with
WORDTETRIS within 1-3 rounds. People enjoyed the chal-
lenges provided by the game, which includes a speeding up
with play. On the downside, translations decline in quality
as the game speeds up.

Evaluating Contributors
It is vital to have a means of determining the quality of solu-
tions provided by human computation. In preliminary stud-
ies, users’ linguistic abilities are determined through a short
questionnaire about the languages that they know and their
self-assessment about their competence. Contributors can
take optional qualiﬁcation tests to raise the system’s con-
ﬁdence about their abilities (and to be offered better chal-
lenges). Beyond explicit testing, with time and repeat con-
tributions, the system can learn about competencies using in-
stream testing based on known answers and on alignments of
output with those received from others working on the same

991instance, and a natural language tool that provides scores on
grammar correctness (Cherry and Quirk 2008).

6 Experiments

We now review results from studies of several aspects of the
Lingua Mechanica prototype, including the performance of
the task federation planner and the performance of people
on two interrelated translation tasks.

Participant Base
For the studies, we sought a set of participants with a rep-
resentative distribution over translation skills. We collected
a snapshot of language competencies from 388 participants
from 70 different countries willing to contribute to Lingua
Mechanica, out of a larger group of 2,500 people we invited
to help. The invitees were selected at random from the full
Microsoft employee address book, spanning all Microsoft
sites throughout the world. An incentive of a lottery ticket
for a gift certiﬁcate was provided to invitees.

Before being engaged in translation challenges, the par-
ticipants were asked to provide information on their coun-
try of origin, current location, and language competencies.
The participants were asked to rank their skill level in read-
ing and writing for the set of ﬁfteen popular languages un-
der consideration. Participants assessed for each language
whether they were a beginner, intermediate, or advanced,
based on the topics they can understand (very limited vocab-
ulary/ conventional/ sophisticated) and the amount of editing
required to ﬁx sentences they construct.

Figure 7: Main languages spoken by study participants, broken
down by skill level.

Figure 7 shows the main languages spoken by the partici-
pants. To assess the accuracy of self-assessments of com-
petency, participants were asked to answer a short list of
questions for each language they speak. The questions were
designed to test grammar, vocabulary, and familiarity with
idioms. We found that in general, people who assess them-
selves as experts typically can be trusted. Interestingly, we
found experts could sometimes even recover mistranslated
cultural terms. For example, several experts managed to
reverse-translate the mistranslated Sherlock Holmes book ti-
tle, ”bright red novel study,” back to ”A Study in Scarlet.”
However, people who report themselves as being intermedi-
ate in skill often over-estimate their abilities. Indeed, we had
several people entering comments at the end of the compe-
tency survey such as, “My French is probably rustier than I
remembered.”

Figure 8: Comparative analysis of federation planners. For each
dataset, we show the average task quality for Flow, Greedy and
Random strategies.

Studies of GTM Planner
We ﬁrst report on a comparative analyses of the performance
of GTM planning algorithms. We used the inputs gathered
from participants as a proxy and source of distributional in-
formation. We ran task federation algorithms on a set of
benchmark cases, each containing representative tasks and
agents. Benchmark cases are lists of tasks, each deﬁned by
source and target languages and the minimal quality desired.
We compared the average task quality generated by a round-
ing of the primary Flow procedure, as described in Section
3, with the performance of a greedy and a random selection
strategies. Average task quality is the average over the end
quality over all tasks in the dataset. A task which was not
completed is considered to have zero quality. The Greedy
strategy iterates over the tasks, selecting paths of acceptable
quality that use the fewest resources (humans and comput-
ers each cost a single unit; some edges are free, as discussed
in Section 4). The Random strategy chooses uniformly at
random among the same paths. The results (Figure 8) show
that the performance of Flow nearly always dominates (by a
signiﬁcant factor) Greedy, which in turn nearly always dom-
inates Random.

The datasets consisted of random translation tasks. The
language pairs were determined from our survey distribu-
tion: some datasets focused on translation between lan-
guages with high correlation (e.g. Spanish/Portuguese),
while others focused on the opposite case (e.g. Ger-
man/Chinese).

Studies of Human Repair of Machine Translation
We also explored the abilities of the participants in our study
to repair machine-translated sentences in the languages they
indicated they had some degree of competence with. The
base machine translations and the repaired sentences were
checked for grammatical correctness and understandabil-
ity by experts, checking whether the intended meaning of
the sentence is clear, even if there are some mistakes. We
expected that experts would be accurate raters. We con-
ﬁrmed that their relative ratings were frequently consistent
with other experts.

Translation challenges were created by translating sen-
tences drawn from Polish Wikipedia to multiple languages

992via the Bing Translator. We found that participants could
improve machine translation to an acceptable level of un-
derstandability. Detailed traces of machine translations and
repairs are provided in (Shahaf and Horvitz 2009).

We next explored if game playing could provide a valu-
able signal about the accuracy of the sentences that partici-
pants had attempted to repair in the ﬁrst phase of the study.
We invited a group of 6 new participants from our organiza-
tion to play a version of the HANGMAN game (Section 5),
for an incentive of a lunch coupon. In each trial of the game,
a sentence that had been reﬁned by participants in the ear-
lier phase described above was displayed in Hangman style,
with empty slots appearing in lieu of letters. Players were
asked to try to ﬁll in all of the missing letters in the sen-
tence by issuing a sequence of letter guesses. Letters cor-
rectly guessed were immediately rendered in the appropri-
ate places in the sentence. Players gained points for each
completed sentence and worked as fast as they could under
a total time allotted for the overall session.

We noted that poor translations would often take players
longer to complete. As an example, some of the translators
in the ﬁrst phase of the reﬁnement (especially beginners),
ﬁxed the machine translation in a way that is grammatically
correct and understandable, yet wrong. For instance, one
of the participants had translated that, “Sherlock Holmes
wrote books about Arthur Conan Doyle.” During the gaming
study, players encountering such translations often looked
confused, and paused to reﬂect about potential alternatives
before guessing letters. Similarly, sentences using the wrong
prepositions caused almost all translators to make the same
mistakes (e.g. trying to type the correct preposition ’in’, in-
stead of the wrong ’at’).

We collected logs of the guesses and timing and con-
structed a library of cases. Each case was labeled with a
measure of the translation accuracy. Labels were provided
by experts. Features included knowledge about the initial
translator (skill level, both reported and assessed), knowl-
edge about the sentence itself (number of words, grammar
correctness score, number of conjunctions), and features
drawn from logs of activity by players (the maximal pause
time, minimal pause time, number of letters guessed cor-
rectly, and total time to guess). Those features were nor-
malized across players (dividing by the length of the longest
round for each player), to compensate for speed differences.
We trained a classiﬁer using logistic regression from the case
library. We used leave-one-out cross-validation. The clas-
siﬁer allowed us to boost classiﬁcation accuracy of the ac-
curacy of translations by 12% (from a baseline marginal of
∼ 31% to ∼ 43%). We believe that adding richer features
could further improve the classiﬁcation accuracy.

7 Summary and Directions

We described the opportunity for developing generalized
task market platforms, with a focus on human-computer
task markets that mesh human expertise and machine intel-
ligence to solve problems. We discussed key components
of such systems, centering on the challenge of generating
plans for distributing subproblems to people and machines
based on competencies, availabilities, and pricing of the dif-
ferent solvers. We examined the hardness of planning and

introduced approximations. We described how we can har-
ness a multi-commodity ﬂow procedure for federating tasks.
We illustrated ideas in the context of efforts on Lingua Me-
chanica, a human-computer task market prototype for lan-
guage translation. We reviewed components of the prototype
that provide methods for authoring translation tasks and for
acquiring human computation via rewards and games with
a purpose. Finally, we reviewed several evaluation studies
performed to probe the performance of different aspects of
the machine and human computation harnessed by a Lingua
Mechanica prototype. Future research includes exploration
of applications that involve the weaving together of broader
classes of resources, including perceptual, inferential, and
motor skills to solve new classes of problems. In the most
general case, task markets should handle tasks requiring per-
ception, reﬂection, and action in the world, generating solu-
tions that weave together combinations of these skills. There
are a number of challenges ahead with the creation of a rich
ecosystem of machine and human resources and with oper-
ating generalized task markets in the open world. However,
we foresee multiple opportunities on the horizon with the
development and ﬁelding of generalized task markets that
can ideally tap human and machine intelligence.

Acknowledgments The authors thank Vikram Dendi,
Paul Koch, Raman Sarin, Paul Newson and Tommy A. Bros-
man for their assistance on the Lingua Mechanica project.

References

Callison-Burch, C.; Bannard, C.; and Schroeder, J. 2004. Improved
statistical translation through editing. In EAMT-2004.
Cherry, C., and Quirk, C. 2008. Discriminative, syntactic language
modeling through latent svms. In AMTA ’08.
Feige, U. 2006. On maximizing welfare when utility functions are
subadditive. In STOC ’06.
Horvitz, E., and Paek, T. 2007. Complementary computing: Poli-
cies for transferring callers from dialog systems to human recep-
tionists. User Modeling and User Adapted Interaction.
Horvitz, E. 2007. Reﬂections on challenges and promises of
mixed-initiative interaction. AI Magazine 28(2).
Kapoor, A.; Tan, D.; Shenoy, P.; and Horvitz, E. 2008. Comple-
mentary computing for visual tasks: Meshing computer vision with
human visual processing.
Mirrokni, V.; Schapira, M.; and Vondrak, J.
Tight
information-theoretic lower bounds for welfare maximization in
combinatorial auctions. In EC ’08. ACM.
Nemhauser, G. L.; Wolsey, L. A.; and Fisher, M. L. 1978. An anal-
ysis of approximations for maximizing submodular set functions I.
Mathematical Programming 14(1).
Shahaf, D., and Horvitz, E. 2009. Investigation of human-computer
task markets: Methods and prototype. Technical Report Mi-
crosoft Research Technical Report MSR-TR-2009-181, Microsoft
Research.
Shehory, O., and Kraus, S. 1995. Task allocation via coalition
formation among autonomous agents. In IJCAI ’95.
von Ahn, L., and Dabbish, L. 2004. Labeling images with a com-
puter game. In CHI.
von Ahn, L. 2008. Human computation. In ICDE. IEEE.
Vondrak, J. 2008. Optimal approximation for the submodular wel-
fare problem in the value oracle model. In STOC ’08.

2008.

993