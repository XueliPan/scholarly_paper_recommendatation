Modeling Buffers with Data Refresh Semantics

in Automotive Architectures∗

Linh T.X. Phan1

Reinhard Schneider2

Samarjit Chakraborty2

Insup Lee1

1Department of Computer and Information Science, University of Pennsylvania, USA

2Institute for Real-Time Computer Systems, TU Munich, Germany

{linhphan,lee}@cis.upenn.edu

reinhard.schneider@rcs.ei.tum.de samarjit@tum.de

ABSTRACT
Automotive architectures consist of multiple electronic control units
(ECUs) which run distributed control applications. Such ECUs are
connected to sensors and actuators and communicate via shared
buses. Resource arbitration at the ECUs and also in the commu-
nication medium, coupled with variabilities in execution require-
ments of tasks results in jitter in the signal/data streams existing in
the system. As a result, buffers are required at the ECUs and bus
controllers. However, these buffers often implement different se-
mantics – FIFO queuing, which is the most straightforward buffer-
ing scheme, and data refreshing, where stale data is overwritten by
freshly sampled data. Traditional timing and schedulability analy-
sis that are used to compute, e.g., end-to-end delays, in such auto-
motive architectures can only model FIFO buffering. As a result,
they return pessimistic delay and resource estimates because in re-
ality overwritten data items do not get processed by the system.
In this paper we propose an analytical framework for accurately
modeling such data refresh semantics. Our model exploits a novel
feedback control mechanism and is purely functional in nature. As
a result, it is scalable and does not involve any explicit state mod-
eling. Using this model we can estimate various timing and perfor-
mance metrics for automotive ECU networks consisting of buffers
implementing different data handling semantics. We illustrate the
utility of this model through three case studies from the automotive
electronics domain.
Categories and Subject Descriptors: C.3 [Special-purpose and
application-based systems]: Real-time and embedded systems
General Terms: Design, veriﬁcation, performance.
Keywords: Modeling, analysis, buffer management.

1.

INTRODUCTION

Automotive architectures typically consist of a collection of elec-
tronic control units (ECUs) that are connected by multiple com-
munication buses implementing various protocols such as CAN
and FlexRay. Such a platform is used to execute multiple dis-
tributed control applications that obtain their input from various
∗

This research was supported in part by NSF CNS-0931239, NSF
CNS-0834524, NSF CNS-0721541, NSF CNS-0720703 and the
DFG (Germany) through the SFB/TR28 Cognitive Automobiles.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
EMSOFT’10, October 24–29, 2010, Scottsdale, Arizona, USA.
Copyright 2010 ACM 978-1-60558-904-6/10/10 ...$10.00.

data refresh …

r
o
s
n
e
s

finite

FIFO

FIFO

infinite

infinite

r
o
t
a
u
t
c
a

Figure 1: Network having buffers with different semantics.

sensors. Hence, there is information processing and propagation
in the form of data/message streams, originating from sensors and
terminating at actuators and passing through various ECUs and
buses. Although the sensors typically sample data at periodic inter-
vals and tasks are also periodically activated, resource arbitration at
the ECUs and buses, coupled with variable processing demands of
tasks, introduce jitters in the data streams. As a result, buffers need
to be placed at various positions in the architecture, e.g., at the bus
controllers.

Given such a setup, there are two predominant data handling se-
mantics implemented in the buffers: (i) First In First Out or FIFO,
where data should not be lost and the history of transmission or
ordering of data is important. Data streams containing incremen-
tal information, e.g., the speed increase of a car, are buffered in
this manner. (ii) Data refresh semantics, where buffers are of re-
stricted size and stale data is overwritten by freshly sampled sen-
sor data. Such semantics is used where data with the most recent
values are of interest, e.g., the actual speed of a car. However,
modeling such buffer overwrites – while doing timing/performance
analysis of the system – turns out to be a challenging problem and
has mostly been ignored in the past. Ignoring such overwrites is ac-
ceptable in streaming multimedia applications, e.g., where encoded
video data is stored in a buffer and existing data is never overwrit-
ten (in fact overwriting or data loss is not desirable). However, for
many control applications, stale data is not useful and is replaced
by newly sampled data. Neglecting overwrites in this case leads to
pessimistic estimates on computation and communication resource
requirements (since the overwritten data is also assumed to be pro-
cessed in the model).

In this paper we develop an analytical model for ECU networks
containing buffers implementing both the above semantics (e.g.,
see Fig. 1). Our model is motivated by previous work on modeling
and analysis of applications processing continuous data streams us-
ing the Real-Time Calculus (RTC) framework [16]. We extend the
RTC framework using a novel feedback control mechanism that
enables the modeling of complex “state information”, which in this
case is the deletion of existing data in the buffer by fresh data. The
main challenge in modeling such overwrites stems from the rel-
atively complex overwriting process, viz., the oldest data in the
buffer is overwritten by freshly sampled data. Typically such data
refresh semantics requires explicit state-based modeling, e.g., using
timed automata (see [8]). This leads to the well-known state space
explosion problem, which has also been reported in the context of
state-based modeling of applications processing data streams [6].

119Our main contribution is to suitably modify the RTC framework
with an appropriate abstraction and a feedback control construct,
that avoids any state-based modeling, but nevertheless accurately
captures the complex buffer refresh semantics.

The importance of this problem has recently been pointed out in
a number of studies. In [11] buffering mechanisms with overwrit-
ing have been discussed to bridge the gap between synchronous
semantics at the model level and the asynchronous nature of im-
plementation platforms. The semantics of tag systems [1] has been
used to model systems with buffers implementing the data refresh
semantics as we do in this paper [2]. This has been extended to
model loosely time-triggered architectures in [3]. However, all of
these efforts were directed towards studying the functional correct-
ness of the system. In this paper, we model the data refresh seman-
tics to quantitatively capture the load on system resources and the
volume of actual data that is processed by the system. Our goal
is to factor this into the computation of timing properties of the
system, e.g., delays suffered by messages. The need for quantita-
tively capturing this in performance analysis techniques has also
been pointed out in [7].

Developed on top of the Network Calculus [4] theory from the
communication networks domain, the RTC framework [5] we use
in this paper has been extensively adapted to model and analyze
heterogeneous real-time systems in a compositional manner (e.g.,
see [17]). The central concept in this framework is its use of arrival
functions to model the timing properties of data streams and service
functions to capture the availability of resources. Speciﬁcally, each
data stream is modeled by a pair of arrival functions, αu(Δ) and
αl (Δ), which denote the upper- and lower-bound on the number of
data items that may arrive in any time interval of length Δ. Simi-
larly, a resource is modeled by a pair of service functions, βu(Δ)
and βl (Δ), which specify the maximum and minimum number of
items that can be processed by the resource within any time inter-
val of length Δ. Given the arrival functions of an input data stream
and the service functions of a resource, one can compute – using
purely algebraic techniques – various bounds on system properties
such as the maximum backlog of data items at a buffer, the maxi-
mum delay suffered by the input stream, the arrival functions of the
output stream, and the service functions of the remaining resource.
The output arrival functions can then be fed as inputs to the next
resource whereas the remaining service functions can be used to
process the next data stream.

The arrival and service functions in the RTC framework admit
a much richer collection of arrival sequences and resource patterns
than the classical event and resource models (e.g., periodic, spo-
radic, bounded delay) do. Its algebraic feature also enables efﬁ-
cient computation of system’s performance in a fully compositional
manner. However, the standard RTC formalism assumes an un-
bounded buffer size and does not model buffering schemes that are
dependent on the state of the buffer. As mentioned earlier, this as-
sumption is not only unrealistic but also prevents RTC from being
applicable to many common practical systems.

When data refresh semantics is implemented in a buffer, the
smaller the buffer size, fewer will be the the number of data items
to be processed downstream (e.g., on the ECU next to the actuator
in Fig. 1). This is because certain data items will be overwritten and
will therefore not have to be processed subsequently (i.e., beyond
the buffer which implements data refresh in Fig. 1). In such cases,
assuming that no data is lost – as in the standard RTC framework –
results in a higher system load and hence pessimistic timing bounds
and resource estimates.

Our contributions.
In this paper, we extend the existing RTC
framework to model and analyze systems with buffers implement-

ing both FIFO as well as the data refresh semantics. The key idea
in our technique is to use in combination the concept of a virtual
processor to encapsulate the data overwriting scheme and a feed-
back control mechanism to capture the overﬂow constraints. Our
analysis relies solely on algebraic manipulations and thus can be
computed efﬁciently. The technique we propose here signiﬁcantly
enhances the modeling power of the existing RTC framework while
sidestepping the problems associated with other ﬁne-grain state-
space models [6, 8]. At the same time, it is modular and fully com-
positional. Through case studies, we illustrate how our method can
be seamlessly integrated into the current RTC framework, and at
the same time we show the effects of capturing buffer overwrites
on the accuracy of the analysis. We also provide an experimental
validation of our analysis method against simulation. It is worth
noting, however, that our analytical method is not only faster but
also able to provide guaranteed bounds on the system properties,
which cannot be achieved using simulation.

Related work.
The ﬁrst line of work targeted towards state-
dependent systems comes from the formal methods domain. Timed
automata and related automata-theoretic formalisms have been em-
ployed to model task scheduling of hard real-time systems [8] as
well as systems processing data streams [6]. Although automata-
theoretic models are highly expressive, they often suffer from the
state explosion problem when applied to realistic settings.

The effect of ﬁnite buffer capacities has been studied in the con-
text of data ﬂow graphs [10]. For instance, an algorithm for com-
puting the buffer capacities that satisfy throughput constraints was
presented in [18]. Analysis of self-time scheduling for multirate
data ﬂow with ﬁnite buffer capacities was studied in [12]. Back-
pressure was used in [15] as a mechanism to allow a semantics pre-
serving implementation of synchronous models on Loosely Time
Triggered Architectures.

Further, as mentioned above, [2, 3, 11] have proposed techniques
for modeling systems with data refreshing, although from a func-
tional correctness perspective. The main goals of these frameworks
are to investigate communication and clock synchronization proto-
cols that are data semantics preserving in a distributed time trig-
gered platform with asynchronous communication. This is done
by means of tag structures [1], which hold information about the
freshness levels of the data, and an enforcement of constraints on
these tags to ensure correctness of data values. Unlike these tech-
niques, our framework does not deal with the functional aspects of
the system and imposes no constraints on the system. Instead, it
provides methods to compute timing and workload related perfor-
mance properties in presence the of data refresh, which was not
addressed in [2, 3, 11].

Lastly, various data management mechanisms have also been in-
vestigated to handle overﬂow conditions in bounded buffers. For
instance, [14] identiﬁes four different overﬂow policies – namely
Drop Newest, Drop Oldest, Drop Random and Drop All – and presents
a simulation-based framework for analyzing properties such as the
number of dropped data items and the average delay of the pro-
cessed data. The refresh semantics we consider here is identical to
the Drop Oldest policy, which is most relevant in automotive ar-
chitectures that involve transmission of sensor data. We further ex-
tend our analysis to other data management mechanisms proposed
in [14]. It is worth noting that our method is purely analytical and
thus applicable to safety-critical applications (which is not the case
with simulation-based approaches such as [14] that fail to provide
any guaranteed timing bounds). Our method also works faster com-
pared to simulation, which is time consuming.
Organization of the paper.
In the next section we describe the
basic concepts of the RTC framework. Section 3 focuses on our

120analysis technique for the basic data refresh semantics, followed
by an extension to other data refresh semantics in Section 4. We
present our case studies in Section 5 and conclude in Section 6 by
outlining some directions for future work.
2. RTC BACKGROUND

The RTC framework was developed based on (min,+) and (max,+)
algebra [4] and models data streams and processing resources us-
ing a count-based abstraction. Speciﬁcally, an arrival pattern of
a stream is modeled as a cumulative function A(t) that gives the
number of items arriving over the time interval (0,t]. The set of all
arrival patterns of a stream is represented by a pair of arrival func-
tions α = (αu,αl ), where αu(Δ) and αl (Δ) specify the maximum
and minimum number of data items that can arrive from this stream
over any time interval of length Δ. In other words, for all A(t),

∀Δ ≥ 0, ∀t ≥ 0 : αl(Δ) ≤ A(Δ + t) − A(t) ≤ αu(Δ).

Similarly, a service pattern of a resource is captured by a cumula-
tive function C(t), with C(t) denoting the number of items that can
be processed by the resource in (0,t]. The set of all service patterns
of a resource is modeled by a pair of service functions β = (βu,βl),
where βu(Δ) and βl(Δ) give the maximum and minimum number
of items that can be processed by the resource over any time inter-
val of length Δ respectively.

Formally, let R = R ∪ {+∞, −∞} where R is the set of real num-
bers. Let F be the set of monotonic functions, i.e., F = { f :
R+ → R | ∀s < t, 0 ≤ f (s) ≤ f (t)} where R+
is the set of non-
negative real numbers. The minimum operator in F, denoted by
⊕, is deﬁned for all f , g ∈ F as usual: ∀t ∈ R+
(t) =
. Similarly, f ∼ g iff f (t) ∼ g(t) for all t ∈ R+
,
min
where ∼∈ {≤, ≥, =}. Further, the supremum (sup), if it exists, of
a set S ⊆ F is the smallest U ∈ F such that h ≤ U for all h ∈ S.
Similarly, the inﬁmum (inf) of S is the largest L ∈ F such that h ≥ L
for all h ∈ S. The deﬁnition of sup and inf can also be similarly
deﬁned over the set R.

¯
f (t), g(t)

´
f ⊕ g

We can now deﬁne the (min,+) convolution ⊗ and deconvolution

˘

`

,

(cid:13) operators as follows. For all f , g ∈ F and for all t ∈ R+
,

¯

˘
f (s) + g(t − s) | 0 ≤ s ≤ t
˘
¯
f (t + u) − g(u) | u ≥ 0
.

`
`

,

One can verify the following results: (i) f ⊗ g = g ⊗ f , (ii) f ⊗ g ≤
f ⊕g, (iii)
+c = ( f +c)⊗g = f ⊗(g+c), and (iv) f (cid:13)g ∼ h
iff f ∼ h ⊗ g where ∼∈ {≤, ≥}.

Let ε ∈ F be such that ε(0) = 0 and ε(t) = +∞ for all t > 0. The
, where

¯
f n | n ≥ 0

˘
∗ = min

sub-additive closure of f is given by f
f 0 = ε and f n+1 = f n ⊗ f for all n ∈ N, n ≥ 0.

∗(g).

THEOREM 2.1

([4], THEOREM 4.3.1). For any given f , g ∈
F, the inequality h ≤ g ⊕ f (h) has one unique maximal solution,
given by h = f
We denote by ◦ the composition of two operators: (O1 ◦ O2)(x) =
O1(O2(x)). The linear idempotent operator Ig for any ﬁxed g ∈ F
is deﬁned by Ig( f )(t) = inf{g(t) − g(s) + f (s) | 0 ≤ s ≤ t}. Then,
the following holds [4],

( f ⊕ Ig)∗ = (Ig ◦ f )∗ ◦ Ig.

(1)
The (max,+) convolution ⊗ and deconvolution (cid:13) operators are de-
ﬁned as: for all f , g ∈ F and for all t ∈ R+

,

´
f ⊗ g
´
f (cid:13) g
`

(t) = inf
(t) = sup
´
f ⊗g

`

`

˘
´
(t) = sup
f ⊗g
´
˘
f (cid:13)g
(t) = inf

f (s) + g(t − s) | 0 ≤ s ≤ t
¯
.
f (t + u) − g(u) | u ≥ 0

¯

,

Next, let ε(0) = 0 and ε(t) = −∞ for all t > 0. The super-additive
˘
closure of f is deﬁned by f ∗ = max
, where f 0 = ε and
f n+1 = f n ⊗ f for all n ∈ N, where N is the set of natural numbers.

¯
f n | n ≥ 0

. Similarly,

In the context of RTC, we often assume that upper arrival (ser-
vice) functions are sub-additive and lower arrival (service) func-
tions are super-additive. A function f ∈ F is sub-additive iff f (x +
y) ≤ f (x) + f (y) for all x and y in R+
is super-
f
additive iff f (x + y) ≥ f (x) + f (y) for all x and y in R+
. A func-
tion can be made sub-additive (super-additive) by taking its sub-
additive (super-additive) closure. In this paper, we assume that all
given upper (lower) functions are reﬁned to satisfy sub-additivity
(super-additivity) before the analysis. Further, we require that each
pair of upper and lower functions satisﬁes causality, i.e., it does
not include infeasible bounds. Speciﬁcally, for any given pair of
upper and lower functions ( f u, f l), we must have for all t ≥ 0,
f l(t) ≤ f l (x) + f u(t − x) ≤ f u(t) for all 0 ≤ x ≤ t.

Lastly, the maximum vertical and horizontal deviation (distance)

between two functions f , g ∈ F are given by:

vdist( f , g) def= sup{ f (t) − g(t) | t ≥ 0 }
˘
hdist( f , g) def= sup

¯
˘
τ ≥ 0 | f (t) ≤ g(t +τ)

inf

¯
| t ≥ 0

(2)

(3)

Performance bounds with unbounded FIFO buffers. Consider
an input data stream with arrival functions α = (αu,αl ) that is pro-
cessed by a resource with service functions β = (βu,βl ). Suppose
the buffer that stores the data items from the input stream has inﬁ-
nite capacity. Let A(t) be an input arrival pattern of the stream and
A(cid:15)(t) be the corresponding output arrival pattern. Then, from [5],
(4)

(cid:15) ≤ A ⊗βu

A ⊗βl ≤ A

The maximum backlog at the input buffer and the maximum de-
lay experienced by the input stream are given by vdist(αu,βl ) and
hdist(αu,βl ), respectively. Further, the output arrival functions
(cid:15)
(cid:15)
inf and remaining service functions β
inf are computed as follows.
α
´

˘`

¯

˘`

αu(cid:15)
inf = min
αl(cid:15)
inf = min
βu(cid:15)
inf =
βl(cid:15)
inf =

`
βu −αl
`
βl −αu

αu ⊗βu
´
αl (cid:13)βu
(cid:13) 0
⊗ 0

´
´

(cid:13)βl , βu
¯
⊗βl , βl

(5)

(6)
(7)
(8)

Terminology. We refer to the conventional RTC for unbounded
FIFO buffers described above as RTC-INF and the method pro-
posed in the next section for bounded buffers with data refresh se-
mantics as RTC-DRF. The subscript “inf” (“drf”) stands for the
results computed by the RTC-INF (RTC-DRF) method. Lastly, an
arrival pattern of an input/output stream is also known as an in-
put/output function, and we use them interchangeably in this paper.

3. MODELING FINITE BUFFERS WITH

DATA REFRESH SEMANTICS

We now extend the RTC-INFresults to capture systems contain-
ing buffers that implement data refresh semantics. In such systems,
buffers have bounded capacities and incoming data items are stored
in the buffer in the order of their arrivals. However, if an incoming
item arrives at a buffer when the buffer is full, the oldest data item
– at the head of the buffer – is discarded/overwritten, and the fresh
data item is written to the end of the buffer.

EXAMPLE 1. Consider a buffer B of size 3. Given B = [e1 e2 e3]
when item e4 arrives, where items e1, e2, e3 arrived earlier in that
order. Then, e1 will be overwritten and B will be [e2 e3 e4], which
contains the three most recent data items.

Objectives. Given such a system, our goal is to compute the
standard performance-related metrics mentioned earlier. Since the
RTC-INF assumes inﬁnite FIFO buffers, its analysis results become
overly pessimistic in presence of data refresh. We present here an

121extension of RTC-INF to model and analyze systems with data re-
fresh semantics, including methods for computing:

• The maximum delay experienced by the input stream, con-
sidering only items that are not overwritten1. (Section 3.1.1)

• The arrival functions of the output stream. (Section 3.1.2)
• The remaining service functions of the PE after processing

the stream. (Section 3.1.3)

We further extend our method to analyze systems with a mixture
of FIFO and data refresh semantics (Section 3.2) and other buffer
management schemes (Section 4).

Note that the maximum backlog of the buffer that implements
data refresh semantics is either the buffer capacity or the maximum
backlog computed by RTC-INF, whichever is smaller.

3.1 Systems with a single input stream

Consider a system consisting of a single input stream that is pro-
cessed by a processing element (PE) given in Fig. 2. As shown in
the ﬁgure, upon arriving at the system, the stream is written to a
buffer B before being processed by the PE. We assume that (i) the
input stream is modeled by the arrival functions α = (αu,αl ), (ii) the
resource availability of the PE is modeled by the service functions
β = (βu,βl), and (iii) data refresh semantics is implemented at
buffer B, which has a ﬁnite capacity of Bmax (items).

input 
stream

α

Bmax

β

PE

B

data refresh

α′

…

Figure 2: A system with a buffer having data refresh semantics.
Basic modeling ideas. Let A1 be an arrival pattern of the input
stream, C be a service pattern of the PE, and A3 be the correspond-
ing arrival pattern of the output stream. We denote by A2 the effec-
tive input function of A1, i.e., A2(t) speciﬁes the number of items
of A1 that arrive in (0,t] and that will not be overwritten. Since all
and only the items captured by A2 will be processed by the PE, A3
is the actual output function of A2. Observe that A2 is dependent

A3 + Bmax

input 
stream

A1

A2

Pv

B

C
PE

A3

…

Pv :  a virtual processor that controls
 the data going through the system.

no data refresh

A2′  (discarded)

Figure 3: A virtual system equivalent to the one in Fig. 2.

not only on the original arrival pattern A1 but also on the service
pattern C and the size of B. To compute an arrival function that
bounds A2, we employ a feedback control mechanism, where the
arrival pattern A3 of the processed stream is used as feedback in-
formation to control the data items going through the system. The
original system in Fig. 2 can be recast as an equivalent system that
has an additional virtual processor Pv in front of B (see Fig. 3). Pv
serves as an admission controller, which splits the original input
stream (captured by A1) into two separate streams:

(i) the former, modeled by the effective input function A2, con-

sists of all items that will be processed by the PE, and

(ii) the latter, modeled by the input function A(cid:15)

2, consists of all
items that will be overwritten, which will be discarded by Pv.
1Overwritten data items are lost and hence have no notion of delay.

Pv guarantees that A2 contains as many items as possible while en-
suring that none of these items will be overwritten (i.e., buffer B
never overﬂows).
In essence, the data refresh semantics of the
buffer in the original system is now captured completely by the pro-
cessing semantics of this virtual processor; as a result, the corre-
sponding buffer in the virtual system behaves exactly like an un-
bounded FIFO buffer. Note that Pv does not impose any additional
delay on the input items as it does not perform any real processing.

EXAMPLE 2. Fig. 4 shows the effective input function A2 and
the output function A3 (in solid lines) corresponding to a given in-
put arrival pattern A1 and a service pattern C, where Bmax = 3. In
the ﬁgure, the ﬁlled black circles represent the items that go through
the system (captured by A2). The unﬁlled pink circles represent the
(cid:15)
items that are discarded by Pv (captured by A
2). Each blue rectan-
gle corresponds to an item that can be processed (captured by C).
The number associated with a blue rectangle denotes the index of
the corresponding item in A1 that is processed by the PE. Note that
the second and third rightmost blue rectangles are wasted because
there is nothing to process.

#items

B=[1]

B=[2,3]

B=[3,4,5]

B=[6,7] B=[6,7,8]

B=[9]

B=[ ]

B=[1,2]

B=[3, 4]

B=[4,5,6]

10

9

B=[ ]

11

A2(t)

A3(t)

11

unused

unused

7

6

8

5

10

9

8

7

5

2

4

1

3

2

1

j

item k  is discarded because

k

buffer B is full when item j arrives

B=[ ]

B=[1, 2, 3]

B=[2, 3, 4]

B=[5, 6, 7]

B=[7,8,9]

B=[9,10]

B=[ ] B=[ ] B=[11]

B=[8,9]

B=[10]

time

Figure 4: Actual data items that go through the system.
One can verify that, when item 6 arrives, the buffer is full (B =
[3, 4, 5]). Therefore, item 3 (the oldest) is overwritten and item 6 is
written to the buffer (B = [4, 5, 6]). Similarly, items 4 and 6 will be
overwritten when items 7 and 9 arrive, respectively. Thus, in the
virtual system, Pv will discard items 3, 4, 6 when they arrive.

The performance-related metrics of the original system can now
be analyzed based on this virtual system as outlined in the com-
ing subsections. Fundamentally, the maximum delay is computed
based on the conditions for which data refresh occurs. To obtain the
remaining service and output arrival functions, we ﬁrst compute the
service function βv for Pv such that A2 is the largest effective func-
tion possible and B never overﬂows (cf. Fig. 3). This βv is then
used to derive the arrival function αv of A2. From αv and β, we
can apply the RTC-INF to derive the remaining service function of
the PE (since B behaves like an inﬁnite FIFO buffer). Further, βv
can also be combined with β to form the overall service function
e
β for the entire system. We then derive the arrival functions of the
processed stream based on α and

e
β.

3.1.1 Computing maximum delay

Recall the virtual system in Fig. 3. Denote d(t) as the delay
experienced by an input item that arrives at time t. Lemma 3.1
states two basic bounds on d(t) due to data refresh. These bounds
are shown in Fig. 5.

122#items

b(t) items in the buffer 

after item e arrives

Bmax

A1(t)

C(t)

d1(t)

d2(t)

e

b(t)

t

time

 instant at which b(t) items 

are fully processed

lastest instant at which 
e is processed/overwritten

Further, b(t) ≤ Bmax. Hence, d2(t) ≤ del(βl, Bmax) for all t ≥ 0.
The above two bounds on d(t) are depicted in Fig. 6(i-ii).

We shall now prove that for all t ≥ 0, d2(t) ≤ hdist(eαu,βl). In-
tuitively, this means the delay of an input item that goes through
the system is bounded by the maximum horizontal distance be-
tween eαu and βl (see Fig. 6(iii)). From Corollary 3.2, we have
A2(t) ≤ eαu(t). Consider an input item e arriving at time t. Since
(cid:15)] for any
there are at least βl (t
t(cid:15) > 0, item e will be processed latest at the ﬁrst instant t
(cid:15) = t + Δ at
(cid:15)). In other words, the amount of time required
which eαu(t) ≥ βl (t
to process e satisﬁes

(cid:15)) items that can be processed in (0,t

d2(t) ≤ inf{Δ ≥ 0 | eαu(t) ≥ βl (t + Δ)} ≤ hdist(eαu,βl ).

Figure 5: Upper bounds on delay of an output data item.

Thus, Eq. (12) holds and hence the lemma.

LEMMA 3.1. Let b(t) be the number of items in the buffer B at
time t, i.e., b(t) = A2(t) − A3(t). Then, d(t) ≤ min{d1(t), d2(t)},
where

d1(t) = min
d2(t) = min

˘
Δ ≥ 0 | A1(t + Δ) − A1(t) ≥ Bmax
¯
˘
Δ ≥ 0 | C(t + Δ) −C(t) ≥ b(t)
.

¯

,

(10)
PROOF. Consider an item of A1, called e, that arrives at time
t. Observe that e is only overwritten when it is the oldest item in
B and B is full. Because B contains at most Bmax items, e will
not be overwritten iff it is processed before the next Bmax items of
A1 arrive. (Otherwise, it would be overwritten by the (k + Bmax)th
item). For example, in Fig. 4, item 5 must be processed before item
8 arrives. Thus, the delay d(t) of e satisﬁes

(9)

˘
d(t) ≤ min

Δ ≥ 0 | A1(t + Δ) − A1(t) ≥ Bmax

= d1(t).

¯

Further, at time t, there are b(t) = A2(t) − A3(t) items currently in
the buffer (with e included). Thus, d(t) will be no more than the
amount of time needed to process these b(t) items, i.e.,

˘
d(t) ≤ min

¯
Δ ≥ 0 | C(t + Δ) −C(t) ≥ b(t)

= d2(t).

As a result, d(t) ≤ min{d1(t), d2(t)}.

A2 ≤ eαu.

Further, since αu is the upper arrival function of A1 and βu is the
upper service function of C, A1(t) ≤ αu(t) and C(t) ≤ βu(t) for
all t ≥ 0. Observe that (i) the buffer can hold at most Bmax items,
and (ii) at most βu(t) items can be processed over any interval of
length t. Hence, the number of items that are not overwritten in
(0,t], given by A2(t), is no more than the minimum of αu(t) and
βu(t) + Bmax. As a result, the following corollary holds, which in
turn implies Lemma 3.3.

COROLLARY 3.2. Deﬁne eαu def= min{αu,βu + Bmax}. Then,

˘
LEMMA 3.3. Let del( f , k) = min
t ≥ 0 | f (t) ≥ k

¯

for all f ∈

F and for all k ≥ 0. For all t ≥ 0,
d1(t) ≤ del(αl, Bmax),
˘
d2(t) ≤ min

¯
del(βl, Bmax), hdist(eαu,βl )
.

(12)
PROOF. Recall that αl is the lower arrival function of A1. Thus,
A1(t + Δ) − A1(t) ≥ αl (Δ) for all t ≥ 0 and for all Δ ≥ 0. By deﬁ-
nition of d1(t), we imply for all t ≥ 0,

(11)

˘
d1(t) ≤ min

Δ ≥ 0 | αl (Δ) ≥ Bmax

= del(αl, Bmax).

¯

Similarly, since βl is the lower service function of C, we have
C(t + Δ) −C(t) ≥ βl for all t ≥ 0 and for all Δ ≥ 0. Hence,

˘
∀t ≥ 0 : d2(t) ≤ min

¯
Δ ≥ 0 | βl(Δ) ≥ b(t)

.

#items

Bmax

2

0

#items

αu

e

Bmax

Bmax

#items

Bmax

1

2

D1

(i)   

Bmax

time

0

1

2

time

D2

(ii)   

Bmax

βu + Bmax

 

αu∼

Bmax

hdist(αu, βl )

∼

βu

βl

αu

 
βu + Bmax
αu∼

0

t0

(iii)   hdist( αu, βl )

∼

time

t0 + D3

Figure 6: The maximum delay experienced by the input
stream is the minimum of del(αl , Bmax), del(βl, Bmax), and
hdist(eαu,βl ).

From Lemma 3.3, we imply Theorem 3.4, which gives the max-

imum delay experienced by the input stream.

THEOREM 3.4. The maximum delay experienced by the input

stream is given by

˘
del(αl,Bmax), del(βl ,Bmax), hdist(eαu,βl )
deldrf (α,β,Bmax) = min

¯
.

Fig. 6 illustrates the delay computation given by Theorem 3.4.

THEOREM 3.5. The delay bound given by Theorem 3.4 is tight.
PROOF. Denote D = deldrf (α,β, Bmax) and D3 = hdist(eαu,βl ).
Then, D ≤ D3 and D ≤ del(αl , Bmax). For any given f , g ∈ F and
t ∈ R+
, we deﬁne hdist( f , g,t) to be the horizontal distance be-
tween f and g at time t, i.e.,

hdist( f , g,t) def= inf{ Δ ≥ 0 | f (t) ≤ g(t + Δ) }.

We denote by Πh( f , g, D) the ﬁrst instant t at which hdist( f , g,t) is
at least D, i.e.,

Πh( f , g, D) def= min{t ≥ 0 | hdist( f , g,t) ≥ D }.

123We will construct an input arrival pattern cA1(t) constrained by α
and a service pattern bC(t) constrained by β, such that there is an
item of cA1(t) which will be fully processed after D time units. In
other words, there exists T ≥ 0 such that hdist(cA1, cA3, T ) = D and
c
c
A1(T + D) − c
A3(t) is the resulting output
function of cA1(t) when the PE offers the service pattern bC(t). The
ﬁrst condition speciﬁes that the amount of time required to fully
process an item arriving at time T is D. The second states that there
are no more than Bmax items arriving over the interval (T, T + D]
(which implies that the item arriving at time T will not be overwrit-
ten over this interval).

A1(T ) ≤ Bmax, where

Since eαu = min{αu,βu + Bmax} ≤ αu and all the given arrival/

service functions are non-decreasing,

hdist(αu,βl) ≥ hdist(eαu,βl) = D3 ≥ D.

Hence, there exists t ≥ 0 such that hdist(αu,βl ,t) ≥ D. Let t0 be
the smallest of such t, i.e., t0 = Πh(αu,βl , D). Deﬁne cA1(t) =
αu(t) if t ≤ t0, and cA1(t) = αu(t0) +αl(t − x0) otherwise. Further,
deﬁne bC(t) = βl (t) for all t ≥ 0. Since αl (t) ≤ αu(x) +αl (t − x) ≤
αu(t) for all 0 ≤ x ≤ t, αl (t) ≤ cA1(t) ≤ αu(t) for all t ≥ 0. Hence,
cA1(t) is a valid arrival pattern of the input stream. By construction,
bC(t) is a valid service pattern of the PE.

Since αu is sub-additive and βl is super-additive, αu(t) ≥ βl (t)

for all 0 ≤ t ≤ t0. Indeed, if αu(s) < βl (s) for some s < t0, then
´
`
βl (s) +βl (t0 + D − s)

αu(t0) −βl (t0 + D) ≤ αu(s) +αu(t0 − s) −

< αu(t0 − s) −βl (t0 − s + D).

As a result, hdist(αu,βl,t0) < hdist(αu,βl,t0 − s). Hence,

Πh(αu,βl, D) ≤ t0 − s < t0 = Πh(αu,βl , D),

which is always false.

From the above, we imply cA1(t) ≥ bC(t) for all 0 ≤ t ≤ t0. This
means all resource offered by bC in [0,t0 + D] will be used to pro-
cess the items. Thus, the corresponding output function cA3 satisﬁes
cA3(t) = bC(t) for all 0 ≤ t ≤ t0 + D. Hence, hdist(cA1, cA3,t0) = D
(recall that t0 = Πh(αu,βl , D). In other words, the delay of an item
e arriving at T = t0 is D. Besides, D ≤ del(αl, Bmax) implies that
αl (D) ≤ Bmax. Hence, the number of items arriving in (t0,t0 + D]
is cA1(t0 + D) − cA1(t0) = αl (D) ≤ Bmax, which means e is not over-
written. As a result, the constructed system consisting of cA1 and bC
achieves the delay D given by Theorem 3.4.

3.1.2 Computing output arrival functions

Recall that A2(t) is the effective input function of A1(t), which
captures the items that will indeed be processed by the PE (see
Fig. 3). Lemma 3.6 states the relationship between these two func-
tions.

LEMMA 3.6. The effective input function A2 is bounded by:
A1 ⊗ (βl + Bmax)∗ ≤ A2 ≤ A1 ⊗αu ⊗ (αu ⊗βu + Bmax)∗.
PROOF SKETCH. Since none of the items in A2 is overwritten,
for all t ≥ 0, b(t) = A2(t)−A3(t) ≤ Bmax, or A2 ≤ A3 +Bmax. Let f
be the function that maps the input A2 to the output A3, assuming f
is monotonic. Then A3 + Bmax = f (A2) + Bmax = ( f + Bmax)(A2).
Further, the number of items that pass the admission test at Pv
(i.e., not overwritten) over any time interval (s,t] is no more than
the number of original items that enter the system over the same
interval. In other words,

∀t ≥ 0, ∀ 0 ≤ s ≤ t : A2(t) − A2(s) ≤ A1(t) − A1(s).

Recall that IA1
Then, A2 ≤ IA1

(A2)(t) = inf
(A2). Hence,

˘

A2(s) + A1(t) − A1(s) | 0 ≤ s ≤ t

˘
A2 ≤ min
⇔ A2 ≤ A1 ⊕

¯
(A2), ( f + Bmax)(A2)

´
⊕ ( f + Bmax)
(A2).

A1, IA1
`
IA1

¯

.

(13)

(14)

Hence, the input function of the items that actually go through the
system is the maximum solution for Eq. (14). By Theorem 2.1,

A2 =

`
IA1

⊕ ( f + Bmax)

´∗(A1).

By applying Eq. (1) (cf. Section 2), the above is equivalent to

A2 =

`
IA1

◦ ( f + Bmax)

´∗ ◦ IA1

(A1).

Denote Cz(x) = x ⊗ z. Since f is the mapping from A2 to A3, and β
is the service function of the PE, f (A2) ≤ A2 ⊗βu, or equivalently,
f ≤ C
βu . Similarly, α is the arrival function of A1 implies that
A1(t) − A1(s) ≤ αu(t − s). Thus, IA1
≤ Cαu.
Hence,

(A2) ≤ αu ⊗ A2, or IA1

`
Cαu ◦ (C

A2 ≤

βu + Bmax)

´∗ ◦ Cαu (A1),

which can be rewritten as A2 ≤ A1 ⊗ (αu ⊗βu + Bmax)∗ ⊗αu.
By similar arguments, we can also imply A2 ≥ A1 ⊗(βl +Bmax)∗

.

This proves the lemma.

Lemma 3.7 is derived directly from the bounds established in the
above lemma, which holds true due to A1 ⊗βl
v

≤ A2 ≤ A1 ⊗βu
v .

and βl
v

. Then, βu

LEMMA 3.7. Let βu
v
v and βl

= αu ⊗ (αu ⊗βu + Bmax)∗
= (βl +
v are valid upper and lower service func-

Bmax)∗
tions for Pv.
≤ A3 ≤
By deﬁnition, A2 ⊗βl ≤ A3 ≤ A2 ⊗βu. Thus, A1 ⊗βl ⊗βl
v
A1 ⊗βu ⊗βu
v are the over-
all service functions given to the input stream when there is data
e
β, we can compute the output arrival functions.
refresh. Based on

e
βu = βu ⊗βu

e
βl = βl ⊗βl

v . Hence,

v and

THEOREM 3.8. The arrival functions of the output stream (A3)
when data refresh semantics is implemented at the input buffer is
given by α

(cid:15) = (αu(cid:15) ,αl(cid:15) ), where

αu(cid:15) = min
αl(cid:15) = min
e
βl = βl ⊗(βl +Bmax)∗

with

˘`

˘`

´

¯

αu ⊗ e
βu
´
αl (cid:13) e
βu

βl , e
(cid:13) e
βu
¯
⊗ e
βl , e
βl

.

,

(16)
e
βu = βu ⊗αu ⊗(αu ⊗βu +Bmax)∗

.

and

(15)

We note that when Bmax is unbounded,

e
β = β and α

(cid:15)
(cid:15) = α
inf .

Lemma 3.9 further reﬁnes the effective output arrival functions
to ensure the sub-additivity property of αu(cid:15)
, the super-additivity of
αl(cid:15)
and their causal relationship. Its proof can easily be established
based on the deﬁnition of upper and lower arrival functions. The
details are available in [13].

LEMMA 3.9. Let bαu = (αu(cid:15) )∗

and bαl = (αl(cid:15) )∗

. Denote

αu(cid:15)
drf (Δ) = min
αl(cid:15)
drf (Δ) = max

¯
˘
bαu(Δ +τ) − bαl (τ) | τ ≥ 0
,
¯
˘
bαl (Δ +τ) − bαu(τ) | τ ≥ 0

.

(17)

(18)

drf (αl(cid:15)

Then, αu(cid:15)
output stream that is smaller (larger) than the αu(cid:15)

drf ) is a valid upper (lower) arrival function for the

(αl(cid:15)

).

3.1.3 Computing remaining service functions

Since βu

v and βl

v are the upper and lower service functions of the
virtual processor Pv (Lemma 3.7), the effective input function A2 is
bounded by the output arrival functions of Pv, given by

αu
v
αl
v

= min
= min

˘`
˘`

´
αu ⊗βu
´
v
αl (cid:13)βu
v

¯
(cid:13)βl
, βu
¯
v
v
, βl
⊗βl
v
v

,
.

124Using αv = (αu
) as input arrival functions to the PE, we can
v
derive the remaining service functions of the PE as in the conven-
tional case as below (since there is no buffer overﬂows).

,αl
v

βu(cid:15)
drf =
βl(cid:15)
drf =

´

´

`
βu −αl
v
`
βl −αu
v

(cid:13) 0
⊗ 0

of analysis accuracy for the overall system. Speciﬁcally, it provides
a tighter output arrival function than RTC-INF does, and hence
ensures accurate analysis at the subsequent PEs (Theorem 3.11).
Further, by taking into account data refresh, RTC-DRF is able to
capture the service unused by the overwritten items, thereby guar-
antees more service for the lower priority streams (Theorem 3.12).

(19)

(20)

Thus, the remaining service function of the PE when the buffer
drf ).
implements data refresh semantics is given by β

drf = (βu(cid:15)
(cid:15)

drf ,βl(cid:15)

3.2 Heterogeneous systems with a mixture of

buffer semantics

In this section, we show how one can apply the RTC-DRF pre-
sented in the previous section to analyze heterogeneous systems
with different buffer semantics in a compositional manner. Through
this, we demonstrate how RTC-DRF can be integrated directly into
the conventional RTC-INF while guaranteeing that the overall anal-
ysis is at least as tight as the RTC-INF alone.

The systems we consider consist of multiple input streams, namely
s1, . . . , sn, that are processed by a sequence of PEs under Fixed Pri-
ority (FP) scheduling policy. Each buffer in a system can be either
an inﬁnite FIFO buffer or a ﬁnite buffer with data refresh semantics.
An example of such systems is shown in Fig. 7. In this example,
B1 is a ﬁnite buffer of size Bmax that has data refresh semantics. On
the other hand, B(cid:15)
1 is an unbounded FIFO buffer. Given such a sys-
tem, we would like to compute the standard performance-related
metrics as discussed in the previous sections.

input 

stream s1

input 

stream s2

α

α2

B1

B2

data refresh

β

PE1

αdrf′

′
B1

α2′

  FIFO     
…

β2
PE2

αdrf″
to subsequent PE
t

…

 Buffer semantics :   infinite FIFO  or finite with data refresh 

Figure 7: Systems with a mixture of buffer semantics.

Consider the ﬁrst PE in Fig. 7. Suppose si has higher priority
than s j if i < j. Then, PE1’s resource will ﬁrst be given to s1 and
the remaining will be given to s2, then s3 and so on. Denote α
and β as the arrival function of s1 and the service function of PE1,
respectively. Applying the RTC-DRF results obtained in the single
stream case, we compute the maximum delay using Theorem 3.4
and the output arrival function using Lemma 3.9. The remaining
(cid:15)
service function β
drf that is used to process the next input stream
(cid:15)
s2 can also be computed using Eq. (19) and (20). Based on β
drf , we
then analyze s2, taking into consideration the semantics of B2. If
B2 is an inﬁnite FIFO buffer, we apply the RTC-INF. However, if
B2 implements data refresh semantics, we analyze using RTC-DRF
as done for s1. The analysis is repeated until we reach sn.

(cid:15)
At the same time, the output arrival function α
drf of s1 produced
by PE1 is fed as input arrival function to PE2. At this PE, we repeat
the same analysis as above with respect to the semantics of its input
buffer B(cid:15)
(cid:15)(cid:15)
1. The computed output arrival function α
drf is then fed as
input to the subsequent PEs2.

Correctness of our compositional analysis. As seen above, the
RTC-DRF combined with the RTC-INF enables complex systems
with a mixture of different buffer types to be analyzed composition-
ally. We claim that RTC-DRF does not introduce any loss in terms

with

2A tighter output arrival function for PE j can be obtained by ap-
plying RTC-INF to the overall effective service function for part
of the system comprising PE1 to PE j (i.e., the convolution of the
individual effective service functions).

drf = (αu(cid:15)
(cid:15)
LEMMA 3.10. Let α

inf ) be
the output arrival functions of s1 at PE1 (Fig. 7) that are computed
using RTC-DRF and RTC-INF, respectively. Then, αu(cid:15)

inf = (αu(cid:15)
(cid:15)
drf ) and α

drf ≤ αu(cid:15)
inf .

drf ,αl(cid:15)

inf ,αl(cid:15)

PROOF SKETCH. Since αu(cid:15)

drf ≤ αu(cid:15)

orem holds if αu(cid:15) ≤ αu(cid:15)
¯
e
βl , e
. Thus, αu(cid:15) ≤ e
βu
By deﬁnition, we have
´
αu(cid:15)
inf = min
e
βu ≤ βu. Thus,

e
βu ≤ αu(cid:15)

αu ⊗βu

˘`

(cid:13)βl, βu
inf iff

(due to Lemma 3.9), the the-
(cid:13)

αu ⊗ e
βu

˘`

´

inf . By Theorem 3.8, αu(cid:15) = min
βu. We will prove that

e
βu ≤ αu(cid:15)
inf .

e
βu = βu ⊗αu ⊗ (αu ⊗βu + Bmax)∗

and
¯
. Since βu ⊗ g ≤ βu for all g ∈ F,

βu ⊗αu ⊗ (αu ⊗βu + Bmax)∗ ≤

`
αu ⊗βu

´

(cid:13)βl

(21)

Let f = αu ⊗βu = βu ⊗αu. Then,
(21) ⇔ f ⊗ ( f + Bmax)∗ ≤ f (cid:13)βl ⇔ f ⊗ ( f + Bmax)∗ ⊗βl ≤ f

⇔ f ⊗ g ≤ f where g = ( f + Bmax)∗ ⊗βl ,

which is always true. Hence,

e
βu ≤ αu(cid:15)
inf and thus, αu(cid:15) ≤ αu(cid:15)
inf .
(cid:15)
(cid:15)
inf be deﬁned in Lemma 3.10.
drf and α
THEOREM 3.11. Let α
(cid:15)(cid:15)
(cid:15)(cid:15)
Denote by bufdrf , deldrf and α
drf (resp. bufinf , delinf and α
inf ) the
maximum backlog, maximum delay and output arrival function at
(cid:15)
(cid:15)
PE2 where α
drf (resp. α
inf ) is used as the input arrival function to
PE2. Then, bufdrf ≤ bufinf , deldrf ≤ delinf and αu(cid:15)(cid:15)

drf ≤ αu(cid:15)(cid:15)
inf .

PROOF. Since the input buffer of PE2 is a simple inﬁnite FIFO
buffer, we analyze it using RTC-INF. Let β2 be the service function
of PE2. Following RTC-INF and Lemma 3.10, we have:

(cid:15)
drf ,βl
bufdrf = vdist(α
2
˘
αu(cid:15)
inf (Δ) −βl
≤ sup
2
(cid:15)
inf ,βl
= vdist(α
2

) = bufinf .

˘
αu(cid:15)
drf (Δ) −βl
) = sup
2
(since αu(cid:15)

¯
(Δ) | Δ ≥ 0

¯
(Δ) | Δ ≥ 0

drf ≤ αu(cid:15)
inf )

Thus, bufdrf ≤ bufinf . The remaining properties can be proved
similarly.

THEOREM 3.12. Let β

(cid:15)
inf ) be the remaining service func-
tion of PE1 after processing s1, which is computed using RTC-DRF
(RTC-INF), Then, (i) βl(cid:15)

inf and (ii) βu(cid:15)

drf ≥ βu(cid:15)
inf .

drf ≥ βl(cid:15)

(cid:15)
drf (β

PROOF. First, for any f , g ∈ F such that f ≥ g, we have:

∀ Δ ≥ 0 : ( f ⊗ 0)(Δ) = sup
0≤x≤Δ

f (x) ≥ sup
0≤x≤Δ
Thus, f ⊗ 0 ≥ g ⊗ 0. Similarly, f (cid:13) 0 ≥ g (cid:13) 0.

g(x) = (g ⊗ 0)(Δ).

(i) Recall that βl(cid:15)

drf = (βl −αu
v

) ⊗ 0 and βl(cid:15)

inf = (βl −αu) (cid:13) 0,

˘`

´

¯

αu
v

αu ⊗βu
v

= min
, βu
v
= αu ⊗ (αu ⊗βu + Bmax)∗

(cid:13)βl
v

≤ βu
v
≤ αu.

Thus, βl −αu
v
In other words, βl(cid:15)

drf ≥ βl(cid:15)
inf .

≥ βl −αu. Hence, (βl −αu
v

) ⊗ 0 ≥ (βl −αu) ⊗ 0.

125(ii) By deﬁnition, βu(cid:15)
We will ﬁrst show that αl
v

˘`

αl
v

= min

αl (cid:13) βu
v

drf = (βu −αl
v

) (cid:13) 0 and βu(cid:15)

inf = (βu −αl ) (cid:13) 0.

≤ αl . Indeed,
¯

´

⊗βl
, βl
v
v
`
αl (cid:13)βu
v

≤
´

´

`
αl (cid:13)βu
v

⊗βl
v

,

which implies that αl
v
alent to αl (cid:13)βu
v

≤ αl (cid:13)βl

≤ αl if

⊗βl
v

≤ αl . This is equiv-

v, which always holds due to βu
v

≥ βl
v.

≤ αl , we imply βu −αl
v

From αl
v
) (cid:13) 0 ≥ (βu −αl ) (cid:13) 0. In other words, βu(cid:15)

≥ βu −αl . As a result,
drf ≥ βu(cid:15)
inf .

(βu −αl
v

4. EXTENSIONS TO OTHER BUFFER MAN-

AGEMENT SEMANTICS

In the data refresh semantics considered thus far, if an item ar-
rives when the input buffer is full, the oldest item in the buffer is
discarded (also known as Drop Oldest in [14]). We now extend
our analysis method for other buffer management semantics, such
as those deﬁned in [14]. We ﬁrst consider the Drop Newest (DN)
policy, where incoming items are discarded if the buffer is full.

Observe that the number of items that are discarded in the DN
and the data refresh semantics are identical. Since only the number
of items that are discarded (and not which speciﬁc items) affects
the number of items that will be processed by the PE, the num-
ber of items that are processed over any given time interval in both
cases are the same. Hence, the system produces the same number
of output items in both semantics. In other words, the output ar-
rival functions and the remaining service functions in both refresh
semantics are the same, which are given by Lemma 3.9 and Eq. (19)
and Eq. (20).

Further, note that in the DN semantics, once an input item is
written to the buffer, it will not be overwritten. Hence, the delay
experienced by an input item is bounded by the maximum amount
of time required to process the item. Using the same argument as in
the data refresh semantics case, we imply that the maximum delay
experienced by the input stream is

delDN = min{del(βl, Bmax), hdist(eαu,βl)}.

Based on similar arguments, one could also obtain the analysis
results for the Drop All and Drop Random policies [14] where all
items or a random item in the buffer will be discarded when the
buffer overﬂows. Further, systems with multiple PEs and/or multi-
ple input streams under Fixed Priority scheduling which implement
a mixture of these semantics can also be analyzed in a composi-
tional manner as detailed in Section 3.2.

5. CASE STUDY

We now present three case studies to demonstrate the applicabil-
ity of our analysis methods. The ﬁrst shows how our technique can
be used to compute bounds on the amount of data guaranteed to
go through the system when the buffer implements the data refresh
semantics. The second illustrates the effect of buffer size on the
freshness of output data in a traction control application. The last
one presents a sensitivity analysis of the variation in the maximum
delay experienced by the input stream with respect to changes in
the input workload.
5.1 Case study 1: Output guarantees in pres-

ence of data refreshing

In this case study, we analyze the bounds on the output stream
of the system described in Fig. 2 using our technique in Section 3.1
and a SystemC simulation.

Simulation setup. Using our SystemC event simulator, we gen-
erate an arbitrary input event trace Rsim(t) that comprises different

event types of varying processing cycle requirements. Events from
Rsim(t) are ﬁrst kept at a ﬁnite buffer that implements data refresh
semantics. Here, we are interested in the freshest event and hence
the size of the buffer is set to 1. The processor processes the events
from the buffer in a greedy fashion, where it is set to run at fre-
quency f = 5 MHz. We then observe the output arrival pattern
R(cid:15)
sim

(t) of the processed stream.

−u( f Δ) and βu(Δ) = γ

Obtaining the arrival and service functions. Based on the gener-
ated trace Rsim(t), we derive the arrival functions αu(Δ) and αl (Δ)
of the input stream by sliding a window of size Δ along the time axis
and determine the maximum and minimum number of events gen-
erated across all the windows. Further, from the execution require-
ments of the generated events, we compute the workload functions
γu(k) and γl (k), which give the maximum and minimum number
of processor cycles required to process any k consecutive events.
The service functions of the processor can be then obtained from
the workload functions and the frequency f using the formulas
−l ( f Δ). We then use α and β
βl (Δ) = γ
as input arrival and service functions to compute the output arrival
functions using RTC-DRF and RTC-INF.
Simulation vs. analytical results.3 The upper output arrival func-
tion computed by RTC-DRF correctly upper bounds the output sim-
ulation trace R(cid:15)
(t) than the upper output
arrival function given by RTC-INF. Similarly, the lower output ar-
rival function given by RTC-DRF correctly lower bounds R(cid:15)
(t);
the RTC-INF, however, gives a wrong bound as its computed value
is above the output simulation trace. Hence, by taking into account
buffer refresh, our method gives a tighter upper bound than the con-
ventional RTC does, at the same time avoids invalid results faced
by the conventional RTC.

(t), and it is closer to R

(cid:15)
sim

sim

sim

 

B=10

B=5

B=15

B=2

B=1

1000

2000

3000

4000

6000

7000

8000

9000

10000

5000
Δ  ms

Figure 8: Guarantees on the output stream for different input
buffer sizes.

115

116

116

103

70

40

B = 1

B = 2

B = 5

B = 8

B = 10

B = 15

Figure 9: Effect of buffer size on the system’s throughput.

Effect of buffer size on the output stream and throughput. Fig. 8
depicts the lower output arrival functions obtained by our technique
when varying the input buffer size for the same input stream α(Δ).
As shown in the ﬁgure, the lower arrival function corresponding
to a lower buffer size is located below the one corresponding to a

3Due to space constraints, we do not show the detailed results here.

 

 
f
o
n
o
i
t
c
n
u
f
 
l
a
v
i
r
r
a
 
r
e
w
o
L

 
]
s
t
n
e
v
e
#
[
 
 

m
a
e
r
t
s
 
t
u
p
t
u
o
e
h
t

 

120

100

80

60

40

20

0
 
0

 
t
u
p
h
g
u
o
r
h
t
 

m
u
m
n
M

i

i

]
s
0
1
/
s
t
n
e
v
e
#
[
 
 
 

126larger buffer size. This is because, as the buffer size is increased,
fewer items are overwritten and thus, more items are processed. It
can also be observed from the ﬁgure that the output arrival func-
tions for buffer sizes B = 10 and B = 15 coincide, which happens
when all input items are processed.

The buffer size also has a large impact on the minimum through-
put of the system. As illustrated in Fig. 9, initially when we double
the size of the buffer, the throughput increases nearly by a factor
of two. However, the increasing factor is reduced as we further in-
crease the buffer size, and the throughput will ﬁnally converge (at
value B ≥ 10) when the buffer is large enough to avoid overﬂows.
5.2 Case study 2: A traction control system

Strong acceleration can lead to wheel spinning, especially on
poorly prepared roads. A traction control system prevents spinning
of the driving wheels and provides an optimal traction. Fig. 10
depicts a traction control system application mapped on a CAN ar-
chitecture we would like to analyze.

ECU1 

ECU2 

Fixed Priority 

Fixed Priority 

(cid:1)2 

(cid:1)1 

s2 

s1 

T2 

T1 

‚ 

‚ 
(cid:1)1  (cid:1)2 

T4 

T3 

A
c
t
u
a
t
o
r
 

B1 

m2 

CAN 

m1 

B4 

m4 

m3 

Residual Bus 

T5 

… 

Tn 

… 

m5  mn 

Figure 10: A traction control system on a CAN architecture.

The system consists of a wheel speed sensor cluster s1, two
ECUs for computing the traction control and an actuator perform-
ing the wheel braking. ECU1 receives the wheel-speed values from
the sensor cluster s1, and processes the current slip by executing
task T1. The processed slip value is sent via message m1 to ECU2.
Task T4 is computing the brake force according to the input slip
value, especially if a wheel is going to spin. Subsequently, the
brake force value is sent via m4 to the wheel brake actuator which
performs the brake application and therefore prevents wheel spin-
ning. As the delay of such a system has to be very short, it is
important that the most recent slip value is available for comput-
ing the brake forces and that the most recent computed brake force
value is sent to the actuator. To achieve this, the buffers B1 and B4
are conﬁgured to non-queued buffers that allow updating their con-
tents with a new processed value in case the previous value could
not be transmitted on the bus according to the CAN scheduling pol-
icy. This may happen if too many messages with a higher priority
than m1 and m4 are transmitted on the CAN bus for a certain period
of time (e.g., due to some event triggered higher priority messages
which have to be transmitted because of changing system states of
other ECUs). Besides, there is a second application running on
ECU1 and ECU2. Messages are sent on the CAN bus according to
ﬁxed priority non-preemptive scheduling (FPNS).

Given the above system, we are interested in how fresh the wheel-
speed values are when they arrive at the actuator. To derive this, we
calculate the maximum end-to-end delay of the messages that are
transmitted from the sensor s1 to the actuator through the colored
path (in solid blue line).

Analysis results. We employ the method in [9] for modeling FPNS
policy used by the CAN bus. The residual bus depicted in Fig. 10
consists of n strictly periodic messages with priorities higher than
>
m1 to m4. The message priority Pm is deﬁned by Pm1

> Pm2

> Pm4

 

infinite

31.25

36.00

delay(T
)
1
delay(m
)
1
delay(T
)
4
delay(m
)
4

21.25

]
s
m

[
 
 

 

y
a
l
e
d
d
n
e
-
o
t
-
d
n
e
m
u
m
i
x
a
M

 

Inf

80

60

40

20

0

 

B1 = 1

B1 = 3
RTC with data refresh semantics

B1 = 2

conventional RTC

Figure 11: Maximum delay from s1 to the actuator.

> PT2 and PT3

> PT4. For
Pm3 and the task priority PT is given by PT1
the analysis, we assume a low speed CAN bus providing a data rate
of 125 kbit/s and a ﬁxed frame length for every CAN frame in the
system. The sensor task s1 has a period of 10 ms and an additional
jitter of 2 ms. Both B1 and B4 are ﬁnite buffers with data refresh
semantics, where B4 has a ﬁxed size of 1 and B1 has a variable size.
Fig. 11 depicts the corresponding maximum delay experienced
by a message originated from the sensor s1 to the actuator when we
vary the size of the buffer B1, computed by RTC-INF (assuming
no data refresh) and by RTC-DRF. Here, the longer the delay, the
less fresh the data. As illustrated in the ﬁgure, according to the
RTC-INF, a message may experience unbounded end-to-end delay,
which is overly pessimistic. By taking into consideration the buffer
size and the data refresh semantics, our RTC-DRF method gives
a ﬁnite delay. It can also be observed from the ﬁgure that, as we
increase the buffer size, the delay increases and the data becomes
more stale. This is expected because when the buffer is small, it
keeps only the most recent data items, which is not the case for a
large buffer.

Based on the above observations, it is often appropriate to keep
the buffer size small in applications where the freshness of data is
critical. On the contrary, applications that require high throughput
often need sufﬁcient on-chip memory to maintain the desired level
of quality of service.

5.3 Case study 3: Sensitivity analysis

To evaluate the robustness of our method as well as the rela-
tionship between input parameters and system performance-related
metrics, we study the sensitivity of our analysis with respect to
variations in the input stream. Towards this, we consider a sin-
gle periodic-with-jitter input stream that is processed by a system
which implements data refresh semantics, and examine the impact
of input jitter variation on the delay of the output stream.

ideal periodic points

P

jitter window of size J

arrival input data item

time

Figure 12: Periodic sensor stream with jitter.

As shown in Fig. 12, such an input stream arrives at a constant
period P in average; however, the arrival times of the items may
deviate within an interval of length J (called the jitter) surrounding
the ideal periodic arrival points. Besides modeling an input source
that is not strictly periodic, this jitter is also often used as a means
to capture possible errors in the period measurement of an input
stream.

127input 
stream

s1

b

s1′

PE1

B

CAN bus

PE2

…

data refresh

message delay?

Figure 13: Example system for sensitivity analysis.

System description. Fig. 13 shows the architecture of the system.
The periodic sensor stream s1, with period P and jitter J, upon ar-
riving at the system will be stored in the input buffer b prior to being
(cid:15)
processed by the processor PE1. Its output stream s
1 is then writ-
ten to a transmit buffer B before being transmitted to the CAN bus
(denoting as PE2). Here, b is an unbounded FIFO buffer; however,
B implements data refresh semantics.

We assume that B has a ﬁxed depth of 1 and the CAN bus pro-
vides data rate of 125 kbit/s. The input stream s1 has a period of
10 ms and a variable jitter of J ms. In our experiment, we vary J
from 0 to 7 ms in steps of 0.5 ms, and compute the corresponding
maximum delay experienced by an input message.

Constant bound on the message delay

h
s
e
r
f
e
r
a
t
a
D

 

14

12

10

8

6

4

2

0

]
s
m

 

 

[
 
1
s
y
b
d
e
c
n
e
i
r
e
p
x
e
y
a
l
e
d
m
u
m
i
x
a
M

 

 

0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

Jitter of input stream s1 [ms]

Figure 14: Effect of input jitter on the message delay.

Fig. 14 depicts the maximum delay experienced by an input mes-
sage corresponding to different input jitter values. As shown in the
ﬁgure, the maximum message delay increases linearly as we in-
crease the jitter of the input stream. This is expected because when
the jitter is increased, more items may arrive in a ﬁxed interval of
time, which increases the worst case resource demand of the in-
put stream. As a result, a more jittery stream experiences a larger
maximum delay.

On the other hand, the maximum delay stabilizes after the in-
put jitter exceeds a certain value (i.e., J ≥ 3 in the ﬁgure). This
convergence of delay is guaranteed due to the enforcement of data
refresh semantics. Speciﬁcally, since the service functions of the
PEs do not change, the maximum workload that can be processed
by PE2 stays constant regardless of the input demand. Further, in a
high load scenario (e.g., with high input jitter value), the maximum
number of items that wait in the buffer will be limited by the depth
of the buffer (since the excessive input items will all be discarded
due to the data refresh semantics). Hence, when the input jitter goes
beyond a certain threshold, the maximum number of items that will
indeed be processed is only limited by the service function and the
buffer size. As a result, the maximum delay remains constant as
the jitter continues to increase.

From the above sensitivity analysis, one can derive the correla-
tion between the input measurements and the system behavior. In
scenarios where jitter is used to accommodate possible input mea-
surement errors, the tightness of the delay results is linearly pro-
portional to the tightness of the input jitter value; however, it is
guaranteed to be bounded by a constant accuracy despite how pes-
simistic the input measurement is.

Lastly, the RTC-DRF results also showcase interesting system
behaviors that are not easily visible otherwise. For instance, when

data refresh is implemented, the maximum message delay is no
longer inﬂuenced by the input load once the load is larger than the
maximum service provided added with the buffer size. On the con-
trary, the number of messages that are overwritten is susceptible to
input workload, especially when the input load is high. Based on
these observation, one can also determine the maximum workload
acceptable by the system to guarantee a delay constraint or to min-
imize the amount of data loss. It is worth noting that such insights
into the effects of various design parameters and their trade-offs
would have not been possible by using RTC-INF alone.

6. CONCLUDING REMARKS

We have presented an analytical framework to model and analyze
systems with buffers implementing data refresh semantics. Our
analysis is tight and based solely on algebraic techniques, which
can be computed efﬁciently and compositionally. Further, it can
be easily integrated into the existing RTC framework and extended
to analyze similar buffer management policies. We have also il-
lustrated the utility of our method using three realistic case stud-
ies from the automotive domain. We plan to extend the theoreti-
cal results established in this paper to capture more complex sys-
tem behaviors, such as dynamic scheduling policies, dependen-
cies between input/output streams, and more complex buffer update
schemes.

7. REFERENCES
[1] A. Benveniste et al. Heterogeneous reactive systems modeling: capturing

causality and the correctness of loosely time-triggered architectures (LTTA). In
EMSOFT, 2004.

[2] A. Benveniste et al. Communication by sampling in time-sensitive distributed

systems. In EMSOFT, 2006.

[3] A. Benveniste et al. Loosely time-triggered architectures based on

communication-by-sampling. In EMSOFT, 2007.

[4] J.-Y. Le Boudec and P. Thiran. Network Calculus: A Theory of Deterministic

Queuing Systems for the Internet, volume LNCS 2050. Springer, 2001.

[5] S. Chakraborty, S. Künzli, and L. Thiele. A general framework for analysing

system properties in platform-based embedded system designs. In DATE, 2003.
[6] S. Chakraborty, L. T. X. Phan, and P. S. Thiagarajan. Event count automata: A

state-based model for stream processing systems. In RTSS, 2005.

[7] N. Feiertag, K. Richter, J. Nordlander, and J. Jonsson. A compositional

framework for end-to-end path delay calculation of automotive systems under
different path semantics. In CRTS, 2008.

[8] E. Fersman, P. Krcál, P. Pettersson, and W. Yi. Task automata: Schedulability,

decidability and undecidability. Information and Computation,
205(8):1149–1172, 2007.

[9] W. Haid and L. Thiele. Complex task activation schemes in system level

performance analysis. In CODES+ISSS, 2007.

[10] E. A. Lee and D. G. Messerschmitt. Synchronous data ﬂow. Proceedings of the

IEEE, 75(9):1235–1245, 1987.

[11] L. Mangeruca, M. Baleani, A. Ferrari, and A. Sangiovanni-Vincentelli.

Semantics-preserving design of embedded control software from synchronous
models. IEEE Transactions on Software Engineering, 33(8), 2007.

[12] O. Moreira and M. Bekooij. Self-timed scheduling analysis for real-time
applications. EURASIP Journal on Advances in Signal Processing, 2007.

[13] L. T. X. Phan, R. Schneider, S. Chakraborty, and I. Lee. Modeling buffers with

data refresh semantics in automotive architectures.
www.cis.upenn.edu/~linhphan/papers/emsoft10TR.pdf, 2010.

[14] J. Ray and P. Koopman. Data management mechanisms for embedded system

gateways. In DSN, 2009.

[15] S. Tripakis, C. Pinello, A. Benveniste, A. Sangiovanni-Vincentelli, P. Caspi, and

M. Di Natale. Implementing synchronous models on loosely time triggered
architectures. IEEE Transactions on Computers, 57(10), 2008.

[16] E. Wandeler and L. Thiele. Workload correlations in multi-processor hard

real-time systems. Journal of Computer and System Sciences (JCSS),
73(2):207–224, 2007.

[17] W. Wandeler, A. Maxiaguine, and L. Thiele. Quantitative characterization of
event streams in analysis of hard real-time applications. Real-Time Systems,
29(2-3):205–225, 2005.

[18] M. Wiggers, M. Bekooij, P. Jansen, and G. Smit. Efﬁcient computation of

buffer capacities for multi-rate real- time systems with back-pressure. In
CODES+ISSS, 2006.

128