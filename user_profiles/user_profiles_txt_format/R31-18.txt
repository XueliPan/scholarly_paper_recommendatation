Practical Passive Lossy Link Inference

Alexandros Batsakis, Tanu Malik, and Andreas Terzis

Computer Science Department

Johns Hopkins University

abat@cs.jhu.edu, tmalik@cs.jhu.edu, terzis@cs.jhu.edu

Abstract. We propose a practical technique for the identi(cid:12)cation of
lossy network links from end-to-end measurements. Our scheme is based
on a function that computes the likelihood of each link to be lossy. This
function depends on the number of times a link appears in lossy paths
and on the relative loss rates of these paths. Preliminary simulation re-
sults show that our algorithm achieves accuracy higher than previously
proposed heuristic methods and comparable to statistical methods at
signi(cid:12)cantly lower running time.

1

Introduction

Loss inference techniques [1{3] attempt to infer link loss rates from end-to-
end measurements. Active techniques [1] infer link loss by actively probing the
network, while passive techniques [2, 3] estimate packet loss by observing the
evolution of application tra(cid:14)c. Depending on the method used to infer packet
loss, passive techniques can be further divided to analytical [2] and heuristic [3].
Analytical techniques detect more lossy links while techniques are more e(cid:14)cient.
The insight behind our approach is that users are interested in (cid:12)nding lossy
links a(cid:11)ecting the performance of their applications, rather than (cid:12)nding the
exact loss rate of these links. Based on this insight, we present COBALT, a
heuristics-based inference algorithm that detects with high probability the lossy
links a(cid:11)ecting applications’ performance. COBALT assigns a con(cid:12)dence level to
each link|the higher the con(cid:12)dence, the higher the probability that the link is
lossy. The con(cid:12)dence level of a link depends on the number of lossy paths the
link belongs to and how lossy are those paths compared to all the paths in the
network.

2 Algorithm

We start by presenting the network model we use. In our model clients are
connected to servers through a network whose topology is known a-priori. Clients
exchange data with the servers using any TCP-based protocol. Using traces
collected from the servers, we calculate the loss rate of the path between the
server and the each client as the ratio of the retransmitted packets to the total
number of packets sent by the server.

COBALT starts by separating good from bad paths. Paths with loss rate
higher than a threshold T are labeled as bad while the remaining paths are la-
beled as good. The threshold T corresponds to a loss rate above which application
performance is disrupted. Subsequently, links are categorized depending on the
number of good paths they belong to. This approach is based on the intuition
that lossy links dominate the end-to-end path loss rate. If a path contains a lossy
link then the path’s loss rate will be at least equal to the link’s loss rate. Thus,
a lossy link cannot be part of a good path. To make COBALT less susceptible
to path loss rate estimation errors, we classify a link as good (non-lossy) only if
it belongs to at least s good paths. The parameter s, de(cid:12)ned as the sensitivity
of the algorithm, depends on number of paths in the network. Higher values of
s give higher con(cid:12)dence that the identi(cid:12)ed links are truly lossy. At the same
time, the number of false positives can increase because some good links will be
classi(cid:12)ed as lossy if they don’t participate in s paths.

After excluding the links found in good paths, COBALT computes the con-
(cid:12)dence levels of the remaining links. The con(cid:12)dence level cf d(l) for a link l in a
network N is computed as:

cf d(l) = K t(l) (cid:1) avl(l) (cid:1) avp

(cid:0)1(N )

(1)

In Eq.1, avl(l) is the average loss rate of all paths that l belongs to, while
avp(N ) is the average loss rate among all bad paths in the network. t(l) denotes
the number of times l is found in lossy paths and (cid:12)nally K (cid:21) 1 is a constant.
Intuitively, a link is bad if it participates in paths whose loss rate is much higher
than the average loss rate of all network paths. This e(cid:11)ect is covered by the
fraction in Equation 1. Second, we can have higher con(cid:12)dence that a link is bad
if it belongs to many bad paths. This second e(cid:11)ect is covered by the K t(l) term
in Equation 1. The greater the value of K the higher the importance of t(l). We
use an exponential function of t(l) so small di(cid:11)erences in the number of lossy
paths a link belongs to will create large di(cid:11)erence in con(cid:12)dence level simplifying
the (cid:12)nal selection of the most problematic links. More details on how K should
be selected will be discussed in future work.

As its last step, COBALT ranks the links by their con(cid:12)dence levels. The links

with the highest con(cid:12)dence levels are the most likely to be problematic.

2.1 Real-time Extension

Existing loss inference techniques cannot detect transient lossy links existing in
the Internet today [4]. To address this limitation, we extend the basic algorithm
presented above to work incrementally over shorter timescales. This online al-
gorithm works similarly to its o(cid:15)ine variant but uses an exponential moving
average formula to compute the con(cid:12)dence level of a link l:

cf dti+1(l) = (1 (cid:0) w) (cid:1) cf dti(cid:0)1(l) + w (cid:1) cf dti(l)

(2)

where cf dti(cid:0)1 (l) the previous con(cid:12)dence level and cf dti(l) the con(cid:12)dence
level computed by the most recent data. w is the aging constant controlling

the convergence time of the algorithm. An interesting point in our method is
that the value of w might not be the same across all links or even for estimates
made for the same link. Its exact value is a function of two parameters: (a) The
interval (cid:1)t = ti (cid:0) ti(cid:0)1. If (cid:1)t is large, the signi(cid:12)cance of cf dti(cid:0)1(l) decreases, as
it re(cid:13)ects an obsolete network view. Hence, in this case w should be close to one.
(b) The number of packets (cid:1)P received in (cid:1)t. Since our method is based on
statistics, the larger the sample the more con(cid:12)dent we are about the outcome of
our analysis. Therefore, as (cid:1)P increases, w should approach one.

3 Simulation Results

We used ns-2 to simulate our network of clients and servers and The simulated
network was created using BRITE’s two-level hierarchical topologies [5]. The
network consists of 800 nodes and about 1400 links. We randomly chose 100
clients out of a pool of 250 to download a large (cid:12)le from the server using HTTP.
We also picked a fraction f of the links to be lossy. Good links, have loss rates
between 0 (cid:0) 0:5%, while bad links have loss rate between 1:0 (cid:0) 3:0%.

We compare COBALT to three other methods: Random Sampling [2], Bayesian

with Gibbs Sampling [2] and the SCFS algorithm [3]. Given the di(cid:11)erence of
COBALT to previous techniques we need to rede(cid:12)ne coverage and false positives
in terms of the algorithm’s parameters. In our evaluation, a correctly identi(cid:12)ed
lossy link is one whose con(cid:12)dence level exceeds a threshold Tlossy. Consequently,
non-lossy links whose con(cid:12)dence level exceed this threshold count as false posi-
tives. We set Tlossy = K in our experiments. We ran each algorithm three times,
each time with a di(cid:11)erent topology. The reported con(cid:12)dence level is the average
of the con(cid:12)dence levels obtained over the three executions. We choose K to be
3=2, the sensitivity s = 3, while T = 0:01. For random sampling, the mean link
loss rate is chosen over 500 iterations. If the mean exceeds the loss rate threshold
of bad links, the link is said to be lossy. For the Bayesian method, the "burn in"
period in Gibbs sampling is 1000 iterations, and links are marked lossy if 99%
of the samples found are above the loss rate threshold.

Table 1. Comparison of the Random(R),Bayesian(B),SCFS(S),COBALT(C) lossy link
inference methods

Fraction of Lossy Link
Number of Bad Links

5%
64

10%
105

20%
224

Correctly Identi(cid:12)ed
False Positives

R B S C R B S C
75 51 81
13 37 39
20 12
4
23
24

30
62

43
14

9

R
46
100

S

C
B
140 57 144
45
54

15

A comparison of COBALT with the three other methods is shown in Table
1. The results on this table are based on measurements from a single server. It is

Bayes Cor.  Identified

Bayes False Positives

COBALT Cor. Identified

COBALT False Positives

s
k
n

i
l
 
f

o

 
r
e
b
m
u
N

 70

 60

 50

 40

 30

 20

 10

 0

 1

 70

Bayes

COBAT

 60

SCFS

)
n
m

i

(
 
e
m

i
t
 

i

g
n
n
n
u
R

 50

 40

 30

 20

 10

 0

 4

 2

 3

 4

 5

 6

 7

 8

 9

 10

 6

 8

 10

 12

 14

 16

 18

 20

Number of servers

Fraction of lossy links

(a) Coverage as a
function of moni-
tored servers

(b) Running time as
a function of fraction
of lossy links

Fig. 1. Performance and running time of COBALT

evident that COBALT provides the best coverage at the expense of a relatively
high false positive rate compared to SCFS. SCFS has the lowest false positive rate
but its coverage drops dramatically when lossy links are not rare. The Bayesian
method (cid:12)nds about 70% of the truly lossy links with false positive rate close to
20%. Finally, random sampling fails to identify more than 30% of the lossy links,
while at the same time the number of false positives is very high. Our (cid:12)ndings
about the Bayesian and random sampling methods are slightly di(cid:11)erent from
the results presented by Padmanabhan et al in [2] mainly due to our di(cid:11)erent
loss and topology model.

Figure 1(a) shows that the number of false positives for COBALT, as well as
the Bayesian method decreases as the number of measurement points increases.
In this scenario, traces from all the servers are combined and both algorithms
run over the aggregate collected data. Figure 1(b) shows the running time of
Bayesian, SCFS and our approach as the fraction of lossy links increases. The
execution time of the heuristics-based methods, SCFS and COBALT, is almost
ten times faster than the Bayesian method with Gibbs sampling.

References

1. M. Rabbat, R. Nowak, and M. J. Coates, \Multiple Source, Multiple Destination

Network Tomography," in Proceedings of IEEE INFOCOM 2004, Apr. 2004.

2. Venkat Padmanabhan, Lili Qiu, and Helen J. Wang, \Server-based Inference of

Internet Link Lossines," in Proceedings of IEEE INFOCOM 2003, Apr. 2003.

3. Nick Du(cid:14)eld, \Simple Network Performance Tomography," in Proceedings of In-

ternet Measurement Conference (IMC) 2003, Oct. 2003.

4. Y. Zhang, N. Du(cid:14)eld, V. Paxson, and S. Shenker, \On the Constancy of Internet
Path Properties," in Proceedings ACM SIGCOMM Internet Measurement Work-
shop, Nov. 2001.

5. Alberto Medina, Anukool Lakhina, Ibrahim Matta, and John Byers, \Brite: An
approach to universal topology generation," in Proceedings of MASCOTS 2001,
Aug. 2001.

