Latent Semantic Indexing with a Variable Number of Orthogonal

Center for Web Research, University of Chile

georges.dupret@laposte.net

Factors

Georges Dupret

February 21, 2004

Abstract

We seek insight into Latent Semantic Indexing by establishing a method to identify the optimal
number of factors in the approximation matrix. We deﬁne some reasonable property for the ap-
proximation to hold and derive a new, un-parametric query expansion method. Extensive numerical
experiments conﬁrm the value of the new method.

Keywords: Latent semantic analysis, query expansion, information retrieval, text mining, singular value
decomposition.

Introduction

The task of retrieving the documents relevant to a user query in a large text database is complicated by the
fact that different authors use different words to express the same ideas or concepts. Methods related to
Latent Semantic analysis interpret the variability associated with the expression of a concept as a noise,
and use linear algebra techniques to isolate the perennial concept from the variable noise.

Following the words of its authors (Deerwester, Dumais, Furnas & Landauer 1990), in Latent Se-
mantic Analysis, “the approach is to take advantage of implicit higher-order structure in the association
of terms with documents (“semantic structure”) in order to improve the detection of relevant documents
on the basis of terms found in queries. The particular technique used is singular value decomposition,
in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from
which the original matrix can be approximated by linear combination. Documents are represented by ca.
100 item vectors of factor weights. Queries are represented as pseudo-documents vectors formed from
weighted combinations of terms, and documents with supra-threshold cosine values are returned.”

A large number of factor weights provide an approximation close to the original term by document
matrix but retains too much noise. On the other hand, if too many factors are discarded, the information
loss is too large. While the ﬁrst objective in this work was the identiﬁcation of the optimal number of
orthogonal factors, we ﬁnally propose a method that associates a different number of dimensions for each
term.

In Section 1 we deﬁne the bag-of-word representation of documents and the simplest associated doc-
ument retrieval method. We introduce the original Latent Semantic Method in Section 2 and a common
modiﬁcation in Section 3. This paper presents two contributions: A new method in Section 4 to identify
the appropriate number of singular values and a new kind of Latent Semantic indexing - named the ˜S
Method - in Section 5. The ˜S Method doesn’t require the user to specify a number of orthogonal factors.
We use precision/recall curves in Section 6 to compare the ˜S Method to Latent Semantic indexing.

1 Bag-of-Words Representation

The methods discussed in this paper rely on the Bag-of-Words representation in which each document
is replaced by a vector of its attributes (Baeza-Yates & Ribeiro-Neto 1999). These attributes are usually
the keywords present in the document, but other information like a time span or the keyword stem can be
used.

In Boolean models (See Armstrong, Freitag, Joachims & Mitchell (1995); Pazzani, Muramatsu &
Billsus (1996); Tate (1997) or Yang (2001)) a coordinate of a vector is naught when the corresponding
attribute is absent from the document or unity when it is present. Term weighting (Salton & Buckley
1994) is a reﬁnement that takes into account the frequency of appearance of words within and among
documents, and their location of appearance (e.g., in the title, abstract or section header.)

Popular weighting schemes are variation of the TFIDF model, where the coordinates of a vector

representation of a document are given by:

di = TF(Wi, D).IDF(Wi)

(1)

In this expression TF(Wi, D) is the ith term frequency, which is the number of times term Wi occurred
in document D, and IDF(Wi) is the inverse document frequency, which is the inverse of the number of
documents word Wi occurred in.

This representation can be used to retrieve documents relevant to a user query: A vector represen-
tation is derived from the query in the same way as regular documents and then compared with the
bag-of-words representation of the database using a suitable measure of distance or similarity (Caruana
& Freitag (1994); John, Kohavi & Pﬂeger (1994); Skalak (1994)).

Formally, we call A the matrix of D documents by N attributes formed by the bag-of-word repre-
sentations of the documents in the database. Q is the vector representation of the query, and d(Di, Dj)
is the dissimilarity measure between vectors Di and Dj. By computing d(Di, Q) for all i, we can order
the documents with respect to their similarity to the query. A simple measure of similarity is the product
of the normalized bag-of-word representations. For documents Di and query Q, this is written:

(2)

(3)

ri =

Di
kDik

QT
kQk

R = AQT

Extending this to all the documents in the database, if A and Q are previously normalized the vector of
similarities can be written:

Other measures can be found in Caruana & Freitag (1994); John et al. (1994) and Skalak (1994).

The bag-of-word representation usually leads to a matrix including several thousands of attributes.
This causes serious problems in terms of computational complexity and conceptual terms if the goal is to
design an efﬁcient algorithm for further processing. There are several ﬁltering approaches that lead to a
reduction in size of the vector representation. Pruning consists in removing infrequent and very frequent
words, as they usually carry little information (Joachims 1997). Lemmatization replaces inﬂected words
(e.g. noun-plurals, verb-tenses) by their root in order to regroup forms with the same semantic content.

2 Latent Semantic Analysis

A problem with the former retrieval method has been mentioned in the introduction: The similarity be-
tween a query and a given document can be under-estimated because the database user and the document
author use different vocabularies to express the same concepts. The inventors of Latent Semantic Index-
ing (LSI) claim that (Deerwester et al. 1990) is one of the few methods which successfully overcome this
problem because it takes into account synonymy and polysemy. Synonymy refers to the existence in most
languages of equivalent or similar terms to express the same idea, and polysemy refers to the fact that
some words have multiple, unrelated meanings. Not accounting for synonymy leads to over-estimate the

dissimilarity between related documents, while not accounting for polysemy leads to erroneously ﬁnding
similarities between documents.

The idea behind LSI is to reduce the dimension of the information retrieval problem by projecting the
D documents by N attributes matrix A into an adequate subspace of lower dimension. This is achieved
based on the singular value decomposition of A:

A = UΣVT

where U and V are orthogonal matrices, and Σ is a diagonal matrix. The dimension of A being D × N ,
the non-null elements of Σ are denoted σ1, . . . , σp where p = min(D, N ) and we have σ1 ≥ σ2 ≥ . . . ≥
σp−1 ≥ σp.

The closest matrix A(k) of dimension k < rank(A) in terms of the Frobenius norm is obtained by
setting σi = 0 for i > k. This is equivalent to reducing U and V to their k ﬁrst columns and Σ to its k
ﬁrst columns and rows. We denote these reduced matrices by U(k), V(k) and Σ(k) respectively.

Based on Eq. (3), we can rewrite the similarity measure as follows:

R = AQT

= UΣVT QT
= AVVT QT
’ AV(k)V(k)T QT

(4)

(5)

This is equivalent to using V(k) to send both the documents in A and the query to a k dimensional space
and to comparing them in the new, reduced subspace.

When the numbers of documents and attributes are large, LSI involves the manipulation of very large
but sparse matrices. It is possible to improve performance and memory requirements signiﬁcantly by
using algorithms and data representation models specially designed to handle such matrices (Kobayashi
& Dupret 2000).

Presenting formal similarity with Latent Semantic Analysis, Probabilistic Latent Semantic Analysis
(Hofmann 1999) is another method that takes synonymy and polysemy into account. Its starting point
is the so called aspect model (Bartholomew & Knott (1999); Hofmann & Puzicha (1998)) where two
categories of variables are considered: The latent and manifest variables. Latent variables cannot be
observed directly but the manifest variables are conditioned on them, and by modeling this dependency
it is possible to retrieve the underlying concepts. In reference to the problem of a same concept being
expressed with different vocabularies, an analogy is made between the latent variable and the concept on
one hand, and the vocabulary used to express it and the manifest variables on the other hand.

3 Correlation Method

In the former Section, Eq. (5) is understood as a method to send documents and query to a k dimen-
sional subspace, but another possible interpretation goes as follows: V(k)V(k)T being a symmetric, full
matrix, V(k)V(k)T QT contains typically more terms than QT . Eq. (5) is thus equivalent to a query ex-
pansion based on term co-occurrences in documents. This leads naturally to using the correlation matrix
S instead of the factor V(k)V(k)T to provide for the query expansion. The computation of similarities
becomes:

R = ASQT

(6)

The intuitive motivation behind the use of the correlation matrix lies in the fact that if a keyword
is present in a document, correlated keywords should be taken into account as well - even if they aren’t
explicitly present - so that the concepts present in the document aren’t obscured by the choice of a speciﬁc
vocabulary.

Instead of decomposing the document by keyword matrix, the Correlation Method (Kobayashi,
Malassis & Samukawa 2000) applies the singular value decomposition to the correlation matrix. If D is

(7)

(8)

(9)

(10)

(11)

(12)

the number of documents, Ad the vector representing the dth document and A the mean of these vectors
(i.e. A = 1
D

d=1 Ad), the unbiased covariance matrix is written:

(cid:80)D

C =

1

D − 1

D(cid:88)

d=1

(Ad − A)T (Ad − A)

The correlation matrix S of A is deﬁned based on the covariance matrix C:

Si,j =

(cid:112)

Ci,j
Ci,i Cj,j

S = VΣVT

The correlation matrix being symmetric, its singular value decomposition can be written:

where V is orthonormal and Σ is the diagonal matrix of the ordered singular values.

This can be interpreted as sending the documents and the query to a k dimensional subspace:

R = ASQT

= AVΣVT QT
= AVΣ1/2Σ1/2VT QT
’ AV(k)Σ(k)1/2Σ(k)1/2V(k)T QT

A → AV(k)Σ(k)1/2 = A(k)
Q → QV(k)Σ(k)1/2 = Q(k)

and compare the documents in this last subspace as we did in Section 2.

As the correlation matrix dimension depends on the number of keywords and not on the number
of documents, the Correlation Method is able to handle databases of several hundred of thousands of
documents. Moreover, the correlation matrix doesn’t need to be updated each time a batch of new
documents enter the data base as long as these new documents don’t introduce new topics (i.e.
they
are sufﬁciently similar to existing documents). This aspect is particularly important in the context of
electronic networks, where new data become continuously available (See Memmi & Meunier (2000)
or Memmi (2001) for other methods to handle this problem). See Kobayashi & Aono (April 13, 2002a)
and Kobayashi & Aono (2002b) for a further discussion on the advantages of the Correlation Method.

4 Validity Ranks

We present here a criterion to determine the number of singular values necessary for a keyword to be
correctly distinguished from all others in the dictionary, along with a deﬁnition of what we understand
by “correctly distinguished.”

4.1 Single Term Queries

It isn’t clear which rank k will provide the most adequate approximation for the ranking problem, but we
can impose some basic conditions to be met. To start with, imagine the following situations: For a given
collection D of documents, we have identiﬁed a set N of terms and computed the correlation matrix and
its singular value decomposition. We create a new corpus T of documents containing only one term.
There are N such documents, namely T1 through TN . We can impose the following condition on the
the rank k): If we issue a query for term i, we should retrieve
correlation matrix approximation (i.e.
document Ti as the top item on the list.

Based on Eq. (10), this results in the following condition for all i 6= j:

Ri ≥ Rj ⇒ TiS(k)TT

i ≥ TiS(k)TT
j

because the bag-of-word representation of a query for keyword i and Ti coincide and are equal to a vector
with all elements set to zero but the ith equal to one.
The inequality in (13) can easily be simpliﬁed to

S(k)ii ≥ S(k)ij

or S(k)ii ≥ S(k)ji given that S(k) is symmetric.

In terms of the S(k) elements, this means that even though the diagonal elements aren’t equal to
unity, they should be greater than the non-diagonal elements of the same row (or the same column, as the
matrix is symmetric). This provides us with a criteria for k when ranking against a single term query:
set k to N and decrease its value until Eq. (14) is not satisﬁed. The validity rank ρi of term i is equal
to the k such that the condition is veriﬁed for k but not for k − 1. The next property of singular value
decomposition enables the computation of S(k − 1) based on S(k), which simpliﬁes the computation of
the ranks:

S(k) =

ViσiVT
i

k(cid:88)

i=1

For the sake of later discussion we introduce the next deﬁnitions: A keyword is said to be “valid” at
rank k if it satisﬁes Eq. (14) and is said to be of rank k if k − 1 is the largest value for which Eq. (14) is
not veriﬁed. In this case, k is the validity rank of the keyword.

We have shown in Dupret (2003) the signiﬁcance of this condition: For a given keyword i, when the
approximation rank is below ρi, all the information contained in the query isn’t used, while when it is
larger, noise is introduced.

4.2 Two Terms Queries

We can extend this development to multiple terms queries. In the case of a query for two terms, we
redeﬁne the artiﬁcial corpus T as the set of all documents composed of two keywords.

We start imposing that a query for terms i and j should rank the document containing them before

any document containing i but not j. This can be expressed for all i, j 6= n as follows:

S(k)ii + 2S(k)ij + S(k)jj ≥ S(k)ii + S(k)ij + S(k)in + S(k)jn

S(k)ij + S(k)jj ≥ S(k)in + S(k)jn

Similarly we also need to have

S(k)nj + S(k)jj ≥ S(k)in + S(k)ij

because searching for the two terms query {j, n} should return Tn,j ﬁrst. Combining Eq. (16) and (17)
results in the necessary condition that S(k)jj ≥ S(k)in for all i, n 6= j. This further combined with
Eq. (14) gives:

S(k)jj ≥ S(k)in ∀i, j, n

This imposes a common validity rank for all keywords. Although it is always possible to meet this
condition, the resulting rank k is larger than what numerical experiments recommend. It is also intuitively
unappealing: If two words have a perfect correlation - which might happen in practice - the condition in
Eq. (18) imposes that rank k equals the full rank. Nevertheless, we expect most queries of two terms to
satisfy it.

(13)

(14)

(15)

(16)

(17)

(18)

5 Optimal Rank and the ˜S Method

5.1 Global Rank

We saw in Section 4.1 that terms have different validity ranks. A simple method to estimate the optimal
global rank ρ of the correlation matrix approximation consists in requiring that a large proportion of the
keywords be valid at that rank. For example ρ should be large enough for 95% of the ρi to be inferior or
equal to ρ. We will run numerical experiments in Section 6 to test this idea.

5.2 Terms Ranks

If this last approximation appears to be accurate, this is clearly useful, but a method requiring no ad-
justment from the user would be better. In Eq. 10 we showed that the ranking can be approximated
by

R ’ AV(k)Σ(k)1/2Σ(k)1/2V(k)T QT

= AS(k)1/2S(k)1/2T

QT

(19)

In Section 4.1, we saw there is a validity rank for each term. For this reason, instead of truncating all
rows of S1/2 after the kth term, we truncate each row after the validity rank of the corresponding term:
For row i in S1/2, we set to zero all the elements with an index larger than ρi.

We also set the diagonal elements of the recomposed matrix ˜S to unity because we saw in Section 4
that the diagonal terms should be larger than the other terms. This way, the matrix ˜S deﬁned here is exact
on the diagonal and an approximation for other terms.

6 Numerical Experiment

We use different text databases to test the ideas presented in this paper. For each corpus, we compute
the precision/recall curves of the Correlation Method for different values of k and identify the optimal
experimental rank ρe. We then compare this rank with the estimation deﬁned in Subsection 5.1. We also
compare ρe with the ˜S method described in Subsection 5.2 on the precision/recall curves.

For both methods, the approximation of keywords correlation looses quality if documents are too
long because our estimation doesn’t take into account the word distance between terms. To remediate
this, we cut documents in pieces of 25 consecutive words for all corpora (but NPL where documents
are short enough). The correlation matrix is then computed based on these shorter, more numerous
documents. Precision has been found to be higher when applying this method.

If we set A to be the number of relevant documents and B to be the number of retrieved documents,

precision and recall are deﬁned to be

Precision = |A∩B |
|B |

, Recall = |A∩B|
|A|

We use plots of Precision versus Recall to compare different methods with different parameters. A
method is strictly better than another if the precision associated to it is larger for all recall values.
See Van Rijsbergen (1979) for a detailed explanation of the evaluation and comparison methodology.

We use the corpora associated with the SMART (Blake & Merz 1998) project and the REUTERS

data (Lewis 1997).

6.1 The REUTERS Database

This section is taken verbatim from Sanderson (1994) who explains with brievety and great clarity the
method associated with the generation of queries and relevant documents in the REUTERS database.

“The main difference between REUTERS and an IR test collection is that REUTERS doesn’t have a
set of standard queries with corresponding relevant documents. However each document in REUTERS

It is these codes that allow us to use
is tagged with a number of manually assigned subject codes.
REUTERS as a test collection for comparing document representation methods. This use of REUTERS
was ﬁrst described by Lewis (1997) and it is his method, with some modiﬁcations, that is described here.”
“First, R is deﬁned as the set of all documents in the REUTERS collection. This set is then partitioned
into two subsets of equal size: Q (the query set) and T (the test set). The method used to partition R was
chosen to be a random assignment of documents into one of the two subsets. This method ensured that
groups of documents covering common themes would be evenly distributed to both Q and T.”

“Next, S is deﬁned as the set of all subject codes that have been assigned to at least one document
in Q and at least one document in T. If we pick one of the subject codes from S, we can now perform a
retrieval.”

“For example suppose we perform a retrieval for the subject code ‘crude’. First, all documents in Q
tagged with ‘crude’ are selected. Then by performing relevance feedback using the selected documents,
word/weight pairs are generated to form a query. This query is used to retrieve from the T set. The
resulting ranked document list is examined to see where in the ranking, documents tagged with ‘crude’
appear. The position of the tagged documents is used to produce precision/recall ﬁgures. A conserva-
tive interpolation technique (outlined in Van Rijsbergen (1979)) is used to transform these ﬁgures into
precision values at ten standard recall levels (0.1, 0.2, . . . 1.0).”

“This process is repeated for each subject code in S, each time producing another set of precision
values. These precision values are then averaged to give an overall set of values for each of the ten
standard recall levels. So by partitioning REUTERS and using the subject codes, all the components of a
classic IR test collection are created.

• the collection to be searched - T

• a set of queries - generated from Q, for each element of S

• a set of relevant documents for each query - documents in T tagged with the respective element of

S.”

“The use of relevance feedback to generate the queries in place of verbose user generated queries
means that the form of retrieval can be likened to an iteration of relevance feedback during a retrieval
session.”

In the particular experiment presented here, we applied the following procedure:

1. We divide randomly in two the set of documents of a given subject code: The Q and T sets. The

two subsets have approximately the same number of documents.

2. We create one large document composed of all the documents in Q.

3. We identify the four terms with higher TFIDF scores to generate a four terms query (four or ﬁve

have been found to be the optimal number of terms. See (Sanderson 1994)).

4. We compute the precision/recall pairs for recalls equal to

0.05, 0.1, 0.2, . . . , 0.9, 1.0.

We select the 53 subject codes with more than 20 associated documents. For each of them, we repeat the
procedure for 15 random divisions of the initial set. This provides a total of 795 query/relevant document
pairs that we use to test the different methods. Precision/recall curves are computed using the so-called
conservative method mentioned above.

6.2 The SMART Database

The SMART collection proposes seven databases, namely ADI, CACM, CISI, CRAN, MED, NPL
and TIME, each with a set of queries and a list of documents relevant to these queries.

We discarded TIME and CRAN databases from the analysis because the simple cosine, the Correlation
and the ˜S methods gave poor results: Precision around 5 % in the best case. Probably, the bag-of-word
representation of documents entails too much a loss of information with regards to the associated queries.
We summarize some characteristics of the corpora in Table 1: Size in megabytes, number of docu-

ments and number of queries.

Corpus

Size (MB)

Nbr. of

Nbr. of Nbr. of
Documents Topics Queries

ADI
CACM
CISI
MED
NPL
REUTERS

.363
2.1
2.3
1
3.1
25.1

82
3203
1459
1032
11428
21578

135

Table 1: Corpora Characteristics.

35
63
111
29
92
795

Results from REUTERS database will be more reliable than from SMART because of the much larger

size of the database and the higher number of queries.

6.3 Global Ranks

We show in Figs. 1, 3, 5, 7, 9 and 11 the conservative approximation of the precision/recall curves for
all six databases (Ignore for now the curves labeled “Unit” and “Vec”). On the right of these ﬁgures,
we show the histograms of the number of terms that are valid in the sense of Eq. 14 as a function of
the number of eigenvalues.
In all experiments, most terms are valid for a relatively low number of
eigenvalues, but the validity of the last term is obtained only after a signiﬁcantly larger number ρv of
eigenvalues is computed. This suggests to stop computation of new eigenpairs after a reasonable number
of terms are valid, like 90 or 95 % of the vocabulary.

In this experiment, we compare the rank ρv leading to a large proportion of terms to be valid with
the optimal experimental rank ρe as deduced from the observation of Figs. 1, 3, 5, 7, 9 and 11. A single
optimal rank is difﬁcult to estimate in general: When determined experimentally, precision/recall curves
tend to cross each other. On the other hand, the analytical estimation of ρv depends on the proportion
of valid terms we require. Instead of deﬁning a threshold artiﬁcialy, we chose to identify ρv through
the observation of the histogram ﬁgures. The second column gives the number of terms used for the
bag-of-word representation.

Corpus Nbr. of Terms

ρe

ADI
CACM
CISI
MED
NPL
REUTERS

242
629
1016
645
1088
1690

50 ∼ 100
300 ∼ 350
200 ∼ 1000

50 ∼ 100
400 ∼ 500
750 ∼ 1690

ρv
’ 50
’ 300
’ 300

125 ∼ 175

∼ 250

600 ∼ 800

Table 2: Ranges for optimal ranks.

Approximations tend to be under-estimates but they are roughly compatible with experimental op-

tima. More sophisticated method for a global rank approximation can be found in Efron (2002).

6.4 Comparison of Correlation and ˜S Methods
The ˜S Method is labeled “unit” in Figs. 1, 3, 5, 7, 9 and 11. It is further identiﬁed by 11 circles corre-
sponding to the points were the conservative precision has been evaluated. The “vec” Method is described

Figure 1: ADI

Figure 2: ADI ranks

Figure 3: CACM

Figure 4: CACM ranks

204060801000102030405060recallprecisionunitvec255075100125150175200225Histogram of RanksRanks (max= 73 )Counts010203040506070024681012204060801000102030recallprecisionunitvec100200250300350400500Histogram of RanksRanks (max= 494 )Counts0100200300400500010203040Figure 5: CISI

Figure 6: CISI ranks

Figure 7: MED

Figure 8: MED ranks

204060801000510152025recallprecisionunitvec1002003004005001000Histogram of RanksRanks (max= 472 )Counts01002003004000102030405020406080100020406080recallprecisionunitvec50100150200300400500645Histogram of RanksRanks (max= 207 )Counts0501001502000102030Figure 9: NPL

Figure 10: NPL ranks

Figure 11: REUTERS

Figure 12: REUTERS ranks

20406080100010203040recallprecisionunitvec1002003004005007501000Histogram of RanksRanks (max= 633 )Counts0100200300400500600010203040506070204060801000102030405060recallprecisionunitvec20030040050060075010001500Histogram of RanksRanks (max= 1690 )Counts050010001500050100150by Eq. (3): This is simply the cosine distance between query and documents bag-of-word representations.
Other curves are the results of the Correlation Method for various value of the rank k as reported in the
ﬁgures’ legends.

We see that the “vec” Method is sometimes signiﬁcantly better, sometimes signiﬁcantly worse than
˜S and the Correlation Methods. This isn’t uncommun and has been reported in the Literature (Kobayashi
& Aono 2002b).

The ˜S Method gives result close to Correlation Method with rank set to the experimental optimum,
but for the MED data. Given that this database is small ( ’ 1 MB in size and 29 queries compared with
’ 25.1 MB and 795 queries for REUTERS), we believe this result isn’t signiﬁcant enough to invalidate
the ˜S Method. The most reliable experiment is also the most favorable to the ˜S Method: In Fig. 11, its
precision is either above or insigniﬁcantly below the experimental optimal global rank.

Based on simple considerations of what we expect from the Correlation Method, we derived a new way
of estimating the optimal rank at which to stop the singular value decomposition. Similar considerations
lead to a new way of estimating from the data an appropriate query expension scheme.

This method has been extensively tested on experimental data set of small to medium size. It com-
pares favorably to the traditional Correlation Method both in terms of precision/recall curves and because
it doesn’t require the user to set the number of orthogonal factors.

Conclusion

References

Armstrong, R., Freitag, D., Joachims, T. & Mitchell, T. (1995), ‘Webwatcher: A learning apprentice
for the world wide web’, Proc. of the AAAI Spring Symposium on Information Gathering from
Heterogeneous, Distributed Environments .

Baeza-Yates, R. & Ribeiro-Neto, B. (1999), Modern Information Retrievial, ACM Press.

Bartholomew, D. & Knott, M. (1999), Latent Variable Models and Factor Analysis, Second Edition,

Vol. 7, Kendall’s Library of Statistics.

Blake, C. & Merz, C. (1998), ‘UCI repository of machine learning databases’.

*http://www.ics.uci.edu/∼mlearn/MLRepository.html

Caruana, R. & Freitag, D. (1994), Greedy attribute selection, in ‘International Conference on Machine

Learning’, pp. 28–36.

Deerwester, S., Dumais, S., Furnas, G. & Landauer, T. (1990), ‘Indexing by latent semantic analysis’,

Journal of the American Society of Information Science 41, 391–407.

Dupret, G. (2003), Latent concepts and the number orthogonal factors in latent semantic analysis, in ‘Pro-
ceedings of the 26th annual international ACM SIGIR conference on Research and development in
informaion retrieval’, ACM Press, pp. 221–226.

Efron, M. (2002), Amended parallel analysis for optimal dimensionality estimation in latent semantic

indexing, Technical Report SILS Technical Report TR-2002-03, OSRT.
*http://www.ibiblio.org/osrt/people/efron.html

Hofmann, T. (1999), Probabilistic Latent Semantic Indexing, in ‘Proceedings of the 22nd Annual ACM
Conference on Research and Development in Information Retrieval’, Berkeley, California, pp. 50–
57.

Hofmann, T. & Puzicha, J. (1998), Unsupervised learning from dyadic data, Technical Report TR-98-

042, Berkeley, CA.

Joachims, T. (1997), A probabilistic analysis of the rocchio algorithm with TFIDF for text categorization,

in ‘International Conference on Machine Learning’, pp. 143–151.

John, G. H., Kohavi, R. & Pﬂeger, K. (1994), Irrelevant features and the subset selection problem, in
‘International Conference on Machine Learning’, pp. 121–129. Journal version in AIJ, available at
http://citeseer.nj.nec.com/13663.html.

Kobayashi, M. & Aono, M. (2002b), Vector space models for search and cluster mining (invited paper),

in ‘M. Berry (ed.), A Comprehensive Survey of Text Mining’, Springer.

Kobayashi, M. & Aono, M. (April 13, 2002a), ‘Major and outlier cluster analysis using dynamic re-
scaling of document vectors’, Proceedings of the SIAM Text Mining Workshop, Crystal City, VA
pp. 103–113.

Kobayashi, M. & Dupret, G. (2000), ‘Efﬁcient estimation of singular values for searching very large and

dynamic web databases’, WWW9: Ninth World Wide Web Conference, Amsterdam .

Kobayashi, M., Malassis, L. & Samukawa, H. (2000), ‘Retrieval and ranking of documents from a

database’, U.S. Patent .

Lewis, D. (1997), ‘Reuters-21578 text categorization test collection’, AT&T Labs-Research .

*http://www.research.att.com/(cid:101)lewis/

Memmi, D. (2001), ‘Incremental clustering and category drift’, Cahiers Leibniz 2001-22.

Memmi, D. & Meunier, J.-G. (2000), ‘Using competitive networks for text mining’, NC’2000, Berlin .

Pazzani, M., Muramatsu, J. & Billsus, D. (1996), Syskill & webert: Identifying interesting web sites, in
‘AAI Spring Symposium on Machine Learning in Information Access’. Lecture Notes in Computer
Science 11.
*www.parc.xerox.com/istl/projects/mlia/papers/pazzani.ps

Salton, G. & Buckley, C. (1994), ‘Automatic structuring and retrieval of large text ﬁles’, Communications

of the ACM 32(2), 97–107.

Sanderson, M. (1994), Reuters Test Collection, in ‘BSC IRSG’.

Skalak, D. B. (1994), Prototype and feature selection by sampling and random mutation hill climbing

algorithms, in ‘International Conference on Machine Learning’, pp. 293–301.

Tate, A. (1997), ‘Learning and revising user proﬁles: The identiﬁcation of interesting web sites’, In Tate,
A. (Ed.), Advanced Planning Technology, (pp. 206–209). Menlo Park: AAAI Press. Pazzani, M. &
Billsus, D. (1997). Learning and revising user proﬁles: The identiﬁcation of interesting web sites.
Machine Learning 27:313–331. .

Van Rijsbergen, C. J. (1979), Information Retrieval, 2nd edition, Dept. of Computer Science, University

of Glasgow.

Yang, Y. (2001), ‘Expert network: Effective and efﬁcient learning from human decesions in text catego-
rization and retrieval’, Proc. of the 7th Annual International ACN-SIGIR Conference on Research
and Development in Information Retrieval Dublin.

