Training and Testing of Recommender Systems on Data

Missing Not at Random

Harald Steck

Bell Labs, Alcatel-Lucent

600 Mountain Ave

Murray Hill, NJ 07974, USA

Harald.Steck@alcatel-lucent.com

ABSTRACT
Users typically rate only a small fraction of all available
items. We show that the absence of ratings carries useful
information for improving the top-k hit rate concerning all
items, a natural accuracy measure for recommendations. As
to test recommender systems, we present two performance
measures that can be estimated, under mild assumptions,
without bias from data even when ratings are missing not
at random (MNAR). As to achieve optimal test results, we
present appropriate surrogate objective functions for e(cid:14)-
cient training on MNAR data. Their main property is to
account for all ratings{whether observed or missing in the
data. Concerning the top-k hit rate on test data, our exper-
iments indicate dramatic improvements over even sophisti-
cated methods that are optimized on observed ratings only.

Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications{
Data Mining

General Terms
Algorithms

Keywords
Recommender Systems

1.

INTRODUCTION

The idea of recommender systems is to automatically sug-
gest items to each user that s/he may (cid:12)nd appealing. The
quality of recommender systems can be assessed with re-
spect to various criteria, including accuracy, diversity, sur-
prise / serendipity, and explainability of recommendations.
This paper is concerned with accuracy, and we consider the
top-k hit rate{based on all items{as the natural accuracy

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for pro(cid:2)t or commercial advantage and that copies
bear this notice and the full citation on the (cid:2)rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speci(cid:2)c
permission and/or a fee.
KDD’10, July 25(cid:150)28, 2010, Washington, DC, USA.
Copyright 2010 ACM 978-1-4503-0055-1/10/07 ...$10.00.

measure for recommender systems, as only a few out of all
items can be recommended to a user in practice (see Section
2 for exact de(cid:12)nition of top-k hit rate). While this measure
is computationally tractable for testing the predictions of
recommender systems, unfortunately it is computationally
very costly for training recommender systems. For training,
we thus resort to appropriate surrogate objective functions
that are computationally e(cid:14)cient.

The root mean square error (RMSE) has become the most
popular accuracy measure in the literature of recommender
Its computational e(cid:14)-
systems{for training and testing.
ciency is one of its main advantages.
Impressive progress
has been made in predicting rating values with small RMSE,
and it is impossible to name all approaches, e.g., [4, 7, 8, 13,
15]). There is, however, also some work on optimizing the
ranking of items, e.g., measured in terms of normalized Dis-
counted Cumulative Gain (nDCG) [18]. Despite their dif-
ferences, they have in common that they were trained and
tested on observed ratings only. Obviously, these measures
cannot immediately be evaluated if some items have unob-
served ratings. This is our motivation for using the top-k
hit rate as to assess the ranking of all items, as it can deal
with observed and missing ratings (see Section 2 for details).
In support of the validity to use observed ratings only, the
experiments on the Net(cid:13)ix data in [8] show that models with
better RMSE on the observed ratings achieve a better top-k
hit rate on all items.

When using observed ratings only, the implicit underly-
ing assumption is, however, that the ratings in the available
data are missing at random. Missing (completely) at ran-
dom and missing-data mechanisms in general are outlined
in [10, 14]. Missing at random means in the context of rec-
ommender systems that the probability of a rating to be
missing does not depend on its value. If data are missing at
random, using only the observed data for statistical analysis
yields ’correct’ results (e.g., unbiased maximum-likelihood
parameter estimates, and predictions) [10, 14]. In contrast,
if the data are missing not at random (MNAR), the missing
data mechanism cannot be ignored in general, and has to
be modeled precisely as to obtain correct results [10, 14]. In
other words, if the missing data mechanism is ignored, the
resulting recommendation accuracy may be degraded signif-
icantly.

Recent work by Marlin et al.

[11, 12] provided evidence
that the data typically used for training and testing recom-
mender systems are MNAR: their histograms of the distribu-

713Table 1: Simplistic Example for ratings missing not
at random (MNAR): test data where users rated
only what they liked or knew.

users

romance
lovers

horror
fans
5
5

5
5

5

5

5

5

5
5

5

5

5
5

5

5
5
5

5
5

5

h
m o
r
o
.
v
i
r
e
o
s m
.

tion of ratings in the Yahoo!LaunchCast data show that low
ratings are much more likely to be missing from the observed
data than high ratings (see Fig. 2 in [11]). This may be a
consequence of the fact that users are free to choose which
items to rate. This kind of data is inexpensive to collect, and
typically used for training and testing recommender systems
(prior to conducting more costly (cid:12)eld studies).

The e(cid:11)ect of ratings missing not at random (MNAR) is
illustrated by the following simplistic example, see Table 1:
assume that the items comprise horror and romance movies;
the romance-lovers assign the highest rating of 5 stars to
romance movies and choose not to rate any of the horror
movies, while the horror-fans do the opposite. The missing
data mechanism is to rate only movies the users like or know.
Let us further assume a (useless) recommender system that
predicts a 5-star rating for all movies and users. When se-
lecting a movie with the highest predicted rating (ties are
broken at random), a horror or romance movie is recom-
mended with equal probability to each user. While this is
obviously a useless recommender system, it achieves perfect
results on the observed ratings in our test data in Table 1{for
any performance measure, including RMSE, mean absolute
error (MAE), nDCG, expected reciprocal rank (ERR), mean
average precision (MAP), etc. The reason is that the test
data are missing not at random (MNAR), as they do not
contain any test cases for the combinations (horror movie,
romance lover) and (romance movie, horror fan). The solu-
tion is to consider, for each user, the predictions / rankings
of all items{whether their ratings are observed or missing.
This is the scenario of a real-world recommender system,
which has to pick a few items from among all available ones.
Many popular performance measures cannot readily deal
with missing ratings. In this paper, we adopt the top-k hit
rate as a natural measure for assessing the accuracy of rec-
ommender systems. In Section 2, we (cid:12)rst de(cid:12)ne the top-k
hit rate, TOPK, and present a second, related performance
measures, named ATOP; both can be estimated without
bias on MNAR data under mild assumptions. While these
measures are computationally tractable for testing recom-
mender systems, they are too costly for training. We thus
resort to surrogate objective functions that are computation-
ally e(cid:14)cient (Section 3). In Section 4, we take a collabora-
tive (cid:12)ltering approach, and present the update equations for
training a matrix factorization model w.r.t. our new objec-
tive functions. Our experiments on publically available data
in Section 6 show that it is essential to account for the fact

that the data are MNAR, and we obtain a signi(cid:12)cantly im-
proved top-k hit rate. Section 7 summarizes our conclusions
and future work.

2. TEST PERFORMANCE MEASURES

We consider the top-k hit rate as a natural performance
measure for recommender systems in practice, as only a
small number of items can be recommended to a user. Ide-
ally, each of the recommended items is relevant to a user.

De(cid:12)nition (’relevant’): An item is relevant to a user
if s/he (cid:12)nds this item ’appealing’ or ’interesting’. Di(cid:11)erent
items may be relevant to di(cid:11)erent users (personalization).
In our notation, a quantity with the superscript ’+’ refers
to relevant items. As to make this general de(cid:12)nition more
concrete, concerning the data in our experiments, we con-
sider items with the highest (5 star) rating as relevant to a
user. In case of continuous-valued ratings, one may regard
the values above an appropriate threshold (possibly di(cid:11)erent
for each user) as relevant.

2.1 (Unknown) Complete Data

In this section, we assume that complete data are avail-
able, i.e., the ratings of all items by all users. This allows
us to de(cid:12)ne the performance measures we ideally would like
to compute. For a chosen cut-o(cid:11) value k0 2 N, the top-k hit
rate may be de(cid:12)ned in two alternative ways:

(cid:15) precision: N +;k

u

0

=k0,

(cid:15) recall: N +;k

u

=N +
u ,

0

0

u

where N +;k
is the number of items that are relevant to
user u, and which made it into the top k0 items (out of all
items) based on the recommender system’s predictions; and
N +
u denotes the number of items relevant to user u. We
make the following three observations: (1) Precision and re-
call can both be calculated from complete data, i.e., if the
ratings of all N items are known. (2) Typically, however,
most ratings are unobserved:
in contrast to precision, re-
call can be estimated from MNAR data without bias under
mild assumptions (see next section). (3) When comparing
di(cid:11)erent recommender systems to each other on (cid:12)xed data
and (cid:12)xed k’, then precision and recall are proportional to
each other, with a (user-dependent) proportionality-factor
k0=N +
u . In other words, the recommender system with the
larger recall also has the larger precision.

In place of the integer-valued k0 as threshold of the top-k
hit rate, we will use a normalized k 2 [0; 1] in the remainder
of this paper: k = (k0 (cid:0) 1)=(N (cid:0) 1) determines the fraction
of items relative to the total number N of items.

For the reasons outlined above, we de(cid:12)ne the top-k hit

rate as recall (/ precision, for (cid:12)xed data and k):

TOPKu(k) =

(1)

N +;k
u
N +
u

;

u denotes all items relevant to user u, and N +;k
where N +
counts the relevant ones in the top k, as above. Its maximum
value (achieved for perfect recommendations) is k0=N +
u < 1
if k0 < N +
u , and equal to 1 otherwise. The average over all
users u is given by

u

TOPK(k) =

wuTOPKu(k);

(2)

u
X

714with user-weights wu, which are normalized as follows:

u wu = 1.
The following two properties of Eq. 1 may be considered
P
a disadvantage: (1) it depends on the chosen value of k, and
(2) it has a hard cut-o(cid:11) for the top k items, and ignores the
ordering (position bias) of relevant items that are ranked
either below or above the cut-o(cid:11). Both can be overcome by
the Area under the TOPKu-curve (ATOPu) for user u. We
de(cid:12)ne it as follows :

ATOPu =

TOPKu(k) dk:

(3)

1

0

Z

Given that TOPKu(k) is a monotone increasing function of
k 2 [0; 1], we can alternatively consider k as a function of
TOPKu, and integrate along TOPKu instead: ATOPu =
1
0 (1 (cid:0) k) dTOPKu. From complete data, the latter can
be calculated (as a non-parametric estimate) as follows: for
R
(cid:12)xed user u, all items i are ranked according to their pre-
dicted rating ^Ri;u: item i with the highest predicted rating
is assigned the largest rank Ranku;i = N , and the item with
the lowest predicted rating obtains Ranku;i = 1.1 We de-
(cid:12)ne the normalized rank Nranku;i = (Ranku;i (cid:0) 1)=(N (cid:0)
1) 2 [0; 1]. Let S+
u denote the set of relevant items for
user u. Then the non-parametric estimate of the TOPKu-
curve, based on these ranked items, increases by 1=N +
u where
Nrank+
u;i = 1(cid:0)k for each i 2 S+
u , and is constant everywhere
else. For this reason, we have:

ATOPu =

Nrank+

u;i = hNrank+

u;iii2S+

u

;

(4)

1
N +
u

Xi2S+

u

u (cid:0) 1)=(N (cid:0) 1)=2, where N +

where h(cid:1)i denotes the average. The area under the TOPKu-
curve equals the average normalized rank of the relevant
items. This also shows that the maximum value of ATOPu
is 1 (cid:0) (N +
u in the complete data
is typically unknown. Rather than the ordering of all items,
often the ordering above a small cut-o(cid:11) value is of interest;
it is straightforward to de(cid:12)ne a truncated version of ATOP
that captures the area under the initial part of the T OP K
curve only. With the same weights for the users as above,
the average area under the TOPKu-curves is de(cid:12)ned as

ATOP =

wuATOPu:

(5)

2.2 Observed MNAR Data

u
X

In this section, we show that the performance measures
outlined above, TOPK and ATOP, can be estimated from
available MNAR data under mild assumptions. The moti-
vation for our approach is as follows. In the one extreme,
the ratings are indeed missing at random, and the miss-
ing data mechanism can be ignored. For a given data set,
one cannot generally expect this (convenient) assumption to
hold. In the other extreme, there is a (non-trivial) missing
data mechanism, and one is able to model it exactly. Un-
fortunately, the (exact) missing data mechanism is typically
unknown. Even if it were known for one particular data set,
one could not expect it to be valid for other data sets in gen-
eral. But there is also some middle ground between these
1Note that ranks are often de(cid:12)ned the other way around in
the literature: the smallest rank-value is best. Our de(cid:12)ni-
tion has the advantage that it can also be viewed from the
perspective of the Borda count.

two extremes, which is the focus of this paper. Inspired by
general properties of various publically available data sets
for recommender systems, we make the following
Assumptions (User Model):

1. We assume that the relevant2 rating values are missing

at random in the observed data.

2. Concerning the other rating values, we allow for an
arbitrary missing data mechanism, as long as they are
missing with a higher probability than the relevant rat-
ing values do.

Several comments are in order. Typically, the number of
relevant ratings is only a tiny fraction (a few percent) of
all ratings in the complete data, e.g., see Fig. 2 in [11].
Our (cid:12)rst assumption is hence a major relaxation compared
to the assumption that all ratings are missing at random.
Note, however, as every assumption is a simpli(cid:12)cation of
the real world, it may not hold exactly for a given data set
in reality; but it may serve as a reasonable approximation
across a large number of data sets. This is supported by the
large improvements we obtained in our experiments. Our
second assumption can be expected to hold for many data
sets, including the Yahoo!LaunchCast data, see Fig. 2 in
[11]. Note that this also implies that the average unobserved
rating is lower than the average observed rating.

Let S+;obs

denote the set of observed relevant items for
j their number; let N +;obs;k
user u, and N +;obs
be the number of relevant items in the top k. Nrank+;obs
is
calculated for the observed relevant items w.r.t. to all items.
Theorem: Under the above two assumptions, the mea-

= jS+;obs

u;i

u

u

u

u

sures

TOPKobs

u (k) =

N +;obs;k
u
N +;obs

u

;

ATOPobs

u = hNrank+;obs

u;i ii2S+;obs

;

u

(6)

(7)

P

computed from the observed MNAR data, provide unbiased
estimates of the measures TOPKu(k) and ATOPu, respec-
tively. The averages TOPKobs(k) =
u (k)
and ATOPobs =
are unbiased estimators
of the corresponding measures evaluated on complete data.

u wuTOPKobs

u wuATOPobs

P

u

In summary, TOPKobs

Proof: The ranks of the observed relevant items are de-
termined by using all the N items, whether their ratings
are observed or missing. The ranks of the observed relevant
items are identical for complete and MNAR data. With the
assumption that the relevant items are missing at random, it
is apparent that their average rank can be estimated without
bias. The other claims follow analogously from Assumption
1, and the well-known fact that estimates are unbiased from
2
data missing at random.
u (k) is (1) the unbiased estimate
of recall (under the outlined assumptions); and (2) propor-
tional to precision (with unknown user-dependent proportion-
ality-factor k0=N +
u ) when comparing recommender systems
on (cid:12)xed data and (cid:12)xed k. In other words, the recommender
system with larger recall also has larger precision on (cid:12)xed
data and (cid:12)xed k. Analogously, the weighted average over
all users, TOPKobs(k) =
u (k) is (1) an un-
biased estimate of the average recall (weighted by wu), and
(2) proportional to the precision averaged over the users.
2See De(cid:12)nition of ’relevant’ above.

u wuTOPKobs

P

715u

ATOPobs

can be viewed (1) as the unbiased estimate of
the area under the TOPKu(k)-curve; and (2) as the aver-
age normalized rank of the relevant items. Both views also
apply to the weighted average over the users, ATOPobs =
u . Note that ATOP captures position bias
(within the entire list of items), but in a quantitatively dif-
P
ferent way than nDCG, ERR, or MAP. Note that the lat-
ter measures cannot be readily evaluated in the presence of
missing ratings.

u wuATOPobs

2.3 Practical Computation of ATOP Measure

First, our de(cid:12)nition of the normalized rank, Nranku;i =
(Ranku;i (cid:0) 1)=(N (cid:0) 1) 2 [0; 1] (above Eq. 4), can also be
viewed as follows:
for a given user u and item i, it is the
fraction of remaining items (i.e., all items except for item i)
that are ranked lower than item i. One can thus calculate
Nrank+
u;i for each user-item pair (i; u) with an observed rel-
evant rating in the test set individually; in other words, one
can iterate through the list of relevant pairs (i; u) in the test
set in any order and calculate the normalized rank Nrank+
u;i
for one pair (i; u) at a time. This may be more convenient
than calculating the ranks of all relevant items simultane-
ously for each user u. The ATOP measure can then be
calculated as the average of all these normalized ranks; and
the TOPK(k) measure as the fraction of those normalized
ranks with Nranki;u (cid:21) 1 (cid:0) k; note that this implicitly deter-
mines the weights wu over the users: wu / N +;obs
, i.e., the
number of observed relevant ratings of user u. We use these
weights in our experiments, so that our measure becomes
identical to the one used in [8], and thus allows for a fair
comparison of the experimental results.

u

Second, one may rank a relevant pair (i; u) w.r.t.

to a
random subsample of size ~N (cid:0) 1 of the remaining items
(N (cid:0) 1), as suggested in [8]. This results in computationally
e(cid:14)cient estimation of the TOPK(k) and ATOP measures.
Note that this may result in a small positive bias, especially
for TOPK(k) at small values of k;
it is hence important
to use the same value of ~N for fair comparisons (of course,
bias-correction is an alternative).

2.4 Area under the ROC Curve

As an aside, we like to note that the TOPK(k) and ATOP
measures are closely related to the Receiver Operating Char-
acteristics (ROC) Curve, which originates from signal pro-
cessing and has since been adopted by the machine learning
community for assessing classi(cid:12)cation performance and the
quality of rankings. The latter is due to the equivalence of
the Area Under the ROC Curve (AUC) and the Wilcoxon-
Mann-Whitney (WMW) statistic. While the de(cid:12)nition of
WMW is concerned with the correct ranking of all possible
pairs of items, it was shown in [5, 19] that WMW or AUC
can be evaluated in time linear in N given the ranks of the
relevant items. It reads for user u:

AUCu =

1
u N (cid:0)
N +

u 2

0

Rank+

+

i;u1

N +
u

i=1
X

N +

u + 1
2 !3
5

 

;

(8)

@

4
where N (cid:0)
u = N (cid:0) N +
u is the number of irrelevant items for
user u. Note that we have to assume complete data here,
as N +
It follows
immediately:

u would be unknown otherwise.

u and N (cid:0)

A

AUCu =

1 +

ATOPu +

N +

u (cid:0) 1
N (cid:0)

u (cid:19)

(cid:18)

N +
u + 3
2N (cid:0)
u

= ATOPu + O

(9)

N +
u
N (cid:0)

(cid:18)

u (cid:19)

:

This shows that the di(cid:11)erence between AUC and ATOP
becomes negligible if N +
u (cid:28) N . This is the case in the Ya-
hoo!LaunchCast data set, where N +
u =N (cid:25) 3% in the survey
data, see Figure 2 in [11]. A similarly small fraction may
be expected for the other commonly used data sets. Under
this condition, AUC can be computed from MNAR data in
good approximation; the TOPK curve can be expected to
be close to the ROC curve.

3. TRAINING OBJECTIVE-FUNCTION

Good performance of recommender systems w.r.t. our
TOPK and ATOP measures on a held-out test set can only
be expected when they have been trained on an appropriate
objective function. Ideally, one would use the same objective
function (plus some regularization) for training as is later
used as performance measure in testing. While our TOPK
and ATOP measures are computationally tractable for test-
ing, they are unfortunately computationally very expensive
to optimize during training, like other rank-based measures.
For this reason, we resort to an appropriate surrogate mea-
sure for computationally e(cid:14)cient training. As motivated
above, key is to consider the ranking of all items{whether
their ratings are observed or missing in the data. This key
property is captured by our AllRank objective function, of
which we present three versions: they are based on (logis-
tic) regression for computational e(cid:14)ciency. They may also
be viewed as the minimal modi(cid:12)cation necessary to extend
the well-studied approach based on RMSE from observed
to all ratings. Given the large improvement achieved w.r.t.
TOPK and ATOP in our experiments (compared to training
on the observed ratings only), further improvements from
using more sophisticated versions of AllRank might be small
in comparison.

First, note that the ATOPobs-test-measure is the average
rank of the observed relevant items w.r.t. all items. This
suggests to cast the learning task as a binary classi(cid:12)cation
problem: the observed relevant items are assigned to class
1, and the other items (with lower or missing ratings) are
assigned to class 0. In place of the rating matrix R, we use
the corresponding binary matrix Y .

Given that ATOPobs (cid:25) AUCobs for N + (cid:28) N (see above),
we can motivate the likelihood of logistic regression as ap-
propriate objective function for training: in [17], AUC was
related to the so-called hinge loss, which is typically used
for training support vector machines (SVM). The key prop-
erty of the hinge loss is that the loss increases linearly with
the distance from the decision boundary for misclassi(cid:12)ed
data points. The likelihood of logistic regression shows the
same linear behavior in good approximation for reasonably
large distances from the decision boundary. Note that the
only di(cid:11)erence between logistic regression and Fisher’s linear
discriminant analysis (LDA) is that the latter additionally
assumes that the input variables follow a Gaussian distri-
bution. Both approaches can thus be expected to optimize
AUC approximately if the data ful(cid:12)ll their underlying as-
sumptions. Given the large data set size, training an SVM is

716computationally very costly compared to logistic regression.
For this reason, our (cid:12)rst variant, AllRank-Binary-Logistic,
uses the penalized log likelihood of logistic regression,

1
2

Wi;u

Yi;u ^Yi;u (cid:0) log

1 + e

^Yi;u

(cid:0)

jj(cid:18)i;ujj2
2

;

Xall u

Xall i

(cid:26)

(cid:16)

(cid:17)

(cid:27)

(10)
where ^Yi;u 2 [0; 1] denotes the prediction. The sum extends
over all item-user pairs (i; u) (with observed and unobserved
ratings). The weights Wi;u take only two values for simplic-
ity: Wi;u = 1 for observed relevant ratings (class 1), and
Wi;u = wm if the rating of pair (i; u) does not have a rele-
vant value or if the rating is missing (class 0); wm is a tuning
parameter in this objective function, besides the regulariza-
tion factor (cid:21) in the ridge penalty term involving the L2 norm
of the model parameters (cid:18)i;u. These tuning parameters are
determined by cross-validation as to optimize the ATOPobs-
measure on the test set. The objective function in Eq. 10
has two major drawbacks: (1) when cast as a classi(cid:12)cation
problem, all the observed ratings, except for the relevant
ones, are ignored, which may result in a considerable loss of
information; (2) maximizing Eq. 10 is computationally in-
e(cid:14)cient for large data sets, e.g., using the Newton-Raphson
algorithm.

Given the large training set size, computational e(cid:14)ciency
of optimizing Eq. 10 can be increased by using quadratic
approximations as outlined in [16]. A computationally ef-
(cid:12)cient approach can be obtained by replacing Eq. 10 with
least squares for the binary matrix Y , which results in our
second objective function, AllRank-Binary-Regression:

2

(cid:17)

Wi;u

Yi;u (cid:0) ^Yi;u

+ (cid:21)jj(cid:18)i;ujj2
2

:

(11)

Xall u

Xall i

(cid:26)(cid:16)

(cid:27)

Finally, in our third objective function, AllRank-Regression,
we use all observed rating values (i.e., not binarized), as this
allows one to learn from gradual di(cid:11)erences in the rating
values. For all missing rating values, we impute the value
rm. This is the simplest approach to dealing with missing
data; more importantly, though, this allows us to retain the
sparsity of the original rating matrix, which is essential for
e(cid:14)cient computations. We allow for a di(cid:11)erent weight-value
for each rating value Robs
i;u , and an additional weight wm for
the missing ratings:

Wi;u =

(cid:26)

w(Robs
i;u )
wm

i;u observed

if Robs
otherwise

;

(12)

Wi;u (cid:1)

Ro&i

i;u (cid:0) ^Ri;u

+ (cid:21)jj(cid:18)i;ujj2
2

;

(13)

2

Xall u

Xall i
i;u are the observed and imputed ratings, while ^Ri;u
where Ro&i
are the predicted ratings.

(cid:17)

(cid:26)(cid:16)

(cid:27)

4. MODEL TRAINING

We now take a collaborative (cid:12)ltering approach, and use
a (basic) low-rank matrix-factorization model: the matrix
of predicted ratings ^R 2 Ri0(cid:2)u0 , where i0 = N denotes the
number of items, and u0 the number of users, is modeled as

^R = rm + P Q

>

;

(14)

with matrices P 2 Ri0 (cid:2)j0 and Q 2 Ru0(cid:2)j0 , where j0 (cid:28)
i0; u0 is a free parameter of the model and determines the
rank. Note that we choose the o(cid:11)set rm 2 R in Eq. 14
to be equal to the imputed rating rm. All entries in these
matrices are considered independent parameters, with L2-
norm for pair (i; u):

jj(cid:18)i;ujj2

2 =

P 2

i;j + Q2

u;j:

j0

j=1
X

In the remainder of this section, we outline how our third
objective function (see Eq. 13) can be optimized e(cid:14)ciently
for this model.

4.1 Alternating Least Squares

This section outlines the update equations for learning P
and Q for (cid:12)xed tuning parameters Wi;u; rm, and (cid:21). Ad-
ditionally, the tuning parameters have to be optimized via
cross validation as to maximize ATOPobs on the test data.
Determining the values in the two matrices P and Q by
minimizing Eq. 13 is a non-convex minimization problem
[16]. Alternating least squares is an elegant and e(cid:14)cient
method for (cid:12)nding a (close to) minimum solution of Eq. 13
by gradient descent. It can also be used for incremental up-
dates as new rating values arrive over time. At each step
of this procedure, one of the two matrices P and Q is as-
sumed (cid:12)xed, which turns the update of the other one into a
quadratic optimization problem that can be solved exactly.
For (cid:12)xed Q, the matrix P that minimizes Eq. 13 can
be calculated using the usual necessary condition (equate
gradient to zero), and solving for Pi;(cid:1) for each item i. This
results in the following update equation for each row i of P :

Pi;: = (Ro&i

i;: (cid:0) rm) ~W (i)Q

> ~W (i)Q + (cid:21)tr( ~W (i))I

Q

(cid:0)1

;

(cid:16)

(15)
where the dot in the index of a matrix refers to the vector of
all entries; ~W (i) = diag(Wi;:) 2 Ru0(cid:2)u0 is the diagonal ma-
trix containing the ith row of weight matrix W ; I 2 Rj0 (cid:2)j0
is the identity matrix. While the diagonal matrix ~W (i) may
appear to be of computationally prohibitive size, Eq. 15 can
be computed e(cid:14)ciently, as outlined in the next section.

(cid:17)

Analogously, the update equation for each row u of Q is:

Qu;: = (Ro&i
:;u

(cid:0)rm) ~W (u)P

>

P > ~W (u)P + (cid:21)tr( ~W (u)) (cid:1) I

(cid:0)1

;

(cid:16)

(16)
(cid:17)
where ~W (u) = diag(W:;u) 2 Ri0 (cid:2)i0 is the diagonal matrix
containing the uth column of the weight matrix W . The
common random initialization is used to generate the ma-
trices P; Q of expected maximal rank j0 at the beginning.

Apart from that, note that this approach lends itself to
parallelization (each row can be computed independently of
the other rows in either matrix). Moreover, this provides
also{to a good approximation{an e(cid:14)cient incremental up-
date mechanism if a new user or a new item is added, or if a
few ratings concerning a user or an item changes: assuming
that the addition of the new user-ratings does not a(cid:11)ect the
item-matrix P notably, the corresponding row in the user-
matrix Q can be updated in one step according to Eq. 16;
similarly for new user ratings.

7174.2 Practical Computations

Update equation Eq. 15 (and Eq. 16 analogously) can be
easily rewritten in a way that is computationally e(cid:14)cient.
In practice, computations are only slightly more costly com-
pared to the popular objective of optimizing RMSE on the
observed ratings only.
First, because Ro&i
unobserved, we have

i;u (cid:0) rm = 0 if the rating of pair (i; u) is

(Ro&i

i;: (cid:0) rm) ~W (i)Q

(17)

= (Ro&i

i;u2Si (cid:0) rm)

W

>
i;u2Si11(1; j0)

(cid:10) Qu2Si;:

;

(cid:16)(cid:16)

where 11(1; j0) 2 R1(cid:2)j0 is a vector of ones, and (cid:10) denotes the
elementwise product of matrices. Note that this expression
involves only the submatrices for the set of users u who have
rated item i, denoted by Si. This is typically a very small
subset of all users.

(cid:17)

(cid:17)

Second, because Wi;u (cid:0) wm = 0 if the rating of pair (i; u)

is missing, one can decompose:

Q

> ~W (i)Q
= wmQ

>

Q (cid:0) wmQ

>
u2Si;:Qu2Si;:

(18)

+

Q

>
u2Si;: (cid:10) (11(j0; 1)Wi;u2Si )

Qu2Si;:

(cid:16)

(cid:17)

Note that the (cid:12)rst and computationally most expensive term
can be pre-computed at each step for all u, while the other
two terms require only a submatrix of Q concerning the users
u who have rated item i.

Third, the trace simpli(cid:12)es into a sum over the users u who

have rated item i (rather than summing over all users):

tr( ~W (i)) = wmu0 +

(wi;u (cid:0) wm):

(19)

u2Si
X

5. RELATED WORK

Nearly all work on recommender systems is concerned
with explicit feedback data (e.g., ratings assigned by users).
Impressive progress has been made in recent years{and it is
impossible to name all. While almost all work has focused
on predicting the rating values accurately (e.g., [1, 4, 7, 15,
13, 8]), only very few papers (e.g., [18]) were concerned with
optimizing the ranking of items. Despite their di(cid:11)erences,
they have in common that they were tested on the observed
ratings only, and hence trained accordingly: optimizing the
ranking or the distribution over rating values conditioned on
the fact whether the items had been rated by the user or not.
Such a conditional ranking or distribution by itself is only
of limited value for real-world recommender systems, as the
marginal distribution or ranking (i.e., without conditioning)
is needed. To this end, the conditional distribution may be
combined with the additional prediction on which items will
be rated by the user, which seems a similarly challenging
problem [9].

Even when testing on observed ratings only, prediction
accuracy could be improved by training not only on the ob-
served rating values but also by using the indicator matrix,
which captures whether an item was rated by a user or not
[13, 8, 15]. The conditional Restricted Boltzmann Machine
learns the distribution conditioned on this indicator matrix,
which was found to be bene(cid:12)cial for improving RMSE on

observed ratings [15]. Its applicability to ranking all items
is not immediately clear. While the matrix factorization
models NSVD1, NSVD2 and SVD++ [13, 8] used the indi-
cator matrix, the training objective function summed over
the observed ratings only. The original motivation for us-
ing the indicator matrix was to reduce the e(cid:11)ective number
of free parameters in the matrix factorization model [13].
Moreover, the indicator matrix allows one to incorporate
into the model the items that were rated in the test set (and
hence have to be predicted), which was known in the Net(cid:13)ix
competition beforehand, but does not seem applicable to a
real-world recommendation scenario. Whether these models
lead to further improvements when trained with our AllRank
objective function, is subject to future work. Note that our
paper illustrates the importance of accounting for missing
ratings in the training objective function (rather than in our
simple matrix-factorization model ).

It makes sense to ignore missing ratings if one assumes
that the ratings are missing at random. Recently, however,
the validity of the missing at random assumption was ques-
tioned [11, 12]. This work outlines empirical evidence that
the ’typical’ data (i.e., where users are free to choose what
to rate) provide a distorted distribution over the rating-
values compared to the (usually unavailable) complete data:
low ratings tend to be missing with increased probability.
Their work developed multinomial mixture models to ac-
count for the missing data mechanism underlying the train-
ing data. While valuable, computational e(cid:14)ciency of this
approach may need to be improved as to scale up to the
Net(cid:13)ix data and other real-world data sets, which are typ-
ically much larger. Their models achieved signi(cid:12)cant im-
provements w.r.t. their testing procedure, which relied on
having an approximation to the distribution of the complete
data available (by means of the survey data). Such survey
data, unfortunately, is typically not available (at low cost).
Their work was based on the Yahoo!LaunchCast data set,
which is rather small and not available to the general pub-
lic. It is not obvious how their testing procedure could be
modi(cid:12)ed for use on MNAR test data. Being able to test on
(inexpensive) MNAR data is one of the main motivations of
our work.

In the context of implicit feedback data (e.g., log (cid:12)les of
TV consumption for each user), missing feedback was taken
into account when training a matrix factorization model in
[6]. This work points out that accounting for missing feed-
back is important only for implicit feedback data, while this
is unnecessary for explicit feedback data. The reason is that
implicit feedback provides an imbalanced picture, contain-
ing exclusively positive feedback (e.g., TV show watched by
user), while lacking negative feedback (e.g., there are many
reasons for not watching a TV show). In contrast, explicit
feedback provides a balanced picture of a user’s preferences,
as it contains positive and negative feedback, according to
[6]. A main point of our paper is that, even though explicit
feedback data contain negative feedback, it is not su(cid:14)cient;
missing feedback tends to be mainly negative, and hence car-
ries additional negative information that can be used. Apart
from that, while [6] obtained best results on implicit feed-
back data by (cid:12)tting the model to binarized target values
(called preference in their paper), we found that it is best to
use the full range of rating values when training on explicit
feedback data in our experiments.

718Table 2: Test results on MovieLens data for di(cid:11)erent training objective functions.

objective function for training

wm

rm (cid:21)

bestseller list 1: mean of observed ratings
bestseller list 2: count of observed ratings (any value)
bestseller list 3: count of observed 5-star ratings
observed ratings only, optimize (cid:21) for RMSE (Eq. 13)
observed ratings only, optimize (cid:21) for ATOP (Eq. 13)
AllRank-Regression (Eq. 13)
’dense’ SVD (Eq. 13)
AllRank-Binary-Logistic (Eq. 10)
AllRank-Binary-Regression (Eq. 11)

-
-
-
0
0
0.05
1
0.02
0.01

-
-
-
-
-
2
2
0
0

-
-
-
0.06
0.05
0.05
0.03
0.02
0.03

RMSE ATOP ATOP
(test)
(test)
0.803
0.990
0.873
-
0.880
-
0.901
0.861
0.864
0.914
0.933
1.218
0.915
1.766
0.928
-
-
0.924

(XV)
0.804
0.874
0.883
0.863
0.866
0.933
0.915
0.923
0.919

6. EXPERIMENTAL RESULTS

This section summarizes our results on the MovieLens
and Net(cid:13)ix data sets. Unfortunately, we do not have the
Yahoo!LaunchCast data available for experiments. For all
experiments, we chose rank j0 = 50 of our low-rank matrix
factorization model (Eq. 14). We consider 5-star ratings as
relevant 3 to a user (see De(cid:12)nition above), and use the top-k
hit rate, TOPK, and the area under the top-k hit rate curve,
ATOP, as outlined above, as our performance measures on
the test data. The standard deviation (std) of all reported
numbers in the tables is less than 0.005.

Our results con(cid:12)rm the assumption that these data are
MNAR. Training a matrix factorization model such that
it optimizes our new objective functions, we obtain signi(cid:12)-
cantly better results w.r.t. the top-k hit rate than have been
reported in the literature for even sophisticated models that
were trained to optimize the popular RMSE on the observed
data only.

6.1 MovieLens Data

The MovieLens data [2] involves 3,900 movies and 6,040
users, and ratings are available for about 1 million movie-
user-pairs. About 4% of all possible ratings are observed in
this data set. The ratings take integer values from 1 (worst)
to 5 (best). As we train a collaborative (cid:12)ltering model in
this paper, we do not use the provided demographic and
content information. We split this data set into a train-
ing set and a disjoint test set. As the time-stamps of the
ratings are available, we assigned the last 5 ratings of each
user to the test set, and the earlier ratings to the training
set. We further split the test set randomly into two dis-
joint sets of equal size. The (cid:12)rst test set (’XV’) is used for
cross-validation during training as to determine the tuning
parameters in our objective function. The second test set is
used as a truly held-out data set (’test’) for (cid:12)nal evaluation
of the trained model w.r.t. the ATOP measure.

Table 2 shows the results concerning our ATOP measure
on both test sets{XV and test. The small di(cid:11)erences con-
cerning these two test sets are due to the random split of
the data and provide an estimate for the con(cid:12)dence inter-
vals (std(cid:20) 0:005). They also show that there is no signi(cid:12)cant
over(cid:12)tting toward the XV set (used for tuning). Some ap-
proaches provide a ranking of the movies, so that RMSE
does not have any meaning in this case.

Let us now discuss the results obtained for each approach.

3Considering both 4 and 5 star ratings as relevant, exper-
iments showed similar di(cid:11)erences among the various ap-
proaches.

First, we set the weights of all observed ratings to 1, while
we set wm = 0. This ignores missing ratings. Eq. 13 simpli-
(cid:12)es to the extremely popular RMSE measure used for train-
ing recommender systems. (cid:21) remains the only free tuning
parameter for training. We optimized it by cross validation
on the (cid:12)rst test set (XV) in two di(cid:11)erent ways: (1) to mini-
mize RMSE, and (2) to maximize ATOP. In both cases, we
obtained relatively similar results on the test set: relatively
good results w.r.t. RMSE, but relatively poor ones w.r.t.
ATOP, see Table 2.

We obtained a signi(cid:12)cant improvement regarding ATOP,
however, when we took into account the missing ratings by
allowing for wm > 0. This shows that ignoring the missing
ratings in the training data is detrimental w.r.t. ATOP.

Besides wm, we also optimized the weights w(r) for each of
the observed rating values (r = 1; ::; 5). We found that this
does not signi(cid:12)cantly improve the test results w.r.t. ATOP,
and thus used w(r) = 1 for all observed rating values.

Finally, for wm = 1, each rating is weighted equally{
whether observed or missing in the training data. This is
identical to regular / dense singular value decomposition
(SVD) (plus regularization). This approach had already
been used successfully in the past, e.g.,
in latent seman-
tic analysis / indexing [3]. Interestingly, this ’dense’ SVD
achieves better ATOP test results in Table 2 than the ’sparse’
matrix approach (wm = 0), which recently had became pop-
ular for the Net(cid:13)ix prize competition.

Moreover, we also cast the training problem as a binary
classi(cid:12)cation task: the observed 5-star ratings vs. all other
(observed and missing) ratings. As outlined in Section 3,
we optimized the two training objective functions in Eqs.
10 and 11. The ATOP test results were slightly below our
best results (see also Table 2). This may not be surpris-
ing, as these binary classi(cid:12)cation approaches completely ig-
nored the information contained in the observed rating val-
ues 1,...,4. Interestingly, however, these ATOP test results
were considerably better than the ones obtained by ignoring
the missing ratings (wm = 0), see Table 2.

A few comments on the optimal values of the tuning pa-
rameters for the MovieLens data are in order: we found the
optimal value for the imputed rating value to be rm (cid:25) 2.
This makes intuitive sense when the imputed value is in-
terpreted as the average rating in the (unknown) complete
data: it is considerably lower than the mean ((cid:25) 3:6) of the
observed ratings (see also Assumption 2). The optimal value
wm (cid:25) 0:05 of the weight assigned to each missing rating may
appear very small. This value has to be seen in perspective
to the 4% of observed ratings in this data set. With a weight

719−5 −4 −3 −2 −1

1

2

3

4

5

0

0.2

0.4

0.6

0.8

1

0
r
m

w

m

0.96

P
O
T
A

0.94

0.92

0.9

0.96

P
O
T
A

0.94

0.92

0.9

0.96

P
O
T
A

0.94

0.92

0.9

Figure 1: ATOP measure on the Net(cid:13)ix data for
di(cid:11)erent imputed ratings rm; on held-out test set
(solid) and on XV test set used for parameter tun-
ing (dashed); the two curves are almost identical.

of 1 assigned to each observed rating, wm (cid:25) 0:05 essentially
means that all the missing ratings together have about the
same weight as all the observed ratings together. This makes
sense, as it is common practice in machine learning to con-
vert very imbalanced training sets into somewhat balanced
ones as to improve the trained model’s test performance.

As a baseline model for (non-personalized) recommenda-
tions, we considered three variants of the bestseller list: the
rank of each item is determined according to (1) the mean
of the observed ratings (as in [8]), (2) the number of ratings
(of any value), and (3) the number of 5-star ratings in the
training data. Note that the ’test’ and ’XV’ sets are both
held-out test sets for the bestseller lists. Table 2 illustrates
two interesting results. First, among the three bestseller
lists, the one that ignores the missing ratings (mean of ob-
served ratings) is by far worse than the two bestseller lists
that account for the missing values (by counting). Second,
the latter two bestseller lists are competitive with the matrix
factorization model that ignores missing ratings (wm = 0)
concerning ATOP.

We also considered optimizing the nDCG measure on the
observed ratings in the training data, as is readily done
by Co(cid:12)Rank4 [18]. We used the provided con(cid:12)guration
(cid:12)le, changed the hidden dimension to 100 and trained with
nDCG@10 as in [18]. We obtained test results concerning
ATOP and TOPK that were signi(cid:12)cantly worse than the
bestseller lists; however, we did not have time yet to exam-
ine this issue in further detail.

6.2 Net(cid:3)ix Prize Data

The Net(cid:13)ix Prize data [1] contain 17,770 movies and al-
most half a million users. About 100 million ratings are
available. Ratings are observed for about 1% of all possi-
ble movie-user-pairs. The ratings take integer values from 1
(worst) to 5 (best). The provided data are already split into
a training and a probe set. We removed the probe set from
the provided training set as to obtain our training set. A
random split of the probe set provided us with two test sets

4We used release 0.1 from http://www.cofirank.org

0

0.002 0.004 0.006 0.008

0.01

w

m

Figure 2: ATOP measure on the Net(cid:13)ix data for dif-
ferent weights wm of the missing ratings; on held-
out test set (solid) and on XV test set used for pa-
rameter tuning (dashed); the two curves are almost
identical. For wm 2 [0; 1] (top), and zoomed in to
wm 2 [0; 0:01] (bottom). There is a clear maximum
at a small positive weight, while the global mini-
mum is at wm = 0 (i.e., ignoring missing data). For
comparison, ATOP at wm = 1 (horizontal line).

of equal size, one is used for cross-validation during training
(XV) and one for (cid:12)nal testing (test), as before.

Among our three objective functions for training (Eqs. 10,
11 and 13), the (cid:12)rst one (logistic regression) turned out to be
computationally too costly on the much larger Net(cid:13)ix data.
This is expected from our experiments on the MovieLens
data. Training with least squares on the binary classi(cid:12)ca-
tion data (Eq. 11) resulted in ATOP (cid:25) 0:94. Like for the
MovieLens data, this is slightly worse than optimizing Eq.
13 (see Table 3), but considerably better than ignoring the
missing ratings (wm = 0).

When optimizing Eq. 13, we have to determine the opti-
mal values of the tuning parameters wm, rm and (cid:21). Note
that we set the weights for all observed rating values to 1,
w(r) = 1 for r = 1; ::; 5; this was suggested by the Movie-
Lens experiments above. Figure 1 shows the test results
for di(cid:11)erent values of rm imputed for the missing ratings.
For each rm, the other tuning parameters were optimized.

720K
P
O
T

1

0.8

0.6

0.4

0.2

0
0

K
P
O
T

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Several observations are interesting. First, there exists a
maximum. It is located at rm (cid:25) 2, which again is consider-
ably below the average of the observed ratings ((cid:25) 3:6). For
rm < 2, ATOP changes only slightly, i.e., the test results do
not depend very sensitively on the exact imputation value.
Also note that, if the imputed value rm is interpreted as the
average rating of the (unknown) complete data, then values
below 1 are not permissible. For rm > 2 there is a sudden
drop and ATOP reaches a plateau for rm > 3:6, which is the
average of the observed ratings. The reason is that the best
ATOP on the test data is achieved for wm = 0 in our train-
ing objective function Eq. 13; in other words, the imputed
values are ignored in this case, and the standard RMSE on
the observed ratings is optimized for rm > 3:6.

Figure 2 illustrates how ATOP changes as a function of
the weight wm assigned to the missing ratings in Eq. 13.
Again, for each wm the other tuning parameters were op-
timized. The optimum is reached at wm (cid:25) 0:005. While
small, it is again close to the percentage of observed ratings
(1%), resulting in a relatively balanced training set: all the
observed ratings together have twice the weight compared to
all the unobserved ratings together. To the left and right of
the optimum, ATOP drops steadily and signi(cid:12)cantly. Inter-
estingly, ATOP is again larger at wm = 1 than at wm = 0.
For (close to optimal) tuning parameter values wm = 0:01,
rm = 2 and (cid:21) = 0:04, Figure 3 shows the TOPK(k) curve
as a function of k. We estimated TOPK(k) in exactly the
same way as in [8], i.e., we used sample size ~N = 1001,
see also Section 2.3 above (note that we used ~N = 101 for
computing ATOP in all our other experiments). We can
thus compare our best model to two models: (1) the model
that optimizes Eq. 13 with wm = 0 (i.e., the usual RMSE),
but with (cid:21) such that ATOP is maximized on the cross val-
idation set; this resulted in a signi(cid:12)cantly lower curve in
Figure 3; (2) the results reported for the integrated model
in [8], which we summarized in Table 3, alongside our re-
sults. Table 3 shows that it is essential to account for the
missing ratings in the training data as to achieve high top-k
hit rates on the test data. By optimizing our new AllRank
objective function{even though we used a simple model{,
we signi(cid:12)cantly improved ATOP and TOPK compared to
the sophisticated integrated model of [8], which was trained
to optimize RMSE on the way to winning the $1 million
Net(cid:13)ix prize. Our model with optimal weight wm achieved
ATOP = 0:96, which is signi(cid:12)cantly closer to the maximal
possible value of 1. Moreover, the TOPK(k) measure shows
that 39% more relevant items are ranked at the very top
(i.e., none of the 1000 other, randomly chosen items ranks
higher), 48% more relevant items are ranked in the top 0:2%,
and 50% more in the top 2% by our new approach, com-
pared to the integrated model in [8]; see also Figure 3 in [8]
for comparison.

Note that ATOP and AUC refer to the area under the
entire curve. In real-world top-k recommender systems, one
may be interested in maximizing the initial area under the
curve, e.g., up to a pre-determined threshold k. Optimizing
the initial area directly is important in several areas of ma-
chine learning, and currently under active research. While
these two objectives are theoretically di(cid:11)erent, however, op-
timizing the entire area under the curve often provides a
good optimization of the initial part of the area in practice.
In the context of recommender systems, this was also found
for the area under the top-k hit rate in [8], Figure 3.

0.2

0.4

0.6

0.8

1

k

0.005

0.015

0.02

0.01

k

Figure 3: TOPK(k) curves on the Net(cid:13)ix data for
k 2 [0; 1] (top), and zoomed in to top 2% (bottom)
on held-out test set (XV test set resulted in practi-
cally indistinguishable curves in this plot). For all
values of k, signi(cid:12)cant improvements are obtained
by accounting for the missing ratings during train-
ing (solid line), rather than ignoring them (dashed
line).

As a baseline in Table 3, we also calculated the recommen-
dation accuracy of three variants of the bestseller list, like for
the MovieLens data above. Concerning the TOPK(k) mea-
sure, the bestseller lists perform considerably worse than
the methods that ignore the missing data mechanism in Ta-
ble 3 if k is small, which is the relevant case in practice.
For larger k, the situation is the other way around, which
explains the surprisingly good ATOP values of count-based
bestseller lists 2 and 3 in Table 3. Among the bestseller lists,
the variant that ignores the missing values (by calculating
the mean of the observed values), is again by far the worst,
like for the MovieLens data above.

7. CONCLUSIONS AND FUTURE WORK

In this paper, we considered the top-k hit rate on all items
as the natural performance measure for recommender sys-
tems, as only a few out of all items can typically be recom-
mended to a user at a time. We found that optimizing the
popular root mean square error or other (ranking) measures

721Table 3: Net(cid:13)ix Data: Test results for RMSE, ATOP and TOPK measures.

RMSE ATOP TOPK(0%) TOPK(0:2%) TOPK(2%)

training approach
bestseller list 1: mean of observed ratings
bestseller list 2: count of observed ratings (any value)
bestseller list 3: count of observed 5-star ratings
observed ratings only: Eq. 13, wm = 0
integrated model: from [8]
AllRank-Regression: Eq. 13, wm = 0:01

0.999
-
-
0.921
0.887
1.106

0.81
0.90
0.92
0.90
0.91
0.96

1:8%
2:6%
3:4%
4:7%
6:7%
9:3%

4:4% 19:8%
7:9% 37:6%
10:0% 41:9%
11:7% 39:1%
15:7% 43%
23:3% 64:8%

on the observed data can result in dramatically degraded
performance w.r.t.
the top-k hit rate, in particular when
the available data are missing not at random (MNAR), as
is typically the case in practice.

As the exact missing-data mechanism is unknown in gen-
eral, we have developed mild assumptions concerning user
behavior, which were inspired by general properties across
various publically available data sets for recommender sys-
tems. We outlined test performance measures that can be
evaluated in the presence of missing ratings, and which are
unbiased on MNAR data under these assumptions. As these
measures are computationally ine(cid:14)cient for training, we have
developed appropriate surrogate objective functions. Our
new family of objective functions, AllRank, shares the cru-
cial property of accounting for all ratings{whether observed
or missing in MNAR training data.

When optimizing our new objective functions (in a collab-
orative (cid:12)ltering setting with a simple matrix factorization
model), we obtained considerably better results w.r.t. the
top-k hit rate than has been achieved with even sophisti-
cated models that were trained to optimize RMSE on the
observed data. Depending on the value of k, our approach
resulted in a 39 (cid:0) 50% higher top-k hit rate compared to
state-of-the-art recommender systems in our experiments on
the Net(cid:13)ix data.

We consider this work as only the (cid:12)rst step toward improv-
ing recommendation accuracy concerning all items when the
available training and test data are MNAR. The next steps
include the development of more sophisticated surrogate ob-
jective functions for training, as well as more powerful mod-
els.

One cannot take for granted that the numerous results ob-
tained for RMSE are guaranteed to carry over to the top-k
hit rate. Re-evaluation of these results with respect to ac-
curacy measures concerning all items may be in order. One
such observation might be that content-based recommender
systems were clearly inferior to collaborative (cid:12)ltering meth-
ods w.r.t. RMSE on the observed data.

Acknowledgements
I am greatly indebted to Tin Ho for her encouragement and
support of this work, and for fruitful discussions with her. I
am very grateful for useful discussions with Aiyou Chen and
Jin Cao, and for valuable comments from the anonymous
reviewers.

8. REFERENCES
[1] J. Bennet and S. Lanning. The Net(cid:13)ix Prize. In
Workshop at SIGKDD-07, ACM Conference on
Knowledge Discovery and Data Mining, 2007.

[2] MovieLens data. homepage:

http://www.grouplens.org/node/73, 2006.

[3] S. Deerwester, S. Dumais, G. Furnas, R. Harshman,

T. Landauer, K. Lochbaum, Lynn Streeter, et al.
Latent semantic analysis / indexing. homepage:
http://lsa.colorado.edu/.

[4] S. Funk. Net(cid:13)ix update: Try this at home, 2006.

http://sifter.org/ simon/journal/20061211.html.

[5] D. J. Hand and R. J. Till. A simple generalization of

the area under the ROC curve for multiple class
classi(cid:12)cation problems. Machine Learning, 45:171{86,
2001.

[6] Y. Hu, Y. Koren, and C. Volinsky. Collaborative

(cid:12)ltering for implicit feedback datasets. In International
Conference on Data Mining (ICDM), 2008.

[7] R. Keshavan, A. Montanari, and S. Oh. Matrix

completion from noisy entries. arXiv:0906.2027, 2009.

[8] Y. Koren. Factorization meets the neighborhood: a

multifaceted collaborative (cid:12)ltering model. In Conf. on
Knowledge Discovery and Data Mining (KDD), 2008.

[9] M. Kurucz, A. Benczur, T. Kiss, I. Nagy, A. Szabo,

and B. Torma. KDD Cup 2007 task 1 winner report.
ACM SIGKDD Explorations Newsletter, 9:53{6, 2007.

[10] R. Little and D. B. Rubin. Statistical Analysis with

missing data. Wiley, 1986.

[11] B. Marlin and R. Zemel. Collaborative prediction and

ranking with non-random missing data. In ACM
Conference on Recommender Systems (RecSys), 2009.

[12] B. Marlin, R. Zemel, S. Roweis, and M. Slaney.

Collaborative (cid:12)ltering and the missing at random
assumption. In Conf. on Uncertainty in Arti(cid:12)cial
Intelligence (UAI), 2007.

[13] A. Paterek. Improving regularized singular value de-

composition for collaborative (cid:12)ltering. KDDCup 2007.
[14] D. B. Rubin. Inference and missing data. Biometrika,

63:581{92, 1976.

[15] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted
Boltzmann machines for collaborative (cid:12)ltering. In Int.
Conf. on Machine Learning (ICML), 2007.

[16] N. Srebro and T. Jaakkola. Weighted low-rank
approximations. In International Conference on
Machine Learning (ICML), pages 720{7, 2003.

[17] H. Steck. Hinge rank loss and the area under the ROC

curve. In Proceedings of the European Conference on
Machine Learning (ECML), 2007.

[18] M. Weimer, A. Karatzoglou, Q. Le, and A. Smola.

Co(cid:12) rank - maximum margin matrix factorization for
collaborative ranking. In Advances in Neural
Information Processing Systems (NIPS), 2008.
[19] S. Wu and P. Flach. A scored AUC metric for
classi(cid:12)er evaluation and selection. In ROCML
workshop at ICML, 2005.

722