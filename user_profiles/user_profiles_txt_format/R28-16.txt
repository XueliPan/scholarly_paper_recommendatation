Latent Concepts and the Number Orthogonal Factors in

Latent Semantic Analysis

Georges Dupret

georges.dupret@laposte.net

ABSTRACT
We seek insight into Latent Semantic Indexing by establish-
ing a method to identify the optimal number of factors in
the reduced matrix for representing a keyword. This method
is demonstrated empirically by duplicating all documents
containing a term t, and inserting new documents in the
database that replace t with t(cid:1). By examining the number
of times term t is identiﬁed for a search on term t(cid:1) (preci-
sion) using diﬀering ranges of dimensions, we ﬁnd that lower
ranked dimensions identify related terms and higher-ranked
dimensions discriminate between the synonyms.
Categories and Subject Descriptors: H.3.3 Information
Search and Retrieval: Retrieval models
General Terms: Algorithms
Keywords: Correlation method, latent semantic analysis,
information retrieval, text mining, singular value decompo-
sition.

Introduction
The task of retrieving the documents relevant to a user query
in a large text database is complicated by the fact that dif-
ferent authors use diﬀerent words to express the same ideas
or concepts. Methods related to Latent Semantic analysis
interpret the variability associated with the expression of
a concept as a noise, and use linear algebra techniques to
isolate the perennial concept from the variable noise.

Following the words of his authors [6], in Latent Seman-
tic Analysis, “the approach is to take advantage of implicit
higher-order structure in the association of terms with doc-
uments (“semantic structure”) in order to improve the de-
tection of relevant documents on the basis of terms found
in queries. The particular technique used is singular value
decomposition, in which a large term by document matrix
is decomposed into a set of ca. 100 orthogonal factors from
which the original matrix can be approximated by linear
combination. Documents are represented by ca. 100 item
vectors of factor weights. Queries are represented as pseudo-
documents vectors formed from weighted combinations of

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGIR’03, July 28–August 1, 2003, Toronto, Canada.
Copyright 2003 ACM 1-58113-646-3/03/0007 ...$5.00.

terms, and documents with supra-threshold cosine values
are returned.”

A large number of factor weights provide an approxima-
tion close to the original term by document matrix but re-
tains too much noise. On the other hand, if too many factors
are discarded, the information loss is too large. The objec-
tive in this work is the identiﬁcation of the optimal number
of orthogonal factors.

In Section 1 we deﬁne the bag-of-word representation of
documents and present the simplest associated document
retrieval method. In Section 2, the original Latent Semantic
method is presented. A method inspired on Latent Seman-
tic Analysis and developed at IBM Tokyo Research Labo-
ratory [15] is presented in Section 3 and extended in Sec-
tion 4 to enable the determination of the adequate number
of orthogonal factors. Numerical experiments in Section 5
validate the method.

1. BAG-OF-WORDS REPRESENTATION

The methods discussed in this paper rely on the Bag-of-
Words representation in which each document is replaced by
a vector of its attributes [2]. These attributes are usually the
keywords present in the document, but other information
like a time span or the keyword stem can be used.

In Boolean models [1, 20, 24, 25], a coordinate of a vector
is naught when the corresponding attribute is absent from
the document or unity when it is present. Term weight-
ing [22] is a reﬁnement that takes into account the frequency
of appearance of words within and among documents, and
their location of appearance (e.g., in the title, abstract or
section header.)

Popular weighting schemes are variation of the TFIDF
model, where the coordinates of a vector representation of
a document given by:

di = TF(Wi, D).IDF(Wi)

(1)
where TF(Wi, D) is the ith term frequency, which is the
number of times term Wi occurred in document D, and
IDF(Wi) is the inverse document frequency, which is the
inverse of the number of documents word Wi occurred in.

This representation can be used to retrieve documents
relevant to a user query: A vector representation is derived
from the query in the same way as regular documents and
then compared with the bag-of-words representation of the
database using a suitable measure of distance or similar-
ity. The Euclidean distance or the angle between vectors is
commonly used, but other measures are possible [5, 11, 16,
23].

Formally, we call A the matrix of D documents by N at-
tributes formed by the bag-of-word representations of the
documents in the database. Q is the vector representation
of the query, and d(Vi, Vj) is the dissimilarity measure be-
tween vectors Vi and Vj. By computing d(Vi, Q) for all
i, we can order the documents with respect to their (dis-)
similarity to the query.

The bag-of-word representation usually leads to a matrix
including several thousands of attributes. This causes se-
rious problems in terms of computational complexity and
conceptual terms if the goal is to design an eﬃcient algo-
rithm for further processing. There are several ﬁltering ap-
proaches that lead to a reduction in size of the vector repre-
sentation. Pruning consists in removing infrequent and very
frequent words, as they usually carry little information [10].
Lemmatization replaces inﬂected words (e.g. noun-plurals,
verb-tenses) by their root in order to regroup forms with the
same semantic content.

2. LATENT SEMANTIC ANALYSIS

A problem with the former retrieval method has been
mentioned in the introduction: The similarity between a
query and a given document can be under-estimated be-
cause the database user and the document author use dif-
ferent vocabularies to express the same concepts. Their in-
ventors claim that Latent Semantic Indexing (LSI) [6] is
one of the few methods which successfully overcome this
problem because it takes into account synonymy and poly-
semy. Synonymy refers to the existence in most languages
of equivalent or similar terms to express the same idea, and
polysemy refers to the fact that some words have multiple,
unrelated meanings. Not accounting for synonymy leads to
over-estimate the dissimilarity between related documents,
while not accounting for polysemy leads to erroneously ﬁnd-
ing similarities between documents.

The idea behind LSI is to reduce the dimension of the in-
formation retrieval problem by projecting the D documents
by N attributes matrix A into an adequate subspace of lower
dimension. This is achieved based on the singular value de-
composition of A:

A = UΣV

T

(2)

where U and V are orthogonal matrices, and Σ is a diagonal
matrix. The dimension of A being D × N , the non-null
elements of Σ are denoted σ1, . . . , σp where p = min(D, N )
and we have σ1 ≥ σ2 ≥ . . . ≥ σp−1 ≥ σp.

The closest matrix A(k) of dimension k < rank(A) in
terms of the Frobenius norm is obtained by setting σi = 0 for
i > k. This is equivalent to reducing U and V to their k ﬁrst
columns and Σ to its k ﬁrst columns and rows. We denote
these reduced matrices by U(k), V(k) and Σ(k) respectively.
In order to reduce the dimensionality of the problem, only
the k largest singular values are maintained: Instead of com-
paring the N dimensional vectors representing the docu-
ments in the original space deﬁned by A, we compare them
in a k dimensional subspace based on A(k). The projection
of the original document representation gives

T

A

U(k)Σ(k)

−1

= V(k)

that is to be compared with the result of the same opera-
tion on the query vector Q projected in the k-dimensional
subspace

(3)

(4)

The closest document to query Q is identiﬁed in refer-
ence to a dissimilarity function dk(., .) in (cid:2)k between the
k-dimensional vectors:

min

0<α≤m

dk(Q(k), V

T
α (k))

(5)

where VT
closest documents are retrieved in the same way.

α is the αth row of VT . The second, third and later

When the numbers of documents and attributes are large,
LSI involves the manipulation of very large but sparse ma-
trices. It is possible to improve performance and memory
requirements signiﬁcantly by using algorithms and data rep-
resentation models specially designed to handle such matri-
ces [14].

Presenting formal similarity with Latent Semantic Anal-
ysis, Probabilistic Latent Semantic Analysis [8] is another
method that takes synonymy and polysemy into account.
Its starting point is the so called aspect model [3], [9] where
two categories of variables are considered: the latent and
manifest variables. Latent variables cannot be observed di-
rectly but the manifest variables are conditioned on them,
and by modeling this dependency it is possible to retrieve
the underlying concepts. In reference to the problem of a
same concept being expressed with diﬀerent vocabularies,
an analogy is made between the latent variable and the con-
cept on one hand, and the vocabulary used to express it and
the manifest variables on the other hand.

3. COVARIANCE METHOD

Instead of decomposing the document by keyword ma-
trix, the Covariance Method [15] applies the singular value
decomposition to the keywords covariance matrix. As the
covariance matrix dimension depends on the number of key-
words and not on the number of documents, the Covari-
ance Method is able to handle databases of several hundred
of thousands of documents. Moreover, the covariance ma-
trix doesn’t need to be updated each time a batch of new
documents enter the data base. This aspect is particularly
important in the context of electronic networks, where new
data become continuously available (See [19] or [18] for other
methods to handle this problem). See [12] and [13] for a fur-
ther discussion on the advantages of the Covariance Method.
When the number of documents is larger than the number
of attributes, the LSI method involves the manipulation of
very large but sparse matrices.

If D is the number of documents, Ad the vector repre-
senting the dth document and A the mean of these vectors
(i.e. A = 1
D

d=1 Ad), the covariance matrix is written:

PD

C =

(Ad − A)

(Ad − A)

T

DX

d=1

This matrix being symmetric, the singular value decom-

position can be written:

C = VΣV

T

where V is orthogonal and Σ is the diagonal matrix of the
ordered singular values.

Reducing Σ to the k more signiﬁcant (i.e.

larger) sin-
gular values, we can project the keyword space into a k
dimensional subspace:

(6)

(7)

T
Q(k) = Q

U(k)Σ(k)

−1.

(A|Q) → (A|Q)V(k)Σ = (A(k)|Q(k))

(8)

and compare the documents in this last subspace as we did
in Section 2. The motivation behind the multiplication of
the document by keyword matrix by the covariance matrix
or its approximation lies in the fact that if a keyword is
present in a document, correlated keywords should be taken
into account as well - even if they aren’t explicitly present -
so that the concepts present in the document aren’t obscured
by the choice of a speciﬁc vocabulary.

4. EMBEDDED CONCEPTS

Sending the covariance matrix onto a subspace of fewer
dimension implies a loss of information. We will see that it
can be interpreted as the merging of keywords meaning into
a more general concept that encompasses them. As an ex-
ample, “cat” and “mouse” might be merged into the concept
of “mammal,” which itself can be merged into the concept
of “animal.” The results from the Latent Semantic Indexing
and the Covariance Method are of course not expected to be
as clean, but this example illustrates what we understand
by “underlying concept.”

We present here a criteria to determine the number of
singular values necessary for a keyword to be correctly dis-
tinguished from all others in the dictionary, along with a def-
inition of what we understand by “correctly distinguished.”

4.1 Correlation Method

The method we use is based on the correlation matrix, but
is otherwise identical to the Covariance Method presented in
the former Section. The correlation matrix S of A is deﬁned
based on the covariance matrix C:
Ci,jp

Si,j =

(9)

Ci,i Cj,j

This matrix being symmetric, the singular value decomposi-
tion can be written as in Eq. (7). The result of this decom-
position is then used to send the documents into a k dimen-
sional subspace as in Eq. (8). Using the correlation rather
than the covariance matrix results in a diﬀerent weighting of
correlated keywords, the justiﬁcation of the model remain-
ing otherwise identical.

4.2 Keyword Validity

Consider the following property of the singular value de-

composition [4]:

S =

ViσiV

T
i

NX

i=1

(10)

Using the same notation as before, it is easy to see that the
rank k approximation S(k) of S can be written

S(k) =

ViσiV

T
i

(11)

kX

i=1

with k ≤ N . We also have by deﬁnition that S(N ) = S.

S(k) being an approximation of the correlation matrix, we
can argue in favor of the following argument: The k-order
approximation of the correlation matrix correctly represents
a given keyword only if this keyword is more correlated to
itself than to any other attribute.

In terms of the S(k) elements, this means that even though
the diagonal elements aren’t equal to unity, they should be
greater than the non-diagonal elements of the same row (or

the same column, as the matrix is symmetric). For a given
keyword α this condition is written

S(k)α,α > S(k)α,β ∀ β = 1, . . . N and β (cid:6)= α

(12)

To simplify later discussion we introduce the next deﬁni-
tions: A keyword is said to be “valid” at rank k if it satisﬁes
Eq. (12) and is said to be of rank k if k − 1 is the largest
value for which Eq. (12) is not veriﬁed. In this case, k is the
validity rank of the keyword.

5. NUMERICAL EXPERIMENTS

Numerical experiments are based on the REUTERS and
the TREC databases. The REUTERS data base is a collec-
tion of 21,578 articles extracted from the Reuters newswire
in 1987. For complete information, please refer to [17]. The
TREC database [7] is a collection of 131,896 articles of the
Los Angeles Times from the years 1989 and 1990. For the
purpose of this study, we used the paragraphs as documents.
It resulted in a collection of 1,822,531 “documents.”

As a form of pre-processing, we extract the stem of the
words using the Porter algorithm [21], and we removed the
keywords appearing in either more or less than two user
speciﬁed thresholds. We present here the results based on
diﬀerent vocabularies. We then map documents to vectors
using the TFIDF representation as described by Eq. (1).

To facilitate reading, we refer to keywords by one of their
base forms even though the stemmed forms is used in the
experiment.

First Experiment
We claim that a given keyword is correctly represented by
a rank k approximation of the correlation matrix if k is at
least equal to the validity rank of the keyword. In order to
verify this, we conduct the experiment described here:

1. We select a keyword, for example africa, and we ex-
tract from the text data base all the documents con-
taining it.

2. We produce a new copy of these documents, in which
we replace the selected keyword by an arbitrary new
In this example, we replace africa by afrique
one.
which is a translation in French of the word.

3. We add these new documents to the original data base,

and the keyword afrique to the vocabulary.

4. We compute the correlation matrix and the singular
value decomposition of this extended, new data base.
This deﬁnes a new subspace on which to send the doc-
uments for retrieval. Note that as far as the correla-
tion matrix is concerned, afrique and africa are perfect
synonyms.

5. We send the original data base (i.e. the data base with-
out the afrique attribute) to the new subspace and we
issue a query for afrique. We hope to ﬁnd documents
containing africa ﬁrst.

We conduct this experiment for diﬀerent approximation S(k)
of the correlation matrix, and compare the precision. We
deﬁne the precision as the ratio between the number of cor-
rectly retrieved documents (i.e. documents containing the
keyword africa in our example) and the total number of re-
trieved documents [2].

i

i

n
o
s
c
e
r
p

0
0
1

0
8

0
6

0
4

0
2

0

rank=100
rank=400
rank=425
rank=450
rank=500

ercio in a vocabulary of 1241 keywords (we used the Spanish
translation because “commerce” exists both in French and
English). The drop in precision after we reach the validity
rank - 399 - is low, but still present. As usual, the validity
rank is precisely deﬁned: the drop in precision is observed
as soon as the rank reaches 400. In Fig. 3, we used a 2486
keywords vocabulary in which we duplicated network by re-
seau. The validity rank is 982. Precision is 100% for ranks
above this value, and well below 60% for higher ranks.

0

20

40

60

80

100

nbr of document (validity rank = 428)

Figure 1: Keyword africa is replaced by afrique:
Curves corresponding to ranks 450 and 500 start
with a null precision and remain under the curves
of lower validity ranks.

We choose various keywords, database and vocabulary
combinations and we discuss here only representative re-
sult. We choose both highly and poorly correlated keywords:
Keyword trade is frequent in the REUTERS database, while
africa appears in less than half of a percent of the documents.
Keyword network has been studied in the signiﬁcantly larger
TREC database. Statistics relevant to these keywords can
be found in Table 1.

trade

keyword
duplicate
database REUTERS REUTERS

africa
afrique

comercio

stem
vocabulary
count
% doc
rank

yes
1241
6956
(cid:7) 10%

399

yes
1201
116

(cid:7) 0.5%

428

network
reseau
TREC

yes
2486
5171
(cid:7) 0.3%

982

Table 1: Keywords Characteristics. The “vocab-
ulary” entry refers to the number of keywords in
the vocabulary. “Count” is the number of time the
(stemmed) keyword appears. “% doc” is the per-
centage of documents containing the keyword and
“rank” is its validity rank.

With a 1201 keywords vocabulary extracted from the REU-
TERS database, the rank from which the keywords africa
and afrique are valid is 428. In Fig. 1, we plotted the pro-
portion of documents containing the keyword africa against
the number of retrieved documents (precision) computed for
diﬀerent numbers of orthogonal factors (referred as rank in
the ﬁgure’s legend). All precisions for ranks inferior or equal
to 428 start at 100% precicion and remain higher than what
is achieved for ranks superior to 428. Precision increases
with rank until the validity rank is reached and then drops,
sometimes quite dramatically.

For further illustration, we provide the results of two more
experiments: In Figs. 2, keyword trade is replicated by com-

i

i

n
o
s
c
e
r
p

5

.

9
9

5

.

8
9

5

.

7
9

rank=300
rank=375
rank=399
rank=425
rank=500

0

20

40

60

80

100

nbr of document (validity rank = 399)

Figure 2: Keyword trade is replaced by comercio: Pre-
cision improves from rank 300 to 375. Ranks 375
and 399 have identical precision for this range of
documents. Performance deteriorates for values be-
yond the validity rank.

We might wonder if this property of the validity rank is
veriﬁed for all correlation matrices or is speciﬁc to text. The
experiments we conducted show that the second assumption
is correct:
for both correlation matrices based on random
vectors and text databases containing too few documents
for the correlations to be meaningful, we fail to observe a
correspondence between retrieval quality and validity ranks.
We interpret these results as follows: In the ﬁrst range
below the validity rank, the added orthogonal factors im-
prove information about the keywords, which explains the
precision gain. In the second range, from validity rank to
full rank, the subspace contains enough information to dis-
tinguish between the synonym pairs like africa and afrique
or trade and comercio. Consequently, searching for afrique
doesn’t return the documents containing africa with the
same precision, as searching for comercio returns fewer doc-
ument containing trade.

This experiment shows the relation between the “concept”
associated with afrique (or any other concept) and the actual
keyword. For low rank approximations S(k), augmenting
the number of orthogonal factors helps identifying the “con-
cept” common to both afrique and africa, while orthogonal
factors beyond the validity rank help distinguish between
the keyword and its synonym.

i

i

n
o
s
c
e
r
p

0
0
1

0
8

0
6

0
4

0
2

0

rank=800
rank=900
rank=950
rank=1000
rank=1100

o

i
t

a
r

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0

20

40

60

80

100

nbr of document (validity rank = 982)

200

400

600

800

1000

rank

Figure 3: Keyword network is replaced by reseau: Pre-
cision is 100% until the validity rank and deterio-
rates drastically beyond it.

Figure 4: Ratios R and hit = N/G for keyword afrique.
Vocabulary of 1201 keywords. Validity rank is 428

Second Experiment
Returning to the africa versus afrique experiment, if the for-
mer interpretation is correct, the number of returned docu-
ment containing africa must be approximately equal to the
number of documents containing afrique as long as the num-
ber of orthogonal factors is lower than the validity rank, but
as it exceeds it, more documents containing the exact query
should be retrieved. In Fig. 4 we show the result of an exper-
iment conducted to verify this hypothesis. For an increas-
ing number of orthogonal factors represented in abscissa, we
identify the group of G = 50 most similar documents to the
query afrique. We plot in ordinate the ratio R of documents
containing afrique divided by the total number T of docu-
ments containing either afrique or africa. We also plot the
number T divided by G = 50 to be certain that the number
of retrieved documents is large enough for R to make sense.
We see that as forecast R is approximately equal to 50%
until the rank is valid. Beyond this rank, the documents
containing afrique largely dominate the group.

Third Experiment
Given that a keyword is correctly represented at its validity
rank, an histogram of validity ranks for the whole vocab-
ulary can be used to determine the number of orthogonal
factors beyond which there is no much information to be
gained when computing the subspace (see Eq. (8)). Figs. 5
and 6 present such histograms. In the ﬁrst Figure, around
200 orthogonal factors seem to be suﬃcient for a vocabulary
of 1201 keywords applied to the REUTERS database, while
approximately 600 are required for the larger 2486 keywords
vocabulary used with the TREC database.

Conclusion
We examined the dependence of the Latent Semantic struc-
ture on the number of orthogonal factors in the context of
the Correlation Method. We analyzed explicitly the claim
following which the Latent Semantic Analysis provides a
method to take account of synonymy.

We propose a method to determine the number of or-
thogonal factors for which a given keyword best represents
an associated latent semantic concept and showed that the
concept corresponding to a given keyword is captured by the
ﬁrst orthogonal factors up to a validity rank that we deﬁne.
Remaining orthogonal factors help distinguish between this
keyword and its synonyms.

We also showed that the optimal number of orthogonal
factors depends on the query as well as on the document
collection and we give an upper bound on the number of or-
thogonal factors needed to represent the database correctly.
Further directions might include the extension to multiple
keywords queries.

6. ACKNOWLEDGMENTS

The author would like to thank the reviewers for their
usuful comments. He also want to thank Raﬁ Ahmad for
correcting his English.

7. REFERENCES
[1] R. Armstrong, D. Freitag, T. Joachims, and

T. Mitchell. Webwatcher: A learning apprentice for
the world wide web. Proc. of the AAAI Spring
Symposium on Information Gathering from
Heterogeneous, Distributed Environments, 1995.

[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern

Information Retrievial. ACM Press, 1999.

[3] David Bartholomew and Martin Knott. Latent

Variable Models and Factor Analysis, Second Edition.
Kendall’s Library of Statistics, Volume 7, 1999.

[4] Michael W. Berry, Susan T. Dumais, and Gavin W.

O’Brien. Using linear algebra for intelligent
information retrieval. Technical Report UT-CS-94-270,
1994.

[5] Rich Caruana and Dayne Freitag. Greedy attribute

selection. In International Conference on Machine
Learning, pages 28–36, 1994.

Histogram of Validity Ranks

0
0
8

0
0
6

0
0
4

0
0
2

0

0
0
0
1

0
0
8

0
0
6

0
0
4

0
0
2

0

0

200

400

600

800

1000

1200

0

500

1000

1500

2000

2500

rank

rank

Figure 5: Histograms of Validity Ranks: vocabulary
of 1201 keywords in REUTERS database

Figure 6: Histograms of Validity Ranks: vocabulary
of 2486 keywords in TREC database

[8] Thomas Hofmann. Probabilistic Latent Semantic

Cahiers Leibniz, 2001-22, 2001.

[6] S. Deerwester, S. Dumais, G. Furnas, and

T. Landauer. Indexing by latent semantic analysis.
Journal of the American Society of Information
Science, 41:391–407, 1990.

[7] D. Harman and E. M. Voorhees. Nisc special

publication. In Proceedings of the Fifth Text Retrieval
Conference (TREC-5), pages 500–238, Department of
Commerce, National Institute of Standards and
Technology, Gaithersburg, MD, 1997.

Indexing. In Proceedings of the 22nd Annual ACM
Conference on Research and Development in
Information Retrieval, pages 50–57, Berkeley,
California, August 1999.

[9] Thomas Hofmann and Jan Puzicha. Unsupervised

learning from dyadic data. Technical Report
TR-98-042, Berkeley, CA, 1998.

[10] Thorsten Joachims. A probabilistic analysis of the

rocchio algorithm with TFIDF for text categorization.
In International Conference on Machine Learning,
pages 143–151, 1997.

[11] George H. John, Ron Kohavi, and Karl Pﬂeger.

Irrelevant features and the subset selection problem.
In International Conference on Machine Learning,
pages 121–129, 1994. Journal version in AIJ, available
at http://citeseer.nj.nec.com/13663.html.

[12] M. Kobayashi and M. Aono. Major and outlier cluster
analysis using dynamic re-scaling of document vectors.
Proceedings of the SIAM Text Mining Workshop,
Crystal City, VA, pages 103–113, April 13, 2002.

[13] M. Kobayashi and M. Aono. Vector space models for

search and cluster mining (invited paper). In M. Berry
(ed.), A Comprehensive Survey of Text Mining.
Springer, to be published 2002.

[14] M. Kobayashi and G Dupret. Eﬃcient estimation of
singular values for searching very large and dynamic
web databases. WWW9: Ninth World Wide Web
Conference, Amsterdam, 2000.

[15] M. Kobayashi, L. Malassis, and H. Samukawa.

Retrieval and ranking of documents from a database.
U.S. Patent, ﬁlled June 2000.

[16] T. Kohonen. Self-Organizing Maps, volume 30.

Springer Series in Information Sciences, Springer,
Berlin, Heidelberg, New York. Second Edition, 1995.

[17] D. Lewis. Reuters-21578 text categorization test

collection, distribution 1.0. AT&T Labs-Research,
1997.

[18] D. Memmi. Incremental clustering and category drift.

[19] D. Memmi and J.-G. Meunier. Using competitive
networks for text mining. NC’2000, Berlin, 2000.

[20] M. Pazzani, J. Muramatsu, and D. Billsus. Syskill &

webert: Identifying interesting web sites. In AAI
Spring Symposium on Machine Learning in
Information Access, 1996. Lecture Notes in Computer
Science 11.

[21] M. F. Porter. An algorithm for suﬃx stripping.

Program, Vol. 14, no. 3, pages 130–137, 1980.

[22] G. Salton and C. Buckley. Automatic structuring and

retrieval of large text ﬁles. Communications of the
ACM, 32(2):97–107, 1994.

[23] David B. Skalak. Prototype and feature selection by

sampling and random mutation hill climbing
algorithms. In International Conference on Machine
Learning, pages 293–301, 1994.

[24] A. Tate. Learning and revising user proﬁles: The
identiﬁcation of interesting web sites. In Tate, A.
(Ed.), Advanced Planning Technology, (pp. 206–209).
Menlo Park: AAAI Press. Pazzani, M. & Billsus, D.
(1997). Machine Learning 27:313–331., 1997.

[25] Y. Yang. Expert network: Eﬀective and eﬃcient

learning from human decesions in text categorization
and retrieval. Proc. of the 7th Annual International
ACN-SIGIR Conference on Research and
Development in Information Retrieval, Dublin, 2001.

