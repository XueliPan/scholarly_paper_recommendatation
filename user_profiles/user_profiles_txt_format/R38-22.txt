Optimal Outlier Removal in High-Dimensional Spaces

John Dunagan(cid:3)

Santosh Vempala(cid:3)

ABSTRACT

by slowing down or even misleading an algorithm; machine

We study the problem of (cid:12)nding an outlier-free subset of a

learning is an area where outliers in the training data could

set of points (or a probability distribution) in n-dimensional

cause an algorithm to (cid:12)nd a wayward hypothesis. Even from

Euclidean space. A point x is de(cid:12)ned to be a (cid:12) -outlier if

a purely theoretical standpoint, removing outliers could lead

there exists some direction w in which its squared distance

to simpler mathematical models, or the outliers themselves

from the mean along w is greater than (cid:12) times the average

might constitute the phenomenon of interest.

squared distance from the mean along w[1]. Our main the-

How does one (cid:12)nd outliers? To address this question we

orem is that for any (cid:15) > 0, there exists a (1

(cid:15)) fraction of

have to (cid:12)rst answer another:

what precisely is an outlier?

the original distribution that has no O(

(b+log

))-outliers,

(cid:15)

(cid:15)

7

(or a distribution) in n-dimensional Euclidean space. In the

n

n

(cid:0)

In this paper we will assume that the data consists of points

improving on the previous bound of O(n

b=(cid:15)). This bound

is shown to be nearly the best possible. The theorem is

constructive, and results in a

approximation to the fol-

1

1

(cid:15)

(cid:0)

one-dimensional case, one could use one of the de(cid:12)nitions

alluded to above, viz. a point is an outlier if its distance from

the mean is greater than some factor times the standard

lowing optimization problem: given a distribution (cid:22) (i.e. the

deviation. The following generalization to higher dimensions

ability to sample from it), and a parameter (cid:15) > 0, (cid:12)nd the

n

was proposed in [1]. Let P be a set of points in

. A point

minimum (cid:12) for which there exists a subset of probability at

R

x in P is called a (cid:12) -outlier if there exists a vector w such

least (1

(cid:15)) with no (cid:12) -outliers.

that the squared length of x along w is more than (cid:12) times

the average squared length of P along w, i.e. if

(cid:0)

Keywords

Outlier, ellipsoid, sampling, Euclidean space

T

2

T

2

(w

x)

> (cid:12)

[(w

x)

]

x

P

E

2

1.

INTRODUCTION

Note that (w

x)

is the squared distance along w from the

T

2

origin. The (cid:12)rst problem we address is the following: does

The term \outlier" is a familiar one in many contexts.

there exist a small subset of P whose removal ensures that

Statisticians have several notions of outliers. Typically they

the remaining set has no outliers? More precisely, what is

quantify how far the outlier is from the rest of the data,

the smallest (cid:12) such that on removing a subset consisting of

e.g.

the di(cid:11)erence between the outlier and the mean or

at most an (cid:15) fraction of the points, the remaining set has no

the di(cid:11)erence between the outlier and the closest point in

(cid:12) -outliers?

the rest of the data. In addition, this di(cid:11)erence might be

A natural approach might be to (cid:12)nd all (cid:12) -outliers in the

normalized by some measure of the \scatter" of the set, e.g.

set and remove them. This can be done by (cid:12)rst applying a

the range or the standard deviation. Data points that are

linear transformation (described in section 2) that results in

outside some threshold are labelled outliers.

the average squared length of the distribution being 1 along

Identifying outliers is a fundamental and ubiquitous prob-

every unit vector (the so-called

position). Isotropic

isotropic

lem. The outliers in a data set might represent experimental

position was used in [4] to show that any convex set K in

error, in which case it would be desirable to remove them.

isotropic position contains a unit ball and is contained in

They could a(cid:11)ect the performance of a computer program,

a ball of radius n. Bringing a distribution into isotropic

(cid:3)

Department of Mathematics, MIT, Cambridge MA, 02139.

Email:

Supported in

jdunagan, vempala

@math.mit.edu

is a (cid:12) -outlier now simply has squared length more than (cid:12) .

position allows us to identify outliers easily. A point that

part by NSF Career award CCR-9875024.

f

g

The main diÆculty is that the remaining set might still have

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
STOC’01, July 6-8, 2001, Hersonissos, Crete, Greece.
Copyright 2001 ACM 1-58113-349-9/01/0007 ...

5.00.

$

outliers | it is possible that points that were previously not

outliers now become outliers. Can this happen repeatedly,

perhaps forcing us to throw out most of the set?

Our main result is that the answer to this question is \no"

for a surprisingly small value of (cid:12) . We present it below in a

more general framework. Let

=

0

x

: 2

(cid:0)

b

n

n

b

b

R

f

g [ f

2 R

(cid:20)

x

2

, i.e. the subset of n-dimensional space outside of a

j

j (cid:20)

g

very small ball and inside a very large ball, plus the origin.

In place of the point set P in the discussion above we have

any probability distribution (cid:22) on

. Note that

contains

b

n

b

R

R

the set of all b-bit rationals, an ob ject of frequent interest in

2. ALGORITHMS FOR OUTLIER REMOVAL

theoretical computer science.

We state our algorithms for probability distributions over

Theorem 1

(Main Theorem).

(cid:22)

Let

be a probability

R

restriction in a single step at the end of section 3. Let

distribution on

and

be a region in space. Denote by

S

b

b

n

n

J

=

0

[2

; 2

] and J

=

x

J

;

i =

x

i

b

b

(cid:0)

b

n

b

R

(cid:22)(S )

x

the probability that

chosen according to

is in

(cid:22)

S

.

f

g [

f

2 R

j

2

1; 2; : : : ; n

, i.e. the subset of n-dimensional space with co-

Then for every

, there exists

and

(cid:15) > 0

S

g

ordinates in J

. Note that the coordinate axes play a special

b

a restriction of

for ease of exposition. We remove this

n

b

n

n

role in J

. We assume for the remainder of this section that

(cid:12) = O

(b + log

)

(cid:15)

(cid:15)

(cid:16)

(cid:17)

the probability distribution we are interested in is over J

.

n

b

In order to detect outliers, we use a linear transformation.

Let M =

[xx

] where x is drawn according to the prob-

n

b

E

T

such that

(i)

(cid:22)(S )

1

(cid:15)

w

2 R

(ii)

for al l

max

(w

x)

: x

S

(cid:12)

[(w

x)

: x

S ]

2

(cid:21)

(cid:0)

T

2

T

2

E

ability distribution (cid:22). Since M is positive de(cid:12)nite, there

n

f

2

g (cid:20)

2

exists a matrix A such that M = A

. Consider the trans-

formed space z = A

x. This transformation preserves out-

1

(cid:0)

The proof of the theorem (section 3) is constructive. In

liers:

if z is a (cid:12) -outlier in direction w in the transformed

section 2 we describe (two variants of ) an algorithm for out-

1

space, the corresponding x = Az is a (cid:12) -outlier in direction

w

= A

w in the untransformed space, and vice versa. The

0

(cid:0)

lier removal. The theorem can be proven using either vari-

ant. Although the theorem is not obvious, the algorithm is

extremely simple. To convince the reader of this, we include

transformed distribution is in

[4] position, and we

isotropic

refer to the resulting distribution as a

distribution.

centered

a matlab implementation of the algorithm in section 9.

Such transformations have previously been used to make ge-

For a point set with m points (m > n) the algorithm runs

ometric random walks more eÆcient [3]. If M does not have

2

full rank, it is still positive semi-de(cid:12)nite, and we instead

in O(m

n) time. In section 4 we show that the algorithm can

also be used on an unknown distribution if it is allowed to

center (cid:22) in the span of M .

draw random samples from the distribution. The number of

samples required is

O(

) and the running time is

O(

).

2

2

5

n

b

b

n

~

~

(cid:15)

2

(cid:15)

for some direction w is also an outlier in the direction x.

This follows from the facts that a centered distribution has

For a centered distribution, any point x that is an outlier

Our algorithm is similar to the algorithm of [1], the imme-

E

T

2

[(w

x)

] = 1 for every w such that

w

= 1, and that the

diate inspiration for our work. The bound on (cid:12) in the the-

j

j

pro jection of the point x on to a direction w is greatest when

orem improves on the previous best bound of O(

) given

w = x=

x

. Thus outlier identi(cid:12)cation is easy for centered

in [1]. There it was used as a crucial component in the (cid:12)rst

distributions.

j

j

polytime algorithm for learning linear threshold functions in

The (cid:12)rst algorithm has the following simple form: while

the presence of random noise. Due to the high value of (cid:12) ,

there are (cid:12) -outliers, throw them out; if at any point we are

the bound on the running time of the learning algorithm,

very close to a lower dimensional subspace, drop to the lower

although polynomial, is a somewhat prohibitive

O(n

). In

dimensional subspace. Stop when there are no outliers. In

n

b

7

(cid:15)

28

~

5

~

contrast, our theorem implies an improved bound of

O(n

)

the description below, c is an absolute constant.

for learning linear thresholds from arbitrary distributions in

Algorithm 1 (Restriction to Ellipsoids):

the presence of random noise. Further, our bound on (cid:12) is

Let (cid:12) = (cid:13)

= c(

(b + log

)) and (cid:22) be the distribution.

2

n

n

(cid:15)

(cid:15)

5 by an example where for any (cid:15) <

, a bound on (cid:12) better

1. Center (cid:22). If there exists x such that

x

> (cid:13) , let S =

asymptotically the best possible. This is shown in section

than (cid:10)(

(b

log

)) is not possible.

n

1

(cid:15)

(cid:15)

(cid:0)

1

2

Our main theorem gives an extremal bound on (cid:12) . A nat-

x :

x

(cid:13)

. Retain only points in S .

j

j

f

j

j (cid:20)

g

ural follow-up question is whether one can achieve the best

2. Transform back to the original space. If there is some

possible (cid:12) for any particular distribution. Given a distribu-

coordinate axis i such that Pr[x

2

]

, throw

i

(cid:0)

tion (cid:22) and a parameter (cid:15), we want to (cid:12)nd a subset of proba-

out all points x with x

2

.

i

b

(cid:15)

3n

(cid:21)

(cid:20)

b

(cid:0)

(cid:21)

bility at most (cid:15) whose removal leaves an outlier-free set with

the smallest possible (cid:12) . This question can be shown to be

3. Repeat until neither of the above conditions is met.

NP-hard even in the one-dimensional case by a reduction

The following variant of the above algorithm will be sig-

to subset-sum.

In section 6 we prove that our algorithm

ni(cid:12)cantly easier to analyze. Whereas in the previous algo-

achieves a (

)-approximation to the best possible (cid:12) for

rithm, we removed outliers in every direction in one step, in

1

1

(cid:15)

(cid:0)

any given (cid:15).

Algorithm 2, we only remove outliers in one direction per

In some cases it may be desirable to translate the data

set so that the origin coincides with the mean, rather than

step. As before, c is a constant.

Algorithm 2 (Restriction to Slabs):

having a (cid:12)xed origin. We prove the following corollary for

2

n

n

standard deviations from the mean in section 7. Let (cid:22) be

n

p

a probability distribution on I

, where I

=

:

p

;

q

b

b

q

1. Center (cid:22).

If there exists a unit vector w such that

b

n

T

2

2

T

2

2

f

j

j

j

j 2

0; 1; 2; :::; 2

1

; q

= 0

. I

is the set of all n-dimensional

max

(w

x)

> (cid:13)

, let S =

x : (w

x)

(cid:13)

. Re-

b

f

(cid:0)

g

g

f

g

f

(cid:20)

g

vectors of b-bit rationals. (Note that now (cid:22) will satify the

tain only points in S .

Let (cid:12) = (cid:13)

= c(

(b + log

)) and (cid:22) be the distribution.

(cid:15)

(cid:15)

\outside a small ball, inside a large ball" criterion for ever

possible placement of the origin, except for at most one

2. Transform back to the original space. If there is some

point.) The corollary is that, for any (cid:15) > 0, there exists

coordinate axis i such that Pr[x

2

]

, throw

i

(cid:0)

b

(cid:15)

3n

b

(cid:0)

(cid:21)

(cid:20)

a (1

(cid:15)) fraction of the distribution such that along ev-

out all points x with x

2

.

i

(cid:0)

(cid:21)

ery direction, no point is further away from the mean than

O(

(b + log

)) standard deviations in that direction.

3. Repeat until neither of the above conditions is met.

n

n

(cid:15)

(cid:15)

p

6
3. PROOF OF THE MAIN THEOREM

We now construct a vector w

of length 1=a belonging to the

0

When either algorithm terminates, we clearly have a (cid:12) -

dual ellipsoid. Letting w

= w=a suÆces since w is a unit

0

outlier free subset. It remains to show that we do not throw

vector by assumption and

out too much of the distribution. The main idea of the proof

E

2

T

2

T

a

=

[(w

x)

: x

S ] Pr[x

S ] = w

M

w

S

is to show that in every step the volume of an associated dual

2

2

$

ellipsoid increases. By bounding the total growth of the dual

T

ellipsoid volume over the course of the algorithm, we will

1 = w

M

w

w

W (M

)

S

S

0

0

0

$

2

deduce that no more than a certain fraction of the original

We also show that every v

W (M ) also belongs to W (M

).

S

probability mass is thrown out before the algorithm termi-

We have M

= M

(cid:22)(x)xx

. Hence,

S

nates. Special care will be taken to deal with the possibility

that at some step the distribution (cid:22) becomes concentrated

v

W (M )

v

M v

1

2

T

x =

S

(cid:0)

2

P

T

2

$

(cid:20)

on a subspace of lower dimension.

Towards this end, we will need some de(cid:12)nitions. De(cid:12)ne

T

T

T

T

v

M

v = v

M v

(cid:22)(x)v

xx

v =

S

E (M ) to be the ellipsoid

x :

A

x

1

and W (M ) to be

x :

Ax

1

where M = A

. We will refer to E (M ) and

f

j

j (cid:20)

g

2

f

j

j (cid:20)

g

(cid:0)

x =

S

X

2

W (M ) as the primal inertial ellipsoid and the dual ellipsoid

T

T

2

T

respectively. If S is a subset of

, we denote by M

the

S

(cid:0)

(cid:20)

(cid:20)

n

v

M v

(cid:22)(x)(v

x)

v

M v

1

matrix given by

x =

S
X

2

1

(cid:0)

R

M

=

[xx

: x

S ] Pr[x

S ] =

(cid:22)(x)xx

S

E

2

T

T

the boundary of an ellipsoid lower bounds the length of

implying that v

W (M

). The length of a point on

S

2

2

x

S

X

2

the longest axis. Since at least one axis of the dual has

M

is the matrix obtained by restricting (cid:22) to S (zeroing out

S

length 1=a, and all the other axes have length at least 1,

points outside of S ). We denote this restricted probability

V ol(W (M

))

(1=a)f (n) while V ol(W (M )) = f (n), im-

S

(cid:21)

2

(cid:13)

p=2

distribution directly by (cid:22)

. The useful property attained by

plying the dual volume grows by at least a factor of e

.

centering with respect to (cid:22)

(the restriction of the original

If the dual ellipsoid has in(cid:12)nite volume after this iteration,

distribution onto the region S ) is that

then the statement is still true because the dual ellipsoid

[(w

x)

: x

S ] Pr[x

S ] = 1 for every unit vector w,

had (cid:12)nite volume at the beginning of the iteration. This

where the expectation and probability are with respect to x

concludes the proof of lemma 1.

2

2

S

j

S

j

E

T

2

drawn from (cid:22).

We will also need the following elementary facts about lin-

Lemma 2

(Dual Volume Growth).

(cid:22)

Let

be an ini-

ear algebra and ellipsoids: the volume of an ellipsoid is given

tial ful l-dimensional distribution, and let

be the (cid:12)nal

by the product of the axis lengths times the volume of the

distribution resulting from application of either algorithm.

unit ball, which we will denote by f (n), and

x :

A

x

1

Assume

is ful l-dimensional. Let

. Then

L = (b + log

)

1

(cid:0)

(cid:22)

(cid:3)

S

j

f

j

j (cid:20)

g

is an ellipsoid with axes given by the eigenvectors of A. It

follows that V ol(W (M ))V ol(E (M )) = (f (n))

, a function

2

V ol(W (M ))

2

(cid:22)

(cid:3)

S

j

n

(cid:15)

solely of the dimension (although if V ol(E (M )) = 0, then

V ol(W (M )) =

).

We will actually prove theorem 1 with

[(w

x)

: x

1

E

T

2

S ] Pr[x

S ] in place of

[(w

x)

]. Note that this is a

E

T

2

2

stronger statement than the original theorem.

2

Proof. First we lower bound the initial dual volume,

V ol(W (M )). Consider any vector v of length at most 2

=pn.

b

(cid:0)

Lemma 1 relates the dual volume growth to the loss of

Since the longest possible x belonging to (cid:22) is of length at

probability mass, and lemma 2 upper bounds the total dual

b

T

T

2

most 2

n, we have that v

M v =

[(v

x)

]

1 and so

p

E

volume growth.

(cid:20)

v belongs to the dual ellipsoid. Thus the dual ellipsoid ini-

tially has volume at least (2

=n)

= 2

2

(cid:0)

(cid:0)

(cid:0)

(cid:0)

b

n

n(

b

log n)

nL

Lemma 1

(Restriction to a Slab).

(cid:13)

Let

be (cid:12)xed, and

(cid:21)

(using the inscribed cube to lower bound the volume of the

let

be a ful l-dimensional centered distribution. Suppose

(cid:22)

ball).

w;

w

= 1

such that

(cid:3)

9

j

j

Next we upper bound V ol(W (M

)). Consider any vec-

S

T

2

2

T

2

tor v of length

v

. Then v has length at least

v

=

n on

p

max

(w

x)

> (cid:13)

[(w

x)

]

E

T

2

2

f

g

some coordinate axis i. By the property that (cid:22)

is full-

j

j

j

j

(cid:3)

S

j

Let

and

. Then

S =

x : (w

x)

(cid:13)

p = Pr[x =

S ]

dimensional, the test condition in step 2 of either algorithm

f

(cid:20)

g

2

was never satis(cid:12)ed, and thus at least an (cid:15)=3n fraction of

V ol(W (M

))

e

V ol(W (M ))

S

the points x have value at least 2

on this coordinate axis.

b

(cid:0)

2

p(cid:13)

=2

(cid:21)

This yields that

Proof. Let a

=

[(w

x)

: x

S ] Pr[x

S ]. Starting

(cid:21)

j

j

(cid:21)

2

T

2

E

T

T

2

2

2b

b

v

M v =

[(v

x)

]

(

v

=n)(2

) Pr[x

2

]

i

(cid:0)

(cid:0)

E

from the identity

2

2

2

2b

(

v

=n)(2

)((cid:15)=3n)

(cid:0)

E

E

E

T

2

T

2

T

2

(cid:21)

j

j

[(w

x)

] =

[(w

x)

] Pr[x

S ]+

[(w

x)

] Pr[x =

S ]

x

S

T

2

For v to be in the dual ellipsoid, we must have v

M v

2

2

x =

S

2

and using that (w

x)

(cid:13)

for all x not in S , we get

T

2

2

b+log n+1=2 log 1=(cid:15)

(cid:20)

1

v

2

. Thus the ultimate volume of

2

2

(cid:21)

1

a

+ (cid:13)

p, which implies

$ j

j (cid:20)

n(b+log n+1=2 log 1=(cid:15))

nL

the dual ellipsoid is no more than 2

2

(cid:21)

(cid:20)

(using the containing cube to upper bound the volume of the

2

2

(cid:13)

p

2

(cid:0)

(cid:20)

(cid:0)

(cid:20)

a

1

(cid:13)

p

e

ball).

nL

(cid:0)

(cid:21)

V ol(W (M

))

2

S

(cid:3)

nL

(cid:20)

Using lemmas 1 and 2, we now prove that Algorithm 2

throw out no more than an (cid:15)=(3n) fraction of the probability

terminates with S satisfying theorem 1.

mass. Since we might also drop in dimension by explicitly

Proof. Suppose that the algorithm terminates with sub-

mass (the criterion for step 2 of Algorithm 2), we can up-

set S

after having thrown out no more than (cid:15)

of the original

(cid:3)

0

per bound the total amount of probability mass thrown out

throwing out up to an (cid:15)=(3n) fraction of the probability

probability mass. Then we have that for every w,

without any increase in the dual by

max

(w

x)

: x

S

(cid:13)

[(w

x)

: x

S

] Pr[x

S

]

3n

3

(cid:3)

(cid:3)

(cid:3)

E

(total number of dimensions dropped)

. Combin-

T

2

2

T

2

2(cid:15)

2(cid:15)

f

2

g (cid:20)

2

2

ing this with

(cid:15)=3, we see that we throw out no more

p

i

(cid:2)

(cid:20)

We remind the reader again that normalizing (cid:22)

so that

(cid:3)

S

j

(cid:20)

P

than (cid:15) probability mass over the life of the algorithm. Thus

it is a probability distribution on points from (cid:22), rather than

(cid:15)

(cid:15), and our (cid:12)nal bound on (cid:12) is

0

with points outside of S

replaced by zeros, increases the

(cid:3)

(cid:20)

right-hand side of this inequality by the factor 1=(cid:22)(S

), but

(cid:3)

does not increase the left-hand side. Thus the inequality

(cid:12)

(b + log

)

36n

n

(cid:20)

(cid:15)

(cid:15)

will still be true even if we normalize (cid:22)

. We thus achieve

(cid:12) = (cid:13)

= c

(b + log

)

simple lemma about ellipsoids.

We now prove property (b). It follows from the following

(cid:3)

S

j

2

n

n

(cid:15)

(cid:15)

It now remains to show the relation between (cid:15)

and (cid:15) that

0

Lemma 3

(Ellipsoid Slices).

n

Consider an

-dimensional

(cid:15)

(cid:15), i.e. that we do not throw out more of the probability

el lipsoid

with axis lengths

. Now take any

E

a

1

: : : a

n

k

-

mass than claimed. First suppose (cid:22)

is full dimensional.

dimensional slice

through the center of

. Then

C

E

0

(cid:20)

Let L = (b + log

).

n

(cid:15)

(cid:3)

S

j

th

V ol

(C )

f (k)

k

a

i

i=n

k+1

(cid:21)

(cid:0)

n

Q

(cid:21)

L

Suppose that during the i

iteration of the algorithm

(b) is a corollary of lemma 3 because no axis of the dual

through step 1, a p

fraction of the original points are thrown

i

ellipsoid has length more than 2

, and the other axes of

out. Then the total amount thrown out is

p

. By lemma

i

W (M ) can only grow when we throw out probability mass

1, the total amount of dual volume increase is

e

=

from (cid:22). It only helps that f (n) is monotonically decreasing

2

(cid:13)

2

i

P p

e

. Comparing this to our bound on the total increase

Q

in n.

2

p

(cid:13)

=2

i

P

i

in the dual volume from lemma 2 yields

Theorem [5].

Proof of lemma 3.

The main tool is the Courant-Fischer

2

(cid:13)

2

i

P p

2nL

e

2

2

(cid:20)

For our choice of (cid:13)

with c = 36, we have that

(cid:15)=3.

p

i

(cid:20)

Then

(cid:2)

(cid:21)

We now extend the proof to the case that the (cid:12)nal distri-

P

bution is not full-dimensional. This drop in dimension is the

main issue of the proof once we have lemmas 1 and 2. Sup-

pose that at some step of the algorithm, we move from M of

T

x

Ax

(cid:21)

= min

max

i

U

x

U;x

=0

T

x

x

2

dimension k to M

of dimension k

. We then want to restrict

0

0

where the minimum is over al l

-dimensional sub-

(n

i + 1)

ourselves for the rest of the algorithm to the k

-dimensional

0

spaces

.

U

(cid:0)

Theorem 2

(Courant-Fisher).

A

Let

be a real sym-

metric

matrix,

the

eigenvalue,

n

n

(cid:21)

i

i

(cid:21)

1

: : : (cid:21)

n

.

th

subspace spanned by M

. That is, both W (M

) and E (M

)

0

0

0

Since the k dimensional slice is a subspace U with i =

are k

-dimensional ob jects. We denote the fact that we are

0

n

k + 1, and the axis lengths of W (M ) are given by the

now considering the volume of a lower-dimensional ob ject

(cid:0)

eigenvalues of A, we (cid:12)nd that the longest axis of the sliced

by adding a subscript to our V ol(

) function. We show two

things

argument to the k

1 dimensional subspace of C perpendic-

(cid:1)

(cid:0)

ellipsoid has length at least a

. Applying the same

n

k+1

(a) Our upper bound on the total volume of the dual

(cid:0)

0

ular to the longest axis of the sliced ellipsoid, we (cid:12)nd that

ellipsoid decreases by 2

(k

k

)L

(cid:0)

0

(b)

2

0

(cid:0)

V ol

(W (M

))

(cid:20)

k

0

k

V ol

(W (M ))

(k

k

)L

(cid:0)

the next longest axis has length at least a

. Applying

n

k+2

the argument to the remainder of the axes concludes the

Thus the amount by which we are away from our upper

proof of lemma 3. This also concludes the proof of the main

bound on the total growth of the dual volume cannot have

theorem.

increased. (a) is immediate from the way we calculated the

upper bound on the (cid:12)nal dual volume. (b) will be proved

We now give an alternate proof of the main theorem using

shortly. We now conclude the argument assuming (b). Ev-

the construction given by Algorithm 1. We begin by proving

ery time (cid:22) drops in dimension (i.e., the rank of M decreases),

an analogue to lemma 1.

we may not have made any progress in increasing the dual

volume, and thus we cannot apply lemma 1 to the probabil-

Lemma 4

(Restriction to an Ellipsoid).

(cid:13)

Let

be

2

(cid:12)xed, and let

be a ful l-dimensional centered distribution.

(cid:22)

ity mass thrown out in this step. However, taking (cid:13)

3n=(cid:15)

T

2

yields that we won’t throw out more than an (cid:15)=(3n) fraction

f

(cid:20)

g

2

(cid:21)

Let

and

. Then

S =

x : (x

x)

(cid:13)

p = Pr[x =

S ]

of the total probability mass on any one step. This follows

V ol(W (M

))

e

V ol(W (M ))

S

2

p(cid:13)

=2

from

(cid:21)

T

2

2

[(w

x)

]

1

E

T

2

Proof. First we establish the tradeo(cid:11) for a radially sym-

Pr[x =

S ] = Pr[(w

x)

(cid:13)

]

=

2

2

metric distribution, and then we show that a radially sym-

2

(cid:21)

(cid:20)

(cid:13)

(cid:13)

metric distribution is the worst case for the tradeo(cid:11) we want.

Since our choice for (cid:13)

already satis(cid:12)ed the above crite-

Let (cid:22)

be a radially symmetric distribution, and de(cid:12)ne

0

2

rion, we conclude that every time we drop in dimension, we

M

, S , and p as above. We then calculate the increase in

0

6
V ol(W (M

)). Let a

=

[(w

x)

: x

S ] Pr[x

S ] for

Finally, we prove that Algorithm 1 terminates with S sat-

0

2

T

2

E

0

(cid:22)

2

2

any w;

w

= 1. From the center of an n-dimensional sphere

isfying theorem 1.

of radius (cid:13) , the pro jection of the sphere on to any direc-

j

j

tion is sharply concentrated around (cid:13) =

n, and the squared

p

Proof. Lemma 2 still holds. The rate of increase in the

expectation is exactly (cid:13)

=n. Using the identity

2

dual volume as we throw out probability mass when the di-

T

2

T

2

T

2

E

E

E

[(w

x)

] =

[(w

x)

] Pr[x =

S ]+

[(w

x)

] Pr[x

S ]

x

S

x =

S

2

2

2

2

mension remains constant is clearly still good as well. How-

ever, our bound on the amount of probability mass p that

can be thrown out in a single step is no longer (cid:15)=(3n). In-

as in the proof of lemma 1, but now for any w, we deduce

stead, we have from the proof of lemma 4 only that

1

a

+ (cid:13)

p=n, and thus

2

2

(cid:21)

0

1

p(cid:13)

=n which only leads to p = O((cid:15)) for our chosen

2

(cid:20)

(cid:0)

2

value of (cid:13)

. Thus, the analysis for Algorithm 2 does not

n

(cid:13)

p=2

(cid:13)

p

2

n=2

2

(cid:20)

(cid:0)

(cid:20)

(cid:18)

n (cid:19)

a

1

e

(cid:0)

immediately apply.

To analyze Algorithm 2, we argued that any drop in the

As in the proof of lemma 1, we observe that W (M

) includes

0

S

dual on the up to n possible steps in which the dimension

a vector of length 1=a in the direction of w. Since this is

dropped could not lead to more than an (cid:15) overall drop in

now true for every w, the dual ellipsoid volume increases by

the probability mass. Thus we were able to just argue that

at least a factor of (1=a)

. This shows that in the case of a

n

the maximum amount by which the dual volume still might

radially symmetric distribution,

grow couldn’t increase in this step. To successfully analyze

Algorithm 1, we must prove the stronger statement that if

V ol(W (M

))

e

V ol(W (M ))

S

we throw out a lot of probability mass, and the dimension

2

p(cid:13)

=2

(cid:21)

Now we show that a radially symmetric distribution is

drops by only a small amount, then we still make signi(cid:12)cant

the worst case for the tradeo(cid:11) we want. Suppose there were

progress on the overall growth of the dual volume. To be

some centered, full-dimensional distribution (cid:22) for which the

precise, letting M ; M

; p; k; k

; L be as before (in the analysis

0

0

statement of the lemma was not true. We construct a new

centered, full-dimensional and radially symmetric distribu-

of Algorithm 2), we prove

V ol

(W (M

))

0

k

(k

k

)(L+1)

p(cid:13)

=4

0

0

2

(c)

2

e

(cid:0)

(cid:0)

V ol

(W (M ))

k

(cid:21)

tion (cid:22)

for which the statement is also false.

0

(Compare to (b) in the analysis of Algorithm 1.) The

We begin by noting that every point thrown out from (cid:22)

idea for our proof of (c) is that any point either has a large

is also thrown out from any rotation of (cid:22) { this just follows

component in the subspace that vanishes, or a large com-

from the fact that (cid:22) is centered. Let (cid:22)

be the expectation

0

ponent in the subspace that remains | if the dimension

of (cid:22) under a random rotation. That is, (cid:22)

is a radially

0

drops by only a small amount, there cannot have been too

symmetric distribution such that the probability of choosing

many points with a large component in the subspace that

x from (cid:22)

at distance less than r from the origin is exactly

0

vanishes, and the dual volume growth results from the dis-

the same as the probability of choosing x from (cid:22) at distance

carded points with a large component in the subspace that

less than r from the origin, for every r. Let M

correspond

0

0

remains. Let W

be the k

-dimensional space spanned

remain

to (cid:22)

.

0

by M

, and W

be the (k

k

)-dimensional subspace of

tossed

0

0

Consider an axis direction w

of E (M

),

w

= 1. We

i

S

i

span(M ) orthogonal to W

. Let p

be the probability

remain

1

have a

=

[(w

x)

: x

S ] Pr[x

S ]. For E (M

), denote

that the pro jection of x onto W

is at least

, and

remain

2

T

2

E

i

i

j

j

0

S

2

2

the axis length for any axis (also just the radius of E (M

))

0

S

similarly let p

be the probability that the pro jection of x

2

(cid:0)

(cid:13)

p

2

by (cid:22)a. We (cid:12)nd from the construction of (cid:22)

that

0

onto W

is at least

. One of these two events happens

tossed

n

n

for every point x. Thus p

p

+ p

.

1

2

2

T

2

2

1

1

(cid:22)a

=

[(w

x)

: x

S ] Pr[x

S ] =

a

i

i

E

Taking any unit vector w

W

and using our favorite

tossed

n

n

2

2

T

2

T

2

2

i=1

X

i=1

X

E

E

equation

[(w

x)

] =

[(w

x)

] Pr[x =

S ]+

One way to visualize this equality is to take (cid:22) and simply

x

S

[(w

x)

] Pr[x

S ], we have that

E

2

T

2

2

consider ~(cid:22) achieved by rotating the axes of (cid:22) onto the other

axes of (cid:22); since this is a discrete set of rotations, it is clear

that the squared axis lengths of ~(cid:22) are just the arithmetic

(cid:13)

1

2(k

k

)

0

1

0 + (

)p

1

p

1

(cid:0)

2

(cid:21)

)

(cid:20)

2

k

k

(cid:13)

0

averages of the squared axis lengths of (cid:22). Then we can make

~(cid:22) into (cid:22)

by taking a continuous set of rotations, without

0

where the factor 1=(k

k

) comes from the expected squared

pro jection of a (k

k

)-dimensional unit vector on to a ran-

a(cid:11)ecting the axis lengths from ~(cid:22).

We now consider the volume of E (M

). We have

0

S

dom direction.

n

n

(cid:22)a

=

[(w

x)

: x

S ] Pr[x

S ], a similar analysis gives

n

2

T

2

2

E

Taking instead a unit vector w

W

and letting

remain

1

2

2

V ol(E (M

)) = f (n)

(cid:22)a = f (n)

a

0

1

v

0

S

2

i

i=1

Y

u

i=1

X

u

n

(cid:21)

@

A

t

(cid:22)a

1

0

k

p

(cid:13)

2

0

2

k

=2

(cid:20)

(cid:0)

(cid:18)

2k

(cid:19)

0

f (n)

a

= V ol(E (M

)

i

S

0

k

n

i=1

Y

Since 1=((cid:22)a

) is a lower bound on the increase in the vol-

ume of the dual ellipsoid in the subspace W

, and the

remain

where the inequality is the arithmetic mean-geometric mean

dual volume drop in moving to that subspace was at most

inequality. This implies that V ol(W (M

))

V ol(W (M

)).

S

0

S

0

(k

k

)L

This concludes the proof of lemma 4.

(k

k

)L

p

(cid:13)

=4

2

2(k

k

)

(cid:21)

0

2

0

2

, we have an increase in the dual volume of at least

(cid:0)

(cid:0)

(cid:13)

p

2

(cid:20)

x =

S

2

2

2

(cid:0)

0

(cid:0)

0

(cid:0)

2

e

. Using p

p

p

yields that

2

p

1

(cid:0)

(cid:0)

(cid:0)

2

(cid:13)

0

2

(cid:21)

0

(cid:0)

(cid:21)

(cid:0)

(k

k

)L

p(cid:13)

=4

(k

k

)=2

this is 2

e

e

, which then implies (c).

(cid:0)

(cid:0)

(cid:0)

The tradeo(cid:11) expressed in (c) is slightly weaker than (b)

4. EFFICIENCY

stead of

.

n

b

R

n

b

R

from the analysis of Algorithm 2, but it is independent of

In this section we describe polynomial time versions of

whether we drop in dimension on a given iteration or not.

th

both algorithms. The computational model is to allow mul-

Suppose we throw out probability mass p

the i

time

i

tiplications and additions in unit time.

through step 1 of Algorithm 1. From (c), we have that

dropping in dimension only takes away from our progress

4.1 Point sets

towards the upper bound on dual volume growth by at most

Suppose the distribution (cid:22) is speci(cid:12)ed explicitly as a set of

a factor of two per dimension that we drop, and thus we (cid:12)nd

m points with weights corresponding to probabilities. Then

2

(cid:13)

2

i

P

p

2n(L+1)

e

2

2

(cid:20)

we can achieve exactly the stated value of (cid:12) with either

algorithm deterministically. The running time for either al-

Our choice of (cid:13)

with c = 36, implies

2(cid:15)=3. We

p

i

gorithm is given by the time to compute M (O(mn

)), the

throw out less than (cid:15)=3 of the probability mass in step 2

time to center the distribution (O(n

+ mn

)), the time to

(cid:20)

P

2

3

2

over the life of the algorithm. Thus the entire algorithm

(cid:12)nd an outlier (O(mn)), and the need to repeat the whole

throws out no more than an (cid:15) fraction of the probability

process up to m times. The amount of time spent in step 2

mass. This concludes the analysis of Algorithm 1.

of either algorithm is negligible. This yields a time bound

We now remove the technicality of restricting to J

in-

In the above discussion we made the worst case assump-

n

b

of O(m

n

+ mn

).

2

2

3

tion that only one data point was thrown out in each iter-

Lemma 5

(Choice of Axes).

(cid:22)

Let

be a probability dis-

ation of centering and looking for outliers. In the case that

tribution over

. There exists a rotation of

, which we

(cid:22)

a single data point is throw out, centering the distribution

denote by

, a subset of space

, and

such

T

B = O(b + log

)

(cid:22)

0

can be done more eÆciently. If the distribution is initially

that

centered, and v of probability p is removed, then the new

(i)

(cid:22)

(T )

1

0

centered distribution is achieved by replacing each vector u

(cid:15)

2

(cid:21)

(cid:0)

(cid:22)

0

T

j

(ii)

is a probability distribution over

n

J

B

by u

1

1

v

p

An intuitive explanation for

2

(u

v)v

T

2

v

(cid:0)

(cid:0)

(cid:0)

(cid:16)

(cid:17)

p

Proof. We construct (cid:22)

by randomly rotating (cid:22). T is

0

this formula is that we are just correcting the inertial ellip-

soid in the direction of v . Using this observation, we com-

chosen to be the largest subset of space we may retain and

2

pute M from scratch once (O(mn

)), center the distribution

still not have any points with too small a coordinate value

3

2

in any axis (so T equals J

, the set that we want (cid:22)

to be

n

B

from scratch once (O(n

+ mn

)), and then (cid:12)nd an outlier

(O(mn)) and recenter using our formula above (O(mn)) a

over).

total of at most m times. This yields the improved time

Restricting to T consists of throwing away any points with

2

2

3

n

(cid:15)

0

j

T

value less than 2

along any axis. The expected amount

B

(cid:0)

of probability mass thrown away is then at most n times

the amount thrown away by restricting along one axis (i.e.,

removing all points within the slab of width 2

centered

B+1

(cid:0)

bound of O(m

n + mn

+ n

).

4.2 Arbitrary distributions

at the origin and perpendicular to the one axis.)

Consider a single point x

R

; x

= 0. The following

contained in an ellipsoid.

n

b

2

bound on the probability of a small pro jection in the direc-

The more interesting problem is where we are not given (cid:22)

explicitly, but rather only the ability to sample from (cid:22). The

outlier-free restriction of (cid:22) will be speci(cid:12)ed as the part of (cid:22)

tion of a random unit vector is proved in [2].

1. Apply lemma 5 (if necessary) to get a set of \clean"

T

2

x

4

Pr

[(w

x)

]

w

S

n

2

2

2

axes.

(cid:20)

(cid:20)

nC

C

2. Get a set P =

x

; : : : ; x

of m samples from (cid:22).

1

m

f

g

Let C =

and B = b + log

. Since

x

2

, we

(cid:0)

3. Run the outlier removal algorithm algorithm on the

(cid:15)

(cid:15)

16n

16n

b

have for the chosen value of C that 2

. As we only

2

j

j (cid:21)

2B

x

(cid:0)

nC

(cid:20)

2B

discrete point set P with parameter (cid:0)

.

2

throw out the point if it’s pro jection is less than 2

, this

(cid:0)

4. Let P

be the outlier free subset of P . Then the outlier-

0

choice of B implies that the probability of throwing out a

free restriction of (cid:22) is given by (1 + Æ)

(cid:0)

E (

M ), where

2

2

(cid:22)

point is at most

for one axis. Thus with probability at

(cid:22)

M =

x

x

and Æ > 0 is an accuracy param-

(cid:15)

4n

1

T

m

x

P

i

2

0

i

i

eter.

P

least a half, we throw out no more than an

fraction of the

(cid:15)

2

distribution after considering all n axes.

Let us now consider the bound on (cid:12) we achieve when our

n

n

The main theorem of this section is the following.

initial distribution is not over J

, but rather over R

. We

Theorem 3

(Sample Complexity).

Let

b

b

apply the transformation in lemma 5 as an initial step, and

m = (

(n log

+ log

))

. With high probability, ei-

(cid:0)

n

b+log

n

(cid:15)

2

2

Æ

Æ

Æ

then apply the algorithm as before. Our upper bound on (cid:12)

2

ther outlier removal algorithm run with parameter

(cid:0)

=

does increase, but only by a constant factor. This is shown

2

2

in our concluding calculation, where we use

; B in place of

2

(1 + Æ)

(cid:13)

T

returns a set

satisfying

(i)

(cid:22)((1 + Æ)

T )

1

(cid:15)

(cid:15); b.

2

2

(cid:21)

(cid:0)

(ii)

has no

-outliers

(1 + Æ)

T

(1 + O(Æ))(cid:13)

2

where

is achieved by the deterministic omniscient al-

((cid:13)

; (cid:15))

(cid:15)

2

2

n

n

2n

16n

n

gorithm (omniscient in that it knows the distribution ex-

(cid:12) = O

(B + log

)

= O

(b + log

+ log

)

(cid:18)

(cid:15)=2

(cid:15)

(cid:19)

(cid:18)

(cid:15)

(cid:15)

(cid:15)

(cid:19)

actly).

n

n

= O

(b + log

)

(cid:16)

(cid:17)

(cid:15)

(cid:15)

terministic omniscient algorithm with parameter (cid:13)

(cid:12)nds

For the remainder of this section, assume that the de-

2

6
a subset S such that (cid:22)(S )

1

(cid:15), and (cid:22)

has no (cid:13)

-

This occurs with constant probability for m = O(

).

(cid:21)

(cid:0)

2

S

j

2

2

(cid:13)

2

Æ

outliers. The statement \(cid:22)

has no (cid:13)

-outliers", or simply

Now we show (ii). Let T be as above, and again assume

S

j

T

2

\S has no (cid:13)

-outliers" (since (cid:22) is implicit), is exactly that

max

(w

x)

: x

S

= 1 without loss of generality. If S

w; max

(w

x)

: x

S

(cid:13)

[(w

x)

: x

S ] Pr[x

S ].

E

has no (cid:13)

-outliers, then y

, and we would have found

T

2

2

T

2

f

2

g

2

8

f

2

g (cid:20)

2

2

Note that

[

] Pr[

] is taken over (cid:22), but max is just over S .

(cid:22)x to be an accurate estimate by the analysis in the previous

Since max is over S , whenever we have

w; max

(w

x)

:

paragraph. In this case, (cid:22)x

, and S has no (cid:13)

-outliers

(cid:1)

(cid:1)

T

2

y

2

x

S

max

(w

x)

: x

T

, we will be able to conclude

implies max

(w

x)

: x

S

(cid:13)

y

(cid:0)

(cid:22)x. This then

T

2

8

f

T

2

2

2

1

2

(cid:13)

(cid:21)

1+Æ

(cid:21)

2

g (cid:20)

f

2

g

2

bn

f

2

g (cid:20)

(cid:20)

that S

T . We know that (cid:13)

=

O(

) is always achiev-

implies S

T .

~

(cid:15)

able, but in some cases we may do better, and our bound

(cid:18)

(cid:18)

on running time is proved for arbitrary values of (cid:13)

.

2

n

2

Lemma 7

(Outlier Detection, Many Iterations).

Suppose that at some step we can estimate E (M ) to

Fix

. Let

Then either outlier re-

w

m = O(

log

)

within 1

Æ in every direction. Let (cid:0)

= (1 + Æ)

(cid:13)

. Then

moval algorithm restricted to

with parameter

produces

w

(cid:0)

2

2

2

2

(cid:13)

2

b+log

(cid:15)

Æ

Æ

every point that we perceive to be a (cid:0)

-outlier will be at

a region in space

T =

x : (w

x)

t

(for some value

t

)

2

T

2

least a (cid:13)

-outlier with respect to the true distribution, and

such that, with constant probability,

f

(cid:20)

g

2

so removing them does not throw away any point that the

(i) For any subset of space

that has no

-outliers along

S

(cid:13)

2

E

(cid:6)

2

deterministic algorithm keeps. Similarly, if we perceive the

w

S

,

T

.

distribution to have no (cid:0)

-outliers, the true distribution will

(ii)

has no

-outliers along

(1 + Æ)T

(1 + Æ)

(cid:13)

w

.

2

8

2

(cid:18)

have no (1 + Æ)

(cid:0)

-outliers. Before removing outliers, we

2

2

(cid:22)

(cid:6)

may not have that

M is within 1

Æ of M . However, when-

Proof. By \either outlier removal algorithm restricted

(cid:22)

2

2

ever we are wrong by more than 1+ Æ , there is some true out-

to w", we simply mean the one-dimensional versions of the

lier with respect to the original distribution that we throw

two algorithms. Consider S achieved by the deterministic

out even using our (cid:13)awed

M . This line of reasoning (made

omniscient version of the algorithm (restricted to w). Since

rigorous) will allow us to achieve a (1 + O(Æ))(cid:13)

-outlier free

our outlier removal algorithm only throws away probability

region in space, where (cid:13)

is the parameter achieved by the

mass when necessary, this S is the largest possible restriction

deterministic version of the algorithm. In lemma 6 we show

that is (cid:13)

-outlier free. De(cid:12)ne y and (cid:22)x as in lemma 6. By

2

this for a particular direction in a particular iteration. In

lemma 6, we have that (cid:22)x is a good approximation to y . This

lemma 7 we extend this to all iterations, and (cid:12)nally in the

ensures that with good probability, we identify S as (cid:0)

-

proof of theorem 3 we extend this to all directions and all

outlier free, and so (i) is proved. It remains to show that, if

iterations. We also show m = O(

(n log

+ log

))

our algorithm for some reason chooses a substantially larger

samples suÆce to achieve our stated goal of a (1 + O(Æ))(cid:13)

-

region T , then (1 + Æ)T has no (1 + Æ)

(cid:13)

-outliers.

8

2

T

2

outlier free set with high probability.

De(cid:12)ne T

=

x : (w

x)

(cid:11)

. Suppose

(cid:11) such that

(cid:11)

(cid:0)

n

b+log

n

(cid:15)

2

2

Æ

Æ

Æ

2

T

has no (cid:0)

-outliers. Then T

has no (1 + Æ)

(cid:0)

-

(cid:11)

(1+Æ)(cid:11)

2

2

2

f

(cid:20)

g

9

Lemma 6

(Outlier Detection, One Iteration).

Fix

outliers. This follows from the fact that max

(w

x)

: x

T

2

a direction

. Let

be a region in space. Let our number of

w

S

E

T

(1 + Æ)

max

(w

x)

: x

and

[(w

x)

:

T

(cid:11)

(1+Æ)(cid:11)

2

T

2

T

2

f

2

samples be

, and consider the sample distances

(cid:11)

(cid:11)

m = O(

)

x

T

] Pr[x

T

] is a monotonically increasing function of

g (cid:20)

f

2

g

2

(cid:13)

2

Æ

in direction

given by

. Let

w

y =

[(w

x)

: x

w

x

i

E

(cid:11). This analysis holds whether we are considering the true

T

T

2

2

2

S ] Pr[x

S ]

(cid:22)x

and

be the sample variance,

f

g

2

underlying distribution or just the samples.

1

2

T

2

m

x

S

i

2

bility that

P

(cid:22)x =

(w

x

)

i

. Then we have with constant proba-

Suppose we estimate that some set T = T

has no (cid:0)

-

t

(i)

max

(w

x)

: x

S

(cid:13)

y

(1

Æ)y

T

2

2

answer). Then our sample also leads us to calculate that

1

T

2

f

2

g (cid:20)

)

(cid:0)

(cid:20)

2

2

T

has no (1 + Æ)

(cid:0)

-outliers for (cid:11)

[t; (1 + Æ)t]. For every

(cid:11)

2

max

(w

x)

: x

S

(cid:13)

y

T =

x : (w

x)

and

T

2

2

T

2

t, we will show that for some nearby (within a factor of

m

x

S

i

(w

x

)

(1 + Æ)y

i

.

2

f

g (cid:20)

(ii)

P

2

(cid:0)

(cid:22)x

S

T

.

f

2

g (cid:20)

f

(cid:20)

(1+ Æ)) value of (cid:11), we correctly estimate the sample variance

g )

(cid:26)

on the restriction of (cid:22) to T

. Since the range of possible

(cid:11)

outliers (in which case the algorithm might return T as an

2

2

values for t is at most 2

, we can take every value of

2(b+log

)

n

(cid:15)

k

2(b+log

)

n

(cid:15)

b+log

n

(cid:15)

Proof. Property (i) says that we do correctly estimate

t = (1 + Æ)

for some integral k and union bound over the at

the variance of an outlier-free restriction of the distribution,

most log

2

= O(

) possible values for k.

1+Æ

Æ

and property (ii) assures us that any outlier-free restriction

2

2

2

We now show that if we estimate T

to have no (1+ Æ)

(cid:0)

-

(cid:11)

of the distribution has no probability mass past (cid:0)

times

outliers, then with good probability T

actually has no

(cid:11)

the sample variance (i.e., we can always throw away proba-

6

2

(1 + Æ)

(cid:0)

-outliers with respect to the true distribution, and

bility mass using the sample variance). Both claims are for

by our reasoning above, since there is an (cid:11) within (1 + Æ ) of

a (cid:12)xed direction w . Note that S found by the deterministic

8

2

omniscient algorithm satis(cid:12)es the conditions of both (i) and

(ii).

Let X

be the random variable representing the squared

i

t, T

is (1 + Æ)

(cid:0)

outlier free.

(1+Æ)t

We do this by showing that if T

has a (1 + Æ)

(cid:0)

-outlier,

(cid:11)

6

2

2

2

then our sample shows T

to have at least a (1 + Æ)

(cid:0)

-

(cid:11)

distance of x

along the direction w, X

= (w

x

)

. Without

i

i

i

T

2

outlier with good probability. Let X

be the random variable

i

loss of generality, assume max

(w

x)

: x

S

= 1 (by an

T

2

T

2

representing the squared distance of x

along the direction

i

f

2

g

w, X

= (w

x

)

. Without loss of generality, assume (cid:11) = 1.

i

i

2

De(cid:12)ne y and (cid:22)x as in lemma 6 (but with T

in place of S ).

(cid:11)

appropriate scaling). First we show (i). Since (cid:22)

has no

S

j

(cid:13)

-outliers, we have y

. Applying the Cherno(cid:11) bound

1

2

(cid:13)

(cid:21)

for y , we have

Then by assumption on T

, y

. The condition

(cid:11)

1

6

2

(1+Æ)

(cid:0)

that our samples show T

to have at least a (1 + Æ)

(cid:0)

-

(cid:11)

outlier is

. We apply the Cherno(cid:11)

X

i

2

2

1

1

m

x

T

i

(cid:11)

(1+Æ)

(cid:0)

2

(cid:20)

to determine the probability that (cid:22)x is not a good estimate

(cid:20)

2

2

Pr[

(cid:22)x

my

Æmy ]

e

bound,

P

j

(cid:0)

j (cid:21)

(cid:20)

2

Æ

my=3

(cid:0)

Pr[

(1 + (cid:1))m

[X

]]

e

X

i

i

E

(cid:0)

2

E

i

2

g

(cid:1)

m

[X

]=3

of all the half-spaces. Then by the same argument using the

w

T

(1+Æ)T ] Pr[x

(1+Æ)T ]

. Let H =

H

, the intersection

w

(cid:21)

(cid:20)

convexity of H and the spacing of the Æ

-grid, for an arbi-

0

X

T

2

T

2

where we have stated the Cherno(cid:11) bound for the case

trary w, max

(w

x)

: x

H

(1 + Æ) min

max

(w

x)

:

i

i

that (cid:1) < 1. Let (cid:1) =

1 (this yields the

x

H

. Our simultaneous lower bound on the expectation

f

2

g (cid:20)

f

1

E

[X

](1+Æ)

(cid:0)

i

2

2

(cid:0)

2

g

for an arbitrary w and upper bound on the maximum for

correct event in our probability calculation). If (cid:1) < 1, then

10

2

that same w yield that H is (1 + Æ)

(cid:0)

-outlier free.

(cid:1)

[X

]

, and the probability we do not correctly iden-

2

2

E

(cid:1)

m

[X

]=3

i

Using the same Æ

-grid reasoning, we (cid:12)nd that (1 + Æ)

T

0

2

12

2

tify the furthest outlier is at most e

= O(1) for

(cid:0)

contains H , and therefore (1 + Æ)

T is (1 + Æ)

(cid:0)

-outlier

2

Æ

E

i

2

(cid:0)

2

(cid:21)

2

m = O(

).

If (cid:1)

1, then (cid:1)

[X

]

, and the ap-

i

2

2

E

free.

(cid:0)

Æ

Æ

(cid:0)

(cid:21)

(cid:21)

plicable alternate form of the Cherno(cid:11) bound yields that

Pr[

X

i

m=(cid:13)

] is at most e

= O(1) for the

(cid:0)

2

(cid:1)m

[X

]=3

E

i

Before stating the corollary for the running time, we men-

tion that we still have not shown that step 2 of either algo-

Since there are only O(

) di(cid:11)erent values of (cid:11) to

done in our (cid:12)nal lemma of this section, lemma 8. Assuming

n

rithm can be carried out with high probability. This will be

same setting of m.

P

(cid:20)

b+log

(cid:15)

b+log

Æ

n

(cid:15)

2

(cid:0)

2

Æ

Æ

consider, m = O(

log

) allows us to union bound

lemma 8, we have the following bound on the running time.

over all the possible values of (cid:11). This shows that with con-

stant probability, if we estimate T to have no (cid:0)

-outliers (in

2

5

2

Corollary 1

(Running Time).

The algorithm runs in

which case our algorithm might return T ), then (1 + Æ)T has

no (1 + Æ)

(cid:0)

-outliers. This implies (ii).

8

2

time

O(

)

.

2

4

b

n

~

(cid:15)

Æ

2

2

2

bn

(cid:15)Æ

2

5

b

n

~

2

4

(cid:15)

Æ

We extend the analysis of lemmata 6 and 7 from a (cid:12)xed

Proof. We have from section 3 that (cid:12) = (cid:13)

is at most

direction to all directions and argue the correctness of the

O(bn=(cid:15)), and so we never need more than m =

O(

) sam-

~

~

entire algorithm by proving theorem 3.

ples. Plugging in this value for m to our bounds in the dis-

Proof. Let S be the ellipsoid found by the determinis-

that our entire algorithm runs in time

O(

), which is the

tic algorithm (i.e. the outlier free subset of points lies in

bound we referred to in the introduction. In this time we

this ellipsoid). Rather than considering the original space,

achieve a 1 + O(Æ) approximation to the optimal value of

cussion of running time at the beginning of section 4 yields

consider the transformed space where S (not E (M

)) is

S

the unit sphere. Consider the many directions w given

by a Æ

-grid over the sphere, Æ

=

. We form this grid

0

0

We now show that we can carry out step 2 of either algo-

Æ

n

by choosing every w such that the coordinates of w lie

rithm with high probability. Additionally, we solve the fol-

Æ

2Æ

n

n

in

0;

;

; : : : ; 1

. By our choice of m, we can apply

lowing problem. Suppose that we are not given the param-

f

g

n

n

2

lemma 7 part (i) to each of these (

)

directions simul-

Æ

eter (cid:13)

, but rather only (cid:15), and asked to (cid:12)nd the appropriate

taneously. We then have that for every w in the Æ

-grid,

0

(cid:13)

. Lemma 8 will show that we can at any point determine

T

2

T

2

max

(w

x)

: x

T

max

(w

x)

: x

S

(i.e., in this

within a factor of (1 + Æ) how much of the probability mass

f

2

g (cid:21)

f

2

g

direction T contains S ). We now show that for an arbitrary

direction w, (1 + Æ )

T contains S .

2

is within a (cid:12)xed ellipsoid. Since (cid:13)

[1;

O(

)], there are

bn

2

log

(cid:15)

~

bn

2

2

bn

~

(cid:15)

Consider an arbitrary unit vector w. For every axis di-

1+Æ

(cid:15)

Æ

at most log

O(

) =

values of (cid:13)

to consider (with

rection i, there are some vectors w

and w

in the Æ

-grid

0

i

i

1

2

a loss of at most a factor of (1 + Æ) in the value we (cid:12)nd for

that are above and below w, but within distance Æ=n. Since

(cid:13)

). Therefore we can simply try them all, estimating for

T = (cid:0)

E (

M

) is convex, the minimum distance of T from

2

(cid:22)

T

each one whether this (cid:13)

requires us to throw away more

the origin between w

and w

is given by 1

. Bounding

i

i

1

2

Æ

than a (1 + Æ)(cid:15) fraction of the distribution.

2

2

over the maximum decrease in every axis direction gives that

Thus, if the parameters ((cid:13)

; (cid:15)) are achievable for the de-

T is at least distance 1

Æ from the origin in direction w.

terministic algorithm, and we are only given (cid:15), we can (cid:12)nd

Since S is within 1 of the origin everywhere, we have that

(cid:0)

a region in space T

satifying parameters ((1 + O(Æ))(cid:13)

; (1 +

0

n

(cid:0)

2

(1 + Æ)T contains S , and therefore (1 + Æ)

T also contains

O(Æ))(cid:15)). Our asymptotic running time increases to

O(

+

2

2

5

b

n

~

2

4

(cid:15)

Æ

S . This concludes the proof of (i).

), which is an increase of no more than a constant.

Now consider (ii). For every w in our Æ

-grid, we have

0

8

2

that (1 + Æ)T is (1 + Æ)

(cid:0)

-outlier free along w by lemma

Lemma 8

(Probability Mass Location).

7 part (ii). Consider an arbitrary unit vector w not in the

(i) Fix a direction

. Let our number of samples be

w

m =

Æ

-grid. Denote w’s n nearest neighbors within the Æ

-grid

0

0

O(

)

2

, and consider the squared sample distances in direc-

by

w

. Let w

be the vector in direction w

with length

i

i

tion

given by

. Let

be the greatest

such that

w

(w

x

)

i

(cid:22)y

y

f

g

T

2

E

i

0

i

T

2

f

g

given by

[(w

x)

: x

(1 + Æ)T ] Pr[x

(1 + Æ)T ]. Then

a

fraction of our samples are above

. Then with

(1 + Æ)(cid:15)

y

w

de(cid:12)ned similarly is bounded away from the origin by

0

constant probability, at least an

fraction of the distribution

(cid:15)

2

2

the hyperplane formed by the

w

| this follows from the

has squared distance along

of at least

, and at most a

w

(cid:22)y

0

i

convexity of E (M

). Combining this and the spacing

(1+Æ)T

(1 + Æ)

(cid:15)

w

fraction has squared distance along

greater than

f

g

2

of the Æ

-grid as in the previous paragraph, we (cid:12)nd that the

(cid:22)y

.

0

(cid:12) .

2

2

1

3

(cid:15)Æ

1

(cid:15)Æ

maximum drop in moving to w from w

is at most (1

Æ),

i

(ii) Let

be an el lipsoid. Let our number of samples be

E

i.e.,

[(w

x)

: x

(1 + Æ)T ]

(1

Æ) min

[(w

x)

: x

m = O(

)

(1 + Æ)(cid:15)

. Then if we estimate a

fraction of our

i

i

E

E

T

2

T

2

(cid:0)

1

2

(cid:15)Æ

2

(cid:21)

(cid:0)

2

(1 + Æ)T ].

samples to be outside of

, with constant probability at most

E

Since (1+ Æ )T is (1+ Æ)

(cid:0)

-outlier free along w, (1 + Æ)T

a

fraction is outside of

, and at least an

fraction

(1 + Æ)

(cid:15)

E

(cid:15)

8

2

2

H

, where H

is the halfspace (really a slab) correspond-

w

w

is outside of or on

E

.

(cid:26)

ing to w, H

=

x : (w

x)

(1 + Æ)

(cid:0)

[(w

x)

: x

w

T

2

8

2

T

2

E

f

(cid:20)

2

Proof. First we show (i). Let y

=

y : Pr[(w

x

)

where we throw out the same points along every axis that

high

i

y ] = (cid:15)

. Let Y

be a random variable, Y

= 1 i(cid:11) (w

x

)

>

we threw out along axis i in S . Then we have (cid:15)

= n(cid:15)

. If

i

i

i

0

i

y

. The event that (cid:22)y > y

is the same as

(cid:15)=n, then S

contradicts the assumption that there was

Y

i

(cid:15)

i

high

high

0

g

m(1 + Æ )(cid:15). We can upper bound the probability of this event

no symmetric subset we could throw out achieving the same

T

2

f

(cid:21)

T

2

(cid:21)

(cid:20)

P

(cid:22)

using the Cherno(cid:11) bound

((cid:15);

(cid:12) ). If (cid:15)

> (cid:15)=n, then there is some other axis j such that

i

Pr[

m(1 + Æ)

[Y

]]

e

Y

i

i

(cid:22)

(cid:22)

ability distribution, but achieving

(cid:12)

(cid:12) along that axis

j

X

(cid:22)

E

(i.e. max

x

: x

S

[x

: x

S ] Pr[x

S ]). Con-

j

(cid:12)

j

2

j

(cid:20)

(cid:21)

(cid:20)

which is constant for m = O(

). A similar calculation

f

2

g (cid:20)

2

2

structing S

by taking S and replacing our choice of points

00

E

(cid:0)

2

E

Æ

m

[Y

]=3

i

1

2

(cid:15)Æ

T

2

2

along axis j we throw out an (cid:15)

< (cid:15)

fraction of the prob-

j

i

with y

=

y : Pr[(w

x

)

y ] = (1 + Æ )

(cid:15)

shows that

low

i

to throw out along axis i with the points thrown out along

the probability of (cid:22)y < y

is at most a constant for the

low

axis j then yields a contradiction because (cid:15)

< (cid:15). Thus we

00

f

(cid:21)

g

same value of m.

can restrict our attention to S symmetric.

The proof of (ii) is identical. By centering E , we can just

For any direction w along an axis, the pro jection onto w

consider distance from the origin of our samples rather than

of any point on the other n

1 axes is 0, so we obtain

distance along a (cid:12)xed w.

E

E

T

2

2

1

[(w

x)

] =

[x

; (cid:22) one-dimensional]

(cid:0)

n

One consequence of the theorems in this section is that

n

b

~

2

(cid:15)

a sample of size

O(

) is enough to estimate the inertial

We ignore the factor of n for the rest of the proof and re-

ellipsoid of any distribution (after removing at most an (cid:15)

strict our attention to a single coordinate axis. Suppose the

fraction) and thus bring it into nearly isotropic position.

furthest point kept by S achieving parameters ((cid:15);

(cid:12) ) is the

5. A MATCHING LOWER BOUND

point with exponent k. By our choice of distribution, we

cannot have thrown out more than half the points with a

1

2

2

factor, and so we have max

x

: x

S

=

, k > b

.

0

We show that for any (cid:15) < 1=2 there exists a distribution

f

2

g

(cid:22) such that, for any S satisfying (cid:22)(S )

1

(cid:15), there exists

Calculating the expectation

w such that

2

0

2

2b

2

1

2(cid:15)

(cid:21)

(cid:0)

p

(cid:15)

E

[x

: x

S ] Pr[x

S ]

(2

+ 2

+ : : : + 2

)+

2

2

(cid:20)

(cid:0)

2b

0

max

(w

x)

: x

S

(cid:12)

[(w

x)

: x

S ] Pr[x

S ]

T

2

T

2

(cid:22)

E

f

2

g (cid:21)

2

2

(cid:22)

2

k

(cid:15)

0

(cid:0)

(cid:22)

(cid:12)

E

T

2

[(w

x)

: x

S ]

(cid:21)

2

2

0

where

(cid:12) = (cid:10)(

(b

log

)). Based on the comparison be-

+

(cid:22)

n

1

(cid:15)

(cid:15)

(cid:0)

tween our upper and lower bounds on (cid:12) in the case that we

2(cid:15)

1

2b

2b

+2

2k

0

0

(2

+ 2

+ : : : + 2

)

2b

(cid:15)

0

2b

1

2k+1

2k+2

2

2

2

(cid:0)

(cid:20)

(cid:20)

2b

b

b

0

0

0

0

b

4(cid:15)

(cid:22)

(cid:21)

can’t throw out more than half the distribution

yields that

(cid:12)

for the one-dimensional case. Thus our

n

n

n

1

O

(b + log

)

vs: (cid:10)

(b

log

)

lower bound in the n-dimensional case is

(cid:15)

(cid:15)

(cid:18)

(cid:15)

(cid:15)

(cid:19)

(cid:0)

(cid:22)

(cid:16)

(cid:17)

(cid:12)

(b

log

)

n

1

(cid:21)

(cid:0)

8(cid:15)

(cid:15)

we describe our result as asymptotically optimal.

Fix n, (cid:15) and b

=

log

. Let (cid:22) be a copy of the fol-

0

b

1

1

2

4

(cid:15)

(cid:0)

lowing distribution along each axis. Let there be 2b

points

0

6. AN APPROXIMATION ALGORITHM

at distances

0

1

b

1

2

2

2

(cid:0)

2

; 2

; : : : ; 2

;

;

; : : :

0

(cid:0)

0

0

0

b

b

+1

2b

1

p

(cid:15)

p

(cid:15)

p

(cid:15)

We showed earlier in the paper that for any distribution

(cid:22), and any (cid:15) we can achieve (cid:12) = O(

(b+ log

)). A question

n

n

(cid:15)

(cid:15)

that naturally arises is how well we can do on a particular

distribution compared to the best possible on that particular

distribution. Formally, given (cid:22) and (cid:15), we seek S minimizing

and consider the distribution that places a (1

2(cid:15)) fraction

(cid:12) sub ject to the constraints that

of the probability mass uniformly on the (cid:12)rst b

points and

(i) (cid:22)(S )

1

(cid:15)

(cid:0)

0

a 2(cid:15) fraction uniformly on the remaining b

points. This

0

E

(ii)

w; max

(w

x)

: x

S

(cid:12)

[(w

x)

: x

S ]

(cid:21)

(cid:0)

T

2

T

2

distribution satisifes that the maximum bit length along an

This is really a bicriteria approximation problem with pa-

8

f

2

g (cid:20)

2

axis is log

= b.

0

2

b

2

p

(cid:15)

There are many ways of choosing a subset S of this distri-

the

probability distribution to be (cid:12) -outlier free.

normalized

bution, but we can quickly restrict the set of possible choices.

We exhibit a (

; 1)-approximation algorithm for this task

First we show that it never helps to treat the di(cid:11)erent axes

in the case that we are given the distribution explicitly. If

asymmetrically for a distribution that is concentrated on

we can only sample from the distribution (cid:22), our algorithm

the axes. Suppose that this statement is not true. We be-

yields a (

+ Æ; 1+ Æ) approximation for any constant Æ > 0

1

1

(cid:15)

(cid:0)

1

1

(cid:15)

(cid:0)

gin by noting that for a distribution concentrated on the

with high probability.

rameters ((cid:12) ; (cid:15)). Note that in this case, we are looking for

axes, w maximizing

always occurs on an

2

Lemma 9

(Preservation of Outliers).

(cid:22)

Let

be a dis-

max

(w

x)

:x

S

T

2

f

2

g

E

T

[(w

x)

]

axis. Let (cid:22)

be a distribution on which it is possible to

1

tribution. Any

-outlier for

is at least a

-outlier

(cid:12)

(cid:22)

(cid:12) (1

(cid:15))

throw out an (cid:15) fraction of the distribution and achieve pa-

with respect to any subset

satisfying

S

(cid:22)(S )

1

(cid:15)

.

(cid:0)

(cid:21)

(cid:0)

rameter

(cid:12) . Further suppose that this (cid:15) is the minimum (cid:15)

(cid:22)

such that this

(cid:12) is achievable, and the only S achieving

(cid:12)

Proof. Let x be a (cid:12) -outlier in the original distribution.

(cid:22)

(cid:22)

is asymmetric. Let axis i be the axis that the outlier oc-

Then for some w, (w

x)

> (cid:12)

[(w

x)

] For any S , we have

curs on, and suppose that along axis i we throw out an (cid:15)

[(w

x)

: x

S ] Pr[x

S ]

[(w

x)

] and so x satis(cid:12)es

fraction of the total distribution. Let S

be the subset of (cid:22)

(w

x)

> (cid:12) (1

(cid:15))

[(w

x)

: x

S ]

0

T

2

T

2

E

E

E

T

2

T

2

i

1

T

2

T

2

2

2

(cid:20)

E

(cid:0)

2

0

0

(cid:21)

(cid:0)

(cid:12)

OP T

1

(cid:15)

(cid:0)

The approximation algorithm is simply either algorithm

the proof of theorem 1 relied upon showing shrinkage of the

described in section 4, with error parameter Æ in the case

primal ellipsoid (growth of the dual ellipsoid), the algorithm

that we are sampling from (cid:22). We could determine the op-

that at each step translates the origin to the new mean con-

timal (cid:12) for a (cid:12)xed (cid:15) through a binary search. Suppose the

cludes at least as quickly, throwing out no more probability

value (cid:12)

is achievable by the restriction of (cid:22) to some

mass overall.

OP T

S satisfying (cid:22)(S )

1

(cid:15). Anytime our algorithm sees a

It may be that at some point our mean is not an element

point that is a (cid:12)

-outlier with respect to the unnormalized

of I

. We show that even in this case our dual volume

distribution, (cid:12)

>

, we know that this cannot be a

bound still holds. Suppose that at some step the criterion

(

(cid:12)

)-outlier under any restriction of (cid:22) by lemma 9. So

OP T

for step 2 of either algorithm is met for axis i, but with

(cid:20)

this point will have to be thrown out by the optimal solu-

the value of b doubled. Then we throw out all x with x

i

tion. Thus running our algorithm with (cid:12) =

forces

2

(where our mean has x

= 0). Not all the remaining

i

us to throw away no points that the optimal solution does

elements necessarily have x

= 0, but they do all necessarily

i

not also throw away. This yields that we achieve a

-

have the same x

value, because two elements of I

cannot

i

(cid:21)

n

b

(cid:12)

OP T

1

(cid:15)

(cid:0)

1

1

(cid:15)

(cid:0)

approximation in the case of an explicitly provided distri-

have distinct values for a particular coordinate that di(cid:11)er

bution. Correctness and running time are clear from the

by less than 2

. So the dimension does collapse. Our dual

2b

(cid:0)

preceding discussions.

volume bound only required the guarantee that in every non-

There is a more direct method that in fact (cid:12)nds an ap-

collapsed dimension at least an

fraction of the points have

(cid:15)

3n

2b

proximation to (cid:12) for

(cid:15) in one pass. The algorithms of

every

(cid:0)

coordinate value at least 2

. This concludes the proof of

n

b

2b

(cid:0)

section 2 can be used to de(cid:12)ne an

of a point

outlier ordering

corollary 2.

set, namely, the (cid:12)rst point that is an outlier, the second

point, etc. Now to approximate the best possible (cid:12) for a

particular value of (cid:15) we simply remove the initial (cid:15) fraction

8. REFERENCES

of the points in the outlier ordering.

[1] A. Blum, A. Frieze, R. Kannan and S. Vempala, \A

7. STANDARD DEVIATIONS FROM THE

pp35-52.

MEAN

[2] A. Blum, G. Konjevod, R. Ravi, and S. Vempala,

Polynomial-Time Algorithm for Learning Noisy Linear

Threshold Functions," In

, 22(1), 1999,

Algorithmica

We prove a variant of our theorem that shows we can (cid:12)nd

\Semi-De(cid:12)nite Relaxations for Minimum Bandwidth

a large subset of the original probability distribution where

and other Vertex-Ordering Problems," In

Proc. of the

no point is too many standard deviations away from the

30th ACM Symposium on the Theory of Computing

,

mean.

Dallas, 1998. To appear in Theoretical Computer

Science, special issue in honour of Manuel Blum.

Corollary 2

(Std. Deviations from the Mean).

[3] L. Lov(cid:19)asz, R. Kannan and M. Simonovits, \Random

Let

be a probability distribution on

. Let

be a re-

(cid:22)

I

S

walks and an O

(n

) volume algorithm for convex

5

(cid:3)

gion in space. Denote by

the probability that

cho-

(cid:22)(S )

x

bodies," In

11,

Random Structures and Algorithms

n

b

E

sen according to

is in

. Let

and

(cid:22)

S

(cid:22)x =

[x : x

S ]

pp1-50.

2

T

2

2

(cid:27)

=

[(w

(x

(cid:22)x))

: x

S ]

(cid:15) > 0

. Then for every

, there

w

E

exists

and

S

(cid:0)

2

n

n

(cid:12) = O

(b + log

)

(cid:15)

(cid:15)

(cid:16)

(cid:17)

[4] L. Lov(cid:19)asz, R. Kannan and M. Simonovits,

\Isoperimetric problems for convex bodies and a

localization lemma," In

Discrete Computational

Geometry

13, 1995, pp541-559.

such that

[5] R. Horn and C. Johnson,

, Cambridge

Matrix Analysis

(i)

(cid:22)(S )

1

(cid:15)

University Press, 1985, pp179.

(ii)

max

w

(x

(cid:22)x) : x

S

(cid:12)(cid:27)

w

for al l

w

p

(cid:21)

(cid:0)

T

n

f

(cid:0)

2

g (cid:20)

2 R

9.

IMPLEMENTATION

Proof. The proof of the corollary is much like the proof

Let

be an n

m matrix whose columns are the points

X

of theorem 1, but with two additional steps.

In the (cid:12)rst

of our distribution. Let

be the values for

m,beta,epsilon

(cid:2)

step, we show that translating (cid:22) so that the origin coincides

m; (cid:12) ; (cid:15), and let the boolean variable

indicate whether

done

with the mean does not increase the volume of the primal

we are (cid:12)nished removing outliers. In the case that

is full

X

inertial ellipsoid. In the second step, we show that running

dimensional throughout the algorithm (a common case), a

the algorithm with our value of b doubled suÆces to preserve

complete implementation is given by the following matlab

our dual volume bound. These steps taken together with our

code:

previous analysis imply corollary 2.

In order to analyze the volume of the primal inertial ellip-

soid, consider the radius r of the primal inertial ellipsoid in

a (cid:12)xed direction w. Implicitly taking all expectations with

respect to the restricted probability distribution (cid:22)

, and

S

j

done = 0

while(~done)

done = 1

M = X*X’/m

letting x

be the value of the pro jection of x onto w, we

w

Y = M^(-.5)*X %% Y is centered version of X

have r

=

[x

]. Suppose we choose to translate our ori-

for i = 1:m, %% remove current outliers

2

2

E

w

gin to a value z along w. We then have r

=

[(x

z)

].

w

2

2

E

if Y(:,i)’*Y(:,i) > beta,

X(:,i)=0, done = 0, end

Single variable calculus shows that the value minimizing r

(cid:0)

end

is z =

[x

], which is just the mean. Thus translating our

w

E

end

origin to (cid:22)x minimizes the radius of the primal inertial ellip-

soid in every direction simultaneously. Since our analysis in

Handling dimension dropping adds a few more lines of code.

