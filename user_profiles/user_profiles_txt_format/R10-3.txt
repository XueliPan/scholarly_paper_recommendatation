9th IEEE/ACM International Symposium on Cluster Computing and the Grid

MPISec I/O: Providing Data Conﬁdentiality

in MPI-I/O∗

Ramya Prabhakar, Christina Patrick and Mahmut Kandemir

Department of CSE, Pennsylvania State University, University Park, PA – 16802

{rap244, patrick, kandemir}@cse.psu.edu

Abstract—Applications performing scientiﬁc computations or
processing streaming media beneﬁt from parallel I/O signiﬁ-
cantly, as they operate on large data sets that require large
I/O. MPI-I/O is a commonly used library interface in paral-
lel applications to perform I/O efﬁciently. Optimizations like
collective-I/O embedded in MPI-I/O allow multiple processes
executing in parallel to perform I/O by merging requests of other
processes and sharing them later. In such a scenario, preserving
conﬁdentiality of disk-resident data from unauthorized accesses
by processes without signiﬁcantly impacting performance of the
application is a challenging task. In this paper, we evaluate
the impact of ensuring data-conﬁdentiality in MPI-I/O on the
performance of parallel applications and provide an enhanced
interface, called MPISec I/O, which brings an average overhead
of only 5.77% over MPI-I/O in the best case, and about 7.82%
in the average case.

I. INTRODUCTION

In the recent past, complexity of large scale applications
in science and engineering has grown dramatically. Emerging
scientiﬁc applications require fast access to large quantities
of data. Applications envisioned for Tera-GRID systems or
that are executed on large clusters, are typically parallel
applications using MPI as message passing environment [7].
Most of the data sets processed by these large scientiﬁc
and engineering applications are disk resident and therefore,
efﬁcient I/O access mechanisms are crucial for achieving high
performance in these applications. Several I/O libraries [11],
[34], [31], [26] and parallel ﬁle systems [33], [8], [1] allow
application programmers to express parallel I/O operations in
their programs, thereby helping them to improve productivity
as well as I/O performance signiﬁcantly. Implementations of
MPI I/O [11], that are a part of the MPI standard [2] are used
extensively in the high performance computing domain. For
example, ROMIO [35], one of the implementations of MPI
I/O runs on many parallel machines and on top of many ﬁle
systems including Parallel Virtual File System (PVFS) [34],
[1].

Parallel computations that utilize resources spanning several
administrative domains introduce the need to establish security
relationships among processes [16]. Preserving conﬁdentiality
of disk-resident data from unauthorized accesses by pro-
cesses without signiﬁcantly impacting the performance of
applications, is an important step towards establishing such
security relationships. Currently, MPI I/O and other parallel

*This work is supported in part by NSF grants 0833126, 0720749, 0724599,

0621402 and 0444158.

I/O libraries do not directly support any mechanism to preserve
conﬁdentiality of disk-resident data. This makes it difﬁcult to
cater to the security requirements of the applications. While it
is possible for an application programmer to ensure security
by employing different customized approaches, this typically
makes the resulting code difﬁcult to maintain and hard to port
to other systems. Also, such programmer-supplied security
enhancements/solutions can be error-prone and hurt program
performance. Additionally, we ﬁnd that security technologies
are used only if they are incorporated in tools in a seamless
fashion. Therefore, an automated runtime system support for
providing conﬁdentiality of disk-resident data in systems that
widely use MPI I/O can be very useful in practice.

In this paper, we propose an application level solution
that provides a secure interface to MPI I/O, called MPISec
I/O, which combines the advantages of the library based
parallel I/O and preserves data conﬁdentiality with minimal
runtime performance overhead. Speciﬁcally, MPISec I/O en-
hances MPI I/O by encapsulating data encryption in a user
transparent manner. This provides several advantages, which
can be summarized as follows:

• It enables application programmers to write MPI I/O
programs that manipulate encrypted disk-resident data with
minimal changes to the interface. In fact, as we illustrate in
this paper, a conventional MPI I/O program can be converted
to an equivalent secure version (i.e., the corresponding MPISec
I/O program) with minimal addition to the code.

• It supports select portions of data to be encrypted, each
with possibly a different encryption algorithm and key. Such
a feature allows a ﬁne degree of control over how the security
mechanisms are employed. In particular, a single application
can mix secure I/O operations by using different encryption
algorithms and keys as appropriate,
thereby allowing the
programmer to strike a balance between performance and
conﬁdentiality constraints.

• By employing a key management infrastructure, we allow
a parallel program to access the encrypted disk-resident data
using appropriate keys even though the program accesses the
ﬁle in different patterns in different parts of the code. This
ﬂexibility is important as it also allows different applications
to share the same (encrypted) disk-resident data sets.

We implemented our proposed MPISec I/O by modifying
ROMIO implementation of MPI I/O using PVFS2 ﬁlesystem.
Experiments using four different I/O-intensive applications
indicate that integrating conﬁdentiality using MPISec I/O leads
to an average overhead of 5.77% in the best case, when

978-0-7695-3622-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CCGRID.2009.53

388

(cid:9)(cid:10)(cid:10)

(cid:0)

(cid:1)(cid:1)

(cid:3)(cid:8)

(cid:1)(cid:2) (cid:3)

(cid:1)(cid:1)

(cid:3)(cid:8)

(cid:6)(cid:11)(cid:11) (cid:7)(cid:4)

(cid:6)

(cid:4)(cid:5)(cid:6) (cid:7)(cid:4)

(cid:6)

P0

P2

P1

P3

P0

P1

P2

P3

(cid:19)

(cid:1)

(cid:4)

(cid:4)

(cid:19)

(cid:14)

(cid:1) (cid:3)(cid:14)(cid:20)

(cid:1) (cid:14)(cid:2)(cid:8)

(cid:11)

(cid:21)

(cid:18)(cid:22)

(cid:1) (cid:14)(cid:23)(cid:14)

(cid:1)

(cid:19)

(cid:24)

(cid:6)(cid:25)

(cid:4)

(cid:4)

(cid:9)(cid:10)(cid:10)

(cid:6)(cid:11)(cid:11)

P0

P1

P2

P3

(cid:12)

(cid:10)

(cid:2) (cid:13)(cid:13)

(cid:1) (cid:14)

(cid:6)

(cid:6) (cid:16)

(cid:15)

(cid:17)(cid:18)

Fig. 1. Collective I/O.

encryption map is the same as ﬁle view and 7.82% in the
average case, when encryption map is different from ﬁle view.

II. OVERVIEW OF COLLECTIVE I/O

Collective I/O is one of the several optimizations in MPI I/O
to improve the overall performance. Collective I/O allows each
processor to do I/O on behalf of other processors effectively
and later exchange data by communicating with each other,
to optimize data access and to boost I/O performance. In this
section, we explain collective I/O, as we later demonstrate our
secure interface using collective I/O calls of MPI I/O.

Collective I/O is implemented such that I/O is performed
in two phases: an I/O phase and a communication phase [35].
In the I/O phase, processors perform I/O in a way that is
most beneﬁcial from the data layout point of view. That is,
taking into account the disk layout of data, they perform I/O
such that minimum number of I/O calls are issued to the disk.
In the communication phase, they engage in a many-to-many
type of communication to ensure that every process gets its
share of data. Collective I/O generally leads to improvements
in I/O latencies in spite of the extra time spent in performing
inter-processor communication, which is, in most cases, small
compared to I/O latencies.

Collective I/O is mostly used when the storage pattern
and the access pattern are different. Because in such cases,
allowing each processor to perform independent I/O will cause
processors to issue many I/O requests, each for a small amount
of consecutive data. As illustrated in Figure 1, when the
storage and access patterns are different, if the processors
perform independent parallel I/O, then every processor will
have to issue four I/O requests, each for small amount of
consecutive data, resulting in sixteen I/O requests. However,
if collective I/O is used in such a scenario the large number
of small I/O requests are reduced to small number of I/O
requests, each requesting large amounts of contiguous data.
Thus, the total number of I/O requests reduces to four as
shown in the ﬁgure. In collective I/O, processors perform
optimized data access in the I/O phase and then communicate
with each other to exchange data in the communication
phase. The basic collective I/O routines supported by MPI
I/O are: MPI File set view (fh, disp, etype, ﬁletype, datarep,

info), MPI File read all (fh, buf, count, datatype, status),
MPI File write all (fh, buf, count, datatype, status).

MPI File set view() is a collective routine that changes the
process’s view of the data in the ﬁle. The start of the view is
set to disp; the type of data is set to etype; the distribution
of data to processes is set to ﬁletype; and the representation
of data in the ﬁle is set to datarep. The collective read and
write routines, MPI File read all and MPI File write all, use
individual ﬁle pointers to perform read and write operations
and the ﬁle pointer is updated relative to the current view
of the ﬁle. Collective I/O optimizations provide impressive
performance beneﬁts in MPI I/O system, as demonstrated by
prior studies [35]. In this paper, we present how MPI I/O can
be enhanced by a secure interface called MPISec I/O. While
our design applies to all routines in MPI I/O, our focus in this
work is on collective I/O routines.

III. DESCRIPTION OF MPISEC I/O

A. Problem Statement

Our goal in the design of MPISec I/O interface and the
corresponding implementation is to obtain a security enhanced
version of the MPI I/O that protects the conﬁdentiality of
disk resident data operated upon using MPI I/O. That is,
processes that are trusted should be able to access appropriate
keys to correctly decrypt the data encrypted by other trusted
processes, while untrusted processes should not be able to
access keys. In our context, processes that are spawned in
a controlled environment (i.e., processes that are a part of
the communication group1) are trusted and processes that
are not a part of this group are not trusted. And, processes
should be given a ﬁne degree of control over how encryption
is performed, by means of a ﬂexible interface that allows
specifying different encryption algorithms, key sizes and block
sizes, so that programmers are able to strike a balance between
performance and conﬁdentiality constraints.

Parallel I/O operations performed using MPI I/O require
multiple resources that may span several administrative do-
mains (e.g. grid environment). There is a need to establish
data conﬁdentiality among processes that collectively operate
in grid environments. Protecting conﬁdentiality of data in a
multiprocess environment is important because unauthorized
accesses to other processes’ data could lead to potential
threats, especially if some processes are operating on sensitive
portion of the ﬁle. Therefore, there is a need for an application
level solution that supports a ﬂexible interface to specify
different access control policies. The ﬂexibility could be in
terms of specifying the encryption algorithm to be used or the
key size and block size to be used for encryption. To the best of
our knowledge, this is the ﬁrst paper that proposes a solution
providing such a ﬂexibility and ﬁne control over regulating
accesses at the data block level for applications using MPI
I/O.

There are many challenges involved in allowing applications
to specify different encryption mechanisms (algorithms and
keys). Firstly, we have to ensure that a given region of the

1In MPI I/O, communication group represents an ordered set of process

identiﬁers that speciﬁes the processes that operate collectively on the ﬁle.

389

,

,

,

,

)+((cid:27)%%

)+((cid:27)%%

)+((cid:27)%%

)+((cid:27)%%

-

0

.

/

C. Underlying Concepts

In order to achieve seamless integration of security enhance-
ments with existing MPI I/O, we maintain information about
access permissions to various parts of the ﬁle to processes at
the same granularity as MPI I/O. We handle different regions
in the encryption map in terms of displacement, ﬁletype and
etype.

’

"

$

(cid:31)( )

+(cid:31)

(cid:28) *

(cid:26)

(cid:27)

(cid:28)

(cid:29)

(cid:30)

*

(cid:29)

"

(cid:30)(cid:31)(cid:30) (cid:27)!(cid:27) (cid:31)

3

4

$

(cid:27)

(AB

)(cid:27)

=

<

=

@

<

56

(cid:26)

9

9

(cid:27)(& 7

(cid:27)

%

(cid:28)

(cid:29),1

2

(cid:27)(

;

;

:

:

8

?

>

,C D

2

#

$

#

$

#

$

%&

%&

%&

Fig. 2. Architecture of MPISec I/O.

etype

Process 0 filetype

Process 1 filetype

Process 2 filetype

Process 0 File view

Process 1 File view

Process 2 File view

Fig. 3.
encryption map is described using similar constructs.

Various components of a ﬁle view description in MPI I/O. An

ﬁle is encrypted only once using the parameters (encryption
algorithm, key size and block size) speciﬁed by the processes.
Secondly, as the number of processes operating on the ﬁle
changes (either increases or decreases), there is need for a
non-volatile data structure for managing the encrypted regions.
In subsequent subsections, we describe the MPISec I/O ar-
chitecture and the underlying concepts used to address these
challenges.

B. Our Approach

MPISec I/O ensures conﬁdentiality of data in MPI I/O by
incorporating the following modiﬁcations to traditional MPI
I/O: Firstly, all data access routines implementing MPISec I/O
interface implicitly operate on encrypted data, i.e., all routines
that read data from ﬁles implicitly decrypt data that is read
and all routines that write data to ﬁles implicitly encrypt the
data being written. Secondly, a comprehensive infrastructure,
called encryption map manages information about the access
rights of various processes to access different parts of the ﬁle
(essentially an access matrix) and information about encryp-
tion algorithms (potentially different) and meta data of keys
(labels) used for encryption of data and lastly, managing the
encryption keys and differential access restrictions to different
processes that are part of the communication group using a key
management infrastructure, that is indexed by a key label and
a unique identiﬁer representing each of the involved processes
in the communicator group to grant access to authorized
processes for their respective encryption keys.

This architecture of MPISec I/O is described in Figure 2.
The concepts of encryption map and key management infras-
tructure are new to MPISec I/O and require some elaboration.
These are discussed in detail in the following section along
with some of the essential concepts of MPI I/O.

Fig. 4. Best case where encryption map is the same as ﬁle view.

Process File view

Process 
Encryption map

Process File view

Process 
Encryption map

P0

E0, K0

P0

E0, K0

Fig. 5. Generic case where encryption map is different from ﬁle view.

A ﬁle displacement is an absolute byte position relative to
the beginning of a ﬁle where a view begins. In the example
MPI I/O code shown in Figure 6 (a), displacement is the
second parameter to MPI File set view() and is equal to 0
in this particular case. Filetype describes a data access pattern
that can be repeated throughout the ﬁle or part of the ﬁle.
A process can only access the ﬁle data that matches items
in its ﬁle type. Data in areas not described by a process’
ﬁle type (holes) can be accessed by other processes that use
complementary ﬁle types as shown in Figure 3. The example
code shown in Figure 6 (a) has a simple ﬁletype equal to
MPI INT as the fourth parameter to MPI File set view(). A
ﬁle type contains an integral number of etypes. This allows
offsets into a ﬁle to be expressed in etypes rather than bytes
which helps portability. The example code shown in Figure 6
(a) has etype also equal to MPI INT as the third parameter to
MPI File set view().

1) Encryption Map: An encryption map deﬁnes what part
of a ﬁle a task can encrypt (decrypt) and also how it should be
encrypted (decrypted). More speciﬁcally, for each region of the
ﬁle speciﬁed by the triplet {displacement, f iletype, etype},
the encryption map stores information about access rights
of each participating process in the communicator group, in
terms of the key labels that are accessible to it in order to
encrypt/decrypt data. Note that, this approach is scalable as the
encryption map operates at a coarser granularity of regions of
the ﬁle rather than blocks/etypes of the ﬁle. MPISec I/O pro-
vides the essential interface to set the encryption map, namely,
M P I F ile set encryption map(). The details about usage

390

{

}

MPI_File fh;

MPI_File fh;

MPI_File_open (MPI_COMM_WORLD, "pvfs2:/mnt/pvfs2/example1.dat", 

MPI_File_open (MPI_COMM_WORLD, "pvfs2:/mnt/pvfs2/example1.dat", 

MPI_MODE_CREATE |  MPI_MODE_RDWR, 
MPI_INFO_NULL, &fh);

MPI_MODE_CREATE |  MPI_MODE_RDWR, 
MPI_INFO_NULL, &fh);

MPI_File_set_view (fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL);

MPI_File_set_view (fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL);

MPI_File_write_all (fh, buf, FILE_SIZE, MPI_INT, &status);

MPI_File_write_all (fh, buf, FILE_SIZE, MPI_INT, &status);

MPI_File_set_encrypt_map (fh, 0, MPI_INT, MPI_INT, "native", 

MPI_INFO_NULL);

MPI_File_read_all (fh, buf, length, MPI_INT, &status);
MPI_File_close (&fh);

MPI_File_read_all (fh, buf, length, MPI_INT, &status);
MPI_File_close (&fh);

{

}

(a) An example MPI I/O code.

(b) The corresponding MPISec I/O code.

Fig. 6. Running example of a simple code showing usage of MPI File set encrypt map interface. (a) Conventional MPI I/O without any encryption. (b)
I/O using the MPISec I/O interface. The only change in the interface is the addition of the MPI File set encrypt map call.

of this method and its parameters are discussed in Section
III-D. On each reference to a region of the ﬁle by a process, the
encryption map is used to perform a reference check. The ref-
erence check involves checking whether the requested region
of the ﬁle is accessible to the process. The process supplies
its credentials in terms of a unique identiﬁer (received during
the creation of communication group) and the key label. Once
this reference check is successful with the encryption map,
the process uses this key label to obtain the corresponding
key from the key management infrastructure as depicted in
Figure 2.

In the most general sense, there are two possible scenarios in
which an encryption map is used. When encryption map is the
same as ﬁle view, i.e., encryption region boundaries coincide
with ﬁle view boundaries for each process, as depicted in
Figure 4. This is the best case where key management is fairly
straightforward. Processes need not request for keys from other
processes and hence the potential overhead of exchanging keys
is eliminated. When encryption map is different from ﬁle view,
i.e., encryption region boundaries of a process may overlap
across boundaries of ﬁle types of multiple processes, as shown
in Figure 5. This allows a ﬂexible scheme of representing en-
cryption maps independent of ﬁle view. However, in this case,
when a process performs I/O on an element that is not part of
its encryption map, key exchange overheads are incurred. In
our implementation, key management is performed using the
key management infrastructure explained below.

2) Key Management Infrastructure: To ensure secure key
management, storage and distribution, we use a Symmetric
Key Management Infrastructure [6], [27]. It consists of a Key
Management Server (KMS) that has two main components:
Encryption Engine and Secure Key Storage. The KMS uses
the encryption engine to encrypt the keys before storing on the
secure storage. All keys stored on the server are encrypted. The
KMS also maintains information about the key size, key type
and key lifetime, as in [4]. Figure 7 describes such a scheme.
KMS is responsible for generating, escrowing and distributing
keys based on clients request.

The clients running applications performing MPI I/O in-
teract with the KMS using Key Management Interface (KMI).

The client applications typically make a request for a symmet-
ric key using KMI. That is, using KMI, the clients communi-
cate with the key server to either request a new key or retrieve
the stored keys. The key management server, upon receiving
such a request, veriﬁes the client and supplies the required
keys over a secure network as shown in Figure 7. KMS
guarantees that a malicious user cannot gain unauthorized
access to resources (i.e., to encryption keys) [3].

Our KMI is essentially a remote protected table-based
service indexed by a key label and a unique identiﬁer that
identiﬁes the process requesting for the key. Upon request, it
services the authorized processes with keys corresponding to
the key labels provided.

D. Usage

A typical use of collective I/O, as illustrated in Figure 6 (a),

involves the following steps:

• Open ﬁle with M P I F ile open(). M P I F ile open()
is a collective operation which requires all processes to
specify the same communicator group, access mode and
f ilename arguments. It returns a f ilehandle to be used in
the subsequent I/O calls.

• Set the ﬁle view with M P I F ile set view() in terms of
displacement, f iletype and etype as parameters to identify
a unique view of the ﬁle.

• Perform I/O using one of the many available data
for example, M P I F ile write all() or

access routines,
M P I F ile read all().

• Close the ﬁle with M P I F ile close().
From an application programmer’s perspective,

in or-
to perform I/O securely using the MPISec I/O in-
der
the only necessary change is to appropriately in-
terface,
to M P I F ile set encryption map() be-
troduce a call
rou-
fore performing disk I/O using MPI File write all
tine. M P I F ile set encryption map() takes f ile handle,
displacement, etype, f iletype as parameters to identify
a unique partition of the ﬁle. It also takes two additional
parameters, namely, data representation and MPI info. The
data representation argument
is a string that speciﬁes the

391

Client 1

Client 2

Client n

MPI Application

MPI Application

MPI Application

.....

KMI Interface

KMI Interface

KMI Interface

10123

{start, end, algo, key label}

55691

{start, end, algo, key label}

73268

{start, end, algo, key label}

88324

{start, end, algo, key label}

user

10123

55691

73268

88324

User

User

User

10123

10123

10123

.

.

.

.

.

.

start

end

algo

User

10123

.

.

key 
label

Fig. 8.
attributes of PVFS.

Representation of encryption map using hierarchical extended

decrypted at the time of reading it from the disk. After the
data is exchanged between the processes and each process
owns exactly the data requested, each process will decrypt its
own data. Every read call is therefore modiﬁed to behave in
this manner:

Read ⇒ Communicate ⇒ Decrypt

MPI File write all: In a typical collective write operation
such as MPI File write all,
the processes involved in the
communicator group perform encryption of the data to be
written independently on their data before communication.
Encrypted data is further communicated appropriately among
the processes, so as to perform effective collective I/O which is
further written to the disk by corresponding processes. Every
write call is therefore modiﬁed to behave in this manner:

Encrypt ⇒ Communicate ⇒ Write

Viewing a ﬁle as a 2D object, as shown in Figure 9(a),
helps us to better understand the relationship between access
patterns and encryption map and its impact on the performance
of MPISec I/O. Three possible scenarios of application access
patterns are illustrated in Figures 9 (b), (c) and (d). In all cases,
encryption map is assumed to be row-wise. Figure 9 (b) depicts
an access pattern which is row wise. Since in this case the
access pattern and encryption map match exactly, this access
pattern has the least associated overheads. Figure 9 (c) shows
a column wise data access pattern of applications. Note that, in
this case, the access pattern is different from the organization
of the encryption map (which is assumed to be row wise),
leading to signiﬁcantly more entries of the encryption map
being consulted per access. Figure 9 (d) depicts a block access
pattern of two-dimensional data sets. As in the previous case,
the access pattern is different from the organization of the
encryption map, leading to more entries of the encryption map
being consulted per access than in the row wise access but
the parts of the accesses which are row wise do not incur this
additional overhead. Therefore, this represents an intermediate
case between the row access and column access in terms of
the overheads to be incurred during the collective call. To sum
up, the overheads involved in our scheme depends strongly on
the data access pattern and encryption map.

Encryption

Secure Key

Engine

Storage

Key Management Server

Fig. 7. Key Management Infrastructure.

representation of data in the ﬁle. The info argument
is
used to provide information regarding ﬁle access patterns
and ﬁle system speciﬁcs to direct optimization. Note that,
in M P I F ile set encryption map(), the ﬁle displacement
parameter is always 0. This enforces the property that encryp-
tion map would consist of partitions of the ﬁle associated with
different processes that make this call with complementary
ﬁletypes, thereby ensuring that each partition is encrypted only
once. A sample code that uses set encrypt map interface is
shown in Figure 6 (b).

IV. IMPLEMENTATION DETAILS

Implementation of MPISec I/O consists of implementing
storage, retrieval and updating of encryption map apart from
appropriate encryption and decryption of data through I/O
calls. The original MPI I/O implementation we target, runs
on top of PVFS2 [1]. We have implemented storage of
encryption map using extended attributes available as part
of the underlying PVFS ﬁle system. Extended attributes on
PVFS is a ﬁle system feature that enables users to associate
ﬁles with metadata that is not interpreted by the ﬁlesystem
but can be used to store additional ﬁle attributes as key–value
pairs. An encryption map can be composed using a set of
extended attributes. Each constituent attribute of an entry of
the encryption map is under a unique preﬁx of the attribute
identiﬁed by a unique identiﬁer for the process corresponding
to the entry as represented in Figure 8.

Once the encryption map is available to the system,
I/O calls need to be instrumented to seamlessly use the
encryption map in a transparent manner to the application.
We present below the implemented instrumentations of two
representative I/O calls, namely, MPI File read all and
MPI File write all, though our approach can also be used
with other parallel I/O calls as well.

MPI File read all: In a typical collective read operation
such as MPI File read all,
the processes involved in the
communicator group cooperatively perform read on data that
is present in the encrypted form on the disk. The data is not

392

(a)

(b)

(c)

(d)

Fig. 9. Possible scenarios of access patterns of a ﬁle viewed as a 2D object as in (a). (b) Row wise access. (c) Column wise access. (d) Block access.

TABLE I

EXPERIMENTAL SETUP.

Cluster Information

Number of Nodes
Total Number of CPUs
CPU Type
Main Memory
Interconnect
Network
Operating System

16
32
Dual 1.733 GHz AMD Athlon
1 GB
Full-duplex 2 Gb/sec link
1 Gb/sec ethernet
RedHat Linux 7.3, Version 2.6.17

V. EXPERIMENTAL EVALUATION

A. Experimental Setup

We evaluate the performance and throughput of our parallel
applications on a system whose conﬁguration is described in
this section. In our experiments, ﬁles are striped across all
I/O nodes and all the computations in our applications are
striped across all the compute nodes. Detailed conﬁguration
information of this cluster is presented in Table I. The 16 node
cluster is conﬁgured with PVFS2 ﬁle system version 2.6.3.
One of the nodes (Node0 of the cluster) is conﬁgured to run
the metaserver and the remaining nodes are conﬁgured as both
I/O server and client. For all of our tests, we used the default
stripe size of 16,384 bytes. We modiﬁed ROMIO (part of
MPICH2 version 2-1.0.5) to incorporate the set encrypt map.
Both AES [28] and DES [12] encryption algorithms were used
to implement necessary encryption for all I/O calls.

In order to measure the performance impact of our approach
in practice, we used four parallel HPC applications. The
applications we experimented with includes MPI-tile-IO and
Noncontig from the Parallel I/O benchmarking consortium
[15], an out-of-core matrix multiplication from MPI-SIM
software benchmarks [30], and a synthetic kernel benchmark
from Los Alamos National Lab, called MPI-IO Test [25],
which performs row wise aggregation on a very large ﬁle.
The total data set sizes manipulated by these applications vary
between 1GB and 8GB.

B. Performance Implication

Various design parameters of the encryption map in MPISec
I/O have a bearing on the performance of applications. In this
section, we analyze the impact of each of these parameters by
backing our analysis with relevant experimental results.

1) Relationship between Encryption Map and File View:
As explained earlier in Section III-C1, encryption map is
in general independent of the ﬁle view. However, there are

distinct beneﬁts from setting the encryption map to be the
same as the ﬁle view. One advantage is concerned with the
efﬁciency of key management. Since all reads/writes from a
given process happen within its ﬁle view, every MPISec I/O
call needs to acquire only one key in order to decrypt/encrypt
the data. This reduces the overheads involved in acquiring
multiple keys, as may be necessary, when the encryption
map differs from the view. Also, note that encryption map
is not volatile as in the case with the ﬁle view. Therefore, the
encryption map can be set to the ﬁle view which prevails over
the largest fraction of program execution time.

Figure 10 (a) presents the overheads incurred by all our
applications with the encryption map being the same as the ﬁle
view of the processes. The encryption algorithm used is AES,
with 128 bit keys. We see from this ﬁgure that the overheads
incurred due to encryption decrease with an increase in the
number of participating processes. This is because, encryption
overheads are also distributed among a higher number of
processes, effectively parallelizing the overheads brought by
encryption. We note that with 16 processes participating in the
parallel execution of the programs, the overhead incurred by
MPISec I/O is as less as an average of 5.77%.

Figure 10 (b) depicts the overheads incurred by the same
applications with the encryption map being different from the
ﬁle view of the processes in each case. It can be clearly
noticed that overheads incurred in this case are larger than the
case where the encryption map is the same as the ﬁle view.
However, with increasing number of processes, the overheads
incurred by the applications even in these sub optimal cases
are not very high. Average overhead of 7.82% was observed
across applications when each of them speciﬁed an encryption
map which is different from the ﬁle view.

2) Sensitivity to Encryption Algorithm: Encryption map is a
ﬂexible scheme capable of specifying different algorithms and
keys for encrypting different regions of a ﬁle. Due to variations
in performance characteristics of individual encryption algo-
rithms, performance of MPISec I/O is sensitive to the speciﬁc
algorithm chosen. Therefore, we conducted experiments with
two popular encryption algorithms, namely, AES and Triple-
DES. The executions overheads observed for our benchmark
applications using AES and Triple-DES over the default case
of plain MPI I/O (without encryption) are plotted in Figure
11 (a). It is clearly seen that the overheads associated with
Triple-DES are signiﬁcantly greater than AES. On an average,
execution time increase of 4.95% was observed in case of AES

393

Overheads

I/O Time

Overheads

I/O Time

s
c
o
r
P
2

 

s
c
o
r
P
4

 

s
c
o
r
P
8

 

s
c
o
r
P
6
1

 

s
c
o
r
P
2

 

s
c
o
r
P
4

 

s
c
o
r
P
8

 

s
c
o
r
P
6
1

 

s
c
o
r
P
2

 

s
c
o
r
P
4

 

s
c
o
r
P
8

 

s
c
o
r
P
6
1

 

s
c
o
r
P
2

 

s
c
o
r
P
4

 

s
c
o
r
P
8

 

s
c
o
r
P
6
1

 

s
c
o
r
P
2

 

s
c
o
r
P
4

 

s
c
o
r
P
8

 

s
c
o
r
P
6
1

 

s
c
o
r
P
2

 

s
c
o
r
P
4

 

s
c
o
r
P
8

 

s
c
o
r
P
6
1

 

s
c
o
r
P
2

 

s
c
o
r
P
4

 

s
c
o
r
P
8

 

s
c
o
r
P
6
1

 

s
c
o
r
P
2

 

s
c
o
r
P
4

 

s
c
o
r
P
8

 

s
c
o
r
P
6
1

 

mxm

mpi-tile-io

synthetic

noncontig

mxm

mpi-tile-io

synthetic

noncontig

(a) Normalized execution overheads with the
encryption map same as the ﬁle view.

(b) Normalized execution overheads with the
encryption map different from the ﬁle view.

Fig. 10.

Impact of setting the encryption map different from the process’s view of the ﬁle on the overheads incurred by applications.

l

JKG

p q

(cid:153)

wrx(cid:159)

xr(cid:159)t

Q

T S

Ym

N

OT

W

V

W

n o

U

o

U

(cid:130) (cid:129)

(cid:135)(cid:154)

|

}(cid:130)

(cid:133)

(cid:132)

(cid:133)

(cid:155)

(cid:157)

(cid:128)(cid:156) (cid:131) }

(cid:129) (cid:158)

(cid:155)

(cid:157)

(cid:128)(cid:156) (cid:131) }

(cid:129) (cid:158)

(cid:127)

(cid:155)

(cid:157)

(cid:128)(cid:156) (cid:131) }

(cid:129) (cid:158)

(cid:155)

(cid:157)

(cid:128)(cid:156) (cid:131) }

(cid:129) (cid:158)

(cid:159)rv(cid:160)

t wvx

i

e
m
T
d
e
z

 

i
l

a
m
r
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

c

JFJH

^

JFKH

JFKE

JFJE

JFEH

JFEE

a

k

j

\

a

i

h

g

c

f

e

d

c

b

a

‘

^

]

\

_

E FIH

E FGH

E FGE

[

E FIE

r svr

L

L

M

LNO

ORS

OT

U

V W

S

OY

W

W

W

T

YT

OZ

Q

QX

Q

Q

P

P

P

z

z

{

z|}

}(cid:128)(cid:129)

}(cid:130)

(cid:131)

(cid:132) (cid:133)

(cid:129)

}(cid:135)

(cid:133)

(cid:133)

(cid:133)

(cid:130)

(cid:135)(cid:130)

}(cid:136)

~(cid:127)

~

(cid:127)(cid:134)

(cid:127)

~

(cid:127)

(a) Normalized execution times of applications
with different encryption algorithms.

(b) Normalized execution times of applications
with varying encryption block sizes.

Fig. 11. Sensitivity analysis

and 9.68% in case of Triple-DES.

3) Sensitivity to Encryption Block Size: An important de-
sign parameter that has an impact on the performance of our
MPISec I/O scheme is the granularity of block sizes at which
encryption or decryption occurs. Note that, in general, encryp-
tion or decryption in MPISec I/O occurs at ﬁxed encryption
block sizes although it does not impose any restriction on the
sizes of data that can be read or written using standard MPI
I/O calls. However, in order to recover data correctly from
the encrypted form, it is necessary to ensure that the size
of the block being decrypted is the same as the size of the
block which was encrypted. For example, if our encryption
block size is 512 bytes and one block was written in the
encrypted form at offset–A in a ﬁle, and a read call requests
for 256 bytes data from offset–A from the same ﬁle, it is
necessary to decrypt the entire 512 bytes (which have been
encrypted as a block) starting from offset–A and select only
the requested data out of this decrypted block. However, ﬁxing
this parameter has a trade-off associated with it. On one
hand, it is beneﬁcial to use the largest block size that is
compatible with (completely divides) the sizes speciﬁed by
most read/write calls, due to reduced overheads by reducing
the number of encryptions/decryptions. On the other hand, a

large block size would also mean reads and writes of sizes,
different from integral multiples of the block size, and that
could incur a large cost in terms of encrypting/decrypting data
for the sake of correctness, which is not being written or read
by the actual call.

Figure 11 (b) plots the impact of varying encryption block
sizes on the execution time of benchmark applications. It can
be seen that the difference between the optimal encryption
block size for an application and a sub optimal one can be
as large as 20% execution time. Therefore, it is important to
tune this parameter based on application and data set sizes
in question. Overall, our enhancement to the original MPI
I/O interface is ﬂexible and allows users to study trade-offs
between performance and data conﬁdentiality.

VI. RELATED WORK

Many systems have been proposed to address the security
related issues in modern ﬁle systems. Cryptographic storage
systems like Plutus [23], CFS (Cryptographic File System) [5],
Cryptfs [36], TCFS (Transparent Cryptographic File System)
[10], EFS (Encrypted File System) [13], Cepheus [18], and
SFS-RO (SFS-Read Only) [19], provide strong security. These
ﬁle systems provide an end-to-end conﬁdentiality. Since all

i

e
m
T
d
e
z

 

i
l

a
m
r
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

(cid:145)

wsxr

(cid:140)

wsyr

wsxu

wswu

wswr

wsru

wsrr

(cid:140)

r svu

(cid:143)

(cid:151)

(cid:152)

(cid:138)

(cid:143)

(cid:151)

(cid:150)

(cid:149)

(cid:145)

(cid:148)

(cid:147)

(cid:146)

(cid:145)

(cid:144)

(cid:143)

(cid:142)

(cid:141)

(cid:139)

(cid:138)

(cid:137)

r stu

r str

394

data is stored in an encrypted form on the disk, such systems
ensure that the compromised servers have no control over the
stored data. The main drawback of this is that they may incur
signiﬁcant performance degradation on the client machines
because all encryption/decryption burden is imposed on them.
Therefore, they may not be very suitable for high performance
applications targeted by MPI and MPI I/O.

Security solutions developed for traditional client-server
applications [14] do not provide direct support for the dis-
tinctive programming tools, and performance requirements
encountered in high performance parallel applications. Also,
recent work on storage security has primarily been on securing
communication on networked systems between clients and
servers. Secure libraries like SEAL and Nexus [17] provide
mechanisms to secure communication. Other systems, such
as NASD (Network Attached Secure Disks) [20], iSCSI with
IPSEC [24], NFS (Network File System) with secure RPC
[29], AFS (Andrew File System) [21], and SFS (Secure File
System) [22], provide stronger security by encrypting data on
the wire as well as using stronger authentication mechanisms.
These encrypt-on-wire type of ﬁle systems protect the data
from adversaries during communication. Since encrypt-on-
wire repeats the encryption and decryption each time a ﬁle is
transferred, there is an inherent encryption overhead associated
with these approaches. However, it is important to note that
security technologies are used only if they are incorporated
such that overheads associated with increased security are
minimal. Hence, we focus on ensuring conﬁdentiality of disk-
resident datasets of high performance parallel applications,
with minimal overheads.

VII. CONCLUSIONS

Preserving conﬁdentiality of data from unauthorized ac-
cesses by processes in a multiprocess environment is a crucial
task. We investigated the feasibility, design and costs involved
in integrating security into performance sensitive parallel I/O
applications by proposing a secure interface to MPI I/O,
called MPISec I/O, that enables conﬁdentiality of disk-resident
data through encryption. MPISec I/O essentially combines
the advantages of the library based parallel I/O and data
conﬁdentiality in a user transparent manner. We also evaluated
the impact of ensuring data conﬁdentiality as part of MPI
I/O on the performance of applications and also measured
the sensitivity of the performance impact to various design
issues like the encryption map layouts, encryption block sizes,
and algorithms used. Our experiments using four different
I/O-intensive applications show that integrating conﬁdentiality
using MPISec I/O leads to an average overhead of 5.77% when
the encryption map is the same as the ﬁle view, and 7.82%
when the encryption map is different from the ﬁle view.

REFERENCES

[1] The Parallel Virtual File System, version 2. http://www.pvfs.org/pvfs2.
[2] Message Passing Interface Forum. MPI-2: Extensions to the Message-

[3] M. J. Atallah et al. Dynamic and efﬁcient key management for access

[4] E. Barker et al. Recommendation for key management part 1: General.

Passing Interface, Jul 1997.

hierarchies. In Proc. of CCS, 2005.

NIST Special Publication, 2006.

[5] M. Blaze. A cryptographic ﬁle system for UNIX.

In Proc. of CCS,

[6] R. Blom. An optimal class of symmetric key generation systems.

In
Proc. of the EUROCRYPT Workshop on Advances in Cryptology: Theory
and Application of Cryptographic Techniques, 1985.

[7] G. Bosilca et al. MPICH-V: Toward a scalable fault tolerant MPI for

volatile nodes. In Proc. of SC, 2002.

[8] P. H. Carns et al. PVFS: A parallel ﬁle system for linux clusters. In

Proc. of the Linux Showcase and Conference, 2000.

[9] J. M. Carroll. Computer Security, 3rd Edition. Elsevier Publishers.,

1993.

1996.

[10] G. Cattaneo et al. The design and implementation of a transparent
In Proc. of the USENIX Annual

cryptographic ﬁle system for unix.
Technical Conference, 2001.

[11] The MPI-IO Committee. MPI-IO: A parallel ﬁle I/O interface for MPI

version 0.5.

[12] D. Coppersmith et al. Triple DES cipher block chaining with output
feedback masking. IBM Journal of Research and Development, 40(2),
1996.

[13] M. Corporation. Encrypting ﬁle system for windows. 2000.
[14] E. Damiani et al. Samarati. Key management for multiuser encrypted
databases. In Proc. of the International Workshop on Storage Security
and Survivability, 2005.

[15] R. R. et al. Argonne National Laboratory. Parallel I/O benchmarking

[16] I. Foster et al. A security architecture for computational grids. In Proc.

consortium.

of CCS, 1998.

[17] I. Foster et al. A secure communications infrastructure for high-

performance distributed computing. In Proc. of HPDC, 1996.

[18] K. Fu. Group sharing and random access in cryptographic storage

ﬁlesystems. Master’s thesis, MIT, 1999.

[19] K. Fu et al. Fast and secure distributed read-only ﬁle system. Computer

Systems, 20(1):1–24, 2002.

[20] G. A. Gibson et al. A cost-effective, high-bandwidth storage architecture.

SIGOPS Opererating Systems Review, 32(5):92–103, 1998.

[21] J. H. Howard. An overview of the andrew ﬁle system. In Proc. of the

USENIX Winter Technical Conference, 1988.

[22] J. P. Hughes and C. J. Feist. Architecture of the secure ﬁle system. In

[23] M. Kallahalla et al. Plutus — Scalable secure ﬁle sharing on untrusted

Proc. of MSST, 2001.

storage. In Proc. of FAST, 2003.

[24] A. Kent and R. Atkinson. Security architecture for the Internet Protocol.

http://www.ietf.org/rfc/rfc2401.txt, 1998.

[25] D. Lancaster et al. A parallel I/O test suite. In Proc. of the 5th European
PVM/MPI Users’ Group Meeting on Recent Advances in Parallel Virtual
Machine and Message Passing Interface, 1998.

[26] J. Li et al. Parallel netcdf: A high-performance scientiﬁc I/O interface.

In Proc. of SC, 2003.

[27] T. Matsumoto and H. Imai. On the key predistribution system: A prac-
tical solution to the key distribution problem. In Proc. of Conference on
the Theory and Applications of Cryptographic Techniques on Advances
in Cryptology, 1988.

[28] J. Nechvatal et al. Report on the development of the advanced encryption
standards. In Journal of Research of the National Institute of Standards
and Technology, 2000.

[29] B. Pawlowski et al. The NFS version 4 protocol.

In Proc. of the
International System Administration and Networking Conference, 2000.
[30] S. Prakash and R. L. Bagrodia. MPI-SIM: Using parallel simulation to
evaluate MPI programs. In Proc. of the Conference on Winter simulation,
1998.

[31] J.-P. Prost et al. MPI-IO/GPFS, an optimized implementation of MPI-IO

on top of GPFS. In Proc. of SC, 2001.

[32] E. Riedel et al. A framework for evaluating storage system security. In

Proc. of FAST, 2002.

[33] F. Schmuck and R. Haskin. GPFS: A shared-disk ﬁle system for large

computing clusters. In Proc. of FAST, 2002.

[34] H. Taki and G. Utard. MPI-IO on a parallel ﬁle system for cluster
of workstations. In Proc. of the IEEE Computer Society International
Workshop on Cluster Computing, 1999.

[35] R. Thakur et al. Data sieving and collective I/O in ROMIO. In Proc.
of the Symposium on the Frontiers of Massively Parallel Computation,
1999.

[36] E. Zadok et al. Cryptfs: A stackable vnode level encryption ﬁle system.
Technical Report CUCS-021-98, Computer Science Dept, Columbia
University, 1998.

395

