Application Specific Low Latency Instruction Cache

for NAND Flash Memory Based Embedded

Systems

Kwangyoon Lee

Alex Orailoglu

Department of Computer Science and Engineering

Department of Computer Science and Engineering

University of California, San Diego

kwlO02@cs.ucsd.edu

University of California, San Diego

alex@cs.ucsd.edu

Abstract-In embedded systems, the demand for high capacity
and updatable code storage increases as the amount of code
grows. NAND Flash memory is one of the most popular storage
solutions for this purpose. However, an extremely long access
latency makes the NAND flash memory less attractive in the
domain of code storage solutions. In this paper, we introduce a
highly effective cache system which utilizes application specific
HotSpot information and a HotSpot-aware prefetching technique
to effectively hide the long latency of the NAND flash memory
based code storage systems. To lessen the negative impact of the
extremely long access latency, we propose a page prefetching
algorithm utilizing HotSpot information which provides a tem-
poral distance sufficient to load a page from the physical NAND
flash memory. The experimental results show that our proposed
architecture exhibits up to 96% reduction of the average access
time, and the juxtaposition of the HotSpot cache and the HotSpot-
aware prefetching demonstrates enhancement by nearly two
orders of magnitude, up to 98.3% reduction in time, over a
basic direct mapped LI cache architecture.

I. INTRODUCTION

A disproportionate hardware cost in most embedded sys-
tems, especially in consumer electronic systems, is incurred
by the memory subsystem as a variety of different memory
components are concurrently adopted in most embedded sys-
tems. The memory requirements of embedded systems are far
more diverse and constrained than those of generic computer
systems. A single and universal standard memory device like
DRAM fails to satisfy the various requirements and constraints
set by embedded systems. In addition to the cost factor, there
exist a number of other characteristics that the embedded
systems designers need to consider before adopting memory
device(s). The major characteristics of the most common mem-
ory devices considered in embedded systems are summarized
in Table I.

From the vantage point of cost, unifying memory devices in
embedded systems is beneficial in many ways. However, there
has been no ideal memory device which satisfies the whole
slew of design requirements that are typically encountered.
NAND flash memory has long been considered and adopted
in practice as the most preferable data storage device since
it is suitable for embedded systems which generally require
special characteristics such as low cost, low power, high

capacity, high reliability, and high robustness. However, the
adoption of NAND flash memory for code storage has been
severely constrained due to several intrinsic characteristics of
NAND flash memory: the random-bit inaccessibility and the
long access latency. A significant amount of research has
been undertaken to overcome these limitations in utilizing
the NAND flash memory as a unified memory infrastructure.
Many researchers continue to envisage NAND flash memory
as the most plausible candidate for this purpose.

In this paper, as a first step towards the unified memory
system in embedded systems, we concentrate on a new in-
struction memory/cache architecture. Embedded systems are
generally composed of a fixed set of functionalities and exhibit
a great degree of application specific behaviors [9], [10].
This fact encourages many researchers to exploit application
information in various ways. There have been two most popu-
lar approaches in employing application specific information.
One consists of transforming and relocating instructions/data
to exhibit enhanced run-time behavior. This information is
gathered off-line from the given set of applications with the
help of compilers or linkers. The other is extracting appli-
cation specific information from a given set of applications
and tuning the hardware based on the obtained information.
We utilize the second approach in this work as it offers
increased flexibility and efficiency both in design and run-time
behaviors. We introduce a novel instruction cache architecture
that demonstrates significant enhancement in average memory
access time on the NAND flash memory based systems. We
primarily focus on the exploration of the possible ways to
mitigate the impact of the long access latency of the NAND
flash memory by exploiting the program behaviors that exhibit
strong spatial and temporal correlation in a group of run-time
instruction segments and the high predictability among these
groups. Extensive simulation using the SimpleScalar simulator
with MiBench and Spec2000 benchmark suites illustrates the
significant advantages of our architecture over the generic
cache architecture with equivalent hardware cost.

The rest of the paper is organized as follows: Section
II briefly presents NAND flash memory based code storage
systems. Section III delivers background and basic technical

1-4244-2334-7/08/$20.00 (Â¢ 2008 IEEE

69

COMPARISON AMONG MEMORY DEVICES

TABLE I

Device
SRAM
DRAM
NOR FLASH
NAND FLASH

cost
highest
high
medium
low

capacity
low
medium
medium
high

read bandwidth
highest
high
low
low

write bandwidth
highest
high
lowest
low

non-volatility
no
no
yes
yes

energy consumption
high
high
low
low

motivation. Section
IV introduces our cache architecture
and the techniques of access latency reduction and highly
accurate page prediction and prefetching. Section V presents
our experimental results based on simulation and provides a
comparison with the baseline cache architecture. Section VI
provides an overall summary of the proposed architecture.

II. RELATED WORKS

Due to the increasing requirements and complexity in
embedded systems, memory subsystems have been getting
diverse and complicated. A significant amount of research has
been performed in the field to enhance the characteristics of
the systems and overcome the underlying hardware specific
problems. To best utilize the application specific information to
the instruction memory, a number of novel cache architectures
[13] have been introduced to save power and enhance
as in
[11],
performance. Research on Scratchpad Memories as in
[2] has also been widely studied for low power and low cost
embedded systems which typically focus on the intelligent
incorporation of application specific behaviors of the target
program.

For the research area on NAND flash memory, as discussed
in section I, a considerable amount of research has been
initiated to overcome the problems in NAND flash memory
based systems. Park et al. in [8] employs NAND flash memory
as a primary memory. The authors add a simple direct mapped
unified cache between a processor and the NAND flash mem-
ory. To minimize access latency for time critical applications,
they prioritize applications based on their criticality. However,
this system requires a relatively large cache and it suffers
from overall performance degradation. The OneNANDTM [6],
a hybrid NAND flash memory is introduced mainly targeting
embedded systems. While OneNANDTM flash memory basi-
cally relieves the problem of the bit-wise inaccessibility as
described above, the problem caused by long latency remains
[7], [4], [3], the authors develop NAND
still unresolved. In
flash memory based code storage systems by incorporating
the VM (Virtual Memory) subsystem of the processor. These
techniques introduce flash-memory-aware VM subsystems by
exploiting parallelized flash memory access, OneNANDTM_
specific buffer capabilities, and application specific informa-
tion. However, these techniques consume significant DRAM
space and still suffer from high access latency of the NAND
flash memory.

III. BACKGROUND AND MOTIVATION

The instruction cache is the most important component in
the code execution unit of a processor core. Thus, the time

to fetch instructions should be minimized because it will
directly affect the execution performance of the processor. In
accessing code memory, two different timing factors exist. One
is the preload time to change the state of a specified memory
chunk to a retrievable state and the other is the transfer time
to bring the prepared memory chunk to a cache line. The
access timing and performance characteristics of the major
memory components are summarized in Table II. Compared
to modern DRAM/SRAM devices, the NAND flash memory
is outperformed in terms of access time by other memory
alternatives due to the NAND flash specific cell architecture.
However, its other characteristics discussed in Section I have
accelerated its popularity on the storage systems.

For the memory access timing among these memory de-
vices, SRAM/DRAM shows relatively fast initial access and
transfer times to an access unit, whose size is normally small.
However, the NAND flash memory demonstrates a long initial
access time for an access unit (page), whose access gran-
ularity is significantly greater than those of SRAM/DRAM.
Consequently, the initial access time for each page is the most
critical factor in accessing data from the NAND flash memory.
Moreover, advances in processor technologies worsen the
problem of latency in NAND flash memory. Modern embedded
processors also adhere to the well-known Moore's law with
clock frequencies of IGHz having been achieved in high-end
embedded processors. The page access time of the NAND
flash memory nonetheless has been a near constant value over
decades and it tends to increase even as new technologies of
NAND flash memory emerge.

From an architectural point of view, most embedded pro-
cessors incorporate a small size of direct-mapped instruction
cache. For low latency memory devices like a DRAM, the
average cache miss time, represented as the product of cache
miss rate and penalty, is sufficiently low compared to the cache
hit time due to the relatively small cache miss penalty. Conse-
quently, there is no need to use the complex instruction cache,
which may increase the average memory access time due to
the increment in cache hit time caused by the complexity
added to the hardware. However, for the long latency memory
device like the NAND flash memory, a relatively low cache
hit rate also results in unacceptably high average memory
access times due to the extremely high penalty on the cache
miss. Consequently, research to explore techniques to lessen
or hide the latency of the instruction fetch from the NAND
flash memory based embedded systems has been initiated.

A HotSpot denotes a code chunk whose frequency of access
is significantly elevated. As in
[13], further analysis on
benchmarks reveals that programs are usually composed of

70

2008SymposiumonApplication Specific Processors (SASP 2008)

TIMING CHARACTERISTICS OF MEMORY DEVICES

TABLE II

Device
Access Time (ns)
Read Bandwidth (MB/s)
Write Bandwidth (MB/s)

SRAM
5
2500
2500

DRAM
50
1000
1000

NAND FLASH
30000
17
6.8

OneNAND'm
25000
108
9.3

small-sized basic blocks which are delimited by branches
and each program spends most of its execution time on a
highly limited set of basic blocks (HotSpots). In particular,
embedded programs have a fixed set of functionality and
subsequently lend themselves to static Hotspot information ex-
traction. Moreover, HotSpot information is intrinsically static
because the relative frequency of access tends to be time and
space invariant under the fixed set of applications.

To obtain good HotSpot candidates, the application spe-
cific characteristics should be thoroughly studied statically.
Hotspots can be classified into two different basic types:
Loop and SubRoutine. The Loop-type of HotSpots exhibits
both strong spatial and temporal locality and also shows
elevated correlation among the neighboring code segments.
The SubRoutine-type of HotSpots demonstrates high spatial
locality and context dependent access behavior. Thus, proper
employment of HotSpot information in the processor as an in-
struction cache will greatly enhance the code access behavior.
However, HotSpot information is static and unable to capture
the dynamic behavior of the program. The dynamic memory
access behavior aspects that are not exploited by the HotSpot
cache should be captured possibly by introducing an additional
generic LI cache that suffices for this purpose.

Further analysis of program behavior on the aforementioned
hybrid cache architecture lends itself to the following interpre-
tation:

* The HotSpot cache guarantees prioritized accesses for the

entries on the HotSpot cache.

* In steady-state, a set of HotSpot(s) in conjunction with
active code segments in the LI cache forms a pseudo-
working set.

* The transitions between pseudo-working sets are highly

predictable.

The above set of observations constitutes a strong set
of motivations for the use of application specific HotSpot
information to hide or at least lessen the access latency in
the NAND flash memory based systems. Firstly, the HotSpot
cache coupled with the LI cache greatly enhances the cache
hit rate. The HotSpot cache absorbs most of the conflict and
capacity misses at the LI cache by placing those highly active
code segments in the prioritized static cache. Secondly, the
HotSpot cache also reduces the cache miss penalty by ex-
ploiting the HotSpot directed prefetching. A set of HotSpot(s)
can represent a tightly coupled snapshot of codes (a pseudo-
working set) which resides in both caches for an extended
period of time and the transitions between sets are highly
predictable in most embedded applications. Consequently, we
have a high chance of prefetching the next possible code

segments with high accuracy and timeliness.

IV. PROPOSED ARCHITECTURE

A. Application Specific HotSpot Cache

Under a given application set, a HotSpot behaves as a code
segment whose access behavior is considerably repetitive in a
well-formed run-time code execution space as shown in Fig 1.
The code space shown in Fig 1 is not necessarily physically
contiguous but it forms a strongly correlated logical set of
instructions. If the HotSpot information is maintained in a
unique cache, it will enhance cache efficiency and capture
improved temporal and spatial locality. As was discussed in
Section III, the dynamic nature of the code execution at run-
time should be captured by the regular LI cache. In this hybrid
cache design, while the LI cache attempts to dynamically
trace and track locality in the absence of prior knowledge
of the program, the HotSpot cache statically prioritizes the
pre-specified code entries based on known access frequency.
Conceptually, the Loop-type of HotSpot defined in Section III
can lessen the burden of both conflict and capacity misses in
the LI cache and the SubRoutine-type of HotSpot is useful
when capacity misses are involved.

The HotSpot information utilized in an instruction cache
system should be obtained by the off-line multi-phase program
profiling process for each target application. This profiling
process extracts the most useful HotSpot information based
on the following criteria:

* Code with density higher than a specific threshold

(codesize x accessfrequency)

* Code with higher priority (Loop-type > SubRoutine-type)
While the static quality of the HotSpot information can be
defined by the hit rate on HotSpot to total HotSpot size ratio,
the run-time quality of the HotSpot information may vary
based on both the total cache size and the proportion of the

Access
frequency

HotSpot

Logical Code
Execution Space

Fig. 1.

The HotSpot Behavior

2008SymposiumonApplication Specific Processors (SASP 2008)

71

The best practice to reduce access latency in cache design is
prefetching the next possible code segment. There has been
a significant amount of research on instruction prefetching
[5], [12]. However, these techniques are not well-suited for
the NAND flash based systems because fetching a page takes
a huge amount of time compared to a block prefetching in
the DRAM based systems. Most of the prefetching ideas do
not consider the amount of latency that the NAND flash
memory exposes, resulting in prefetching delivering slight
benefit unless the prefetching is commenced sufficiently early.
The appropriate prefetching in a NAND flash memory based
system is a highly challenging task necessitating consideration
of a timely prefetching technique capable of delivering high
accuracy.

As was observed in Section III, a HotSpot represents a
temporally and spatially coupled logical group of execution
code referred to as a pseudo-working set. In this phase of
profiling, we partition the logical program space into a number
of pseudo-working sets represented by HotSpot(s) and then we
examine the program control flow of all the pseudo-working
sets. A pseudo-working set is a well-formed logical entity
whose whole set of corresponding code segments are fully
maintained in the cache system at run-time; only transitions
to the subsequent set may increase the number of cache misses.
Thus, we record all the information on the cache misses from
a HotSpot directed pseudo-working set. This information will
be utilized to select a prefetchable code page from a HotSpot
directed pseudo-working set. However, the effectiveness of
prefetching is very dependent on the following criteria:

. Accuracy of prefetching: threshold on the possibility of

the next page access which raises cache misses

. Timeliness of prefetching: threshold on time spent on
a HotSpot directed pseudo-working set without cache
misses

As the Hotspots with obtained information processed, only
the HotSpots which satisfy all the threshold conditions will
have the prefetching information simply expressed by the next
page address. Subsequently, only the HotSpots that have strong
predictability should be chosen to achieve the required level
of the accuracy and timeliness in prefetching. Inaccurate and
untimely prefetching negatively impacts the average memory
access time. However, the impact of inaccuracy is more
pronounced than the impact of timeliness. A more conservative
value should be applied to set the threshold of controlling
prefetching accuracy.

The obtained information is combined with the HotSpot
information and also stored in the HotSpot container. To in-
corporate this information, the HotSpot associated prefetching
information should be utilized as soon as possible to do a
timely load of the next possible code page when this informa-
tion is ready. A prefetcher and prefetch buffer is additionally
introduced to the existing HotSpot cache as in Fig 3. Whenever
a new HotSpot that contains its associated prefetching infor-
mation is accessed, it notifies the prefetcher to asynchronously
initiate a corresponding prefetch operation. The prefetcher can

Fig. 2.

The HotSpot Cache Architecture

area the HotSpot consumes. If the total cache size is given, the
profiling program explores the design space by controlling the
static HotSpot quality, which is manipulated by the threshold
on code density. Parameter space exploration is required to get
optimal HotSpot information; extensive experiments show the
precise relationship between the static HotSpot quality and the
effectiveness of the HotSpot cache.

The obtained optimal HotSpot information should be trans-
ferred to the special HotSpot cache in the embedded proces-
sor. The HotSpot cache is designed to hold static HotSpot
information for specific applications. Whenever a system is
cold-booted, the processor automatically retrieves this HotSpot
information from a pre-specified memory location called
HotSpot Container.

Based on these principles, we introduce the hybrid cache
architecture shown in Fig 2. The two disjoint caches are placed
and accessed in parallel between the process and the NAND
flash memory and the selector retrieves an instruction from
one of the caches with the HotSpot cache being the preferred
one. Consequently, the cache system may experience reduced
hit time overhead at the expense of an increased number of
cache references. However, the hybrid cache will enjoy the
reduction in main memory accesses due to the elevated high
cache hit rate which is of more pronounced importance in
terms of performance and power consideration for the overall
system.

The HotSpot cache captures most of the accesses which are
involved in loop activities. The basic unit of the loop activity is
subdivided into a basic block which is delimited by conditional
branches. Previous research shows that the basic block size is
relatively small, in the range of 5 to 8 instructions in popular
embedded processors [1]. In our design, the Hotspot cache
line size is kept small to hold most of the basic blocks without
excessive waste of space. The LI cache is organized as direct
mapped with a relatively large cache line size to fully utilize
the spatial locality of a program with minimal access latency.

B. HotSpot-Aware Page Prefetching

The HotSpot cache greatly improves the cache hit rate with
a relatively small hardware overhead. However, the long access
latency of the NAND flash memory remains still unresolved.

72

2008SymposiumonApplication Specific Processors (SASP 2008)

HotSpot efficiency vs. Average memory access time

MAD/MiBench
JPEG/MiBench
CRAFTY/SPEC2000
GAP/SPEC2000
TWOLF/SPEC2000

a)
F=

a)I.

0
F=
a)

a,
m
ZD

a)a
.N
m
90

z

15

10

5

Fig. 3.

The HotSpot Cache with Prefetch Buffer

70

75

80

85

90

95

100

Off-line HotSpot hitrate (%)

be in three different states and corresponding behaviors: (1)
The prefetcher is ready to process the request and immediately
initiates prefetching. (2) The prefetcher is ready to process the
request but rejects it because the page has already been loaded
in the prefetch buffer. (3) The prefetcher is in the midst of
processing a previous request, resulting in the request being
queued until the pending job finishes. Conditions 1 and 2 do
not negatively impact the efficiency of prefetching. However,
the last condition caused by the inaccurate and untimely
prediction mechanism deteriorates the access latency of the
system.

In designing a cache system, there are typically three basic
design goals to be considered: lower hit time, lower miss
penalty and higher hit rate. Our proposed architecture focuses
on providing a higher hit rate by utilizing application specific
information because the miss penalty in NAND flash based
systems is extremely high, of the order of up to 25,000 cycles
for the most advanced embedded processors; furthermore, all
trends show that it will tend to increase in the future. Con-
sequently, we try to maintain a lower hit time by introducing
a small fully-indexed but static cache and a very simple LI
cache. The hit time of the proposed architecture is equivalent
or slightly longer than the standard direct mapped LI cache
but much shorter than any other complex cache architecture
including set associative caches.

V. EXPERIMENTAL RESULTS

A. Experimental Framework

We extensively used the SimpleScalar toolset to perform our
experiments under various conditions. Our base architecture is
the 32KB/64KB direct mapped LI cache with 512B cache line

TABLE III

BENCHMARK APPLICATIONS SUMMARY

A ~~~~~~~~~~~~-

Application
JPEG
MAD
CRAFTY
GAP
TWOLF

Type
Image
Mp3
Game
Interpreter
Simulator

Size
292KB
292KB
432KB
912KB
440KB

Benchmark suite
MiBench
MiBench
Spec2OOO
Spec2000
Spec2000

Fig. 4.

HotSpot quality vs. HotSpot cache effectiveness

size. To obtain the optimal HotSpot information for a given
hardware configuration, we use a parameter space exploration
technique to differentiate the HotSpot code segments from
the normal code segments as described in the subsequent
subsection. The line size of the HotSpot cache is set to 32B.
We simulate two different cache architectures: the HotSpot
cache and the HotSpot cache with page prediction. In both
cases, the total size of cache (the HotSpot cache + the LI
cache + the prefetch buffer, if applicable) is always set to the
size of the base architecture. We perform our simulation for
100 million instructions per program. We simulate a set of
representative programs from MiBench and Spec2000, which
approximate real-life embedded applications. Table III outlines
the benchmarks used for the experiments.

B. Parameter Space Exploration

The effectiveness of the proposed architecture is signifi-
cantly dependent on the quality of information we obtained as
described in Section IV. Fig 4 shows the relationship between
the static HotSpot quality and the average memory access
time of the HotSpot cache. The static HotSpot quality is
characterized by the HotSpot size and the HotSpot hit rate.
The empirical data show that HotSpots which are no more
than half of the total cache size and whose hit rates exceed
90% should be considered as appropriate candidates for the
HotSpot cache. The effectiveness of the HotSpots larger than
half the cache size dramatically deteriorates due to the inability
to fully capture dynamic accesses causing cache misses in
the LI cache as the LI cache shrinks because the total size
of cache systems remains constant. For the relatively simple
prefetching buffer, 0.75 for the accuracy threshold and 0.50
for the timeliness threshold are empirically observed. As can
easily be seen by the results, the effectiveness of prefetching
improves when the target application is able to be partitioned
to a number of working sets whose transition behaviors are
easily identified.

2008SymposiumonApplication Specific Processors (SASP 2008)

73

TABLE IV

BENCHMARK RESULTS: MIss RATE

App
MAD
JPEG
CRAFTY
GAP
TWOLF

32K/Base
0.000111
0.000040
0.002800
0.002300
0.005900

32K/HS+Ll
0.000137
0.000039
0.000813
0.000434
0.000046

32K/HS+L1+PF
0.000104
0.000032
0.000731
0.000193
0.000019

64K/HS+Ll
0.000029
0.000007
0.000213
0.000066
0.000002

64K/HS+L1+PF
0.000028
0.000007
0.000133
0.000032
0.000002

HS Size
6.6KB
2.4KB
13.2KB
6.9KB
5.8KB

HS Hitrate
97.76%
93.26%
87.12%
93.15%
98.50%

64K/Base
0.000042
0.000012
0.000190
0.001000
0.001500

TABLE V

BENCHMARK RESULTS: AVERAGE MEMORY ACCESS TIME

Application
MAD
JPEG
CRAFTY
GAP
TWOLF

32K/Base (DRAM)
1.01
1.00
1.28
1.23
1.59

32K/Base (NAND)
2.37
1.40
29.00
24.00
60.00

32K/HS+L1+PF (NAND)
2.08
1.32
3.95
2.45
1.19

C. Average Memory Access Latency

REFERENCES

In general, the overall cache hit rate of the proposed archi-
tecture outperforms the base architecture due to the reduction
of overall cache misses by the hybrid cache system, and the
accurate and timely page prefetching mechanism. As a rule
of thumb, the cache hit rate of the proposed architecture is
equivalent to a 4-way set associative cache. The results can
be seen in Table IV and V assuming a 100 cycle access
latency for DRAM, 10,000 cycle access latency for NAND
flash memory and two 2KB pages for the prefetching buffer.
As can be seen, the average memory access time in the
proposed architecture is highly reduced in comparison to
the base architecture. Furthermore, the proposed architecture
shows results comparable to the base system with DRAM.
The proposed architecture works perfectly when the program
behavior is concentrated on a small set of HotSpots and
pseudo-working sets whose transitions are highly predictable.

VI. CONCLUSION

In this paper, we have proposed a new instruction cache
system to minimize memory access latency in NAND flash
memory based code storage systems. We have designed a
static HotSpot cache in an embedded processor by extract-
ing application specific access behavior of a given program.
The HotSpot cache prioritizes most frequently accessed code
segments to avoid cache conflicts with other code segments
by isolating those segments. Furthermore, a HotSpot-aware
prediction technique is developed to hide long memory access
latency by utilizing pseudo-working set information character-
ized by HotSpots. Thus, a HotSpot successfully predicts and
loads the next code segment in an accurate and timely manner.
Extensive simulations using the SimpleScalar simulator with
MiBench and Spec2000 benchmark suites show that the pro-
posed architecture exhibits up to 96% reduction of the average
access time, and the combined solution of HotSpot cache and
prefetching demonstrates enhancement by nearly two orders
of magnitude, up to 98.3% reduction in time, over the base
cache architecture while retaining average memory access time
equivalent to the DRAM based systems.

[3]

[4]

[1] K. Ali, M. Aboelaze, and S. Datta. Modified hotspot cache architecture:
A low energy fast cache for embedded processors.
In SAMOS VI:
Proceedings of Embedded Computer Systems: Architecture, Modeling,
and Simulation, 2006.

[2] 0. Avissar, R. Barua, and D. Stewart. An optimal memory allocation
scheme for scratch-pad-based embedded systems. Trans. on Embedded
Computing Sys., 1(1):6-26, 2002.
J. In, I. Shin, and H. Kim. Swl: a search-while-load demand paging
scheme with NAND flash memory. In LCTES '07: Proceedings of the
2007 ACM SIGPLAN/SIGBED Conference on Languages, Compilers,
and Tools, pages 217-226, 2007.
Y. Joo, Y. Choi, C. Park, S. W. Chung, E. Chung, and N. Chang. Demand
paging for OneNANDTMFlash eXecute-in-place. In CODES+ISSS '06:
Proceedings of the 4th International Conference on Hardware/Software
Codesign and System Synthesis, pages 229-234, 2006.

[5] D. Joseph and D. Grunwald. Prefetching using Markov predictors. In
ISCA '97: Proceedings of the 24th Annual International Symposium on
Computer Architecture, pages 252-263, 1997.

[6] B. Kim, S. Cho, and Y. Choi. OneNANDTM: A high performance and
low power memory solution for code and data storage. In Proceedings
of 20th Non-Volatile Semiconductor Workshop, 2004.

[7] C. Park, J. Lim, K. Kwon, J. Lee, and S. L. Min. Compiler-assisted
demand paging for embedded systems with flash memory. In EMSOFT
'04: Proceedings of the 4th ACM International Conference on Embedded
Software, pages 114-124, 2004.

[8] C. Park, J. Seo, S. Bae, H. Kim, S. Kim, and B. Kim. A low-cost
memory architecture with NAND XIP for mobile embedded systems. In
CODES+ISSS '03: Proceedings of the 1st IEEE/ACM/IFIP International
Conference on Hardware/software Codesign and System Synthesis,
pages 138-143, 2003.
P. Petrov and A. Orailoglu.
Energy frugal tags in reprogrammable
I-caches for application-specific embedded processors.
In CODES
'02: Proceedings of the Tenth International Symposium on Hard-
ware/Software Codesign, pages 181-186, 2002.
P. Petrov and A. Orailoglu. A reprogrammable customization framework
for efficient branch resolution in embedded processors. ACM Transac-
tions on Embedded Computing Systems, 4(2):452-468, 2005.

[9]

Dynamic overlay of
[11] M. Verma, L. Wehmeyer, and P. Marwedel.
scratchpad memory for energy minimization.
In CODES+ISSS '04:
Proceedings of the 2nd IEEE/ACM/IFIP International Conference on
Hardware/software Codesign and System Synthesis, pages 104-109,
2004.

[12] C. Xia and J. Torrellas. Instruction prefetching of systems codes with
layout optimized for reduced cache misses. In ISCA '96: Proceedings
of the 23rd Annual International Symposium on Computer Architecture,
pages 271-282, 1996.

[13] C.-L. Yang and C.-H. Lee. Hotspot cache: joint temporal and spatial
locality exploitation for I-cache energy reduction. In ISLPED '04: Pro-
ceedings of the 2004 International Symposium on Low power Electronics
and Design, pages 114-119, 2004.

[10]

74

2008 Symposium on Application Specific Hocessors

(SASP 2008)

