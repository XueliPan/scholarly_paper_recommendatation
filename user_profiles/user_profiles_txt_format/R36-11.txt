Video Segmentation Combining Similarity Analysis and

Classiﬁcation

Matthew Cooper

FX Palo Alto Laboratory

3400 Hillview Ave. Bldg. 4
Palo Alto, CA 94304 USA

cooper@fxpal.com

ABSTRACT
In this paper, we compare several recent approaches to video
segmentation using pairwise similarity. We ﬁrst review and
contrast the approaches within the common framework of
similarity analysis and kernel correlation. We then combine
these approaches with non-parametric supervised classiﬁca-
tion for shot boundary detection. Finally, we discuss com-
parative experimental results using the 2002 TRECVID shot
boundary detection test collection.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Content Anal-
ysis and Indexing—Indexing methods

General Terms
algorithms, management

Keywords
temporal media indexing and segmentation

1.

INTRODUCTION

Numerous video retrieval and management tasks rely on
accurate segmentation of scene boundaries. Many exist-
ing systems compute frame-indexed scores quantifying local
novelty within the media stream. The novelty scores are cal-
culated in two steps. First, an aﬃnity or similarity matrix
is generated, as in Figure 1. Next, the frame-indexed score
is computed by correlating a small kernel function along the
main diagonal of the similarity matrix. Typically, detected
local maxima in the novelty score are labelled as segment
boundaries.

In this paper, we compare several kernels used for media
segmentation based on similarity analysis. We ﬁrst review
similarity analysis in Section 2. Section 3 examines each of
the kernels used to produce a frame-indexed correlation or

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
MM’04, October 10-16, 2004, New York, New York, USA.
Copyright 2004 ACM 1-58113-893-8/04/0010 ...$5.00.

Figure 1: Diagram of the similarity matrix embed-
ding.

novelty score. Section 4 presents comparative experimen-
tal results for shot boundary detection using the TRECVID
data and evaluation tools [1].
In the ﬁrst experiment, we
use the diﬀerent kernels and corresponding novelty scores as
input to a binary K-nearest-neighbor (KNN) classiﬁer that
labels frames as either cut boundaries or non-boundaries. In
the second experiment, we directly use the pairwise similar-
ity data as input to train and test the KNN classiﬁer. For
this experiment, we vary the speciﬁc local set of similarity
data again according to the proposed choices of kernels.

2. SIMILARITY ANALYSIS

2.1 Matrix embedding

We detect scene boundaries by quantifying the similarity
between pairs of video frames. First, low-level features are
computed to represent each frame. Throughout this paper,
we extract histograms in the YUV colorspace; these features
are a common choice for segmentation, e.g. [2]. Denote the
frame-indexed feature data V = {Vn : n, = 1, · · · , N }. A
measure D of the similarity between frame parameters Vi
and Vj is calculated for every pair of video frames i and
j. The similarity matrix S contains the similarity measure
calculated for all frame combinations, as depicted in Figure
1. Throughout this paper, we compare feature vectors using
the squared Euclidean vector distance:

(1)
This choice for D measures dissimilarity. Time, or the frame

S(i, j) = D(Vi, Vj ) ≡ kVi − Vj k2 .

istartendjijsimilaritymatrixSstartendstreamD(i,j)index, runs along both axes as well as the diagonal. S
has minimum dissimilarity (zero) along the leading diago-
nal where each frame is compared to itself. Because D is
symmetric, S is also symmetric.

2.2 Kernel-based features

Segment boundaries exhibit a distinct pattern in S. Specif-
ically, the frames comprising coherent segments exhibit low
within-segment dissimilarity, creating square regions along
the main diagonal of S with low values. The boundary be-
tween two such segments produces a checkerboard pattern in
S if the two segments have high between-segment dissimilar-
ity, creating rectangular regions oﬀ the main diagonal with
high values. This suggests that ﬁnding the scene bound-
ary transitions is as simple as ﬁnding the checkerboards
along the main diagonal of S. This can be done using a
matched ﬁlter: correlating S with a kernel, K, that itself
looks like a checkerboard [3]. The correlation produces a
frame-indexed novelty score that can be processed to detect
segment boundaries. Speciﬁcally, deﬁne the kernel correla-
tion, or equivalently, novelty score:

ν(n) =

K(l, m)S(n + l, n + m) .

(2)

L−1
X

L−1
X

l=−L

m=−L

By varying the kernel width L, the novelty score can be
tuned to detect boundaries between segments of a speciﬁc
minimum length. The kernel function can be viewed as a
generalization of the local linear processing of adjacent frame
diﬀerences used for segmentation. As discussed in Section
3, several diﬀerent kernels have been proposed.

Calculating S requires O(N 2) computations, where N is
the number of frames.
In practice, there is no reason to
calculate similarity matrix values beyond the extent of the
kernel, i.e. elements S(i, j) where |i − j| > L. Addition-
ally, because both S and K are typically symmetric, many
computations are redundant. For this reason, we compute
only a small portion of S near the main diagonal, and the
algorithmic complexity is O(N ).

3. RELATED WORK

There is a vast literature on video segmentation, includ-
ing comparative reviews such as [4]. Here, we review only
the algorithms used in the experiments of Section 4. Each
algorithm is characterized by a speciﬁc kernel used to gen-
erate a novelty score per (2). For comparison, we empha-
size the diﬀerences between the kernels in terms of their
relative weighting of the elements of S to form the novelty
scores. Figure 2 graphically depicts the kernels considered
here. In each panel, a blank element does not contribute to
the corresponding novelty score (i.e. K(l, m) = 0 in (2)).
The elements containing solid circles contribute positively
to the novelty score (K(l, m) > 0). The elements contain-
ing unﬁlled circles contribute negatively to the novelty score
(K(l, m) < 0). Notice that the elements along the main di-
agonal of K align with the main diagonal elements of S in
the correlation, where S(n, n) = D(Vn, Vn) = 0.

The results of comparing adjacent video frames appear in
the ﬁrst diagonal above (and below) the main diagonal, i.e.
the elements S(n, n + 1). Scale space analysis [5] is based
on applying a kernel of the form shown in Figure 2(a). The
full analysis uses a family of Gaussian kernels of varying
standard deviation to calculate a corresponding family of

(a)

(b)

(c)

(d)

Figure 2: The ﬁgure shows diﬀerent kernels pro-
posed for segment boundary detection via kernel
correlation (L = 4). The kernels correspond to
scale-space analysis (a), diagonal cross-similarity
(b), cross-similarity (c), and full similarity (d).

novelty scores. Deﬁne the 2L × 2L scale space (SS) kernel
as:

K(σ)

SS (l, m) =

( 1

Z(σ) exp
0

(cid:16)
− l2
2σ2

(cid:17)

|l − m| = 1

otherwise

.

(3)

where Z(σ) is a normalizing factor1. Scale-space analysis
was used in [6] for video segmentation.

Pye, et al.

[7], presented an alternative approach using
kernels of the form of Figure 2(b). When centered on a
segment boundary, this kernel weights only elements of S
that compare frames from diﬀerent segments. This kernel is
deﬁned:

KDCS (l, m) =

( 1

2L |l − m| = L
0

otherwise

.

(4)

We refer to this kernel as the diagonal cross-similarity (DCS)
kernel, as the elements of S for which KDCS > 0 lie on
the Lth diagonal above (and below) the main diagonal of
S. KDCS has been used in the segmentation systems by
Pickering et al. [8].

Building on these intuitions, we present two additional
kernels for comparison. Including all the inter-segment el-
ements implies the kernel of Figure 2(c). This kernel “in-
cludes” the DCS kernel, and adds the remaining between-
segment (cross-similarity) terms within the kernel’s tempo-
ral extent. The cross-similarity (CS) kernel is deﬁned:

KCS (l, m) =

.

(5)






1

l ≥ 0 and m < 0
2L2
1
2L2 m ≥ 0 and l < 0
0

otherwise

For the Euclidean distance of (1), this kernel is precisely the
matched ﬁlter for an ideal cut boundary in S. The inter-

1The SS and DCS kernels are easily deﬁned using a sin-
gle variable, i.e.
l, but we use the two variables (l, m) for
consistency.

segment (cross-similarity) terms will be maximally dissimi-
lar, while the intra-segment terms will exhibit zero dissimi-
larity.

The ﬁnal kernel Figure 2(d) is the full similarity (FS) ker-
nel used in [3], and it includes both between-segment and
within-segment terms. This kernel replaces the zero ele-
ments in KCS with negative weights. The negative weights
penalize high within-segment dissimilarity:

KF S (l, m) =

.

(6)






1

1

2L2

2L2
− 1
2L2

l ≥ 0 and m < 0
m ≥ 0 and l < 0
otherwise

4. EXPERIMENTAL RESULTS

In this section, we examine cut boundary detection using
pairwise similarity features. For each frame, we extract a
global YUV histogram, and block YUV histograms using a
uniform 4×4 grid. We then compute separate similarity ma-
trices for the global histogram data, S(G) and for the block
histogram data, S(B). For the experiments, we follow the
approach of [9] and employ supervised binary classiﬁcation
for boundary detection. We use an eﬃcient implementation
of KNN classiﬁcation [10] to label each frame as either a
boundary or non-boundary. This algorithm oﬀers potential
speedups of a factor of 20 over the naive implementation
of KNN, as tested on video data. This allows a consistent
boundary detection scheme for comparing the various ker-
nels in the testing below. We concatenate frame-indexed
data computed from S(G) and S(B) to train and test the
KNN classiﬁer to detect cut (abrupt) segment boundaries.
For testing, we use the TRECVID 2002 test data and
evaluation software for the shot boundary detection task [1].
This data was viewed as poorer quality than the 2001 data
used in [9]. From TRECVID 2002, the average recall and
precision for cut detection was 0.86 and 0.84, respectively [8].
The test set consists of almost 6 hours of video containing
1466 cut transitions, per the manual ground truth. For the
KNN training, we use cross-validation and train separate
classiﬁers for each video using the remaining videos in the
test set. The results are combined for the entire test set.
Throughout, K = 11.

4.1 Kernel-based features

We present results for two sets of experiments.

In the
ﬁrst, we produce novelty features for shot boundary detec-
tion corresponding to kernels of extent L = 2, 3, 4, 5. For
each L, we compute a frame-indexed kernel correlation sep-
arately using S(G) and S(B) following (2). We concatenate
these novelty scores across scale, so that we have four scores
for each frame for both the global and the block histogram
features. We ﬁnally combine this data into a single 8 × 1
vector to represent each frame n:

Xn =

(cid:20)
ν(G)
2

(n) ν(G)

(n) ν(G)

(n) ν(G)

(n)

3

4

5

n = 1, · · · , N } with the manual ground truth to train and
test the KNN classiﬁer.

We control the sensitivity of the KNN classiﬁcation using
an integer parameter κ : 1 ≤ κ ≤ K. If at least κ out of
the K nearest neighbors of the vector Xn in the training
data are from the “cut” class, we label frame n as a cut and
otherwise label it as a non-cut. κ is varied to produce the
recall-precision curves of Figure 3 for the FS kernel (circle),
the CS kernel (“x”), the SS kernel (square), and the DCS
kernel (“+”). The best performance is achieved by the CS
and the DCS kernels. As noted above, the CS kernel is the
matched ﬁlter for the expected pattern produced by segment
boundaries in S. Both the CS and DCS kernels emphasize
dissimilarity between the segments evident at multiple time
scales. The FS kernel performs worst, we believe due to the
choice of the Euclidean dissimilarity measure. The FS kernel
may be better suited to dissimilarity measures that take
positive and negative values such as the cosine similarity
measure.

Figure 3: Experimental results for cut detection us-
ing the kernel-based features.

4.2 Pairwise similarity features

In the second experiment, we examine performance using
the raw pairwise similarity data (without kernel correlation)
as input to the KNN classiﬁer. This approach does incur
a computational penalty by increasing the dimensionality
of the input data X for classiﬁcation. Again, we use the
separate similarity matrices S(G) and S(B). For each kernel,
we construct the input feature vectors from those elements of
S(G) and S(B) that contribute to the corresponding novelty
score for L = 5. For example, for the SS features (Figure
2(a)) frame n is represented by the column vector:

(cid:20)
S(G)(n − 5, n − 4) S(G)(n − 4, n − 3) · · · S(G)(n + 4, n + 5)

Xn =

S(B)(n − 5, n − 4) S(B)(n − 4, n − 3) · · · S(B)(n + 4, n + 5)

(cid:21)T

ν(B)
2

(n) ν(B)

(n) ν(B)

(n) ν(B)

(n)

.

3

4

5

(cid:21)T

and for the DCS features (Figure 2(b)):

L

where ν(G)
denotes the novelty score computed using S(G)
with kernel width L, and ν(B)
denotes the novelty score
computed using S(B). We use the input data X = {Xn :

L

(cid:20)
S(G)(n − 5, n) S(G)(n − 4, n + 1) · · · S(G)(n − 1, n + 4)

Xn =

S(B)(n − 5, n) S(B)(n − 4, n + 1) · · · S(B)(n − 1, n + 4)

(cid:21)T

0.750.80.850.90.9510.840.860.880.90.920.940.960.98RecallPrecisionSB02 Cut Detection − Kernel FeaturesFull SimilarityCross SimilarityScale SpaceDCSFigure 4: Experimental results for cut detection us-
ing the pairwise similarity features.

We only include elements above the main diagonal of S since
S and K are symmetric.

The results appear in Figure 4. In this case, the additional
similarity information included in the FS data improves per-
formance. The scale-space approach, however outperforms
the CS features. This is not surprising since cut detection
performance relies largely on ﬁrst order (adjacent frame)
similarity, which is not emphasized by either the CS or DCS
features. We also include performance for “row” features
(triangle) following [9] where each frame n is represented by
the 2L × 1 vector:

(cid:20)
S(G)(n, n − 1) S(G)(n, n − 2) · · · S(G)(n, n − L)

Xn =

S(B)(n, n − 1) S(B)(n, n − 2) · · · S(B)(n, n − L)

.

(cid:21)T

Figure 5 shows the curves for all the approaches tested
on a single plot. The dashed curves show performance us-
ing the kernel-based features, and the solid curves show the
performance using the pairwise similarity features. The use
of the pairwise similarity data corresponding to the FS ker-
nel shows the best overall performance. All the approaches
perform at a high level as input to the KNN classiﬁcation.

5. CONCLUSION

In this short paper, we have presented preliminary results
of an empirical comparison of similarity-based approaches to
shot boundary detection. We presented several techniques
in a common framework based on kernel correlation of a sim-
ilarity matrix, and compared them experimentally via test-
ing in combination with supervised classiﬁcation. Generally,
the pairwise similarity features demonstrate superior perfor-
mance over local correlation-based features. Additionally,
we expect that using the pairwise similarity data will ben-
eﬁt the classiﬁcation of boundaries into subclasses, such as
abrupt (cut) versus gradual transitions. There is a complex-
ity tradeoﬀ here, because gradual boundaries require larger
values for L which can result in a quadratic increase in the
dimensionality of the representation Xn for each frame n.
Exploring this tradeoﬀ and integrating gradual transition
detection is a key aim of current research. We also intend to
continue this study by integrating additional features and
broadening the test collection.

Figure 5: Combined experimental results from Fig-
ures 3 and 4.

6. ACKNOWLEDGMENTS

We thank Ting Liu and Andrew Moore for making their

KNN software available for these experiments.

7. REFERENCES
[1] A Smeaton and P. Over. The TREC 2002 Video Track

Report. Proc. TREC Video Track, 2002.

[2] B. Gunsel, M. Ferman, and A. M. Tekalp, Temporal

video segmentation using unsupervised clustering and
semantic object tracking, J. Electronic Imaging
7(3):592-604, July 1998.

[3] M. Cooper and J. Foote. Scene Boundary Detection
Via Video Self-Similarity Analysis. Proc. IEEE Intl.
Conf. on Image Processing, 2001.

[4] J. Boreczky and L. Rowe. Comparison of video shot

boundary detection techniques. Proc. SPIE Storage and
Retrieval for Image and Video Databases, 1996.

[5] A. Witkin. Scale-space Filtering: A New Approach to

Multi-scale Description. Proc. IEEE ICASSP, 1984.

[6] M. Slaney, D. Ponceleon, and J. Kaufman. Multimedia
edges: ﬁnding hierarchy in all dimensions. Proc. ACM
Multimedia, pp. 29–40, 2001.

[7] D. Pye, N. Hollinghurst, T. Mills, and K. Wood.

Audio-visual Segmentation for Content-Based
Retrieval. Proc. Intl. Conf on Spoken Language
Processing, 1998.

[8] M. Pickering, D. Heesch, et al.. Video Retrieval using

Global Features in Keyframes. Proc. TREC Video
Track, 2002.

[9] Y. Qi, A. Hauptman, and T. Liu. Supervised

Classiﬁcation for Video Shot Segmentation. Proc. IEEE
Intl. Conf. on Multimedia & Expo, 2003.

[10] T. Liu, A. Moore, and A. Gray. Eﬃcient Exact k-NN
and Nonparametric Classiﬁcation in High Dimensions.
Proc. Neural Information Processing Systems, 2003.

0.750.80.850.90.9510.840.860.880.90.920.940.960.98RecallPrecisionSB02 Cut Detection − Pairwise Similarity FeaturesFull SimilarityCross SimilarityScale SpaceDCSRow0.750.80.850.90.9510.840.860.880.90.920.940.960.98RecallPrecisionSB02 Cut Detection − Kernel & Similarity FeaturesFull Similarity − KCross Similarity − KScale Space − KDCS − KRow − SimFull Similarity − SimCross Similarity − SimScale Space − SimDCS − Sim