ViGOR: A Grouping Oriented Interface for Search and 

Retrieval in Video Libraries 

Martin Halvey 

David Vallet  

David Hannah 

Joemon M. Jose 

Department of Computing Science, 

University of Glasgow, 

Glasgow, Scotland, United Kingdom, G12 8QQ. 

{halvey, dvallet, davidh, jj} @ dcs.gla.ac.uk 

 

 

 

 
ABSTRACT 
In this paper, we present ViGOR (Video Grouping, Organisation 
and Retrieval) a video retrieval system that allows users to group 
videos in order to facilitate video retrieval tasks. In this way users 
are  able  to  visualise  and  conceptualise  many  aspects  of  their 
search  tasks  and  carry  out  a  localised  search  in  order  to  solve  a 
more global search problem. The main objective of this work is to 
aid  users  while  carrying  out  explorative  video  retrieval  tasks; 
these  tasks  can  be  often  ambiguous  and  multi-faceted.  Two  user 
evaluations were carried out in order to evaluate the usefulness of 
this  grouping  paradigm  for  assisting  users.  The  first  evaluation 
involved  users  carrying  out  broad  tasks  on  YouTube,  and  gave 
insights into the application of our interface to a vast online video 
collection.  The  second  evaluation  involved  users  carrying  out 
focused tasks on the TRECVID 2007 video collection, allowing a 
comparison  over  a  local  collection,  on  which  we  could  extract  a 
number  of  content-based  features.  The  results  of  our  evaluations 
show that the use of the ViGOR system results in an increase in 
user performance and user satisfaction, showing the potential of a 
grouping paradigm for video search for various tasks in a variety 
of diverse video collections. 

Categories and Subject Descriptors: H.5.1 Multimedia 
Information Systems, H.5.3 Group and Organization Interfaces 
General  Terms:  Management,  Design,  Experimentation, 
Human Factors. 

Keywords: Video, search, visualisation, user studies. 

1.  INTRODUCTION 
In  recent  years  there  has  been  an  explosion  in  the  volume  of 
digital video in people’s personal  collections, online and in work 
environments  e.g.  libraries  and  TV  channels.  However,  this 
growth  in  the  volume  of  video  has  not  been  matched  by  the 
development  of  tools  and  systems  that  are  available  to  organise 
and  search  these  large  and  rapidly  growing  volumes  of  video. 
These problems can be further exacerbated in some domains, e.g. 
home video collections or the media industry,  where the number 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
JCDL’09, June 15–19, 2009, Austin, Texas, USA. 
Copyright 2009 ACM  978-1-60558-322-8/09/06...$5.00. 
 

of users that access the video collections is small in comparison to 
the  quantity  of  data  present  or  indeed  in  comparison  with  the 
number of people that could view video material online. Even so, 
a large proportion of online videos are viewed by relatively small 
numbers  of  users  [12].  This  can  lead  to,  or  be  caused  by,  large 
swaths  of  video  collections  and  archives  having  very  few 
annotations. These videos thus become practically irretrievable as 
many current state of the art systems rely on using annotations. As 
a way to ease the dependency on textual information, either from 
annotations  or  other  sources  such  as  speech  recognition,  current 
state  of  the  art  systems  make  use  of  either  low  level  features  of 
videos  or  high  level  features;  the  latter  in  a  form  of  a  concept-
based  vocabulary  [18].  However,  to  date  none  of  these  methods 
has  proved  to  be  sufficient  to  overcome  the  problems  associated 
with  video  search  (see  Section  3.1  for  a  full  discussion).  Thus 
there is an ever increasing need to develop tools to support users 
in their increasingly complex and difficult video search tasks. 

The  emerging  area  of  interactive  video  retrieval  has  the  goal  of 
alleviating  some  of  the  problems  associated  with  video  search 
[19][14][23],  by  offering  the  user  specialised  tools  to  more 
extensively  and  effectively  explore  a  video  collection.  In  this 
context, we have developed ViGOR, a video retrieval system that 
allows  users  to  create  semantic  groups  of  results  to  help 
conceptualise and organise their results for complex video search 
tasks. This interactive grouping is a flexible means for the user to 
express  short  and  long-term  goals,  as  well  as  specific  and  multi-
faceted information needs.  The localised grouping also allows the 
user  to  focus  on  one  particular  aspect  of  a  global  task.  We  also 
believe  that  the  semantic  gap,  [11],  which  is  the  difference 
between  low  level  features  that  machines  use  to  represent 
multimedia  and  the  high  level  concepts  that  humans  associate 
with  the  same  video,  is  shrunk  by  the  construction  of  high-level 
semantic  groupings  rather  than  focusing  on  the  search  process, 
and  also  by  a  flexible  user  interaction  with  the  video  collection. 
We also believe that the use of this system can result in a number 
of  advantageous  outcomes  for  users:  improved  user  performance 
in  terms  of  task  completion  and  task  exploration,  and  increased 
user satisfaction with their search and their search results.   

In order to evaluate the usefulness of ViGOR for assisting users in 
carrying out video search tasks a number of user evaluations were 
carried out. The first evaluation involved users carrying out broad 
tasks  on  YouTube  and  demonstrated  the  benefit  of  using  this 
grouping  functionality  for  broad,  multi-faceted  tasks  on  a  large 
online  collection.  The  second  evaluation  involved  users  carrying 
out  focused  TRECVID  tasks  on  the  TRECVID  2007  video 
collection, with the benefit of having relevance judgments on each 
task, this allowed us to provide a more rigorous evaluation of  

87Figure 1: Screen shot of ViGOR interface for YouTube.  

 

ViGOR, and further illustrate the benefits of the retrieval system. 
The  remainder  of  the  paper  is  organised  as  follows:  The  next 
section  describes  the  ViGOR  interface  and  an  example  usage 
scenario  that  illustrates  the  benefits  of  searching  with  ViGOR. 
Following  this  we  describe  the  state  of  the  art  in  video  search 
interfaces and highlight the contributions that are being made by 
ViGOR.  Section  4  details  our  evaluation  methodologies  and  the 
experimental  results.  Finally  we  will  provide  some  conclusions 
and directions for future work. 
2.  VIGOR INTERFACE 
ViGOR  (see  Figure  1)  comprises  of  a  search  panel  (A),  results 
display  area  (B),  workspace  (C)  and  playback  panel  (D).  These 
facilities  enable  the  user  to  both  search  and  organise  results 
effectively. The users enter a text based query in the search panel 
to begin their search. The result panel is where users can view the 
search  results  (a).  Additional  information  about  each  video  shot 
can be easily retrieved by placing the mouse cursor over a video 
keyframe for longer than 1.5 seconds, which will result in any text 
associated  with  that  video  being  displayed  to  the  user  (we  will 
hence  forth  refer  to  this  action  as  tooltip)  (e).  If  a  user  clicks  on 
the  play  button  the  highlighted  video  shot  will  play  in  the 
playback panel. Users can play, pause, stop and navigate through 
the video as they can on a normal media player (D). Similar to the 
ImageGrouper  [17]  and  EGO  [22]  systems,  the  main  component 
of  ViGOR  is  the  provision  of  a  workspace  (C).  Groups  can  be 
created  by  clicking  on  the  create  group  button.  Users  must  then 
select  a  textual  label  for  the  group  and  can  potentially  add  any 
number of annotations to the group, but each group must have at 
least one annotation. Drag-and-drop techniques allow the user to 
drag videos into a group or reposition the group in the workspace 
(b).  Groups  can  be  deleted,  minimised  and  moved  around  the 
workspace using a number of buttons (f). It should be noted that 
any  video  can  belong  to  multiple  groups  simultaneously.  The 
workspace is designed to accommodate a large number of groups. 
Each group can also be used as a starting point for further search 
queries. Users can select particular videos and can choose to view 

an  expansion  of  the  group  that  contains  similar  videos  based  a 
number  of  different  features  (c,  d);  full details  of  the  expansions 
used  for  our  evaluations  are  available  in  Section  4.  The 
description above describes the basic functionality of ViGOR; two 
slightly different versions of ViGOR were used for evaluation on 
two  different  datasets  i.e.  YouTube  and  TRECVID.  These 
different  interfaces  as  well  as  the  baseline  systems  used  for 
evaluation  will  be  described  in  the  experimental  methodology.  
The  following  subsection  presents  a  scenario  describing  how  a 
user can potentially use the ViGOR system,  with a discussion of 
the key advantages the system is intended to provide.  
2.1  Example Scenario: Review of Sports 
Stories for 2008 
The example scenario is based on the news domain, and involves 
a user who must simultaneously search and learn about a topic:  

“Xavi  works  in  the  news  department  for  a  large  broadcasting 
corporation based in Barcelona. Coming towards the end of 2008 
the news department decided that they would broadcast a segment 
highlighting  the  major  sports  stories  from  2008.  Xavi  was  given 
the task of finding video clips that highlight the main stories from 
2008.  This  task  is  very  broad  and  ambiguous  and  as  Xavi’s 
expertise is in the area of current affairs he is unsure about what 
all  of  the  major  stories  were.  However,  he  knows  that  the 
Olympics  in  Beijing  and  Euro  2008  were  major  stories  and  also 
that  a  Formula  1  and  MotoGP  races  were  held  close  by.  Xavi 
begins by searching for the Olympics. He creates a group for the 
Olympics and using keyword based search finds videos relating to 
the opening ceremony, Usain Bolt, Michael Phelps and numerous 
Spanish  medal  winners  and  adds  them  to  the  Olympics  group. 
While  watching  the  videos  that  relate  to  the  opening  ceremony 
Xavi notices that Rafael Nadal represented Spain in the Olympics, 
so Xavi creates a separate group for Rafael Nadal and drags a shot 
of Rafael Nadal at the Olympics from the Olympics group into the 
Nadal  group.  Xavi  then  continues  to  add  shots  of  other  events 
relating  to  Rafael  Nadal  that  he  finds  through  text  based  search, 
e.g.  the  Wimbledon  final,  French  Open  etc.  to  the  group.  While 

88searching for videos of Rafael Nadal, Xavi finds a video of Nadal 
in  the  crowd  at  a  Barcelona  football  game.  Xavi  then  decides  to 
also  create  a  group  relating  to  Spanish  domestic  football,  and 
includes stories about Real Madrid winning La Liga and Spanish 
teams  exploits  in  European  competitions.  This  inspires  Xavi  to 
return to one of his two original stories and he creates a Euro 2008 
group.  As  Spain  won  Euro  2008  he  has  numerous  video  shots 
relating to the final and matches involving Spain. Having filled all 
of these groups Xavi finally creates a motorsport group; however 
he  is  unsure  of  the  names  of  any  participants  so  he  begins  by 
searching  for  the  Spanish  GP.  Having  found  a  couple  of  video 
shots  he  adds  them  to  the  group,  using  the  tools  available  in  the 
group he is able to find related video shots and adds them to the 
group.  Xavi  then  deletes  the  original  shots  that  he  had  added  to 
the  group  as  he  feels  that  the  new  shots  are  more  appropriate. 
Having used the available grouping tools Xavi has now completed 
his  task,  during  which  he  has  found  video  shots  that  relate  to 
numerous  sporting  events  and  indeed  many  different  aspects  of 
those events”.    

features 

that  are 
This  scenario  highlights  a  number  of 
implemented in ViGOR to support search and browsing. Perhaps 
the  most  obvious  of  which  is  the  organisation  of  search  results 
into  a  number  of  groups,  all  of  which  can  be  viewed 
simultaneously. Visualising the different groups in the workspace 
can be seen as a way of naturally visualising different aspects of a 
complex search task, yet allowing earlier relevant search results or 
aspects to be immediately accessible. This provides the user with 
an overview of the state of the task at any given time. ViGOR also 
makes it simple to create new groups from results in other groups, 
as results are allowed to be members of multiple groups, also by 
interacting  and  searching  the  user  finds  material  to  create  new 
groups  as  a  by  product.  This  is  difficult  in  most  current  video 
search  systems,  where  search  results  are  typically  viewed  in  a 
single  result  list,  and  the  user  must  navigate  backwards  and 
forwards through their search history to find previous results. By 
allowing the user to organise relevant search results into semantic 
groups,  ViGOR  is  empowering  the  users  by  allowing  them  to 
make  use  of  relevant  material,  irrespective  of  what  stage  of  the 
search  it  is  found.  This  can  be  difficult  in  many  existing  search 
systems; for example, it can be difficult to use a good video found 
via YouTube in a new search with a different system. The re-use 
of  this  material  within  a  group  to  launch  new  searches  also 
benefits  the  user,  as  this  allows  the  user  to  concentrate  on  one 
particular aspect of a task at a time while still accomplishing the 
overall goal of the task in hand. In a nutshell, ViGOR is designed 
to  provide  facilities  for  the  organisation  of  a  search  task  into 
groups to visualise a search task, re-organisation of search results 
between groups, and preservation of valuable search results. It is 
hoped that the combination of these features will allow the user to 
accomplish  a  search  task  more  easily  and  also  to  leverage  the 
serendipity  involved  in  their  search.  In  the  following  section  we 
will discuss in more detail the difficulties involved in interactive 
video  search  and  will  also  describe  a  number  of  innovative 
interfaces for multimedia search and retrieval. 
3.  RELATED WORK 
3.1  Interactive Video Retrieval 
As  a  result  of  the  multimodal  nature  of  video,  e.g.,  speech,  text, 
visual  or  spatiotemporal  information,  automatic  video  retrieval 
systems  find  difficult  to  select  the  optimal  combination  of  this 
available information to be exploited during a specific search task. 

This  problem  is  somehow  alleviated  in  most  interactive  video 
search  systems,  as  users  are  able  to  select  which  information  is 
used  and  how  this  information  is  exploited.  Furthermore, 
interactive  systems  allow  users  to  reformulate  their  search  based 
on previously seen results until their information need is satisfied, 
and a large portion of the collection has been explored.  

This  multimodal  information  associated  to  video  results  in 
different retrieval methodologies being available to the user.  The 
most common and most familiar method of searching for video is 
query  by  text,  the  main  benefit  of  this  approach  is  its  simplicity 
and  familiarity  for  users.  This  approach  is  used  by  most  online 
video  search  services,  e.g.  YouTube,  Blinkx  etc.  and  is  also  the 
most commonly used method at the TRECVID workshop [6]. The 
text that is used to represent the video can come from a variety of 
sources  including  extraction  of  speech,  closed  captions  or  user 
annotations.  However,  these  textual  sources  are  not  always 
available and when they are they may not always be reliable for a 
number of reasons, including language differences and limitations 
in  automatic  speech  recognition  [15].  In  the  online  video  search 
scenario,  search  systems  rely  on  textual  annotations  provided  by 
users. This, however, presents some problems, for instance, users 
often  have  different  views  on  the  same  video  and  thus  tag  the 
video  differently, 
in  synonyms,  polysemy  and 
homonymy  [10].  In  addition,  users  need  special  motivation  in 
order to provide large numbers of tags [12]. 

resulting 

An alternative retrieval approach is to use the actual content of the 
video  in  the  form  of  low  level  features  such  as  colour,  shape,  or 
texture etc. Using these features users can select images or video 
clips  and  submit  them  as  queries,  for  comparison  with  other 
images or videos; this is known as query by example. While this 
query  method  does  not  rely  on  the  availability  of  external 
resources  such  as  text,  it  is  not  without  its  own  difficulties.  For 
each  video  or  image,  all  of  the  low  level  features  must  be 
extracted and stored, creating efficiency problems. In addition, the 
semantic  gap  [11],  that  is  the  difference  between  the  low  level 
representation of features of the video and the high level concepts 
that  users  associate  with  the  video,  provide  difficulties  for  this 
querying  method.  In  an  attempt  to  overcome  this  semantic  gap, 
some  research  has  taken  place  into  search  by  concept.  The  basic 
premise  of  search  by  concept  is  that  semantic  concepts  such  as 
“sunset”  or  “city”  can  be  used  to  search  and  retrieve  video;  one 
example  of  concept  thesaurus  is  the  Large  Scale  Ontology  for 
Multimedia  (LSCOM)  [18].  As  with  the  other  approaches  for 
searching for video query by concept is not without its drawbacks, 
to  date  it  has  not  been  deployed  on  a  large  scale  and  also  to  be 
truly  successful  requires  the  representation  of  a  large  number  of 
all possible concepts.  

Although all of these approaches have proved to be unsuccessful 
when  applied  in  isolation,  a  number  of  systems,  including 
Informedia  [13]  and  MediaMill  [21],  have  applied 
them 
successfully  in  combination  for  recent  TRECVID  interactive 
search evaluations. It should be noted however, that these results 
were  for  expert  users  who  form  the  upper  bound  for  user 
performance  [6].  In  addition,  any  combination of  the  approaches 
outlined  above  would  require  extracting  and  storing  multiple 
representations of the same video in different formats. As none of 
these  query  approaches  has  proved  successful  to  date  some 
innovative  interfaces  have  been  proposed  to  help  overcome  the 
query formulation methods and to bridge the semantic gap.   

893.2  Interactive Multimedia Retrieval 
Interfaces 
Similar to ViGOR some image search systems have allowed users 
to organise search results in a workspace. EGO  [22] is a tool for 
the  management  of  image  collections.  The  main  components  of 
EGO  are  a  workspace  and  a  recommendation  system.  By 
providing  these  facilities,  different  types  of  requirements  are 
catered  for, enabling the user to both search and organise results 
effectively. The workspace serves as an organisational ground for 
the  user  to  construct  groupings  of  images.  The  recommendation 
system in EGO observes the user's actions, which enables EGO to 
adapt  to  their  information  requirements  and  to  make  suggestions 
of  potentially  relevant  images  based  on  a  selected  group  of 
images. ImageGrouper [17] is another interface  for digital image 
search  and  organisation.  It  is  possible  to  search,  annotate,  and 
organise  images  by  dragging  and  grouping  images  on  the 
workspace.  ImageGrouper  allows  users  to  use  an  entire  group to 
query by example and allows a user to annotate an entire group at 
once.  However,  unlike  ViGOR,  users  cannot  decide  which 
members of the group to use for querying.  In addition, no formal 
evaluation  of  ImageGrouper  has  taken  place.  Campbell  et  al  [4] 
present  a  novel  image  search  and  browsing  system  in  the 
Ostensive  Browser.  The  main  component  of  the  interface  is  a 
workspace with objects on it and links between those objects. The 
user begins browsing at a starting image, around which candidate 
next  images  are  displayed.  A  user  selects  a  candidate  which 
becomes  the  centre  of  focus;  the  next  possible  candidate  images 
related  to  the  current  image  are  displayed.  Candidate  images  for 
browsing  are  determined  by  an  ostensive  model,  which 
encompasses  a 
is 
accomplished  by  the  application of  a  particular  class  of  discount 
function with respect to the age of the evidence, thus more recent 
user  interactions  are  more  relevant  for  determining  next  steps  in 
comparison with older interactions. While the Ostensive Browser 
includes  a  workspace,  the  goal  of  this  workspace  is  different  to 
ViGOR; in this case the workspace is to facilitate user navigation 
and  present  navigation  options.  PicturePiper  [8]  provides  a 
mechanism for allowing users access to images on the web related 
to a topic of interest. This system was developed to demonstrate a 
re-configurable  pipeline  architecture  that  is  ideally  suited  for 
applications in which a user is interactively managing a stream of 
data. PicturePiper also contains a workspace for displaying search 
results; however the use of this workspace is very different to the 
workspace  utility  in  ViGOR.  The  distance  between  groups  of 
images  in  the  workspace  illustrates  the  distance  between  the 
centroids  of  the  groups  of  images  as  calculated  using  low  level 
features.  CueFlik  [9]  is  a  Web  image  search  application  that 
allows users to create their own rules for ranking images based on 
their visual characteristics. Users can then re-rank possible search 
results  according  to  these  rules.  In  user  evaluations  it  was  found 
that  users  can  quickly  create  effective  rules  for  a  number  of 
diverse  concepts.  While  CueFlik  allows  users  to  quickly  find 
relevant search results and reuse rules for future searches it does 
not allow users to organise search results or to maintain old search 
results and carry out new searches, unlike ViGOR.  
 
While  the  systems  mentioned  above  have  made  a  number  of 
advances  in  relation  to  image  search,  there  are  a  number  of 
important differences that make video search much more difficult 
than  image  search.  The  first  main  difference  is  the  multimodal 
nature of video, encompassing images, text, audio and a temporal 

temporal  profile  of  uncertainty.  This 

the 

that 

the  user 

is  currently  viewing,  with 

factor  etc.  While  text  and  visual  features  may  be  used  to  aid  or 
hamper image search, these are only two of the many  modalities 
involved  in  video  search.  Secondly,  video  is  a  much  more 
interactive  medium.  Interactive  video  retrieval  systems  have  to 
make  an additional effort to aid the user in deciding whether the 
selected  videos  are  relevant  or  not  for  their  tasks,  whereas  for 
image  retrieval  systems  the  user  can  easily  and  quickly  discern 
relevant and irrelevant results. The result of this is that interaction 
and usage information from interactive video retrieval systems is 
far noisier than the usage information on image retrieval systems. 
For  instance,  on  average,  75%  of  the  user  results  that  the  user 
interacted  with  on  the  image  retrieval  system  developed  by  [7] 
were  relevant,  whereas  only  7-9%  of  search  results  that  the  user 
interacted  with  were  relevant  for  a  similar  interactive  video 
retrieval system [16]. As a result of this, interactive video retrieval 
systems  strive  to  lower  the  effort  for  the  user  to  explore  and 
decide  if  a  result  is  relevant.  To  that  end  a  number  of  video 
retrieval systems have been developed to aid user interaction.  
 
The  ForkBrowser  [19]  embeds  multiple  search  methods  into  a 
single  interface  for  browsing.  The  multiple  search  methods  are 
presented  to  the  user  in  the  form  of  threads.  These  threads  are 
ranked  lists  of  shots  based  on  one  of  the  search  methods 
implemented  in  the  interface.  The  threads  are  visualised  in  the 
shape of a fork. The shot at the top of the stem of the fork is the 
video 
tines 
representing the different threads. The ExtremeBrowser [14] aims 
to  maximise  the  human  capability  for  judging  visual  material 
quickly,  while  at  the  same  time  applying  active  learning 
techniques using the user selected videos. Videos are presented to 
the user via a method called rapid serial visual presentation which 
allows  the  user  to  make  fast  judgements  about  high  numbers  of 
videos.  The  feedback  from  the  user  is  used  in  an  active  learning 
loop, which is used to rank the remaining results that the user will 
review.  The  system  that  is  most  similar  to  ViGOR  is  the 
FacetBrowser  [23];  the  FacetBrowser  is  a  video  search  interface 
that supports the creation of multiple search "facets", to aid users 
carrying  out  complex  video  search  tasks  involving  multiple 
concepts.  Each  facet  represents  a  different  aspect  of  the  video 
search  task.  These  facets  can  be  organised  into  stories  by  users, 
facilitating  the  creation  of  sequences  of  related  searches  and 
material  which  together  can  be  used  to  satisfy  a  work  task.  The 
interface allows more than one search to be executed and viewed 
simultaneously,  and 
to  be 
reorganised between the facets. The focus of the FacetBrowser is 
to merely allow the user to view multiple search threads. ViGOR 
is  much  more  interactive  and  allows  the  user  to  store  and  mix 
results from previous searches to inform future searches, while at 
the same time allowing the user to carry out searches independent 
of  these  groups  if  the  user  wishes.  The  interactive  grouping  is  a 
flexible  means  for  the  user  to  communicate  short  and  long-term, 
specific  and  multi-faceted  information  needs  and  the  grouping 
also allows the user to focus on one particular aspect of a global 
task.  The  following  section  will  provide  details  of  two  user 
evaluations that were carried out using ViGOR. 
4.  USER EVALUATIONS 
In  order  to  measure  the  effectiveness  of  ViGOR  we  conducted 
two user-centred evaluations. The goal of both evaluations was to 
investigate the effect of using the grouping paradigm available in 
ViGOR  to  help  users  complete  video  search  tasks.  There  are  a 
number of research questions that we wanted to address.  

importantly,  allows  material 

901.  Does  user  performance 

improve  with 

the  grouping 

functionality available in ViGOR? 

2.  Do the workspace and the grouping functionality available in 

ViGOR allow users to explore more aspects of their task?  

3.  Will  the  ViGOR  system  increase  user  satisfaction  with  their 

search and the search process? 

Two  user  evaluations  were  carried  out  in  order  to  answer  the 
research  questions  above.  Both  user  evaluations  were  slightly 
different  and  allowed  us  to  use  different  measures.  The  first 
evaluation  was a between subjects evaluation that involved users 
carrying  out  broad  tasks  on  YouTube,  this  provided  us  with  a 
large  and  dynamic  data  collection,  and  facilitated  the  analysis  of 
ViGOR in an online situation. The second evaluation was a within 
subjects  evaluation 
focused 
TRECVID  tasks  on  the  TRECVID  2007  video  collection,  this 
evaluation  provided  us  with  relevance 
judgments  for  ask 
completion  and  also  allowed  users  to  compare  ViGOR  and  a 
related baseline system directly.  

involving  users  carrying  out 

For  research  question  1,  a  number  of  different  measures  of 
performance  were  used.  For  both  evaluations  we  measured  the 
number of videos marked as relevant for ViGOR and the baseline 
system, thus we can see if ViGOR is of benefit for both broad and 
specific tasks. In addition the relevance  judgments supplied with 
the  TRECVID  2007  collection  allowed  the  measurement  of 
precision  and  recall  values  for  the  second  evaluation.    Our 
hypothesis for research question 1 is: 

  Hypothesis  1:  Despite  the  overhead  involved  in  the  extra 
grouping  functionality,  that  user’s  performance  will  improve 
using  the  grouping  functionality  in  the  ViGOR  system  in 
comparison with an appropriate baseline system. (Mark more 
videos as relevant, find better videos, i.e. similar or improved 
precision and recall) 

Research  question  2  is  slightly  more  difficult  to  quantify.  It  has 
been argued that for complex search tasks that an increase in user 
interactions  is  indicative  of  a  good  task  performance,  and  is 
preferable  for  these  kinds  of  tasks  [3].  Thus  for  the  YouTube 
evaluation  we  will  be  looking  at  the  user  interactions  with  both 
systems to see if there is any change. In addition, for the YouTube 
evaluation  we  had  independent  judges  make  judgments  on  the 
results  retrieved  by  users  of  the  baseline  system  to  discern  how 
many unique aspects of the task the participants had investigated; 
this  was  compared  with  the  number  of  groups  that  users  of 
ViGOR  created  the  complete  the  same  tasks.  The  same  measure 
was  not  appropriate  for  the  TRECVID  hypothesis,  as  these  are 
very direct and unambiguous tasks and do not encompass as many 
aspects. Hypothesis for research question 2 is:  

  Hypothesis  2:  Users  will  explore  more  aspects  of  their  task 
using  ViGOR  and  that  the  workspace  will  help  the  users 
explore and see more options in large and unfamiliar datasets. 
(More interactions, explore more aspects for each task) 

In  order  to  address  research  question  3  we  asked  the  users  to 
complete  a  number  of  questionnaires  at  different  stages  of  both 
evaluations. In both evaluations users were asked about the videos 
they were returned by the search system, their interaction with the 
search system, their search process, the task they had carried out 
and the search interface itself. As the TRECVID evaluation was a 
within  subjects  evaluation  we  also  asked  the  participants  in  this 
evaluation to directly  compare ViGOR  with the baseline system. 
Using  the  results  of  all  of  these  questionnaires  we  measured  the 

user reactions to a number of aspects of the searches that they had 
carried out. Hypothesis 3 for research question 3 is: 

  Hypothesis  3:  Users  will  be  more  satisfied  with  their  search 
results  and  the  search  process  using  ViGOR  and  in  a  direct 
comparison  users  will  have  a  preference  for  the  ViGOR 
system. (Satisfaction, user questionnaires) 

In the following sections we will describe both evaluations in full 
detail  and  will  also  outline  the  results  obtained  for  both 
evaluations. 
4.1  Exploratory Task Evaluation 
4.1.1  Task and Collection 
For the purposes of this evaluation we used the YouTube API to 
provide access to YouTube to provide a collection. Four simulated 
work  task  situations  were  created  in  order  to  provide  broad, 
ambiguous, open ended tasks for the users  [2]. These tasks  were 
related  to  different  topics  and  multiple  aspects.  Each  task 
prompted the user to address at least two fixed aspects of the task, 
in  order  to  aid  our  possible  posterior  analysis  and  comparisons. 
Users were encouraged to address at least one additional and open 
aspect of the task, there was no limit on the number of aspects the 
user could search for. The four evaluated simulated tasks were:  

  A task of  finding videos of political figures of 2008: George 
Bush  and  Barack  Obama  and  at  least  one  other  political 
leader. 

  A  task  of  finding  video  clips  about  Paris,  Rome  and  at  least 

other Europe location.  

  A  task  of  finding  videos  that  illustrate  Scottish  culture,  in 

particular Scottish dancing and food, among other aspects. 

  A task of finding the major sport news stories of 2008: Beijing 
Olympics, the Euro 2008 Football Championship and at least 
another event. 

the 

The  ultimate  goal  of  these  simulated  tasks  was  to  write  a  short 
essay (e.g. a class project, a short description for a friend, etc.). In 
this way users were encouraged to carry out a deep exploration of 
the information addressed in the tasks and think thoroughly about 
a possible structure of the retrieved information. We thus avoid a 
“berry  picking”  effect,  as  users  were  encouraged  to  store  only 
those videos that were potentially relevant for each task’s goal. 
4.1.2  Experimental Setup 
A  between  subjects  design  was  adopted  for  this  evaluation.  Two 
interfaces  were  evaluated; 
first  was  ViGOR,  which 
implemented the extra within group search functionality with the 
YouTube  API.  The  interface  offers  three  expansion  options  for 
each  group  (see  Figure  1  (d)):  1)  related  videos;  2)  videos  from 
the same user 3) and text expansion which is the result of a new 
search  using  text  extracted  from  the  selected  videos.  All  of  the 
videos returned by these expansion options are retrieved using the 
YouTube  API.    The  second  interface,  which  we  will  refer  to  as 
YouTube Interface (YI), mimicked the functionality of YouTube. 
Users  could  search  via  text  and  when  a  video  was  playing  users 
were  presented  with  lists  of  related  videos  and  videos  from  the 
same  user,  in  the  same  way  that  YouTube  does,  this  also 
mimicked 
the  group 
expansions  explained  above.  In  addition,  users  of  the  YI  were 
provided  with  a  panel  where  they  could  drag  and  drop  relevant 
videos that they had found. We made the supposition that users of 
ViGOR  would  store  relevant  results  in  each  group  panel.  Each 
participant carried out all four tasks either using the YI or ViGOR. 

functionality  available 

through 

the 

91the  usefulness  of 

The  order  of  tasks  was  varied;  this  was  to  avoid  any  order  or 
learning effect associated with the tasks. Using this experimental 
model  we  can  evaluate 
the  grouping 
functionality in ViGOR’s workspace for helping users to complete 
open  ended  and  broad  tasks.  Each  participant  was  given  five 
minutes training on their search system and was allowed to carry 
out  training  tasks.  Users  had  a  maximum  of  20  minutes  to 
complete each of these tasks. For each participant their interaction 
with  the  system  was  logged,  the  videos  they  marked  as  relevant 
were stored and they also filled out a number of questionnaires at 
different stages of the experiment. 
4.1.3  Users 
16  participants  took  part  in  our  evaluation,  they  were  randomly 
divided  into  two  groups  of  8  and  each  group  used  one  of  the 
systems.  The  participants  were  mostly  postgraduate  students  and 
researchers at a university. The participants consisted of 12 males 
and 4 females with an average age of 29 years (median: 27.5) and 
an advanced proficiency with English.  The participants indicated 
that  they  regularly  interacted  with  and  searched  for  multimedia. 
They  were  paid  a  sum  of  £12  for  their  participation  in  the 
evaluation, which took approximately 2 hours. 
4.2  Exploratory Task Evaluation Results 
4.2.1  System Performance 
In  a  direct  comparison  between  the  two  interfaces  it  was  found 
that  on  average  users  of  ViGOR  marked  35.09  videos  as  being 
relevant (by assigning them to a group) in comparison with 23.16 
videos  for  users  of  the  YI.    This  was  also  achieved  in  less  time, 
with  users  of  ViGOR  completing  their  task  in  18.6  minutes  in 
comparison  with  19.06  minutes  for  users  of  the  YI.  While  the 
difference in time may not be that noteworthy the increase of over 
50% in the number of retrieved videos is. In order to gain a further 
insight  into  the  difference  in  the  performance  between  the  two 
interfaces,    a  further  analysis  of  the  logs  was  carried  out  to 
investigate  the  user  interactions,  the  results  of  this  analysis  is 
shown  in  Table  1.  It  can  be  seen  in  Table  1  that  the  ViGOR’s 
users  have  more  user  interactions  with  the  system  overall  in 
comparison with users of the YI. This is an encouraging result, as 
more interactions for complex tasks are a preferable result [3]. In 
addition, most of this difference is due to the increased use of the 
tooltip  functionality  of  the  ViGOR  users;  this  is  a  lightweight 
functionality  which  is  of  low  cost  for  the  user  to  carry  out.  In 
comparison, actions that require more effort and time decrease on 
ViGOR  in  comparison  with  the  users  of  YI.  Notably,  YI  users 
viewed  19.9%  more  videos  and  issue  8.41%  more  queries.  In 
conclusion, it can be seen quite clearly that users of ViGOR find 
more videos in less time. Also the overall number of interactions 
is  higher  which  is  a  positive  result, however  the  number  of  high 
cost  interactions  in  terms  of  user  effort  are  reduced  by  using 
ViGOR freeing the user to explore the task and collection. Users 
of  the  ViGOR  systems  created  an  average  of  4.5  group  panels, 
which shows that they went well beyond the 3 mandatory aspects 
and  were  comfortable  using  the  interface  to  investigate  the 
number of aspects. We evaluated the results and essays created by 
the  users  of  the  YI  interface  to  determine  the  number  of  aspects 
that were investigated. Users of the YI interface explored slightly 
less aspects than the ViGOR interface, 4.1. This indicates that the 
users  that  were  using  ViGOR  investigated  more  aspects  of  the 
tasks.  Furthermore,  this  fact  together  with  the  higher  number  of 
relevant documents retrieved indicates that ViGOR users  created 
more complex and more detailed aspects than those of the YI. 

YouTube Interface 
% 
Number 
60.74% 
240.25 
5.43% 
25.13 
13.86% 
56.50 
88.50 
19.48% 
0.49% 
2.38 
412.15 
100% 

Interface 
Action 
Tooltip 
View 
Query 
Relevant 
Irrelevant 
Total 
Table 1: Total number of different interactions for each user 
for each interface. Interactions are for unique videos e.g. if a 

ViGOR 
Number  
317.50 
20.13 
51.75 
126.63 
3.13 
654.88 

% 
61.16% 
3.88% 
9.97% 
24.39% 
0.6% 
100% 

user plays the same video twice we only record it once. 

/ 

/ 

Irrelevant” 

(Appropriate), 

(Relevant),  “Appropriate 
“Complete 

4.2.2  User Feedback 
In post search task questionnaires we solicited subjects’ opinions 
on their assigned system and their reaction to the retrieved videos. 
The  following  5-point  Likert  scales  and  semantic  differentials 
were used. “The videos that I have received through the searches 
were”  “Relevant 
/ 
Inappropriate” 
Incomplete” 
(Complete) and “Familiar / Strange” (Familiar). “I had an idea of 
which  kind  of  videos  were  relevant  for  the  topic  before  starting 
the  search”  (Prior).  “I  found  it  easy  to  formulate  queries  on  this 
topic”  (Formulate).  “During  the  search  I  have  discovered  more 
aspects  of  the  topic  than  initially  anticipated”  (Discover).  “The 
video(s)  I  chose  in  the  end  match  what  I  had  in  mind  before 
starting  the  search”  (Match).  “The  tools  provided  allowed  me  to 
find  videos  that  matched  the  topic”  (Tools).  “My  idea  of  what 
videos  and  terms  were  relevant  changed  throughout  the  task” 
(Change). “I am satisfied with my search results” (Satisfy). Table 
2 presents the average responses for each of these scales using the 
labels  after  each  of  the  Likert  scales  in  the  list  above.  The  most 
positive response for each user type is shown in bold.  

Differential 
Relevant 
Appropriate 
Complete 
Familiar 
Prior 
Formulate 
Discover 
Match 
Tools 
Change 
Satisfy 

YouTube Interface 
4.03125 
3.875 
3.12875 
3.90625 
4 
3.8125 
2.875 
3.7185 
3.84375 
2.6875 
3.65625 

ViGOR 
4.09375 
4.09375 
3.375 
3.5 
3.875 
4.15625 
3.3125 
3.65625 
4.1875 
2.875 
3.75 

Table 2: Perceptions of Retrieved Videos (Higher = Better) 

From  the  results  in  Table  2  it  appears  that  participants  have  a 
better  perception  of  the  retrieved  videos  while  interacting  with 
ViGOR. The trend indicates that the users believe that they found 
more relevant, appropriate and diverse videos while using ViGOR 
in  comparison  with  the  YouTube  interface.  We  applied  two-way 
analysis  of  variance  (ANOVA)  to  each  differential  across  both 
systems  and  the  4  tasks  to  test  these  assertions.  None  of  the 
differences  for  the  findings  in  Table  2  were  found  to  be 
statistically  significant.  However  a  number  of  findings  in  other 
parts  of  the  questionnaires  were  found  to  be  significant.  When 
asked  about  their  task  performance,  users  of  YI  had  a  much 
stronger  perception  that  the  video  collection  didn’t  contain 
video(s) they wanted (F=6.5, p=0.0134), that the YI system didn’t 
return relevant videos  (F=10.64, p=0.0018) and that they did not 
have enough time to complete the task (F=5.18, p=0.0265). These 

92though 

through  ViGOR  even 

findings  are  consistent  with  the  findings  of  the  questionnaires 
relating  to  the  retrieved  videos  (see  Table  2).  Also,  when  asked 
for  their  reaction  to  the  system  via  semantic  differentials  a 
significant  difference  was  found  between  ViGOR  and  the  YI  on 
how  novel  (F=4.93,  p=0.0434)  and  flexible  (F=8.14,  p=0.0128) 
the  users  perceived  both  systems  to  be.  In  summary,  the  first 
evaluation has shown that ViGOR users retrieved more videos in 
less time and with less expensive interactions in comparison with 
a  comparable  baseline  interface.  The  users  of  ViGOR  also 
perceived  that  they  retrieved  more  relevant,  appropriate  and 
diverse  videos 
the  retrieval 
mechanisms  were  the  same  for  both  systems.  In  addition,  these 
users found ViGOR to be more novel and flexible than the YI.  
4.3  TRECVID Evaluation 
In order to provide further validation for these findings a second 
evaluation  was  carried  out.  This  evaluation  used  the  TRECVID 
collection  and  tasks.  This  allowed  us  to  calculate  precision  and 
recall  values  for  the  retrieved  results.  However,  the  TRECVID 
tasks are much more focused tasks, ViGOR was not designed with 
these types of task in mind, but it is hoped that ViGOR can still 
aid  user  performance.  Furthermore,  this  evaluation  had  a  within 
user  Latin  Square  design,  which  allowed  us  to  directly  compare 
user  reactions  to  the  two  interfaces,  as  users  used  ViGOR  and  a 
baseline system to complete search tasks. 
4.3.1  Tasks and Collection 
There  are  two  main  reasons  for  using  the  TRECVID  2007 
collection. First, the TRECVID collection is a large,  well known 
and  commonly  used  video  collection.  Secondly,  the  TRECVID 
collection  has  a  number  of  tasks  for  which  the  relevant  and 
irrelevant  video  shots  in  the  collection  are  known.  In  2007  the 
TRECVID collection contained 18,142 shots (over 100 hours) of 
Dutch  magazine  television.  For  the  TRECVID  2007  interactive 
search  evaluations  there  were  a  total  of  24  tasks.  For  our 
evaluation we limited the number of tasks that the users carry out 
to  8.    This  allowed  us  to  carry  out  more  evaluations,  as  24 
individual  search  topics  did  not  have  to  be  carried  out  for  each 
participant.  In  order  to  examine  user  interactions  on  different 
types of tasks we choose the 8 tasks which had the highest number 
of  shots  marked  as  being  relevant during  TRECVID  runs.  The  8 
tasks were 

1.  Find  shots  of  a  person  walking  or  riding  a  bicycle  (1175 

relevant shots) 

2.  Find  shots  of  a  woman  talking  toward  the  camera  in  an 

interview - no other people visible (400 relevant shots) 

3.  Find shots of one or more people playing musical instruments 
such  as  drums,  guitar,  flute,  keyboard,  piano,  etc.  (376 
relevant shots) 

4.  Find shots with hills or mountains visible (343 relevant shots) 
5.  Find  shots  with  3  or  more  people  sitting  at  a  table  (332 

6.  Find  shots  of  waterfront  with  water  and  buildings  (265 

relevant shots) 

relevant shots) 

7.  Find  shots  of  a  very  large  crowd  of  people  (fills  more  than 

half of field of view) (264 relevant shots) 

8.  Find  gray  scale  shots  of  a  street  with  one  or  more  buildings 

and one or more people (210 relevant shots) 

4.3.2  Experimental Setup 
For  our  evaluation  we  adopted  a  2-searcher-by-2-topic  within 
subject’s Latin Square design. Two interfaces were evaluated; the 
first was ViGOR. The interface offers three expansion options for 

each group (see Figure 1 (d)): 1) similar colour; 2) similar shapes, 
this  was  retrieved  using  edge  histograms  3)  and  similar 
homogenous  texture.  This  functionality  allowed  users  to  use 
videos as examples and retrieved the most similar videos based on 
the  low  level  feature  selected  from  the  collection.  The  second 
interface,  which  we  will  refer  to  as  the  Search  Interface  (SI), 
allowed  the  users  to  query  the  collection  via  query  by  text  and 
query  by  example.  The  main  difference  between  both  interfaces 
was the lack of grouping functionality in the SI. Also by including 
query  by  text  and  query  by  example  in  both  interfaces  we  are 
including  the  two  most  widely  used  search  functionalities  at 
TRECVID  [6].  Users  of  both  systems  were  also  provided  with  a 
panel where they could drag and drop relevant videos for the task. 
This  design  encouraged  users  to  add  videos  to  groups  that  may 
help  their  search  task,  but  may  not  be  relevant  for  the  very 
specific  topics  that  are  part  of  TRECVID.  Otherwise  users  may 
have  been  discouraged  from  adding  irrelevant  shots  to  their 
groups.  This  also  gave  the  users  the  option  of  completely 
bypassing the grouping functionality if they wished, as they could 
just interact in the same way as they would with the SI, giving the 
grouping functionality more of an add on feel, rather than it being 
the complete focus of the evaluation, which may bias some users. 
Each participant carried out two tasks using the SI, and two tasks 
using  ViGOR.  The  order  of  system  usage  was  varied  as  was  the 
order  of  the  tasks;  this  was  to  avoid  any  order  effect  associated 
with the tasks or with the systems. Each participant was given five 
minutes  training  on  each  system  and  was  allowed  to  carry  out 
training  tasks.  These  training  tasks  were  also  tasks  from 
TRECVID  2007,  which  we  found  appropriate  as  they  also  had 
relatively  high  numbers  of  relevant  documents.  Each  actual  task 
had  a  fifteen  minute  maximum  time  limit.  For  each  participant 
their  interaction  with  the  system  was  logged,  the  videos  they 
marked as relevant were stored and they also filled out a number 
of questionnaires at different stages of the experiment.  
4.3.3  Users 
16 participants took part in our evaluation. The participants were 
mostly postgraduate students and researchers at a university. The 
participants consisted of 13 males and 3 females with an average 
age of 25.6 years (median: 26) and an advanced proficiency with 
English.  The participants indicated that they regularly interacted 
with and searched for  multimedia. They  were paid a sum of £15 
for 
took 
approximately 2 hours.  
4.4  TRECVID Evaluation Results 
4.4.1  System Performance 
As  we  were  using  the  TRECVID  collection  and  tasks,  we  were 
able  to  calculate  precision  and  recall  values.  The  results  of  our 
evaluation show that ViGOR outperforms the SI on a number of 
measures for the majority of the tasks. When using ViGOR users 
marked more shots as relevant and indeed found more shots in the 
collection  which  are  actually  relevant  for  6  of  the  8  tasks  (see 
Figure 2).  

the  experiment,  which 

their  participation 

in 

For  the  other  2  tasks  (Task  6  and  Task  7)  the  SI  performs 
significantly  better  in  terms  of  the  number  of  videos  marked  as 
relevant  and  the  number  of  relevant  videos  found.  A  further 
analysis  of  the  user  logs  was  carried  out  in  order  to  find  an 
explanation  for  this.  It  was  found  that  for  these  tasks  the  user 
success was down to navigation rather than using the search tools. 
Some  of  the  users  found  a  sequence  of  video  shots  which 
contained  large  numbers  of  relevant  shots;  the  users  would 

93navigate through the video marking each shot as relevant, this is 
not normally a realistic search solution for large video collections. 
For example in YouTube where navigation facilities are available, 
most of the video views can be attributed to searching rather than 
navigation  [12].  Also,  for  the  same  tasks  using  ViGOR,  users 
persisted  with  using  the  search  tools.  The  relative  success  of  the 
SI  for  these  tasks  can  then  be  attributed  to  the  nature  of  the 
collection  and  tasks.  In  order  to  gauge  the  actual  success  of 
ViGOR  we  also  analysed  the  user  performance  in  terms  of 
precision, recall and mean average precision (MAP).  MAP is the 
average of the precision values calculated at every position where 
a relevant document appears in the result list, and is normally used 
for  a  simple  and  convenient  system  performance  comparison. 
MAP  combines  both  recall  and  precision  oriented  measures  and 
gives an overall measure of the performance of the system, so for 
this reason we concentrate on the MAP of the results obtained by 
the  users.  We  see  that  for  4  of  the  8  tasks  the  MAP  when  using 
ViGOR is higher (see Figure 3). This is somewhat skewed by the 
fact  that  the  users  of  ViGOR  retrieved  more  videos  and  also  the 
performance  of  SI  users  for  tasks  6  and  7.  Overall  it  was  found 
that  when  using  ViGOR  users  retrieved  more  videos,  more 
relevant videos and despite the fact that ViGOR was not designed 
for direct TRECVID type tasks, it is comparable with and in some 
cases outperforms a more traditional video search interface. 

ViGOR

SI

60

50

40

30

20

10

0

0.15

0.1

0.05

0

1

2

3

4

5

6

7

8

Figure 2: Number of relevant shots found for each task and 

system combination. 

ViGOR

SI

1

2

3

4

5

6

7

8

Figure 3: MAP for each task and system combination. 

4.4.2  User Feedback 
With the intention of providing further validation for our findings 
and to gauge user perceptions, we analysed the post task and post 
experiment questionnaires that our participants filled out.  

 

 

to 

“rigid/flexible”, 

4.4.2.1  System and Interaction 
In post search task questionnaires we solicited subjects’ opinions 
on  and  reaction 
the  system.  The  following  semantic 
differentials  were  used  to  solicit  user’s  reaction  to  the  system; 
“terrible/wonderful”,  “satisfying/frustrating”,  “dull/stimulating”, 
“easy/difficult”, 
“efficient/inefficient”, 
“novel/standard”  and  “effective/ineffective”.  For  each  of  these 
differentials we assign a value of 5 to the most positive response 
and  1  to  the  most  negative.    Table  3  presents  the  average 
responses  for  each  of  these  differentials.  The  most  positive 
response across for each differential is shown in bold.  
Differential 
Terrible/Wonderful 
Satisfying/Frustrating 
Dull/Stimulating 
Easy/Difficult 
Rigid/Flexible 
Efficient/Inefficient 
Novel/Standard 
Effective/Ineffective 

ViGOR 
3.6451 
3.5806 
3.4193 
3.9356 
3.7097 
3.4838 
3.9555 
3.6129 

SI 
3.2222 
3.3056 
2.9722 
3.8056 
2.9444 
2.9444 
2.8056 
3.0556 

Table 3: Perceptions of System and Interaction (Higher = 

Better) 

flexible 

two-way  analysis  of  variance  (ANOVA) 

From  the  results  in  Table  3  it  appears  that  participants  have  a 
better perception of the system while interacting with ViGOR. We 
applied 
to  each 
differential  across  both  systems  and  the  8  tasks  to  test  these 
assertions.  It  was  found  that  the  differences  in  how  wonderful 
(F=5.21,  p=0.0263), 
(F=6.58,  p=0.0131),  efficient 
(F=5.83,  0.0191),  novel  (F=15.07,  p=0.0003)  and  effective 
(F=4.61,  p=0.0362)  the  users  found  the  system,  was  system 
dependent.  This  demonstrates  that  the  users  had  a  strong 
preference for ViGOR, and that they felt it was more flexible and 
effective  for  their  search  tasks,  thus  providing  a  better  user 
experience. This finding shows that ViGOR is providing  a better 
search experience for the user, and that they are more at ease and 
confident  using  the  this  system.  These  results  also  validate  the 
results  from  user  questionnaires  that  were  found  in  the  first 
evaluation. 
4.4.2.2  System Support 
 In  post  search  task  questionnaires  we  also  solicited  subjects’ 
opinions on their interaction with the systems and the support for 
their  search  tasks  that  each  system  provided.  The  following  5-
point  Likert  scales  were  used.  “The  system  was  effective  for 
solving  the  task”  (effective)  and  “the  system  helped  me  to”  … 
“explore 
the  collection”  (explore),  “find  relevant  videos” 
(relevant), “express different aspects of the task” (aspect), “focus 
my search” (focus), “find videos that I would not have otherwise 
considered” (consider). Table 4 presents the average responses for 
each of these scales using the labels after each of the Likert scales 
in the list above. The most positive response for each user type is 
shown in bold. 

Once  again  it  can  be  seen  from  the  results  in  Table  4  that 
participants have a better perception of ViGOR. The users found 
ViGOR to be superior to the SI system with regards to a number 
of  aspects.  We  applied  two-way  ANOVA  to  each  scale  across 
both systems and the 8 tasks to test these assertions. It was found 
that  the  differences  in  the  scales  effective  (F=6.14,  p=0.0146), 
aspect  (F=6.9,  p=0.0111)  and  focus  (F=5.07,  p=0.0284)  were 
statistically  significant  with  regards  to  the  system  used.  Once 
again this demonstrates that the users had a strong preference for 
ViGOR,  and  that  they  felt  while  they  were  able  to  express 

94different aspects of the task in hand, the system also helped them 
to  focus  on  solving  the  task.  As  the  second  evaluation  is  within 
subjects,  as  opposed  to  the  first  evaluation  which  was  between 
subjects,  we  were  able  to  solicit  user’s  opinions  about  both 
interfaces.  In  the  exit  questionnaires  while  the  users  stated  that 
they found the SI easier to use (ViGOR = 2, SI = 10, Undecided = 
4) and easier to learn to use (ViGOR  = 1, SI = 10, Undecided = 
5), that they still had a preference for the ViGOR (ViGOR = 12, 
SI = 3, Undecided = 1) and found it to be better overall(ViGOR  = 
11, SI = 3, Undecided = 2), highlighting the potential of ViGOR 
as  far  as  the  users  were  concerned.  The  following  section  will 
provide a discussion of all of the findings of both evaluations and 
the ViGOR system overall. 

Scale 
Effective 
Explore 
Relevant 
Aspect 
Focus 
Consider 

ViGOR 
3.8065 
3.3226 
3.3548 
3.4516 
3.5161 
3.2258 

Search 
3.25 
2.8333 
2.8889 
2.8889 
3.0556 
2.8611 

Table 4: Perceptions of System Support (Higher = Better) 

5.  DISCUSSION 
In  this  paper  we  have  introduced  the  ViGOR  system,  a  video 
search  and  retrieval  system  that  allow  users  to  create  groups  of 
results  to  help  conceptualise  and  organise  their  results  for 
complex  video  search  tasks.  It  was  hoped  that  grouping  search 
results  on  the  workspace  would  motivate  the  user  to  organise 
results for their search/work task. This should enable the users to 
break  up  their  global  search  task  into  a  small  set  of  individual 
search  tasks.    Although  the  concept  of  grouping  has  been 
investigated  in  a  number  of  retrieval  scenarios  [17][22],  its 
application and value for searching video collections and archives 
has not been formally evaluated. As has been discussed previously 
(see Section 3.2), video provides a number of unique problems for 
search and retrieval that are not present in other search scenarios. 
Thus there are a number of important contributions that this paper 
makes.  First,  we  present  the  first  system  that  allows  explicit 
grouping  as  part  of  the  video  search  process.  Second,  we  show 
how  this  search  paradigm  can  be  applied  to  a  number  of  video 
search scenarios and video libraries, i.e. YouTube and TRECVID. 
Our goal in this paper was to investigate three hypotheses relating 
to  the  use  of  ViGOR:  1)  that  user  performance  would  improve 
through  the  use  of  ViGOR,  2)  that  ViGOR  can  aid  user 
exploration of the task at hand and 3) that the use of ViGOR can 
also  increase  user  satisfaction  with  their  search  and  their  search 
results.  To  that  end  we  have  conducted  two  user  evaluations, 
involving  in  total  32  participants,  on  a  variety  of  very  different 
video search tasks that incorporate different user goals. In the first 
part  of  the  evaluation  users  searched  over  YouTube  and  carried 
out  broad,  multi-faceted  search  tasks.  In  the  second  part  of  the 
evaluation the users searched over the TRECVID 2007 collection 
and carried focused search tasks. 

There  are  a  number  of  interesting  points  that  can  be  made  about 
the  results  of  these  evaluations. For  both  search  scenarios  it  was 
found that the use of the grouping functionality resulted in users 
retrieving  more  search  results  in  comparison  with  a  baseline 
system. In the YouTube evaluation users retrieved approximately 
50%  more  videos  when  using  the  grouping  interface.  This 
increase in the number of retrieved videos was also coupled with 
an  increase  in  user  interactions.  However  most  of  this  can  be 

functionality  decreases, 

attributed  to  non-expensive  lightweight  functionalities,  while 
more  expensive  heavyweight 
in 
comparison  with  the  baseline  interface,  users  of  the  grouping 
interface viewed 18% less videos and carried out 5% less queries. 
These videos were also retrieved by these users in less time than 
users  of  the  baseline  system.  For  the  TRECVID  collection  the 
difference in the number of retrieved videos is not as large as in 
the  YouTube  evaluation.  However,  the  ViGOR  system  was 
designed with vague and multi-faceted search tasks in mind. The 
fact  that  there  is  any  increase  for  these  focused  search  tasks  is  a 
positive  result.  As  we  were  using  the  TRECVID  collection  and 
tasks,  we  have  relevance  judgements  which  allowed  us  to  carry 
our  some  analysis  that  we  could  not  carry  out  for  the  YouTube 
evaluation. It was found that as well as the users of ViGOR were 
retrieving more video shots, that they were also in fact retrieving 
more  relevant  shots  overall.  In  terms  of  MAP  the  grouping 
interface  is  more  than  comparable  with  the  baseline,  indeed 
outperforming  it  for  half  of  the  tasks.  Once  again  this  result  is 
encouraging as this type of grouping interface has been designed 
for broad, exploratory search tasks and not the focused tasks that 
are part of TRECVID. Overall it can be seen that the availability 
of  the  grouping  functionality  improves  user  performance  when 
searching  digital  video  libraries. These  results  provide  validation 
for our first two hypotheses. 

There were also a number of interesting findings in terms of user 
perceptions. In both user evaluations the differences in how novel 
and  flexible  the  users  found  the  system  were  both  system 
dependent  and  statistically  significant.  In  addition,  for  the 
TRECVID  evaluation  where  the  users  were  exposed  to  both  a 
baseline  system  and  a  system  with  grouping  functionality  it  was 
found  that  the  differences  in  how  wonderful,  efficient  and 
effective the users found the system, was system dependent. Also 
for this evaluation the users stated that the grouping functionality 
helped  them  to  focus  on  the  task  in  hand  while  being  able  to 
express  different  aspects  of  the  search  tasks.  For  the  TRECVID 
evaluation  we  also  asked  users  for  a  direct  comparison  between 
the grouping and baseline interfaces. Whereas the users stated that 
they  found  the  baseline  easier  to  use  and  easier  to  learn  to  use, 
they still had a preference for the grouping interface and found it 
to be better overall. Although users could not directly compare the 
interfaces  in  the  YouTube  evaluation  the  users  did  not  highlight 
any  statistically  significant  difference  between  how  easy  to  use 
and  how  easy  it  was  to  learn  how  to  use  the  grouping 
functionality. These results demonstrate that the users had a strong 
preference 
the  grouping 
functionality.  They  found 
it  more  flexible,  effective  and 
interesting  than  the  baseline,  and  also  found  themselves  able  to 
express more aspects of the topics while still focusing on the task 
in hand, thus providing a better user experience and validating our 
third hypothesis. 

that  provide 

interfaces 

the 

for 

Overall it can be seen that the addition of grouping functionality 
for  video  search  tasks  can  lead  to  a  number  of  favourable 
outcomes. In terms of task performance users retrieve more search 
results  in  less  time  with  less  search  interactions.  With  respect  to 
user  perceptions  the  users  find  the  grouping  systems  to  be  more 
flexible  and  novel,  in  a  direct  comparison  between  a  grouping 
interface and a baseline system the grouping functionality helped 
users  to  focus  on  the  task  in  hand  while  being  able  to  express 
different  aspects  of  the  search  tasks.  While  these  findings 
illustrate  the  benefit  of  the  grouping  metaphor  for  video  search, 
there are also a number of benefits that occur as a result of using a 

95grouping  search  metaphor.  The  interactive  grouping  is  a  supple 
means  of  communicating  a  multitude  of  information  needs  e.g. 
short-term vs. long-term, specific vs. multi-faceted. The semantic 
gap  is  narrowed  by  the  abstraction  to  high-level  semantic 
groupings, reflecting an individual's task-specific mental model of 
the data. Finally, the user leaves a trail of their interactions, which 
can not only be exploited by the system for adaption but by which 
can be used by other users for collaboration.  
6.  CONCLUSIONS AND FUTURE WORK 
In  conclusion,  ViGOR  is  a  system  that  helps  users  to  carry  out 
complex  video  search  tasks  by  allowing  users  to  organise  their 
results into semantic groups and to share results between multiple 
groups. Results of user evaluations have shown the benefits of this 
search  paradigm  for  various  video  search  tasks  in  a  number  of 
scenarios. In addition to this there are a number of supplementary 
benefits  attached  to  the  use  of  ViGOR  that  may  be  exploited  in 
future work. The user leaves trails of their actions behind, which 
other people can exploit; hence ViGOR is ideal in a collaborative 
work context. In addition, ViGOR provides richer feedback from 
the users’ interaction comparison with other video search systems. 
It  may  be  possible  to  exploit  this  interaction  to  provide  various 
types  of  recommendations  to  users,  including  recommendations 
that express the  multi-faceted nature of some  video search tasks. 
Also,  as  part  of  the  search  process  users  add  annotations  to 
groups,  and  thus  to  groups  of  videos.  These  annotations  that  are 
inherently part of the search process with ViGOR which could be 
exploited  to  provide  sufficient  annotations;  in  a  similar  way  that 
reCaptcha [1] uses the work inherent in users translating captchas 
for  security  to  translate  text.  In  conclusion  ViGOR  is  a  first 
important  step  to  changing  the  way  in  which  people  search  for 
video  in  varying  scenarios  and  a  step  towards  bridging  the 
semantic gap.  
7.  ACKNOWLEDGEMENTS 
This  research  work  was  partially  supported  by  the  EC  under 
contracts: SEMEDIA (FP6-045032) and Salero (FP6-027122). 

8.  REFERENCES 
[1]  Von Ahn, L., Maurer, B., McMillen, C., Abraham, D. And 

Blum, M. reCAPTCHA: Human-Based Character 
Recognition via Web Security Measures. Science, September 
2008, pp 1465 – 1468. (2008) 

[2]  Borlund, P. The IIR evaluation model: a framework for 

evaluation of interactive information retrieval systems. Inf. 
Res. 8(3). (2003). 

[3]  Byström, K. and Järvelin, K. 1995. Task complexity affects 

information seeking and use. Inf. Process. Manage. 31, 2, pp 
191-213. (2005)  

[4]  Campbell, I. Interactive evaluation of the Ostensive Model, 

using a new test-collection of images with multiple relevance 
assessments.Inf Retr 2(1):89–114. (2000) 

[5]  Christel, M.G, and Conescu, R.M. Mining Novice User 

Activity in TRECVID Interactive Retrieval Tasks. In Proc 
CIVR 2006, 21-30. (2006) 

[6]  Christel, M.G. Establishing the Utility of Non-Text Search 
for News Video Retrieval with Real World Users. In Proc 
ACM MM 2007, 707-716, (2007). 

 

[7]  Craswell, N. and Szummer, M., Random walks on the click 
graph. In Proc. SIGIR 2007, ACM Press (2007), 239-246.  

[8]  Fass, A.M., Bier, E.A. and Adar, E. PicturePiper: using a re-

configurable pipeline to find images on the Web. In 
Proceedings of UIST 2000. pp 51-62 

[9]  Fogarty, J., Tan, D.S., Kapoor, A. and Winder, S.A.J. 

CueFlik: interactive concept learning in image search. In 
Proceedings of CHI 2008. pp 29-38. (2008) 

[10] Guy, M. and Tonkin, E. Folksonomies Tidying Up Tags, D-

Lib Magazine, Volume 12, Number 1, (2006). 

[11] Jaimes, A., Christel, M., Gilles, S., Ramesh, S., and Ma, W-

Y. Multimedia Information Retrieval: What is it, and why 
isn’t anyone using it? In Proc MIR, ACM Press, 3–8, (2005).  

[12] Halvey, M. and Keane, M.T. Analysis of Online Video 

Search and Sharing. In Proc. ACM HT 2007, ACM Press 
(2007), 217-226. 

[13] Hauptmann, A.G. and Christel, M.G. Successful approaches 

in the TREC video retrieval evaluations, ACM Multimedia 
2004: 668 – 675. 

[14] Hauptmann, A.G., Lin, W-H, Yan, R., Yang, J. and Chen M-
Y. Extreme video retrieval: joint maximization of human and 
computer performance. In proceedings of ACM Multimedia 
2006, pp 385-394. (2006) 

[15] Hopfgartner, F. Understanding Video Retrieval. VDM 

Verlag (2007) 

[16] Hopfgartner, F., Vallet, D., Halvey, M. and Jose, J.M. Search 

trails using user feedback to improve video search. In 
Proceedings of ACM Multimedia 2008, pp 339-348. (2008) 

[17] Nakazato, M., Manola, L. and Huang, T.S. ImageGrouper: A 

Group-Oriented User Interface for Content-Based Image 
Retrieval and Digital Image Arrangement. J. Vis. Lang. 
Comput. 14, 363-386, (2003). 

[18] Naphade, M., Smith, J.R., Tesic, J., Chang, J-S., Hsu, W., 

Kennedy, L., Hauptmann, A. and Curtis, J. Large-Scale 
Ontology for Multimedia. In IEEE MultiMedia 13(3), 2006, 
86-91. 

[19] De Rooij, O., Snoek, C.G.M. and Worring, M. MediaMill: 

fast and effective video search using the forkbrowser. In 
proceedings of CIVR 2008, pp 561-562. (2008). 

[20] Smeulders, AWM, Worring, M, Santini, S, Gupta, A, and 
Jain, R. Content-Based Image Retrieval at the End of the 
Early Years. IEEE Trans. Pattern Anal. Mach. Intell. 22(12): 
1349-1380 (2000) 

[21] Snoek, C., Worring, M., Koelma, D., and Smeulders, A. 

Learned Lexicon-Driven Interactive Video Retrieval. In Proc 
CIVR 2006, 11-20. 

[22] Urban, J and Jose J.M. EGO: A Personalised Multimedia 

Management and Retrieval Tool. In the International Journal 
of Intelligent Systems, Wiley, Vol 21, Issue 7, 725-745, 
(2006). 

[23] Villa, R., Gildea, N. and Jose, J.M. A Faceted Interface for 
Multimedia Search. In Proceedings of ACM SIGIR 2009, 
775 – 776, (2008)

96