Smoothed Analysis of Renegar’s Condition Number for Linear

Programming

John Dunagan∗

Microsoft Research

Daniel A. Spielman †

Department of Mathematics

Massachusetts Institute of Technology

Shang-Hua Teng ‡

Department of Computer Science

Boston University and

Akamai Technologies Inc.

February 1, 2008

Abstract

1 and every σ

We perform a smoothed analysis of Renegar’s condition number for linear programming.
In particular, we show that for every n-by-d matrix ¯A, n-vector ¯b and d-vector ¯c satisfying
¯A, ¯b, ¯c
1/√dn, the expectation of the logarithm of C(A, b, c) is
O(log(nd/σ)), where A, b and c are Gaussian perturbations of ¯A, ¯b and ¯c of variance σ2.
(cid:13)
(cid:13)
From this bound, we obtain a smoothed analysis of Renegar’s interior point algorithm. By
combining this with the smoothed analysis of ﬁnite termination Spielman and Teng (Math.
Prog. Ser. B, 2003), we show that the smoothed complexity of linear programming is
O(n3 log(nd/σ)).

F ≤
(cid:13)
(cid:13)

≤

3
0
0
2

 
t
c
O
7
1

 

 
 
]
S
D
.
s
c
[
 
 

2
v
1
1
0
2
0
3
0
/
s
c
:
v
i
X
r
a

∗Supported in part by NSF Grant CCR-9875024. jdunagan@microsoft.com. Part of the work was done while

†Partially supported by an Alfred P. Sloan Foundation Fellowship, and NSF Grant CCR-0112487.

‡Partially supported by an Alfred P. Sloan Foundation Fellowship, and NSF Grants CCR-9972532, and CCR-

the author was at MIT.

spielman@math.mit.edu

0112487. steng@cs.bu.edu

1

1

Introduction

In [ST03b], Spielman and Teng introduced the smoothed analysis of algorithms as an alternative
to worst-case and average-case analyses in the hope that it could provide a measure of the
complexity of algorithms that better agrees with practical experience. The smoothed complexity
of an algorithm is the maximum over its inputs of the expected running time of the algorithm
under slight perturbations of that input.
In this paper, we perform a smoothed analysis of
Renegar’s condition number for linear programs, and thereby obtain a smoothed analysis of
his interior-point algorithm. Interior point algorithms for linear programming are exciting both
because they are known to run in polynomial time [Kar84] in the worst case and because they
have been used to eﬃciently solve linear programs in practice.
In fact, the speed of interior
point methods in practice is much better than that proved in their worst-case analyses [IL94,
LMS90, EA96]. This discrepancy between worst-case analysis and practical experience is our
main motivation for studying the smoothed complexity of interior point methods.

Our main result is that the smoothed value of Renegar’s condition number, to be deﬁned in
Section 1.2, is O(log(nd/σ)). That is, for each ( ¯A, ¯b, ¯c) and σ

1/√dn,

≤

(A,b,c)

E
(( ¯A,¯b,¯c),σ)
←N

[C(A, b, c)] = O(log(nd/σ)),

N

(( ¯A, ¯b, ¯c), σ) is the distribution of Gaussian perturbations of ( ¯A, ¯b, ¯c) of variance σ2,
where
(( ¯A, ¯b, ¯c), σ) indicates that (A, b, c) is chosen according to this distribution.
and (A, b, c)
As Renegar’s algorithm [Ren95b] takes O (√n ln (C(A, b, c)/ǫ)) iterations to ﬁnd a solution of
relative accuracy ǫ, we ﬁnd that the smoothed complexity of Renegar’s algorithm when it is
asked for a solution of relative accuracy ǫ is O(n3 log(nd/σǫ)).

← N

As explained in [ST03c], when one combines this analysis with the smoothed analysis of the
ﬁnite termination procedure in that paper, one obtains an interior point algorithm that returns
the exact answer to the linear program and has smoothed complexity O(n3 log(nd/σ)).
In
comparison, the best-known bound on the worst-case complexity of any linear programming
algorithm is Vaidya’s [Vai90] bound of O((n + d)d2 + (n + d)1.5d)L), and the best known bound
for an interior point method is O(n3L), ﬁrst due to Gonzaga [Gon88].

1.1 The Complexity of Linear Programming Algorithms

A linear program is typically speciﬁed by a matrix A together with two vectors b and c. If A is
an n by d matrix, then b is an n-vector and c is a d-vector. There are several canonical forms of
linear programs speciﬁed by (A, b, c). The following are four commonly used canonical forms:

max cT x s.t. Ax
max cT x s.t. Ax
b, x
max cT x s.t. Ax = b, x
= 0 s.t. Ax

ﬁnd x

≤

b

0

0

0

≤

≥

≥

≤

and its dual min bT y s.t AT y = c, y
and its dual min bT y s.t. AT y
c, y
and its dual min bT y s.t. AT y
c
and its dual

= 0 s.t. AT y = 0, y

ﬁnd y

≥

≥

0

0

≥

≥

0

≥

(1)

(2)

(3)

(4)

2

6
6
Without loss of generality, we assume that n
d for the remainder of the paper. The worst-case
complexity of solving linear programs has traditionally been stated in terms of n, d, and L,
where L is commonly called the “bit-length” of the input linear program, but is rarely deﬁned
to actually be the number of bits necessary to specify the linear program. For integer A, b, c,
Khachiyan [Kha79] and Karmarkar [Kar84] deﬁned L to be some constant times

≥

log(largest absolute value of the determinant of any square sub-matrix of A)

In the smoothed model, complexity estimates in terms of L are quite pessimistic: even if one
perturbs just the least signiﬁcant digit of each entry of A, the resulting L value is at least
some constant times d with high probability. Thus, in the smoothed model, our analysis of the
complexity of interior point methods the replaces L, which is typically Ω(d), with log(nd/σ).

c
+ log(
k

b
) + log(
k

k∞

k∞

) + log(n + d).

1.2 Renegar’s Condition Number

In [Ren95b, Ren95a, Ren94], Renegar deﬁned the condition number C(A, b, c) of a linear pro-
gram and proved that an interior point algorithm whose complexity was O(n3 log(C(A, b, c)/ǫ))
could solve a linear program to relative accuracy ǫ, or determine that the program was infeasible
or unbounded.

For a linear program in the canonical form (1), we follow Renegar [Ren94, Ren95a, Ren95b] in
deﬁning the primal condition number, C (1)
P (A, b), of the program to be the normalized reciprocal
of the distance to ill-posedness. A program is ill-posed if the program can be made both feasible
and infeasible by arbitrarily small changes to the pair (A, b). The distance to ill-posedness
of the pair (A, b) is the distance to the set of ill-posed programs under the Frobenius norm.
We similarly deﬁne the dual condition number, C (1)
D (A, c), to be the normalized reciprocal of
the distance to ill-posedness of the dual program. The condition number, C (1)(A, b, c), is the
maximum of C (1)

P (A, b) and C (1)

D (A, c).

We can equivalently deﬁne the condition number without introducing the concept of ill-posedness.
For programs of form (1), we deﬁne C (1)

P (A, b) by

Deﬁnition 1.2.1 (Primal Condition Number).

(a) if Ax

b is feasible, then

C (1)

P (A, b) =

≤

≤

C (1)

P (A, b) =

sup

δ :
{

∆A, ∆b
k

kF ≤

δ implies (A + ∆A)x

(b + ∆b) is feasible

(b) if Ax

b is infeasible, then

sup

δ :

∆A, ∆b

δ implies (A + ∆A)x

(b + ∆b) is infeasible

{

k

kF ≤

It follows from the deﬁnition above that C (1)
C (1)

D (A, c), analogously.

P (A, b)

≥

1. We deﬁne the dual condition number,

,

}

.

}

A, b

k

kF

A, b

k

kF

≤

≤

3

To reader familiar with condition numbers in contexts outside of linear programming, the above
deﬁnition may be surprising: the condition numbers for numerous other problems are deﬁned
as the sensitivity of the output to perturbations in the input, and are then often related to
the distance to ill-posedness. Renegar inverts this scheme by deﬁning the condition number for
linear programming to be the distance to ill-posedness, and then proving that the condition
number does bound the sensitivity of the output to perturbations in the input [Ren94, Ren95a].

Any linear program may be expressed in form (1); however, transformations among linear
programming formulations do not in general (and commonly do not) preserve condition num-
ber [Ren95a]. We will therefore have to deﬁne diﬀerent condition numbers for each normal form
we consider. For linear programs with canonical forms (2), (3), and (4) we deﬁne their condition
numbers, C (2)(A, b, c), C (3)(A, b, c) and C (4)(A), analogously. We follow the convention that 0
is not considered a feasible solution to (4). Just as for C (1)

1 for all i.

P (A, b), C (i)

≥

For linear programs given in form (2), Renegar [Ren95b, Ren95a, Ren94] developed an ini-
O(nC(A, b, c))
tialization phase that returns a feasible point with initial optimality gap R
for a linear program (A, b, c) or determines that the program is infeasible or unbounded, in
O(n3 log(C(A, b, c))) operations. By applying O(√n log(nC(A, b, c))/ǫ) iterations of a primal
interior point method, for a total of O(n3 log(nC(A, b, c))/ǫ) arithmetic operations, Renegar
proved:

≤

Theorem 1.2.2 (Renegar). For any linear program of form (2) speciﬁed by (A, b, c) and
parameter ǫ, Renegar’s interior-point algorithm, in O(n3 log(nC(A, b, c)/ǫ)) operations, ﬁnds a
feasible solution x with optimality gap ǫ
kF , or determines that the program is infeasible
or unbounded.

A, b, c

k

Subsequently, algorithms with complexity logarithmic in the condition number were developed
by Vera [Ver96] for forms (1) and (3) and by Cucker and Pe˜na [CP01] for form (4). The
complexities of their algorithms are similar to that of Renegar’s. In [FV00], Freund and Vera
give a uniﬁed approach which both eﬃciently estimates the condition number and solves the
linear programs in any of these forms.

1.3 Smoothed Analysis of Condition Number: Our Results

In this paper, we consider linear programming problems in which the data is subject to slight
Gaussian perturbations. Recall that the probability density function of a Gaussian random
variable with mean ¯x and variance σ2 is given by

A Gaussian perturbation of a vector ¯x of variance σ2 is a vector whose ith element is a Gaussian
random variable of variance σ2 and mean ¯xi, and in which each element is independently chosen.
Thus, the probability density function of a d-dimensional Gaussian perturbation of ¯x of variance
σ2 is given by

µ(x) =

1

σ√2π

(x

¯x)2/(2σ2).

−

e−

µ(x ) =

1

(σ√2π)d

x
e−k

−

¯x

2/(2σ2).
k

4

A Gaussian perturbation of a matrix may be deﬁned similarly.
For each ( ¯A, ¯b, ¯c) and σ
bations of ( ¯A, ¯b, ¯c) of variance σ2, and we let (A, b, c)
is drawn from the distribution

(( ¯A, ¯b, ¯c), σ).

0, we let

← N

N

≥

(( ¯A, ¯b, ¯c), σ) denote the distribution of Gaussian pertur-
(( ¯A, ¯b, ¯c), σ) indicate that (A, b, c)

Our main result, which is proved in Section 4, is

N

Theorem 1.3.1 (Smoothed Complexity of Renegar’s Condition Number). For every
n-by-d matrix ¯A, n-vector ¯b and d-vector ¯c such that
1/√nd, and
every i

1, every σ

¯A, ¯b, ¯c

F ≤

≤

,

1, 2, 3, 4
}

∈ {

C (i)(A, b, c) >

Pr
A,b,c "

213 (n + 1)2(d + 1)1.5

δσ2

log

(cid:18)

(cid:13)
(cid:13)
210 (n + 1)2(d + 1)1.5

(cid:13)
(cid:13)

2

δσ2

< δ,

#

(cid:19)

and

(A,b,c)

←N

E
(( ¯A,¯b,¯c),σ)

log C (i)(A, b, c)
i

h

≤

15 + 4.5 log

nd
σ

.

Theorem 1.3.1 implies a bound on the smoothed complexity of Renegar’s algorithm as well as
a bound on the smoothed complexity of the interior point methods that were developed for the
other canonical forms. Note that in the statement of Theorem 1.3.1, we abuse the notation
C (4)(A, b, c) for C (4)(A). The ﬁrst bound of the theorem means that, with high probability, the
condition number of a perturbed linear program is polynomial in n, d, and 1/σ.

The following theorem follows immediately from Renegar’s analysis (Theorem 1.2.2) and the
previous theorem.

Theorem 1.3.2 (Smoothed Complexity of IPM). Let T ((A, b, c), ǫ) be the time complexity
of Renegar’s interior point algorithm for ﬁnding ǫ-accurate solutions of the linear program deﬁned
by (A, b, c) or determining that the program is infeasible or unbounded. For every n-by-d matrix
¯A, n-vector ¯b and d-vector ¯c such that

1 and every σ

1/√nd,

¯A, ¯b, ¯c

E
(( ¯A,¯b,¯c),σ)

(A,b,c)

←N

F ≤

(cid:13)
[T ((A, b, c), ǫ)] = O
(cid:13)

(cid:13)
(cid:13)

n3 log

≤
n
σǫ

.

(cid:16)

(cid:16)

(cid:17)(cid:17)

In order to analyze Renegar’s condition number for the primal and dual of each of the four
canonical forms, we found it necessary to develop several extensions to the theory of condi-
tion numbers that may be of independent interest. For example, Lemma 2.2.2 generalizes the
geometric condition on distance to ill-posedness developed in [CC01] by incorporating an ar-
bitrary non-pointed convex cone that is not subject to perturbation, and this generalization is
necessary for the application of our techniques. Additionally, Lemmas 2.3.2, 3.3.1, and 3.3.2 all
provide geometric conditions on the distance to ill-posedness whose import to us is on par with
Lemma 2.2.2.

1.4 Organization of the Paper

In our analysis, we divide the eight condition numbers C (i)
D , C (3)
two groups. The ﬁrst group includes C (1)

P , C (2)

P , C (2)

P and C (i)
1, 2, 3, 4
, into
}
D , and with some additional work,

D , for i

∈ {

5

C (4)
P . The remaining condition numbers belong to the second group. We will refer to a condition
number from the ﬁrst group as a primal condition number and a condition number from the
second group as a dual condition number.

Section 2 is devoted to establishing a smoothed bound on the primal condition number. We
remark that the techniques used in Section 2 do not critically depend upon A, b and c being
Gaussian distributed, and similar theorems could be proved using slight modiﬁcations of our
techniques if these were smoothly distributed within spheres or cubes. It follows from the result
of Section 2 alone that Theorem 1.3.1 holds for linear program given in Form (2).

In Section 3, we establish the smoothed bound on the dual condition number. Our bounds in
this section do critically make use of the Gaussian distribution on A, b and c.

In Section 4, we prove Theorem 1.3.1 using the smoothed bounds of the previous two sections.
We conclude the paper in Section 5 with some open questions.

In the remainder of this Section, we review some of the previous work on smoothed analysis,
some earlier results on the average-case analysis of interior-point algorithms, and lower bounds
on the complexity of interior-point algorithms.

1.5 Prior Smoothed Analyses of Linear Programming Algorithms

In their paper introducing Smoothed Analysis [ST03b], Spielman and Teng proved that the
smoothed complexity of a two-phase shadow vertex simplex method was polynomial in n, d
and 1/σ. Shortly thereafter, Blum and Dunagan [BD02] performed a smoothed analysis of the
perceptron algorithm for linear programming. They showed that the probability the perceptron
algorithm would take more than a polynomial in the input size times k steps was inversely
proportional to √k. Their analysis had the advantage of being signiﬁcantly simpler than that
of [ST03b], and it is their analysis that we build upon in this work. Blum and Dunagan’s analysis
used the fact that the number of steps taken by the perceptron algorithm can be bounded by the
reciprocal of the “wiggle room” in its input, and the bulk of their analysis was a bound on the
probability that this “wiggle room” was small. The “wiggle room” turns out to be a condition
number of the input to the perceptron algorithm.

1.6 Prior Average-Case Analyses of Interior Point Algorithms

There has been an enormous body of work on interior point algorithms, some of which has
addressed their average-case complexity. Anstreicher, Ji, Potra and Ye [AJPY93, AJPY99],
have shown that under Todd’s degenerate model for random linear programs [Tod91], a ho-
mogeneous self-dual interior point method runs in O(√n log n) expected iterations. Borgwardt
and Huhn [HB02] have obtained similar results under any spherically symmetric distribution.
The performance of other interior point methods on random inputs has been heuristically an-
alyzed through “one-step analyses”, but it is not clear that these analyses can be made rigor-
ous [Nem88, GT92, MTY93].

6

1.7 Lower Bounds for Interior Point Algorithms

The best known lower bound on the complexity of interior point methods is Ω(n1/3) iterations
due to Todd [Tod94] and Todd and Ye [TY96]. However, the programs for which these lower
bounds hold are very ill-conditioned. There are no known bounds of the form Ω(nǫ) for well-
conditioned linear programs. It would be interesting to know whether such a lower bound can
be proved for a well-conditioned program, or whether interior point algorithms always require
fewer iterations when their input is well-conditioned.

1.8 Notation and Basic Geometric Deﬁnitions

Throughout this paper we use the following notational conventions. The material up to this
point has obeyed these conventions.

lower case letters such as a and α denote scalars,

bold lower case letters such as aaa and b denote vectors, and for a vector aaa, ai denotes the
ith entry of aaa.

capital letters such as A denote matrices, and

bold capital letters such as C denote convex sets.

•

•

•

•

k

aaa
k

If aaa 1, . . . , aaan are vectors, we let [aaa 1, . . . , aaa n] denote the matrix whose rows are the aaais. For a
vector aaa, we let
denote the standard Euclidean norm of the vector. We will make frequent
kF , which is the square root of the sum of squares
use of the Frobenius norm of a matrix,
A
k
A, x 1, . . . , x kkF denote the square
of the entries in the matrix. We extend this notation to let
k
root of the sum of squares of the entries in A and in x 1, . . . , x k. Diﬀerent choices of norm are
possible; we use the Frobenius norm throughout this paper. The following proposition relates
several common choices of norm:

Proposition 1.8.1 (Choice of norm). For an n-by-d matrix A,

A

kF

A
k
√dn ≤ k
A
k
√d ≤ k

kF

A

k∞ ≤ k

A
kF , and

kOP ≤ k

A
kF ,

where

A
kOP denotes the operator norm of A, maxx
k

=0 k
k

Ax
x

k
k

.

We let log denote the logarithm to base 2 and ln denote the logarithm to base e.

We also make use of the following geometric deﬁnitions:

Deﬁnition 1.8.2 (Ray). For a vector p, let Ray (p) denote

αp : α > 0
}
{

.

7

6
Deﬁnition 1.8.3 (Non-pointed convex cone). A non-pointed convex cone is a convex set C
C , and there exists a vector t such that t T x < 0
such that for all x
for all x

C and all α > 0, αx

C .

∈

∈

∈

Deﬁnition 1.8.4 (Positive half-space). For a vector aaa we let
points with non-negative inner product with aaa.

H

(aaa) denote the half-space of

(x ) are not non-pointed convex cones, while

For example, IRd and
and Ray (p)
are non-pointed convex cones. Note that a non-pointed convex cone cannot contain the origin.
All of the cones that we introduce through the process of homogenization are non-pointed convex
cones.

x : x 0 > 0
{
}

H

These deﬁnitions enable us to express the feasible x for the linear program

as

Ax

0 and x

C

≥

∈

x

C

∈

∩

(aaa i),

H

n

\i=1

where aaa 1, . . . , aaan are the rows of A. Throughout this paper, we will call a set feasible if it is
non-empty, and infeasible if it is empty. Thus, we say that the set C
(aaai) is feasible if
the corresponding linear program is feasible.

n
i=1 H

∩

T

8

2 Primal Condition Number

In this section we show that the smoothed value of the primal condition numbers is polynomial
in n, d, and 1/σ with polynomially high probability. As in the work of Pe˜na [Pe˜n00], we unify
this study by transforming each canonical form to conic form.

The primal program of form (1) can be put into conic form with the introduction of the homoge-
nizing variable x0. Setting C =
, the homogenized primal program of form (1)
is

(x , x0) : x0 > 0
{
}

[
−

A, b](x , x0)

0, (x , x0)

C .

≥

∈

(x , x0) : x0 > 0 and x

By setting C =
0
, one can similarly homogenize the primal program
}
of form (2). The dual programs of form (2) and form (3) can be homogenized by setting
C =
, respectively, and considering the
}
program

(y , y0) : y0 > 0
}

(y , y0) : y0 > 0 and y

and C =

≥

≥

0

{

{

{
AT , c](y , y0)

[
−

0, (y , y0)

C .

∈

≥

P below. Note that in each of these homogenized programs, the variables

We will comment on C (4)
lie in a non-pointed convex cone.

Pe˜na [Pe˜n00] proves:

Fact 2.0.1 (Preserving feasibility). Each of the homogenized programs is feasible if and only
if its original program is feasible.

In Section 2.1, we extend the notion of distance to ill-posedness and condition number to conic
linear programs and note that the transformation by homogenization does not alter the distance
to ill-posedness. The rest of the section will be devoted to analyzing the condition number of the
conic program, and this will imply the bound on the condition number of the original program.

2.1 Linear Programs in Conic Form and Basic Convex Probability Theory

The feasibility problem for a conic linear program can be written:

ﬁnd x such that Ax

0, x

C ,

≥

∈

where C is a non-pointed convex cone in IRd and A is an n-by-d matrix. Note that because C
is a non-pointed convex cone, 0 cannot be a feasible solution of this program. The following
deﬁnition generalizes distance to ill-posedness by explicitly taking into account the non-pointed
convex cone, C .

Deﬁnition 2.1.1 (Generalized distance to ill-posedness). For a non-pointed convex cone,
C , that is not subject to perturbation, and a matrix, A, we deﬁne ρ(A, C ) by

a. if Ax

0, x

C is feasible, then

∈

≥
ρ(A, C ) = sup

ǫ :
{

∆A
k

kF < ǫ implies (A + ∆A)x

≥

∈

0, x

C is feasible

;

}

9

b. if Ax

0, x

C is infeasible, then

∈

≥
ρ(A, C ) = sup

ǫ :

∆A

{

k

kF < ǫ implies (A + ∆A)x

≥

∈

0, x

C is infeasible

.

}

We note that this deﬁnition makes sense even when A is a row vector. In this case, ρ(aaa, C )
measures the distance to ill-posedness when we only allow perturbation to aaa. Even though trans-
formations among linear programming formulations in general do not preserve condition number,
Pe˜na [Pe˜n00] has proved that homogenization does not alter the distance to ill-posedness. For
convenience, we will state the lemma for form (1), and note that similar statements hold for
P , C (2)
C (2)
Lemma 2.1.2 (Preserving the condition number). Let

D , and C (3)
D .

max cT x

s.t. Ax

b

. Then, C (1)

≤
P (A, b) =

be a linear program. Let C =

(x , x0) : x0 > 0
}

{

A, b

kF /ρ([
−

k

A, b], C ).

The primal program of form (4) is not quite in conic form; to handle it, we need the following
deﬁnition.

Deﬁnition 2.1.3 (Pointed generalized primal distance to ill-posedness). For a convex
cone that is not non-pointed, C , and a matrix, A, we deﬁne ρ(A, C ) by

a. if Ax

0, x

= 0, x

C is feasible, then

∈

≥

≥

ρ(A, C ) = sup

ǫ :
{

∆A
k

kF < ǫ implies (A + ∆A)x

≥

0, x

= 0, x

C is feasible

b. if Ax

0, x

= 0, x

C is infeasible, then

ρ(A, C ) = sup

ǫ :

kF < ǫ implies (A + ∆A)x

≥

0, x

= 0, x

C is infeasible

∈
∆A

k

{

∈

∈

}

}

This deﬁnition would allow us to prove the analogs of Lemmas 2.1.4 and 2.1.5 for primal
programs of form (4). We omit the details of this variation on the arguments in the interest of
simplicity.

The following two Lemmas are the main result of this section. To see how they may be applied,
we note that a simple union bound over C (2)
D using Lemma 2.1.4 yields Theorem 1.3.1
for form (2).

P and C (2)

Lemma 2.1.4 (Condition number is likely polynomial). For any non-pointed convex cone
C and a matrix ¯A satisfying

1/√nd,

1, for σ

¯A

F ≤
(cid:13)
A
(cid:13)
kF

(cid:13)
(cid:13)
k
ρ(A, C ) ≥

212n2d1.5

≤

δσ2

Pr

A

←N

( ¯A,σ) (cid:20)

29n2d1.5

δσ2

log2

(cid:18)

δ.

≤

Lemma 2.1.5 (Smoothed analysis of log of primal condition number). For any non-
pointed convex cone C and a matrix ¯A satisfying

1/√nd,

1, for σ

¯A

(cid:19)(cid:21)

≤

F ≤

E
( ¯A,σ) (cid:20)

A

←N

log k

A

kF

ρ(A, C )

(cid:13)
(cid:13)

(cid:13)
(cid:13)
14 + 4.5 log
≤

nd
σ

.

(cid:21)

10

6
6
6
6
We will prove Lemma 2.1.4 by separately considering the cases in which the program is feasible
and infeasible. In Section 2.2, we show that it is unlikely that a program is feasible and yet can
be made infeasible by a small change to its constraints (Lemma 2.2.1). In Section 2.3, we show
that it is unlikely that a program is infeasible and yet can be made feasible by a small change to
its constraints (Lemma 2.3.1). In Section 2.4, we combine these results to show that the primal
condition number is polynomial with high probability (Lemma 2.1.4). In Section 2.5 we prove
Lemma 2.1.5.

The thread of argument in these sections consists of a geometric characterization of those pro-
grams with poor condition number, followed by a probabilistic argument demonstrating that
this characterization is rarely satisﬁed. Throughout the proofs in this section, C will always
refer to the original non-pointed cone, and a subscripted C (e.g., C 0) will refer to a modiﬁcation
of this cone.

The key probabilistic tool used in the analysis is Lemma 2.1.7, which we will derive from the
following result of [Bal93]. A slightly weaker version of this lemma was proved in [BD02], and
also in [BR76].

Theorem 2.1.6 (Ball [Bal93]). Let K be a convex body in IRd and let µ be the density function
of a

(0, σ) Gaussian random variable. Then,

N

4d1/4.

µ

≤

Z∂K

(cid:8)

\

∩

4ǫd1/4

,

≤

σ

4ǫd1/4

≤

σ

Lemma 2.1.7 (ǫ-Boundaries are likely to be missed). Let K be an arbitrary convex body
in IRd, and let bdry(K , ǫ) denote the ǫ-boundary of K ; that is,

For any ¯x

IRd,

bdry(K , ǫ) =

x :

x ′

∃

∈

∂K ,

x

x ′

−

ǫ

≤

(cid:9)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∈

x

x

(¯x ,σ)

←N

(¯x ,σ)

←N

∈

∈

Pr

[x

bdry(K , ǫ)

K ]

(outside boundary)

Pr

[x

bdry(K , ǫ)

K ]

(inside boundary)

Proof. We derive the result assuming σ = 1. The result for general σ follows by scaling.

Let µ denote the density according to which x is distributed. To derive the ﬁrst inequality, we
let K ǫ denote the points of distance at most ǫ from K , and observe that K ǫ is convex.

Integrating by shells, we obtain

Pr [x

bdry(K , ǫ)

K ]

∈

\

ǫ

µ

≤

≤

t=0 Z∂K t
Z
ǫ4d1/4,

by Theorem 2.1.6.
We similarly derive the second inequality by deﬁning K ǫ to be the set of points inside K of
distance at least ǫ from the boundary of K and observing that K ǫ is convex for any ǫ.

11

In this section and the next, we use the following consequence of Lemma 2.1.7 repeatedly.

Lemma 2.1.8 (Feasible likely quite feasible, single constraint). Let C 0 be any convex
cone in IRd and, for any ¯aaa

IRd, let aaa be a Gaussian perturbation of ¯aaa of variance σ2. Then,

∈

Pr
aaa [C 0 ∩ H

(aaa) is feasible and ρ(aaa, C 0)

, and

ǫ]

ǫ]

≤

≤

≤

≤

4ǫd1/4

σ

σ

4ǫd1/4

.

Pr
aaa [C 0 ∩ H

(aaa) is infeasible and ρ(aaa, C 0)

(aaa) is infeasible. Observe that ρ(aaa, C 0) is exactly
Proof. Let K be the set of aaa for which C 0 ∩ H
the distance from aaa to the boundary of K . Since K is a convex cone, the ﬁrst inequality follows
from the ﬁrst inequality (the outside boundary inequality) of Lemma 2.1.7, which tells us that
the probability that aaa has distance at most ǫ to the boundary of K and is outside K is at most
4ǫd1/4
. The second inequality similarly follows from the second inequality (the inside boundary

σ

inequality) of Lemma 2.1.7.

2.2 Primal condition number, feasible case

In this subsection, we analyze the primal condition number in the feasible case and prove:

Lemma 2.2.1 (Feasible is likely quite feasible, all constraints). Let C be a non-pointed
convex cone in IRd and let ¯A be any n-by-d matrix. Then for any σ

0,

Pr

[(Ax

0, x

C is feasible) and (ρ(A, C )

ǫ)]

A

←N

( ¯A,σ)

≥

∈

≥

4ǫnd5/4

.

≤

≤

σ

To prove Lemma 2.2.1, we ﬁrst establish a necessary geometric condition for ρ to be small.
This condition is stated and proved in Lemma 2.2.2. In Lemma 2.2.6, we apply Helly’s Theo-
rem [LDK63] to simplify this geometric condition, expressing it in terms of the minimum of ρ
over individual constraints. This allows us to use Lemma 2.1.8 to establish Lemma 2.2.9, which
shows that this geometric condition is unlikely to be met. Lemma 2.2.1 is then a corollary of
Lemmas 2.2.9 and 2.2.2.

We remark that a result similar to Lemma 2.2.2 appears in [CC01].

Lemma 2.2.2 (Bounding ρ by a max of min of inner products). Let C be a non-pointed
convex cone and let aaa1, . . . , aaa n be vectors in IRd for which C

(aaai) is feasible. Then

ρ([aaa 1, . . . , aaa n], C )

∩
T
min

i H
aaa T
i p.

(aaa i)

i

≥

p

C

∈

max
∩
p
k
T

n
i=1 H
=1
k

Proof. Lemma 2.2.2 follows directly from Lemmas 2.2.5, 2.2.3, and 2.2.4 below. These three
lemmas develop a characterization of ρ, the distance to ill-posedness, in the feasible case.

Lemma 2.2.3 (Lower bounding ρ by rays). Under the conditions of Lemma 2.2.2,

ρ([aaa 1, . . . , aaan], C )

ρ([aaa 1, . . . , aaa n], Ray (p)).

≥

p

C

∈

max
i H
∩

(aaa i)

T

12

Proof. Let ∆aaa1, . . . , ∆aaa n be such that C
C

(aaa i), Ray (p)

(∆aaai + aaai) is also infeasible.

∩

i H

∩

i H

∩

i H

T

(∆aaai + aaai) is infeasible. Then, for all p

∈

T

Lemma 2.2.4 (ρ of a ray as a min over constraints). For every set of vectors aaa1, . . . , aaa n
and p such that Ray (p)

(aaai) is feasible,

T

i H

∩
ρ([aaa 1, . . . , aaa n], Ray (p)) = min

T

ρ(aaa i, Ray (p)).

i

Proof. Observe that Ray (p)
is feasible for all i.

∩

i H

T

(aaai + ∆aaai) is feasible if and only if Ray (p)

(aaa i + ∆aaai)

∩ H

Lemma 2.2.5 (ρ of a ray and single constraint as an inner product). For every vector
aaa and every unit vector p,

ρ(aaa, Ray (p)) =

aaaT p

Proof. If aaa T p = 0, then ρ(aaa, Ray (p)) = 0. If aaaT p
only if Ray (
is feasible. So, we assume aaa T p > 0, in which case Ray (p)
that ρ(aaa, Ray (p))

∩ H

p)

−

aaaT p. For every vector ∆aaa of norm at most aaaT p, we have

∩ H

(aaa) is infeasible; so, it suﬃces to consider the case where Ray (p)

∩ H

(aaa) is feasible if and
(aaa)
(aaa) is feasible. We ﬁrst prove

∩ H

(cid:12)
(cid:12)

(cid:12)
(cid:12)

= 0, then Ray (p)

≥

(aaa + ∆aaa)T p = aaa T p + ∆aaaT p

aaaT p

≥

∆aaa

− k

k ≥

0.

That is, p
aaaT p.

∈ H

(aaa+∆aaa). As this holds for every ∆aaa of norm at most aaaT p, we have ρ(aaa, Ray (p))

≥

To show that ρ(aaa, Ray (p))

aaa T p, note that setting ∆aaa =

(ǫ + aaaT p)p, for any ǫ > 0, yields

≤

−

(aaa + ∆aaa)T p = aaaT p + ∆aaaT p = aaaT p

(ǫ + aaa T p)p T p = aaa T p

(ǫ + aaaT p) =

−

−

so, Ray (p)

(aaa + ∆aaa) is infeasible. As this holds for every ǫ > 0, ρ(aaa, Ray (p))

∩ H

Lemma 2.2.6 (Bounding the max of min of inner products). Let C be a non-pointed
convex cone and let aaa1, . . . , aaa n be vectors in IRd for which C

(aaai) is feasible. Then

ǫ;

−

aaa T p.

≤

p

C

∈

max
∩
p
k
T

n
i=1 H
=1
k

aaaT

i p

min

i

min

ρ

≥

i

(aaa i)

aaai, C





∩

i H

T

H

(aaa j)

∩

=i
\j

d.

,





We will derive Lemma 2.2.6 from Lemmas 2.2.7 and 2.2.8, which we now state and prove.

Lemma 2.2.7 (Quite feasible region implies quite feasible point, single constraint).
For every aaa and every non-pointed convex cone C 0 for which C 0 ∩ H

(aaa) is feasible,

ρ(aaa, C 0) = max
∩H
=1
k

C 0
p

∈

p

k

(aaa)

aaaT p.

13

6
6
Proof. The “

” direction follows from Lemmas 2.2.3 and 2.2.5; so, we concentrate on showing

≥

ρ(aaa, C 0)

aaaT p.

≤

p

∈

max
C 0
∩H
p
=1
k

k

(aaa)

We recall that, as C 0 is non-pointed, there exists a vector t such that t T x < 0 for all x
We now divide the proof into two cases depending on whether aaa

C 0.

∈

C 0.

If aaa

C 0, then we let p = aaa/

∈

∈

aaa
k

. It is easy to verify that
k
aaa T p =

aaa
k

k

= max
=1

p

k

k

aaaT p = max
∩H
=1
k

C 0
p

∈

p

k

(aaa)

aaaT p.

Moreover, C 0 ∩ H
If aaa

−

(aaa

(aaa + ǫt)) is infeasible for every ǫ > 0. So, ρ(aaa, C 0)

C 0, let q be the point of C 0 that is closest to aaa. As C 0 ∩ H

(aaa) and is not the origin. Let p = q /

q

6∈

(aaa) is feasible, q lies inside
q. Thus,

. As C 0 is a cone, q is perpendicular to aaa
k
2 =

(aaa T p)2, as aaa T p =

−

k
q

. Conversely, for

H
the distance from aaa to q is

aaa
k

2
k

− k

k

q

aaa
k

2
k

−

q

C 0, the distance from Ray (r ) to aaa is

any unit vector r
vector r

∈

C 0 maximizing aaaT r must be p.

2

aaa
k

k

−

q

(aaa T r )2. Thus, the unit

As C 0 is convex, there is a plane through q separating C 0 from aaa and perpendicular to the line
segment aaa

q , and thus ρ(aaa, C 0)

= aaaT p.

q

≤ k

k

∈

−

aaa

.
k

≤ k

q
k

k

Lemma 2.2.8 (Quite feasible individually implies quite feasible collectively). Let C 0
be a non-pointed convex cone and let aaa1, . . . , aaa n be vectors in IRd. If there exist unit vectors
p 1, . . . , p n ∈

C 0, such that

then there exists a unit vector p

ǫ, for all i, and

0, for all i and j,

aaaT
i p i ≥
aaa T
i p j ≥
C 0 such that

∈

aaa T

i p

≥

ǫ/d, for all i.

Proof. We prove this using Helly’s Theorem [LDK63] which says that if a collection of convex
sets in IRd has the property that every subcollection of d + 1 of the sets has a common point,
then the entire collection has a common point. Let

We begin by proving that every d of the SSis contain a point in common. Without loss of
generality, we consider SS1, . . . , SSd. Let p =

d
i=1 p i/d. Then, for each 1

d,

j

≤

≤

SSi =

x
{

∈

C 0 : aaaT

i x /

x

k

k ≥

ǫ/d
.
}

aaaT
j p = aaa T
j

d

P

p i/d

! ≥

 

Xi=1

aaa T
j

p j/d

ǫ/d.

≥

(cid:0)

(cid:1)

14

As p has norm at most one, aaaT

aaaT
j p, so p is contained in each of S1, . . . , Sd.

j p/

p

k

k ≥

As C 0 is non-pointed, there exists t such that t T x < 0,

. Then, x
1
}

SSi implies

x /t T x

∈

−
As these are convex sets lying in a d
exists a point p that lies within all of the SS′is. As SS′i ⊂
SSis.

−

−

x

C 0. Let SS′i = SSi

x : t T x =
{
SS′i. So, every d of the SS′i have a point in common.
∈
1 dimensional space, Helly’s Theorem tells us that there
SSi, this point lies inside all the

T

∈

∀

Proof of Lemma 2.2.6. For each i, we apply Lemma 2.2.7, to the vector aaai and the cone C

∩

j

=i H

(aaa j) to ﬁnd a unit vector pi ∈

C

∩

(aaa j) such that

As p i ∈

C

∩

j H

(aaaj), we also have

T

p T
i aaaj ≥

0

for all j. Applying Lemma 2.2.8, we ﬁnd a unit vector p

C

∈

∩

n
j=1 H

(aaaj) satisfying

n
j=1 H

T
aaa i, C

p T

i aaai = ρ

∩

H

=i
\j

(aaaj)

.





aaaT

i p

min

ρ

≥

i

aaai, C

T
d,

,

∩

H

=i
\j

(aaa j)













T

for all i.

Lemma 2.2.9 (Max of min of inner products is likely large). Let C be a non-pointed
convex cone in IRd and let ¯aaa1, . . . , ¯aaa n be vectors in IRd. Let aaa 1, . . . , aaa n be Gaussian perturbations
of ¯aaa1, . . . , ¯aaa n of variance σ2. Then,

Pr 

C




∩

H

\i

(aaai) is feasible and

p

C

∈

max
∩
p
k
T

n
i=1 H
=1
k

min

i

aaa T
i p < ǫ

(aaa i)

4ǫnd5/4

.

≤

σ




Proof. By Lemma 2.2.6,

Pr 

C




∩

H

\i

(aaa i) is feasible and

p

C

∈

max
∩
p
k
T

n
i=1 H
=1
k

min

i

aaaT
i p < ǫ

(aaa i)

(aaai) is feasible and min

ρ

aaa i, C

(aaaj)

< dǫ

.

Pr

C

≤





∩

H

\i

Applying a union bound over i and then Lemma 2.1.8, we ﬁnd this probability is at most

n

Xi=1

∩

H

\j





Pr

C

(aaa j) is feasible and ρ

aaai, C

(aaa j)

< dǫ





∩

H

=i
\j







≤



Xi=1

σ








i

∩

H

=i
\j





=





.

σ

n

4(ǫd)d1/4

4nǫd5/4

15

6
6
6
6
6
Proof of Lemma 2.2.1. Follows immediately from Lemmas 2.2.2 and 2.2.9.

This concludes the analysis that it is unlikely that the primal program is both feasible and has
small distance to ill-posedness. Next, we show that it is unlikely that the primal program is
both infeasible and has small distance to ill-posedness.

2.3 Primal number, infeasible case

The main result of this subsection is:

Lemma 2.3.1 (Infeasible is likely quite infeasible). Let C be a non-pointed convex cone
in IRd and let ¯A be any n-by-d matrix such that
1/√d and
ǫ < 1/2,

1. Then, for any 0 < σ

F ≤

¯A

≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Pr

[(Ax

0, x

C is infeasible) and (ρ(A, C )

ǫ)]

A

←N

( ¯A,σ)

≥

∈

361 ǫn2d1.5log1.5(1/ǫ)

.

≤

≤

σ2

To prove Lemma 2.3.1, we consider adding the constraints one at a time.
If the program is
infeasible in the end, then there must be some constraint, which we call the critical constraint,
that takes it from being feasible to being infeasible. Lemma 2.3.2 gives a suﬃcient geometric
condition for the program to be quite infeasible when the critical constraint is added. We then
prove Lemma 2.3.1 by showing that this condition is met with good probability. The geometric
condition is that the program is quite feasible before the critical constraint is added and that
every previously feasible point is far from being feasible for the critical constraint.

Lemma 2.3.2 (The feasible-to-infeasible transition). Let C be a non-pointed convex cone
in IRd, p

C be a unit vector, and aaa1, . . . , aaa k+1 be vectors in IRd such that

∈

Then,

aaa T

i p

≥

i

α, for 1

≤
β, for all x

k, and
k
i=1 H

∩

C

≤

∈

aaa T

k+1x

≤ −

ρ([aaa1, . . . , aaa k+1] , C )

min

≥

(aaa i),

= 1.

x

k

k

T
α
2

,

(cid:26)

αβ

4α + 2

aaa k+1k (cid:27)
k

.

We will derive Lemma 2.3.2 from the following geometric lemma.

Lemma 2.3.3 (ρ bound on inner product). Let C be a non-pointed convex cone and let aaa
be a vector for which C

(aaa) is infeasible. Then,

∩ H

Proof. Let p be the unit vector in C maximizing p T aaa. If we set

max
p
C ,
k

k

p

∈

=1

p T aaa

ρ(aaa, C ).

≤ −

∆aaa =

ǫ

p T aaa

p,

(cid:0)

−

16

(cid:1)

for any ǫ > 0, then we can see that C

(aaa + ∆aaa) is feasible from

∩ H

p T (aaa + ∆aaa) = p T aaa +
= p T aaa +
= ǫ.

−

−

ǫ

ǫ
(cid:0)

(cid:0)

p T aaa
p T aaa

(cid:1)

(cid:1)

pT p

So, we may conclude ρ(aaa, C )

p T aaa

.

≤

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Proof of Lemma 2.3.2. The conditions of the lemma imply that C
∩
So, we may prove the lemma by demonstrating that for all ǫ satisfying

(aaai) is infeasible.

k+1
i=1 H

T

(5)

(6)

(7)

and all

∆aaa 1, . . . , ∆aaak+1}
{

satisfying

k + 1, we have

\i=1
Assume by way of contradiction that

ǫ

≤
ǫ <

∆aaa ik
k
k+1

α/2, and
β
aaa k+1k
k
≤

< ǫ for 1

4 + 2

,

/α

i

≤

C

∩

H

(aaa i + ∆aaai) is infeasible.

k+1

\i=1
C

C

∩

H

(aaa i + ∆aaai) is feasible.

k+1
i=1 H

∩
ǫ
α

T
p

x ′ +

C

∈

∩

(aaa i).

H

k

\i=1

∆aaaik
ǫ
α

p

Then, there exists a unit vector x ′ ∈

(aaa i + ∆aaai). We ﬁrst show that

To see this, consider any i

k and note that (aaa i + ∆aaai)T x ′ ≥
≤
aaa T
i x ′

∆aaaT

i x ′

x ′

≥ − k

ǫ.

≥ −

0 implies

≥ −

ǫ
α

p

(cid:17)

Thus,

aaa T
i

x ′ +

= aaaT

i x ′ + aaa T
i

(cid:16)

To ﬁnish our proof of (7), we observe that x ′ ∈
Let x = x ′ + ǫ
1

ǫ/α. To derive a contradiction, we now compute
(aaa k+1 + ∆aaak+1)T x ′ = (aaa k+1 + ∆aaak+1)T (x

α p. Then x

k
i=1 H

T

C

−

∩

∈

(cid:13)
(cid:13)
ǫ +

(cid:13)
(cid:13)
≥ −

ǫ
α

α

0.

≥

C and p

C imply x ′ + ǫ

α p

∈

C .

∈

(aaai) and x has norm at most 1 + ǫ/α and at least

k+1x + ∆aaaT
= aaa T
β

+

x

≤ −

≤ −
=

−

k
β(1

β(1

k

−

−

(ǫ/α)p )
k+1p
+ (ǫ/α)

−
(ǫ/α)aaa T
k+1x
−
x
∆aaa k+1k k
k

(ǫ/α)∆aaa T
−
aaak+1k
k
aaak+1k
ǫ/α) + ǫ(1 + ǫ/α) + (ǫ/α)
k
aaak+1k
/α + ǫ/α
k

ǫ/α) + ǫ

k+1p
+ (ǫ/α)

k

(1 + ǫ/α) +
aaak+1k
(cid:0)
k

/α) , by (5)

(cid:1)

β/2 + ǫ (2 +

≤ −
< 0 by (6),

∆aaak+1k

k

+ (ǫ2/α)

17

C

which contradicts x ′ ∈
We now prove that the geometric condition of Lemma 2.3.2 holds with high probability. First,
we establish two basic statements.

(aaai + ∆aaa i).

T

∩

k+1
i=1 H

Proposition 2.3.4. For positive α, β and any vector aaak+1,

2α +

αβ
aaa k+1k
k

min

≥

2 +

(cid:26)

αβ
aaak+1k
k

,

β
aaak+1k (cid:27)
k

.

2 +

Proof. For α

1, we have

≥

while for α

1 we have

≤

2α +

αβ
aaa k+1k
k

=

2 +

β
aaa k+1k
k

/α ≥

2 +

β
aaa k+1k
k

,

2α +

≥

2 +

αβ
aaa k+1k
k

αβ
aaa k+1k
k

.

Proposition 2.3.5. If C

(aaa i) is infeasible, then

k
i=1 H

∩
T
ρ ([aaa1, . . . , aaa k], C )

ρ ([aaa1, . . . , aaa n], C ) .

≤

Proof. Adding constraints cannot make it easier to change the program to make it feasible.

Proof of Lemma 2.3.1. Let aaa 1, . . . , aaan be the rows of A, and let

C 0 = C and C k = C

(aaak).

k

∩

H

\i=1

Note that C n is the ﬁnal program. Let Ek denote the event that C k
1 is feasible and C k is
infeasible. Using Proposition 2.3.5 and the fact that C n infeasible implies that Ek must hold
for some k, we obtain

−

Pr [C n is infeasible and ρ ([aaa1, . . . , aaa n], C )

ǫ]

Pr [Ek+1 and ρ ([aaa 1, . . . , aaa n], C )

≤

≤

ǫ]

≤

n

1

−

Xk=0
1
n
−

Xk=0

≤

≤

Pr [Ek+1 and ρ ([aaa 1, . . . , aaa k+1], C )

ǫ] .

(8)

If Ek+1 occurs, then C k is feasible, and we may deﬁne

κ(aaa 1, . . . , aaa k) = max
C k
=1

min
k
i
1
≤
≤

p
∈
p
k

k

aaaT

i p.

18

Then, Ek+1 implies

Lemma 2.3.3 implies

aaa T

i p

≥

κ(aaa 1, . . . , aaa k), for 1

i

k, and

≤

≤

aaaT

k+1x

≤ −

ρ(aaak+1, C k) for all x

C k,

∈

= 1.

x
k

k

So, we may apply Lemma 2.3.2 and Proposition 2.3.4 to show that Ek+1 implies

ρ ([aaa1, . . . , aaak+1], C )

min

,

κ(aaa 1, . . . , aaak)

κ(aaa 1, . . . , aaa k)ρ(aaa k+1, C k)

,

ρ(aaa k+1, C k)
4 + 2

2

aaak+1k (cid:27)
k
κ(aaa 1, . . . , aaa k), κ(aaa 1, . . . , aaak)ρ(aaa k+1, C k), ρ(aaa k+1, C k)
}

aaak+1k
k

4 + 2

(cid:26)
{

min

(9)

≥

≥

4 + 2

aaak+1k
k

We now proceed to bound the probability that the numerator of this fraction is small.

κ(aaa 1, . . . , aaa k)ρ(aaa k+1, C k)

λ

≤

λ, ρ(aaa k+1, C k)

λ, or there exists an l between 1 and

≤

We ﬁrst note that

implies that either κ(aaa 1, . . . , aaa k)
log(1/λ)
⌈
⌉

for which

≤

We apply Lemma 2.1.8 to bound

κ(aaa 1, . . . , aaa k)

2−

l+1 and ρ(aaa k+1, C k)

2lλ.

≤

≤

Praaa k+1

[Ek+1 and ρ(aaa k+1, C k)

λ]

≤

≤

4λd1/4

,

σ

and Lemma 2.2.9 to bound

Pr

aaa 1,...,aaa k

[C k is feasible and κ(aaa 1, . . . , aaa k)

λ]

≤

≤

4λnd5/4

.

σ

So, for 1

l

≤

≤ ⌈

log(1/λ)
⌉

, we obtain

Pr

aaa 1,...,aaak+1

Ek+1 and κ(aaa 1, . . . , aaak)

2−

l+1 and ρ(aaa k+1, C k)

2lλ

≤

≤

i

and κ(aaa 1, . . . , aaa k)

l+1

2−

=

≤

≤

=

=

C k 6
h
C k+1 =

∅

h
Pr

aaa1,...,aaa k
Praaak+1

h
Pr

C k 6
l+14nd5/4

aaa1,...,aaa k
2−

h

σ

32λnd1.5

.

σ2

∅

∅

and ρ(aaak+1, C k)

2lλ

and κ(aaa 1, . . . , aaak)

l+1

2−

≤

i

=

and κ(aaa 1, . . . , aaa k)

l+1

2−

, by (10)

·
i
C k 6
2l4λd1/4

=

∅

|

σ

i

2l4λd1/4

σ

, by (11),

≤

≤

≤

19

(10)

(11)

Summing over the choices for l, we obtain

Pr [Ek+1 and min

κ(aaa 1, . . . , aaa k), κ(aaa 1, . . . , aaa k)ρ(aaa k+1, C k), ρ(aaa k+1, C k)
{
}

< λ]

4λnd5/4

4λd1/4

+

σ
4nd3/4 + 4 + 32

σ

32λnd1.5

log(1/λ)
⌉
⌈
nd1.5
log(1/λ)
⌉

σ2

+

⌈
σ2

, by σ

1/√d,

≤

!

(32

⌈

log(1/λ)
⌉
σ2

+ 8)nd1.5

.

(cid:19)

≤

≤

≤

λ

λ

 

(cid:18)

(12)

(13)

This concludes our analysis of the numerator of (9). We can bound the probability that the
denominator of (9) is small by observing that aaa k+1 is a Gaussian centered at a point ¯aaa k+1 of
norm at most 1; so, Corollary A.0.3 implies

We now set λ = ǫ(6 + 2σ

Pr

4 + 2

aaa k+1k ≥
k

6 + 2σ

2d ln(e/ǫ)

h

i
2d ln(e/ǫ)) and observe that if we had

p

ǫ.

≤

min

κ(aaa 1, . . . , aaak), κ(aaa 1, . . . , aaa k)ρ(aaa k+1, C k), ρ(aaa k+1, C k)
{
}

p

4 + 2

aaa k+1k
k

this would imply

min

κ(aaa 1, . . . , aaa k), κ(aaa 1, . . . , aaak)ρ(aaa k+1, C k), ρ(aaa k+1, C k)
}

{

< λ, or

4 + 2

aaa k+1k ≥
k

6 + 2σ

2d ln(e/ǫ).

(32

log(1/ǫ(6 + 2σ

d log(e/ǫ)))

+ 8)nd1.5

m

≤

, using σ

1/√d in the ﬁrst term

So, we may apply (12) and (13) to obtain

Pr [Ek+1 and ρ ([aaa 1, . . . , aaak+1], C )

ǫ]

≤

l

≤

≤

≤

≤

ǫ + ǫ

6 + 2σ

2d ln(e/ǫ)

p



(cid:17)

(32
⌈

ǫ + ǫ

6 + 3

ln(e/ǫ)

ǫ + ǫ

ǫ + ǫ

p

ln(e/ǫ)

(cid:17) (cid:18)
(32
⌈

log(1/(6ǫ))
⌉
σ2

p
360log1.5(1/ǫ)nd1.5

(cid:17) (cid:18)

σ2

,

(cid:19)

(cid:16)

(cid:16)
9

(cid:16)

(cid:18)

+ 8)nd1.5

log(1/(6ǫ))
⌉
σ2
+ 8)nd1.5

(cid:19)

σ2

p

(cid:19)

log (1/6ǫ)

+ 1/4)

log1.5(1/ǫ) for ǫ < 1/361,

≤

since (

ln(e/ǫ))(
⌈

p

(cid:18)

ǫ

≤

361log1.5(1/ǫ)nd1.5

σ2

⌉

(cid:19)

20

ǫ

≤

p





Plugging this in to (8), we get

Pr [C 0 is infeasible and ρ ([aaa1, . . . , aaa n], C )

ǫ]

≤

≤

361ǫn2d1.5log1.5(1/ǫ)

.

σ2

2.4 Primal condition number, putting the feasible and infeasible cases to-

gether

We combine the results of Sections 2.2 and 2.3 to prove Lemma 2.1.4, which says that the primal
condition number is probably low.

Proof of Lemma 2.1.4. In Lemma 2.2.1, we show that

Pr [(Ax

0, x

C is feasible) and (ρ(A, C )

ǫ)]

≥

∈

4ǫnd5/4

,

≤

≤

σ

while in Lemma 2.3.1, we show

Pr [(Ax

0, x

C is infeasible) and (ρ(A, C )

ǫ)]

≥

∈

≤

≤

361ǫlog1.5(1/ǫ)n2d1.5

.

Thus,

Pr [ρ(A, C )

ǫ] = Pr [(Ax

0, x

C is feasible) and (ρ(A, C )

ǫ)]

σ2

≤

C is infeasible) and (ρ(A, C )

ǫ)]

≤

≤

≥
+ Pr [(Ax

∈

0, x

≥

∈

σ2

4ǫnd5/4

361ǫlog1.5(1/ǫ)n2d1.5

≤

≤

+

σ

σ2

365ǫlog1.5(1/ǫ)n2d1.5

Setting ǫ = δ/(3α log1.5(α/δ)) where α = 365 n2d1.5
σ2

(note that this satisﬁes ǫ < 1/2), we obtain

1

1100 n2d1.5

365 n2d1.5

log1.5

ρ(A, C ) ≥

δσ2

δσ2

(cid:18)

(cid:19)(cid:21)

Pr

(cid:20)

αδ log1.5

3α

δ log1.5
3α log1.5( α
δ )

(cid:0)

α
δ

(cid:0)

(cid:1)(cid:1)

0.74 δ,

≤

≤

as α/δ

365.

≥

At the same time, Corollary A.0.3 tells us that

The lemma now follows by applying this bound, σ

1/√nd, and (15), to get

Pr

A

1 + σ

nd 2 ln(4e/δ)

δ/4.

kF ≥

k

h

≤

i

Pr

A
k

kF

ρ(A, C ) ≥

"

p

δσ2

(1 +

2 ln(4e/δ))1100 n2d1.5

365 n2d1.5

(cid:18)

δσ2

(cid:19)# ≤

(0.74 + 0.25)δ < δ

p

≤

log1.5

21

(14)

(15)

To derive the lemma as stated, we note

(1 +

2 ln(4e/δ))1100 n2d1.5

365 n2d1.5

212 n2d1.5

29 n2d1.5

log1.5

(cid:18)

δσ2

≤

(cid:19)

δσ2

log2

δσ2

(cid:18)

.

(cid:19)

p

δσ2

2.5 Log of the Primal Condition Number

In this section, we prove we prove Lemma 2.1.5.

Proof of Lemma 2.1.5. First notice that

E

log k

A

kF

ρ(A, C )

(cid:20)

= E

log

kF + log
A

k

ρ(A, C )

(cid:21)

(cid:20)

.

(cid:21)

1

We ﬁrst focus on E [log

A
kF ]. Because logarithm is a convex function, we have

k

E [log

A
kF ]
k

≤

A
log(E [
kF ])
k

≤

log

E

2
F

A
k
k

.

i

r

h

2
F is a dn-dimensional non-central χ2 random variable with non-centrality parameter

As
¯A

(cid:13)
(cid:13)

A
k
k
F , its expectation is nd +
(cid:13)
(cid:13)

¯A

F [AS70, 26.4.37]. Therefore,

(cid:13)
(cid:13)

(cid:13)
E [log
(cid:13)

A
kF ]
k

≤

log √nd + 1.

We will use the following simple fact which is easy to verify numerically:

Fact 2.5.1. For all α

100 and x

2 log α, x

1.5 log x

x/2.

≥

≥

−

≥

Let

as before. By Equation (14) in the proof of Lemma 2.1.4,

α =

365n2d1.5

,

σ2

1

ρ(A, C ) ≥

Pr

(cid:20)

x

≤

(cid:21)

α log1.5 x

.

x

22

Therefore,

1

E

log

(cid:20)

ρ(A, C )

(cid:21)

∞

Pr

log

∞

Pr

(cid:20)

(cid:20)

∞

min

1,

2 log α

(cid:18)

dx +

0

Z

0

Z

0

Z

0

Z

1

> x

dx

ρ(A, C )
1

> ex

(cid:21)
dx

ρ(A, C )
αx1.5

(cid:21)

dx

ex

∞

(cid:19)

2 log α

αx1.5
ex dx

Z
∞

∞

2 log α

2 log α

Z

Z

2 log α + α

e−

x/2dx

2 log α + 2,

=

=

≤

≤

≤

≤

= 2 log α + α

e−

x+1.5 log xdx

where the second-to-last inequality follows from Fact 2.5.1.

Thus,

E

log k

A

kF

ρ(A, C )

(cid:20)

(cid:21)

= E

log

kF + log
A

k

(cid:20)

ρ(A, C )

≤

(cid:21)

1

log √nd + 1 + 2 log α + 2

14 + 4.5 log

≤

nd
σ

.

23

3 Dual Condition Number

In this section, we consider linear programs of the form

AT y = c, y

0.

≥

The dual program of form (1) and the primal program of form (3) are both of this type. The dual
program of form (4) can be handled using a slightly diﬀerent argument than the one we present.
As in section 2, we omit the details of the modiﬁcations necessary for form (4). We begin by
deﬁning distance to ill-posedness appropriately for the form of linear program considered in this
section:

Deﬁnition 3.0.1 (Dual distance to ill-posedness). For a matrix, A, and a vector c, we
deﬁne ρ(A, c) by

a. if AT y = c, y

0 is feasible, then ρ(A, c) =

sup

ǫ :

∆A

k

∆c
k

kF < ǫ implies (A + ∆A)T y = c + ∆c, y

≥

0 is feasible

b. if AT y = c, y

0 is infeasible, then ρ(A, c) =

sup

ǫ :

∆A
k

∆c

kF < ǫ implies (A + ∆A)T y = c + ∆c, y

k

≥

0 is infeasible

≥
kF +

≥
kF +

(cid:8)

(cid:8)

(cid:9)

(cid:9)

The main result of this section is:

Lemma 3.0.2 (Dual condition number is likely low). Let ¯A be an n-by-d matrix and c be
a vector in IRd such that

1. Then for any σ

1/√nd,

1 and

¯A

¯c

F ≤

k

k ≤

Pr

(cid:13)
(cid:13)
(( ¯A,¯c),σ) "

(cid:13)
A, c
(cid:13)
kF
k
ρ(A, c)

>

(A,c)

←N

50000 d1/4n1/2

200 d1/4n1/2

ǫσ2

log2

 

ǫ.

!# ≤

In addition,

(A,c)

E
(( ¯A,¯c),σ) (cid:20)

←N

log k

A, c
kF
ρ(A, c)

≤

(cid:21)

14 + 4 log

≤

ǫσ2

nd
σ

.

We begin by giving several common deﬁnitions that will be useful in our analysis of the dual
condition number (Section 3.1). We deﬁne a change of variables (Section 3.2), and we then
develop a suﬃcient geometric condition for the dual condition number to be low (Section 3.3). In
Section 3.4, we use Lemma 3.2.3 to prove Lemma 3.0.2, thereby establishing that this geometric
condition is met with good probability.

3.1 Geometric Basics

Deﬁnition 3.1.1 (Cone). For a set of vectors aaa 1, . . . , aaa n, let Cone (aaa 1, . . . , aaan) denote
x : x =
{

i λiaaai, λi ≥

0
}

.

P

24

Deﬁnition 3.1.2 (Hull). For a set of vectors aaa1, . . . , aaa n, let Hull (aaa1, . . . , aaa n) denote
x : x =
{
Deﬁnition 3.1.3 (Boundary of a set). For a convex set SS, let bdry(SS) denote the bound-
ary of SS, i.e.,

i λiaaai, λi ≥

i λi = 1
}

ǫ, s.t. x + e

P
e,

SS, x

ǫ > 0,

SS

P

0,

.

.

x :
{

∀

e
k

k ≤

∃

∈

e /
∈

−

}

Deﬁnition 3.1.4 (Point-to-set distance). Let dist (x , SS) denote the distance of x to SS,
i.e.,
min

ǫ, s.t. x + e

SS

e,

e

.

ǫ :
{

∃

k

k ≤

∈

}

Note that Cone (aaa1, . . . , aaa n) is not a non-pointed convex cone, while Hull (aaa1, . . . , aaa n) is the
standard convex hull of

aaa 1, . . . , aaa n}
.
{

3.2 Change of variables

We observe that there exists a solution to the system AT y = c, y

0 if and only if

≥

and that for c

= 0, this holds if and only if

Cone (aaa1, . . . , aaa n) ,

c

∈

Ray (c) intersects Hull (aaa 1, . . . , aaan) .

In this Section, we need one technique beyond those used in Section 2—a change of variables.
We set

n

z = (1/n)

aaai, and

Xi=1
z , for i = 1 to n

x i = aaai −
For notational convenience, we let x n = aaa n−
We can restate the condition for the linear program to be ill-posed in these new variables:

z , although x n is not independent of

z , x 1, . . . , x n
{

−

1.

.

1}

−

Lemma 3.2.1 (Ill-posedness in new variables).

0, c

= 0 is ill-posed if and only if z

bdry(Ray (c)

Hull (x 1, . . . , x n)).

∈

−

AT y = c, y

≥

Proof. We observe

AT y = c, y

0 is feasible

≥

⇐⇒

⇐⇒

⇐⇒

Ray (c) intersects Hull (aaa 1, . . . , aaan)
Ray (c) intersects z + Hull (x 1, . . . , x n)
z

Hull (x 1, . . . , x n) .

Ray (c)

∈

−

= 0, Ray (c)

For c
Hull (x 1, . . . , x n) is a continuous mapping from c, x 1, . . . , x n to subsets
of Euclidean space, and so for z in the set and not on the boundary, a suﬃciently small change
to all the variables simultaneously will always leave z in the set, and similarly for z not in the
set and not on the boundary.

−

25

6
6
6
To establish the other direction, we observe that if z is on the boundary, then can perturb z to
bring it in or out of the set. Although z , x 1, . . . , x n are determined by the aaa 1, . . . , aaa n, we can
perturb the aaa1, . . . , aaa n so as to change the value of z without changing the values of any of the
x 1, . . . , x n. This can be done because each x i is a relative oﬀset from the average z , while each
aaai is an absolute oﬀset from the origin; the proof of lemma 3.2.2 below establishes formally that
the change of variables permits this.

The lemma is also true for c = 0, but we will not need this fact.

Note that Ray (c)
Hull (x 1, . . . , x n) is a convex set. The following lemma will allow us to
apply lemma 2.1.7 to determine the probability that z is near the boundary of this convex set.

−

Lemma 3.2.2 (Independence of mean among new variables). Let ¯aaa 1, . . . , ¯aaan be n vectors
in IRd. Let aaa1, . . . , aaa n be a Gaussian perturbation of ¯aaa1, . . . , ¯aaa n of variance σ2. Let

z =

1
n

Xi

aaa i and x i = aaai −

z , for 1

i

n.

≤

≤

Then, z is a Gaussian perturbation of

Xi
of variance σ2/n and is independent of x 1, . . . , x n.

¯z =

1
n

¯aaai,

Proof. As z is the average of Gaussian perturbations of variance σ2 of n vectors ¯aaa1, . . . , ¯aaan, it
is a Gaussian perturbation of variance σ2/n of the average of these n vectors, that is, of

¯z =

1
n

¯aaai.

Xi

The vector z is independent of x 1, . . . , x n because the linear combination of aaa 1, . . . , aaa n used to
obtain z is orthogonal to the linear combinations of aaa 1, . . . , aaan used to obtain the x is.

Lemma 3.2.3 (Mean is likely far from ill-posedness). Let ¯aaa 1, . . . , ¯aaa n be n vectors in IRd
and ¯c be a vector in IRd. Let aaa 1, . . . , aaa n be a Gaussian perturbation of ¯aaa1, . . . , ¯aaa n of variance σ2
and let c be a Gaussian perturbation of ¯c of variance σ2. Let

z =

1
n

Xi

aaa i and x i = aaai −

z , for 1

i

n.

≤

≤

Then, for all c and x 1, . . . , x n,

Pr
z

[dist (z , bdry(Ray (c)

Hull (x 1, . . . , x n)))

−

ǫ]

≤

≤

8ǫd1/4n1/2

.

σ

Proof. Let c be arbitrary. By Lemma 3.2.2, we can choose x 1, . . . , x n and then choose z
independently. Having chosen x 1, . . . , x n, we ﬁx the convex body Ray (c)
Hull (x 1, . . . , x n)
and apply Lemma 2.1.7 twice: once for the inside ǫ-boundary, and once for the outside ǫ-
boundary.

−

26

3.3 A geometric characterization of dual condition number

We now give a geometric characterization of the dual condition number that uses both the
original and the new variables. In the next section, we will use this characterization to prove
Lemma 3.0.2.

Lemma 3.3.1 (Reciprocal of distance to ill-posedness). Let c and aaa1, . . . , aaa n be vectors
in IRd. Let

z =

1
n

Xi

aaa i and x i = aaai −

z , for 1

i

n.

≤

≤

k1 = dist (z , bdry(Ray (c)

Hull (x 1, . . . , x n)))

k2 =

−

c
k

k

1

ρ(A, c) ≤

max

(cid:26)

8
k1

,

4
k2

,

24 maxi k
k1k2

aaaik

.

(cid:27)

Then

Proof. By the deﬁnition of k1 and k2 and Lemma 3.3.2, we can tolerate any change of magnitude
) in c without the
up to k1/4 in z , and x 1, . . . , x n, and any change of up to
program becoming ill-posed. We show that this means we can tolerate any change of up to k1/8
in aaa i without the program becoming ill-posed. Formally, we need to show that if
∆aaaik ≤
k1/8
k
for all i, then
k1/8. Since
k ≤
∆x i = ∆aaai −

k1/8 + k1/8 = k1/4. Thus

k1/4. Since ∆z = (1/n)

k ≤
∆x ik ≤
k

∆x ik ≤

∆z
k
∆z ,

k1/4 and

k1k2
z

∆aaa i,

x ik
k

2k1+4(

+max

∆z

P

k

k

k

k

which implies

as

ρ(A, c)

min

≥

k1
8

,

(cid:26)

2k1 + 4(
k

+ max

x ik
)
k

(cid:27)

k1k2
z

k

1

ρ(A, c) ≤

max

8
k1

,

4
k2

z

8(
k

k

,

+ max
k1k2

)

x ik

k

,

(cid:27)

(cid:26)

4
k2
8(

2k1 + 4(
k

+ max

z
k
k1k2

x ik
)
k

≤ (

z

k

+max
k1k2

k

x ik
)

k

4(
if k1 ≥
k
otherwise.

z

k

+ max

x ik

k

), and

Since z = (1/n)
2 max

aaa ik
k

, we have
P

aaai implies

z

max

k

k ≤

, and x i = aaa i −
aaa ik
k
aaaik
8
24 max
k
k1k2
k1

4
k2

,

,

.

(cid:27)

1

ρ(A, c) ≤

max

(cid:26)

z implies

x ik ≤ k

aaa ik

k

+

z

k

k ≤

Lemma 3.3.2. (Geometric condition to be far from ill-posedness in new variables.)
If

dist (z , bdry(Ray (c)

Hull (x 1, . . . , x n))) > α

(16)

−

27

and

then

α/4,

α/4,

∆x ik ≤
k
∆z
k ≤
k

∆c

k

k ≤

α
z

k

k

c
k
+ maxi k

,

)

x ik

2α + 4(
k

z + ∆z

bdry(Ray (c + ∆c)

Hull (x 1 + ∆x 1, . . . , x n + ∆x n))

Proof. Assume by way of contradiction that

6∈

∈

−

−

z + ∆z

bdry(Ray (c + ∆c)

Hull (x 1 + ∆x 1, . . . , x n + ∆x n)).

We ﬁrst consider the case that z
that dist (z , bdry(Ray (c)
z + ∆z

−
bdry(Ray (c + ∆c)

∈

−

Ray (c)

6∈

Hull (x 1, . . . , x n)))

−

Hull (x 1, . . . , x n).

In this case, we will show
α, contradicting assumption (16). Since

Hull (x 1 + ∆x 1, . . . , x n + ∆x n)),

≤

z + ∆z = λ(c + ∆c)

γi(x i + ∆x i),

−

Xi

for some λ

0 and γ1, . . . , γn ≥

0,

≥

i γi = 1. We establish an upper bound on λ by noting that

P
λ = k

z + ∆z +

i γi(x i + ∆x i)
k

.

(17)

We lower bound the denominator of (17) by

2α + 4(
k
We upper bound the numerator of (17) by

k ≤

∆c
k

c + ∆c
P
c

k
/2 by

k
c
k
+ maxi k

k

k

k
α
z

k

c

/2.

) ≤ k

k

x ik

z + ∆z +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi

γi(x i + ∆x i)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

z

z

z

≤ k

k

≤ k
=

k

k

k

+ α/4 +

x ik
γi(
k
x ik
+ α/2.

Xi
+ α/4 + max
x ik

+ max

i k

i k

+

∆x ik
)
k

+ α/4

Thus,

Since

λ

≤

z

k

k

x ik
+ maxi k
/2
k

c

k

+ α/2

z + ∆z

λ∆c +

γi∆x i

=

λc

γix i

Ray (c)

Hull (x 1, . . . , x n) ,

 

−

Xi

!

 

! ∈

−

−

Xi

28

we ﬁnd that

dist (z , bdry(Ray (c)

Hull (x 1, . . . , x n)))

−

Xi
+
k

∆z

λ∆c +

γi∆x i

−

≤ (cid:13)
(cid:13)
(cid:13)
∆z
(cid:13)
(cid:13)
≤ k

k

+ λ

∆c

α
4
≤
= α,

z
k

k

+

(cid:18)

k
Xi
x ik
+ maxi k
/2
k

c
k

(cid:13)
(cid:13)
(cid:13)
∆x ik
γi k
(cid:13)
(cid:13)
+ α/2

(cid:19) (cid:18)

2α + 4(
k

α
z

k

k

c
k
+ maxi k

)

x ik

(cid:19)

+

α
4

contradicting assumption (16).

We now consider the case that z

Ray (c)

Hull (x 1, . . . , x n). Since

z + ∆z

bdry(Ray (c + ∆c)

Hull (x 1 + ∆x 1, . . . , x n + ∆x n)),

∈

−

−

∈

−

Hull (x 1 + ∆x 1, . . . , x n + ∆x n). By the assumption that

there exists a hyperplane H passing through z + ∆z and tangent to the convex set
Ray (c + ∆c)
dist (z , bdry(Ray (c)
δ
δ
in the direction perpendicular to H. Since dist (z , z + ∆z )

Hull (x 1, . . . , x n))) > α, there is some δ0 > 0 such that, for every
Hull (x 1, . . . , x n). Choose
4 +δ from z +∆z
4 + δ,

(0, δ0), every point within α + δ of z lies within Ray (c)
(0, δ0) that also satisﬁes δ

4 , and dist (z + ∆z , q ) = 3α

. Let q be a point at distance 3α

+maxi k

x ik

∈
∈

≤ k

−

−

k

z

α

≤

At the same time,

q

Ray (c)

Hull (x 1, . . . , x n)

∈

−

dist (q , Ray (c + ∆c)

Hull (x 1 + ∆x 1, . . . , x n + ∆x n)) >

(18)

−

3α
4
0 and γ1, . . . , γn ≥

0,

.

≥

i γi = 1 such

P

Because q
that

∈

−

Ray (c)

Hull (x 1, . . . , x n), there exist λ

We upper bound λ as before,

q = λc

γix i.

−

Xi

q +

λ = k

i γix ik
c
P
k
k

≤

z
k

k

+ α + δ + maxi k

x ik

z
k

k

≤

c
k

k

+ α/2

x ik
+ maxi k
/2
k

c
k

Hence,

q + λ∆c

γi∆x i = λ(c + ∆c)

γi(x i + ∆x i)

−

Xi

−

Xi

Ray (c + ∆c)

Hull (x 1 + ∆x 1, . . . , x n + ∆x n) ,

∈

−

29

and thus

dist (q , Ray (c + ∆c)

Hull (x 1 + ∆x 1, . . . , x n + ∆x n))

−

λ∆c

γi∆x i

−

≤ (cid:13)
(cid:13)
(cid:13)
λ
(cid:13)
(cid:13)
α/2 + α/4

∆c
k

≤

k

Xi
+ max

i k

(cid:13)
(cid:13)
(cid:13)
∆x ik
(cid:13)
(cid:13)

≤

≤

3α/4,

which contradicts (18).

3.4 Dual condition number is likely low

Proof of Lemma 3.0.2. Let

z =

1
n

Xi

aaai and x i = aaa i −

z , for 1

i

n,

≤

≤

k1 = dist (z , bdry(Ray (c)

Hull (x 1, . . . , x n))) and k2 =

We will apply the bound of Lemma 3.3.1. We ﬁrst lower bound min
observing that if

min

k1, k2, k1k2}

{

< ǫ,

c

k

.
k
k1, k2, k1k2}
{

. We begin by

then either

or

dist (z , bdry(Ray (c)

Hull (x 1, . . . , x n))) < ǫ,

−

−

< ǫ,

k

c
k
log 1
ǫ

or there exists some integer l, 1

dist (z , bdry(Ray (c)

, for which

≤
Hull (x 1, . . . , x n))) < 2lǫ and

(cid:6)

(cid:7)

l

≤

−

c

k

k ≤

2−

l+1.

The probabilities of events (19) and (20) will also be bounded in our analysis of event (21). By
Proposition A.0.4, for d

2, we have

≥

c
Pr [
k

ǫ]

k ≤

≤

eǫ
σ

,

which translates to

while Lemma 3.2.3 implies

Pr

c

k

k ≤

l+1

2−

l+1

,

e2−
σ

≤

i

dist (z , bdry(Ray (c)

Hull (x 1, . . . , x n))) < 2lǫ

Pr
z

h

8

2lǫd1/4n1/2

·

σ

.

≤

i

h

−

30

(19)

(20)

(21)

Thus, we compute

Pr [min

k1, k2, k1k2}
{

< ǫ]

8 ǫd1/4n1/2

+

+

8 ǫd1/4n1/2

+

+

55 ǫd1/4n1/2

eǫ
σ

eǫ
σ

log

log 1
ǫ ⌉
⌈

e2−
σ

Xl=1
16eǫd1/4n1/2

σ2

.

1
ǫ

(cid:24)

(cid:18)

(cid:19)(cid:25)

σ

σ

σ2

≤

=

≤

l+1

8

2lǫd1/4n1/2

·

log

σ

1
ǫ

(cid:24)

(cid:18)

(cid:19)(cid:25)

Setting

we ﬁnd that

for δ

1. So, we obtain

≤

δ

(cid:16)

ǫ =

200d1/4n1/2 log

200d1/4n1/2

σ2δ

,

σ2

55 ǫd1/4n1/2

σ2

log

1
ǫ

≤

(cid:18)

(cid:19)

(cid:17) (cid:14)

δ/2,

Pr

min

k1, k2, k1k2}
{

<

200d1/4n1/2 log

200d1/4n1/2

σ2δ





< δ/2,

σ2 


(cid:17) (cid:14)

δ

(cid:16)

which we re-write this as

Pr

max

"

1
k1

,

1
k2

,

1
k1k2 (cid:27)

>

(cid:26)

200 d1/4n1/2

200 d1/4n1/2

δσ2

log

 

δσ2

<

δ
2

.

!#

(22)

By Lemma 3.3.1, we have

1

ρ(A, c) ≤

8
k1

,

4
k2

,

24 maxi k
k1k2

aaaik

max

(cid:26)

24 max(max

aaaik

i k

≤

, 1) max

(cid:27)
1
k1

(cid:26)

,

1
k2

,

1
k1k2 (cid:27)

.

By Corollary A.0.3, we have

Pr

max(
k

A, c

kF , 1) > 3 + σ

(d + 1)n 2 ln(4e/δ)

<

(23)

δ
4

,

i

and max(
k

A, c

h
kF , 1)
≥

max(maxi k

aaaik

, 1).

p

From a union bound of inequalities 22 and 23, we obtain

A, c
kF
k
ρ(A, c)

Pr

"

>

·

δσ2

log

 

24

200 d1/4n1/2

200 d1/4n1/2

(3 + σ

(d + 1)n2 ln(2e/δ))2

δ.

# ≤

!

p

δσ2

31

The proof of the ﬁrst part of the lemma now follows by computing

24

200 d1/4n1/2

200 d1/4n1/2

·

δσ2

log

 

δσ2

!

(3 + σ

(d + 1)n 2 ln(2e/δ))2

≤

p

δσ2

,

!

50000 d1/4n1/2

200 d1/4n1/2

log2

 

δσ2

1/√dn.

≤

where we used the assumption σ

We now establish the smoothed bound on the log of expectation. Note that

E

log k

(cid:20)

A, c
kF
ρ(A, c)

(cid:21)

= E [log

A, c
k

kF ] + E

log

1

ρ(A, c)
(cid:21)
1
k1

log max

(cid:20)

(cid:20)

,

1
k2

,

(cid:26)
log max

1
k1k2 (cid:27)(cid:21)
1
1
,
k1
k2

,

(cid:20)

1
k1k2 (cid:27)(cid:21)

+ E

log(24 max(max

aaa ik

i k

, 1))

(cid:21)

+ E [24 log max(
k

A
kF , 1)]

(cid:20)

55d1/4n1/2

(cid:26)
+ 2 + log 24 + log √nd + 1

σ2

E [log

A, c
k

kF ] + E

E [log max(
k

A, c

kF , 1)] + E

log

n(d + 1) + 1 + 2 log

p

14 + 4 log

nd
σ

,

≤

≤

≤

≤

where the bound is derived using the same argument as in the proof of Lemma 2.1.5.

32

4 Combining the Primal and Dual Analyses

Proof-of-theorem 1.3.1. Note that the transformation of each canonical form into the conic form
leaves the Frobenius norm unchanged. Also, a random Gaussian perturbation in the original
form maps to a random Gaussian perturbation in the conic form. Therefore, by Lemma 2.1.2, the
smoothed bounds on the primal and dual condition numbers of the conic forms imply smoothed
bounds on each of the condition numbers C (1)
By Lemmas 2.1.4 and Lemma 3.0.2, we have that for all ¯A, ¯b and ¯c satisfying
and σ

D , C (3)
D .

P , C (2)

P , C (2)

1/√nd,

¯A, ¯b, ¯c

1

≤

F ≤
(cid:13)
(cid:13)

(cid:13)
(cid:13)

#

(cid:19)

C (i)(A, b, c) >

Pr
A,b,c "

δσ2

213 (n + 1)2(d + 1)1.5

210 (n + 1)2(d + 1)1.5

2

log

(cid:18)
29 n2(d + 1)1.5

δσ2

2

Pr
A,b,c "

≤

C (i)

P (A, b) >

212 n2(d + 1)1.5

(δ/2)σ2

log

(cid:18)

+ Pr

A,b,c "

C (i)

D (A, c) >

212 (n + 1)2d1.5

(δ/2)σ2

log

(cid:18)

(δ/2)σ2

#
29 (n + 1)2d1.5

(cid:19)

(δ/2)σ2

2

#

(cid:19)

δ/2 + δ/2 = δ.

≤

To bound the log of the condition number, we use Lemmas 2.1.5 and Lemma 3.0.2 to show

log C (i)(A, b, c)
i

E

A,b,c

h
E

≤

A,b,c

log

h

P (A, b) + C (i)
C (i)
(cid:16)

D (A, c)

max

E

A,b,c

log

(cid:18)

h
15 + 4.5 log

2C (i)
(cid:16)
nd
σ

,

≤

≤

(cid:18)

(cid:19)

P (A, b)

(cid:17)i
, E
A,b,c

(cid:17)i

h

log

D (A, c)

2C (i)
(cid:16)

(cid:17)i(cid:19)

E [log(β + γ)]

max

E [log(2β)] , E [log(2γ)]

.

≤

(cid:16)

(cid:17)

where in the second-to-last inequality used that fact that for positive random variables β and γ,

33

5 Open Problems and Conclusion

The best way to strengthen the results in this paper would be to prove that they hold under
more restrictive models of perturbation. For example, we ask whether similar results can be
proved if one perturbs the linear program subject to maintaining feasibility or infeasibility. This
would be an example of a property-preserving perturbation, as deﬁned in [ST03a].

A related question is whether these results can be proved under zero-preserving perturbations
in which only non-zero entries of A are subject to perturbations. Unfortunately, the following
example shows that in this model of zero-preserving perturbations, it is not possible to bound the
condition number by poly(n, d, 1
σ ) with probability at least 1/2. Therefore, if such a result were
to hold in the model of zero-preserving perturbations, it would not be because of a polynomial
bound on the condition number.
Let A be a zero preserving Gaussian perturbation of ¯A with variance σ2. For ease of exposition,
we will normalize

¯A

F to be 1 at the end of formulation. Deﬁne the matrix
(cid:13)
(cid:13)

1

(cid:13)
(cid:13)

−

¯A = 

ǫ
1

−





ǫ

· · ·
−

1 ǫ







≥
xi

ǫxi+1 ≥

where ǫ is a parameter to be chosen later, and consider the linear program Ax
where C =

0 is exactly

x : x > 0
{

. The ith constraint of ¯Ax
}

0, x

C

∈

≥

We apply fact A.0.2 with c = δ2/σ2 assumed to be at least 6 (so that (1
This yields

−

c + ln c)

c/2).

≤ −

Pr[
ai,i −
|
ai,i+1 −
Pr[
|

1
| ≥

ǫ

| ≥

δ]

δ]

≤

≤

1
2 (1

1
2 (1

−

−

e−

e−

2

2

δ

σ2 +ln δ

δ

σ2 +ln δ

2
σ2 )
2
σ2 )

2
δ
4σ2

2
δ
4σ2

e−

e−

≤

≤

(24)

(25)

Setting δ = σ√8 log n yields that, with probability at least 1/2, none of the events (24), (25)
happen for any i. Assuming that none of the events (24), (25) occur, and that ǫ > δ (which we
will ensure later), we have that Ax

C is feasible, and

0, x

is one such feasible solution. We also have that (ǫ + δ)xi+1 ≥

(1

−

δ)xi for every i. Deﬁne

≥

ǫ
δ
1 + δ

−

∈
n

,

n

1

−

ǫ
δ
1 + δ

−

(cid:19)

(cid:18)

(cid:19)

x =

"(cid:18)

,

. . . , 1

#

0 . . . 0
0 . . . 0

( ǫ+δ
δ )n
1
−
0

−

2

−

· · ·

0 . . . 0

0



.





∆A = 





34

We now show that (A+∆A)x
∆A
To see infeasibility, note that the constraint given by the top row of (A + ∆A) is

C is infeasible, and hence ρ(A, C )

0, x

≤ k

≥

∈

kF = ( ǫ+δ

1

−

δ )n

2.

−

x1 + ǫx2 −

−

(cid:18)
( ǫ+δ
δ )n
while we simultaneously have that x2 ≤
1
−
this constraint is impossible to satisfy for x
∈

−
C .

n

2

−

ǫ + δ
1
δ

xn ≥

0

(cid:19)

−
2xn. Assuming ǫ

1 (which we ensure later),

≤

1

n2

δ )n

A
k

kF ≤

2 = ( O(1)

n and σ = 1

n(1 + δ)2 + n(ǫ + δ)2

n2 (and hence δ = √8 log n

) yields ρ(A, C ) = ( ǫ+δ
−

Letting ǫ = 1
2,
which is exponentially small and also satisﬁes the requirements on ǫ. We can upper bound
2√n. Thus the condition number, which is equal
A
kF by
k
kF /ρ(A, C ), is at least Ω(n)n
A
to
k

p
If we had normalized
F = 1 at the beginning of the proof, the corresponding normalization
would have been ǫ
n2√n , which still shows the negative result. This analysis also
shows the impossibility of a theorem like theorem 1.3.1 for another natural model of perturbation,
relative perturbation, that is also zero-preserving: multiplying each entry of ¯A by an N (1, σ2)
Gaussian random variable. This concludes our discussion of impossibility results for smoothed
analysis.

¯A
1
n√n , σ
(cid:13)
(cid:13)

n )n

(cid:13)
(cid:13)

3.

≤

≈

≈

−

−

−

1

We would like to point out that condition numbers appear throughout Numerical Analysis and
that condition numbers may be deﬁned for many non-linear problems. The speed of algorithms
for optimizing linear functions over convex bodies (including semideﬁnite programming) has
been related to their condition numbers [Fre02, FV00], and it seems that one might be able to
extend our results to these algorithms as well. Condition numbers have also been deﬁned for
non-linear programming problems, and one could attempt to perform a smoothed analysis of
non-linear optimization algorithms by relating their performance to the condition numbers of
their inputs, and then performing a smoothed analysis of their condition numbers.

The approach of proving smoothed complexity bounds by relating the performance of an algo-
rithm to some property of its input, such as a condition number, and the performing a smoothed
analysis of this quantity has also been recently used in [ST03a, SST02]. Finally, we hope that this
work illuminates some of the shared interests of the Numerical Analysis, Operations Research,
and Theoretical Computer Science communities.

35

References

[AJPY93] K. M. Anstreicher, J. Ji, F. A. Potra, and Y. Ye. Average performance of a self–
dual interior–point algorithm for linear programming.
In P. M. Pardalos, editor,
Complexity in Numerical Optimization, pages 1–15. World Scientiﬁc Publishing Co.,
London, United Kingdom, 1993.

[AJPY99] K. M. Anstreicher, J. Ji, F. A. Potra, and Y. Ye. Probabilistic analysis of an
infeasible-interior-point algorithm for linear programming. Mathematics of Opera-
tions Research, 24(1):176–192, 1999.

[AS70]

[Bal93]

[BD02]

[BR76]

[CC01]

[CP01]

[DG99]

[EA96]

[Fre02]

[FV00]

Milton Abramowitz and Irene A. Stegun, editors. Handbook of mathematical func-
tions, volume 55 of Applied Mathematics Series. National Bureau of Standards, 9
edition, 1970.

K. Ball. The reverse isoperimetric problem for gaussian measure. Discrete and Com-
putational Geometry, 10(4):411–420, 1993.

Avrim Blum and John Dunagan.
tron algorithm for linear programming.
SIAM Symposium on Discrete Algorithms (SODA ’02), 2002.
http://theory.lcs.mit.edu/

Smoothed analysis
In Proceedings of

jdunagan/.

the percep-
of
the 13th ACM-
Available at

∼

Bhattacharya and Rao. Normal approximation and asymptotic expansion. pages
23–38, 1976.

D. Cheung and F. Cucker. A new condition number for linear programming. Math.
Programming, 91(1 (Ser. A)):163–174, 2001.

F. Cucker and J. Pe˜na. A primal-dual algorithm for solving polyhedral conic systems
with a ﬁnite-precision machine. Submitted to SIAM Journal on Optimization, 2001.

S. Dasgupta and A. Gupta. An elementary proof of the johnson-lindenstrauss lemma.
International Computer Science Institute, Technical Report 99-006, 1999.

C. Mezaros X. Xu E. Andersen, J. Gondzio. Implementation of interior point methods
for large scale linear programming. In T. Terlaky, editor, Interior point methods in
mathematical programming. Kluwer Academic Publisher, 1996.

R. Freund. Complexity of convex optimization using geometry-based measures and
a reference point. Technical Report OR358-01, MIT Operations Research Center
Working Paper, 2002. submitted to Mathematical Programming.

Robert Freund and Jorge Vera. Condition-based complexity of convex optimization
in conic linear form via the ellipsoid algorithm. SIAM Journal on Optimization,
10(1):155–176, 2000.

[Gon88] Clovis C. Gonzaga. An Algorithm for Solving Linear Programming Problems in
O(n3L) Operations, pages 1–28. Springer-Verlag, 1988. Progress in Mathematical
Programming, N. Megiddo ed..

36

[GT92]

[HB02]

C. C. Gonzaga and M. J. Todd. An O(√nL)–iteration large–step primal–dual aﬃne
algorithm for linear programming. SIAM Journal on Optimization, 2:349–359, 1992.

Petra Huhn and Karl Heinz Borgwardt.
Interior-point methods: Worst case and
average case analysis of a phase-I algorithm and a termination procedure. Journal of
Complexity, 18:833–910, 2002.

[IL94]

D. Shanno I. Lustig, R. Marsten. Interior point methods: computational state of the
art. ORSA Journal on Computing, 6(1):1–14, 1994.

[Kar84]

N. Karmarkar. A new polynomial time algorithm for linear programming. Combina-
torica, 4:373–395, 1984.

[Kha79]

L. G. Khachiyan. A polynomial algorithm in linear programming. Doklady Akademia
Nauk SSSR, pages 1093–1096, 1979.

[LDK63] B. Grunbaum L. Danzer and V. Klee. Helly’s theorem and its relatives. In Convexity
( Proceedings of the Symposia on Pure Mathematics 7 ), pages 101–180. American
Mathematical Society, 1963.

[LMS90]

I. J. Lustig, R. E. Marsten, and D. F. Shanno. The primal–dual interior point method
on the Cray supercomputer. In T. F. Coleman and Y. Li, editors, Large–Scale Nu-
merical Optimization, Papers from the Workshop held at Cornell University, Ithaca,
NY, USA, October 1989, volume 46 of SIAM Proceedings in Applied Mathematics,
pages 70–80. Society of Industrial and Applied Mathematics (SIAM), Philadelphia,
PA, USA, 1990.

[MTY93] S. Mizuno, M. J. Todd, and Y. Ye. On adaptive–step primal–dual interior–point
algorithms for linear programming. Mathematics of Operations Research, 18:964–
981, 1993.

[Nem88] A. S. Nemirovskii. An new polynomial algorithm for linear programming. Doklady
Akademii Nauk SSSR, 298(6):1321–1325, 1988. Translated in : Soviet Mathematics
Doklady 37(1): 264–269, 1988.

[Pe˜n00]

J. Pe˜na. Understanding the geometry of infeasible perturbations of a conic linear
system. SIAM Journal on Optimization, 10:534–550, 2000.

[Ren94]

J. Renegar. Some perturbation theory for linear programming. Math. Programming,
65(1, Ser. A):73–91, 1994.

[Ren95a] J. Renegar. Incorporating condition measures into the complexity theory of linear

programming. SIAM J. Optim., 5(3):506–524, 1995.

[Ren95b] J. Renegar. Linear programming, complexity theory and elementary functional anal-

ysis. Math. Programming, 70(3, Ser. A):279–351, 1995.

[SST02] Arvind Sankar, Daniel A. Spielman, and Shang-Hua Teng. Smoothed analysis of
the condition numbers and growth factors of matrices. Submitted for publication.
Available at http://arxiv.org/cs.NA/0310022, 2002.

37

[ST03a] Daniel Spielman and Shang-Hua Teng. Smoothed analysis: Motivation and discrete
models. In Proceedings of WADS 2003, Lecture Notes in Computer Science 2748,
pages 256–270, 2003.

[ST03b] Daniel Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the
to

simplex algorithm usually takes polynomial time. Journal of the ACM, 2003.
appear.

[ST03c] Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis termination of linear

programming algorithms. Math. Program., Ser. B, 97:375–404, 2003.

[Tod91] M. J. Todd. Probabilistic models for linear programming. Mathematics of Operations

Research, 16(4):671–693, 1991.

[Tod94] M. J. Todd. A lower bound on the number of iterations of primal-dual interior-
point methods for linear programming. In G. A. Watson and D. F. Griﬃths, editors,
Numerical Analysis 1993, pages 237–259. Longman Press, Harlow, 1994.

[TY96] M. J. Todd and Y. Ye. A lower bound on the number of iterations of long-step and
polynomial interior-point methods for linear programming. Annals of Operations
Research, 62:233–252, 1996.

[Vai90]

An algorithm for

linear programming which requires
P. M. Vaidya.
O((m + n)n2 + (m + n)1.5nL) arithmetic operations. Mathematical Programming,
47:175–201, 1990. Condensed version in : Proceedings of the 19th Annual ACM Sym-
posium on Theory of Computing, 29–38, 1987.

[Ver96]

Jorge Vera.
linear programs. SIAM Journal on Optimization, 6(3), 1996.

Ill-posedness and the complexity of deciding existence of solutions to

38

A Gaussian random variables

We now derive particular versions of well-known bounds on the Chi-Squared distribution. These
bounds are used in the body of the paper, and bounds of this form are well-known. We thank
DasGupta and Gupta [DG99] for this particular derivation.

Fact A.0.1 (Sum of gaussians). Let X1, . . . , Xd be independent N (0, σ) random variables.
Then

d

Pr[

Xi=1

X 2

i ≥

κ2]

≤

d
2 (1

e

−

2

κ

dσ2 +ln κ

dσ2 )

2

Proof. For simplicity, we begin with Yi ∼
then E[etY 2

(t < 1

2 ). We proceed with

] = 1
√1
−

2t

N (0, 1). A simple integration shows that if Y

N (0, 1)

∼

Pr[

k

0] =

(for t > 0)

Y 2
i ≥

k] =

d

Pr[

d

Xi=1
Y 2
i −

≥

1]

k)

i −
i=1 Y 2
d

≥
i −

k)

d/2

i
kt

e−

≤
=

≤

Xi=1

d
i=1 Y 2

Pr[et(

P

et(

E

P

h

1

1

(cid:18)

2t

(cid:19)
d/2

−
k
d

(cid:18)

(cid:19)

k
2 + d

e−

2 = e

d
2 (1

−

k

d +ln k
d )

d

Pr[

Xi=1
d +ln k

k

Y 2
i ≥

k] = Pr[

X 2

i ≥

σ2k]

d

Xi=1
dσ2 +ln κ

κ

2

2

(by Markov’s Ineq.)

(letting t =

1
2 −

d
2k

)

Since

we set k = κ2

σ2 and obtain e

d
2 (1

−

d ) = e

d
2 (1

−

dσ2 ) which was our desired bound.

In particular, this implies:

Fact A.0.2 (Alternative sum of gaussians). Let X1, . . . , Xd be independent N (0, σ) random
variables. Then for c

1,

Corollary A.0.3. Let x be a d-dimensional Gaussian random vector of variance σ2 centered
at the origin. Then, for d

2 and ǫ

1/e2,

≥

d

Pr[

Xi=1

X 2

i ≥

cdσ2]

≤

d
2 (1

e

c+ln c).

−

≥

≤
x
k

k ≥

Pr

h

σ

d(1 + 2 ln(1/ǫ)

ǫ

≤

i

p

39

Proof. Set c = 1 + 2 ln(1/ǫ) in fact A.0.2. We then compute

d
2 (1

e

−

c+ln c)

c+ln c

e1
−

2 ln 1

ǫ +ln(1+2 ln 1

ǫ ) = ǫe−

ln 1

ǫ +ln(1+2 ln 1
ǫ )

≤

e−

≤

We now seek to show

⇔ −

ln

+ ln(1 + 2 ln

e−
1
ǫ

ln 1

≤

ǫ +ln(1+2 ln 1
ǫ )
1
ǫ
1
ǫ ≤

1 + 2 ln

≤

)

⇔

1

0

1
ǫ

For ǫ = 1/e2, the left-hand side of the last inequality is 5, while the right-hand side is greater
than 7. Taking derivatives with respect to 1/ǫ, we see that the right-hand side grows faster as
we increase 1/ǫ (decrease ǫ), and therefore will always be greater.

We also use the following easy-to-prove fact, a proof of which may be found in [ST03b, Propo-
sition 2.4.7]

Proposition A.0.4. Let x be a d-dimensional Gaussian random vector of variance σ2 centered
at the origin. Then,

Pr [

x
k

ǫ]

k ≤

≤

d

.

ǫ
σ

(cid:16)

(cid:17)

40

