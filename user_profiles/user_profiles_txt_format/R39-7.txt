[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 36

..........................................................................................................................................................................................................................

PUTTING FAULTY CORES TO WORK

..........................................................................................................................................................................................................................

NECROMANCER, A ROBUST AND HETEROGENEOUS CORE COUPLING EXECUTION

SCHEME, EXPLOITS A FUNCTIONALLY DEAD CORE TO IMPROVE SYSTEM THROUGHPUT

BY SUPPLYING HINTS REGARDING HIGH-LEVEL PROGRAM BEHAVIOR. NECROMANCER

PARTITIONS A CMP SYSTEM’S CORES INTO MULTIPLE GROUPS, EACH OF WHICH SHARES

A LIGHTWEIGHT CORE THAT CAN BE SUBSTANTIALLY ACCELERATED USING EXECUTION

HINTS FROM THE FAULTY CORE.

......Over the past decade, shrinking

process technologies and rising power den-
sities have led to many reliability challenges
such as defects, wear-out, and parametric var-
iations.1 One challenge for the semiconduc-
tor
industry is manufacturing defects,
which directly impact yield. From each pro-
cess generation to the next, microprocessors
become more susceptible to manufacturing
defects, owing to higher sensitivity of materi-
als, random particles attaching to the wafer
surface, and subwavelength lithography
issues. So, a substantial
is
required to maintain an acceptable level of
manufacturing yield. Traditionally, hardware
reliability was a concern only for high-end
systems, such as HP Tandem Nonstop, for
which applying high-cost redundancy solu-
tions such as triple modular redundancy
was acceptable. However, hardware reliabili-
ty is now a major issue for mainstream com-
puting, where high-cost reliability solutions
aren’t acceptable.

investment

A large fraction of die area is devoted to
caches, which manufacturers can protect
using techniques such as column redun-
dancy. Using appropriate protection mecha-
nisms
the processing cores
become the die’s major source of defect vul-
nerability. Consequently, we try to tackle
hard faults appearing in the processing
core’s noncache portions. Those noncache

for caches,

portions are inherently irregular, and han-
dling defects in them is more challenging.2
A common solution is to disable the faulty
core.3 However,
chip multiprocessor
(CMP) systems with only a modest number
of high-performance cores (such as Intel
Core 2) dominate the industry. So, losing a
core significantly reduces system throughput
and sale price. At the other extreme lies fine-
redundancy.4
grained microarchitectural
Unfortunately, as most of the core logic is
nonredundant, fault coverage from these
approaches is very limited.2 StageNet sug-
gests breaking each core into pipeline stages
and allowing one core to borrow stages
from other cores through interconnection
networks.5 Introducing these interconnec-
tion networks in the processor pipeline
presents performance, power consumption,
and design complexity challenges. DIVA, a
proposal for dynamically verifying complex
microprocessors, employs a checker pipeline
that reruns the same instruction stream to
ensure correct program execution.6 Given
that DIVA wasn’t
intended to tolerate
defects, a hard fault can result in about ten-
fold slowdown.

As we will show, for most defect instan-
ces, the program’s execution flow on a faulty
core coarsely resembles the fault-free pro-
gram execution when starting from the
same architectural state. Given this, we

Amin Ansari
Shuguang Feng
Shantanu Gupta
Scott Mahlke
University of Michigan,
Ann Arbor

..............................................................

36

Published by the IEEE Computer Society

0272-1732/10/$26.00 (cid:2)c 2010 IEEE

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 37

100%

90%

80%

70%

60%

50%

40%

30%

20%

10%

0%

am sv

am sv

am sv

am sv

am sv

am sv

am sv

am sv

am sv

am

sv

am

sv

am sv

am sv

am sv

mgrid

applu

mesa

art

equake

ammp

gzip

vpr

gcc

crafty

parser

bzip2

twolf

Average

<10K (Cl)

<100K (Cl)

>100K (Cl) or masked

Figure 1. The first set of bars shows the distribution of injected hard faults that manifest as architectural state mismatches
(labeled as ‘‘am’’) across different latencies, where latency is measured as the number of committed instructions (CI).
The second set of bars shows the number of CIs before an injected fault results in a violation of a 90-percent similarity
index (labeled as ‘‘sv’’).

propose Necromancer to enhance overall sys-
tem throughput and mitigate the perfor-
mance loss caused by defects in the core’s
noncache parts. To accomplish this, we first
relax the correct execution constraint on a
faulty core (the undead core). Next, we lever-
age high-level execution information (hints)
from the undead core to accelerate the execu-
tion of an animator core. Necromancer intro-
duces an additional core called the animator
core, which is an older generation micro-
processor. The animator core treats the
hints only as performance enhancers without
impacting execution correctness.

Motivation

We can’t trust an aggressive out-of-order
core with a hard fault to operate correctly.
However, as we will see, it can still provide
useful execution hints.

Effect of hard faults on program execution

To illustrate how hard faults negatively
impact program execution, we identify the
average number of instructions that can be
committed before observing an architectural
state mismatch. This result, for 5,000 area-
weighted hard fault injections, is the first
set of bars in Figure 1. For these experiments,
we have a golden execution, which com-
pares its architectural state with the faulty
execution. When a mismatch happens, it

terminates the simulation and reports the
number of committed instructions up to
that point. For instance, looking at eQuake,
42 percent of the injected hard faults cause
an architectural state mismatch in fewer
than 10 thousand committed instructions.
As this figure shows, more than 40 percent
of the injected hard faults can cause an im-
mediate architectural state mismatch. Thus,
we can’t trust a faulty core to provide correct
functionality even for short periods of pro-
gram execution.

Relaxing correctness constraints

Here, we try to determine the quality of
program execution on a faulty core when
relaxing the absolute correctness constraints.
The second set of bars, in Figure 1, depicts
how many instructions can be committed
in a faulty core before it gets considerably
off the correct execution path. We define a
similarity index that measures the similarity
between the program counter of committed
instructions in the faulty core and a golden
execution of the same program. We calculate
the similarity index every 1,000 instructions,
and whenever it drops beneath a prespecified
threshold, we stop the simulation and record
the number of committed instructions. Here,
we use a similarity index of 90 percent,
which means that during each 1,000 instruc-
tion window, 90 percent of PCs must hit

....................................................................

NOVEMBER/DECEMBER 2010

37

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 38

...............................................................................................................................................................................................
CHIP MULTIPROCESSORS

exactly the same instruction cache line in
both the golden execution and program exe-
cution on the faulty core. As you can see,
even for this high threshold value, in more
than 85 percent of cases, the faulty core
can successfully commit at least 100 thou-
sand instructions before its execution differs
by more than 10 percent.

Because a faulty core’s execution behavior
coarsely matches the golden program execu-
tion for long time periods, we can extract
useful information from the program’s exe-
cution on the faulty core and send these
hints to the animator core running the
same program. This symbiotic relationship
between the two cores lets the animator
core achieve significantly higher perfor-
mance. We let the undead core run without
requiring absolutely correct functionality.
Later, we’ll evaluate the performance boost
possible with the Necromancer system.

Challenges in coupling with a faulty core
Given a CMP system, two cores can be
coupled together to achieve higher single-
thread performance. Because the performance
of the slower core limits the coupled core’s
overall performance, these two cores were tra-
ditionally identical. However, to accelerate
program execution, one core must progress
through the program faster. Three methods
have been proposed to accomplish this:

(cid:3) In Paceline, the core that runs ahead
(the leader) operates at a higher fre-
quency than the core that receives exe-
cution hints (the checker).7 When an
architectural state mismatch happens,
the leader’s frequency gets adjusted.

(cid:3) In Slipstream, processors need two ver-
sions of the same program.8 The leader
core runs a shorter version, based on
the removal of ineffectual instructions,
while
checker
the
unmodified program.

runs

core

the

(cid:3) In Flea-Flicker, two-pass pipelining lets
the leader core return an invalid value
on long-latency operations and proceed.9

In most of these schemes, the checker
core takes advantage of program execution
on the leader core by receiving preprocessed
instruction streams, resolved branches, and

L2 cache prefetches. Although a simple ex-
tension of these ideas seems plausible, Necro-
mancer encounters two major difficulties
owing to the presence of defects.

In the first type of defect, global divergences,
hints become ineffective when the undead
core gets completely off the correct execution
path. To bring the undead core back to a
valid execution point, the system can copy
the animator core’s architectural state over
to the undead core. Although prior works
have used exact state matching by check-
pointing the register file,7 it’s not applicable
for animating a faulty core because architec-
tural state mismatches occur so frequently.
Therefore, we need coarse-grained online
monitoring of the hints’ effectiveness over a
long time period to decide whether the sys-
tem should resynchronize the undead core
with the animator core. Moreover, resynch-
ronizations should be cheap and relatively in-
frequent to avoid a noticeable impact on the
animator core’s overall performance.

In the other type of defect, fine-grained
variations, the undead core might execute
and commit more or fewer instructions,
causing variations in the similarity of pro-
gram executions between the two cores. For
instance, the undead core can take the
wrong direction on an IF statement. Al-
though it quickly returns to the correct exe-
cution path, a perfect data or instruction
stream for the animator core is unattainable.
This necessitates employing generic hints
that are more resilient to these local abnor-
malities. Moreover, a mechanism must help
the animator core identify the proper time
for getting the hints from the dead core.
Given that the hints’ usefulness varies, we
can also leverage fine-grained hint disabling
to enhance system efficiency.

Necromancer architecture

To mitigate system throughput loss due
to defects, Necromancer employs a robust,
flexible heterogeneous core coupling tech-
nique. Figure 2 illustrates the high-level
Necromancer design. In our design, most
communication is unidirectional from the
undead core to the animator core, except
for the resynchronization and hint-disabling
signals. Consequently, Necromancer uses a
single queue for sending the hints and

....................................................................

38

IEEE MICRO

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 39

Queue

l
i

a
T

d
a
e
H

 

e
r
o
c
d
a
e
d
n
U

Cache fingerprint

Hint gathering

FET

DEC REN DIS

EXE MEM COM

Resynchronization

signal and hint

disabling information

Hint distribution

Hint disabling

FE DE RE DI EX ME CO

e
r
o
c
 
r
o
a
m
n
A

t

i

L1-inst

L1-data

Memory
hierarchy

L1-inst

L1-data

Read-only

Shared L2 cache

Figure 2. Necromancer’s high-level architecture. The modules that are added or modified
are highlighted.

cache fingerprints to the animator core. The
hint-gathering unit attaches a tag to each
queue entry to indicate its type. When this
queue gets full and the undead core wants
to insert a new entry, it stalls. To preserve
correct memory state, we don’t allow
the dirty lines of the undead core’s data
cache to be written back to the shared L2
cache. Furthermore, we also disabled excep-
tion handling within the undead core because
the animator core maintains the precise state.
In Necromancer, we don’t rely on over-
clocking the undead core or having multiple
versions of the same program. Furthermore,
Necromancer is a hardware-based approach
that’s transparent to the workload and oper-
ating system. It doesn’t require register file
checkpointing for performing exact state
matching between two cores. Instead, we
employ a fuzzy hint-disabling approach
based on the continuous monitoring of
hint effectiveness, initiating resynchroniza-
tions when appropriate. To make the hints
more robust against microarchitectural dif-
ferences between the two cores and variations
in the number and order of executed instruc-
tions, we leverage the number of committed
instructions for hint synchronization and at-
tach this number to every queue entry as an
age tag. Moreover, we introduce the concept
of a release window to make the hints more
robust in the presence of the aforementioned

variations. The release window helps the ani-
mator core determine the right time to use a
hint. For instance, assuming the data cache
release window is 20, and assuming that
1,030 instructions have already been commit-
ted in the animator core, data cache hints
with age tags of less than or equal to 1,050
can be pulled off the queue and applied.

Hint gathering and distribution

We must send our branch prediction and
cache hints (except L2 prefetches) through
the queue to the animator core. The undead
core’s hint-gathering unit is responsible for
gathering hints and cache fingerprints,
attaching the age and type tags, and inserting
them in the queue. The program counter of
committed instructions and addresses of
committed loads and stores are considered
hints. For branch prediction hints,
the
hint-gathering unit sends a hint through
the queue every time the faulty core’s branch
predictor gets updates. On the animator core
side, the hint-distribution unit receives these
packets from the queue and compares their
age tag with the local number of committed
instructions plus the corresponding release
window sizes. It treats the incoming cache
hints as prefetching information to warm
up its local caches.

The animator core’s default branch
predictor is a simple bimodal predictor.

....................................................................

NOVEMBER/DECEMBER 2010

39

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 40

...............................................................................................................................................................................................
CHIP MULTIPROCESSORS

Branch prediction release window size = 10 committed instructions

C/C++ code

sum = 0;
for(i=0; i<100; i++){
for(j=0; j<2; j++){

sum = sum+

arr[i][j];

}

}

Chronologically sorted branch prediction hints for

0X19000020 [sent from the undead core]

NM BP entry for PC = 0X19000020 at different

times [in the animator core]

Age tag

PC

Taken or not

Taken or not

Number of
committed
instructions

0X19000020

Not taken

0X19000020

0X19000020

0X19000020

0X19000020

taken

Taken

Taken

Taken

Taken

0X19000020

Not taken

0X19000020

Taken

···

···

9

15

21

31

37

43

53

···

0X19000020

Not taken

PC

0X19000020

0X19000020

0X19000020

0X19000020

0X19000020

···

taken

Taken

Taken

Taken

Taken

Taken

···

0X19000020

Not taken

9

15

21

31

37

43

53

···

Perfect
branch

prediction

Not taken

Taken

Taken

Taken

Taken

Taken

···

Not taken

DEC Alpha Assembly Code

0X19000000: xor  $1, $1, $1  #sum=0
0X19000004: xor  $2, $2, $2  #i=0
0X19000008: xor  $3, $3, $3  #j=0
0X1900000C: ldq  $4, 0($5) 
0X19000010: addq  $1, 0($5) 
0X19000014: addq  $3, 1, $3 
0X19000018: addq  $5, 1, $5 
0X1900001C: cmplt $3, 2, $6 
0X19000020: bne  $6, 0X1900000C
0X19000024: addq  $2, 1, $2 
0X19000028: cmplt $2, 100, $7  #i<100
0X1900002C: bne  $7, 0X19000008

#i++

#load from arr
#sum=sum+arr[i][j]
#j++
#arr pointer proceeds
#j<2

Figure 3. A code example in which the animator core receives hints at improper times, resulting in low branch prediction
accuracy. Switching to the animator core’s original branch predictor is therefore beneficial. The code simply calculates the
sum of 2D-array elements. The branch prediction release window size is normally set to maximize the branch prediction
accuracy for the entire execution.

We first add an extra bimodal predictor
(NMBP) to track incoming branch predic-
tion hints. Furthermore, we employ a hierar-
chical tournament predictor to decide, for a
given program counter, whether the original
branch predictor or the NMBP should take
over. As we mentioned earlier, we leverage
release windows to apply the hints just before
they’re needed. However, owing to the varia-
tions in the number of executed instructions
on the undead core, even the release window
can’t guarantee perfect timing of the hints. In
such a scenario, for a subset of instructions,
the tournament predictor can give priority
to the animator core’s original branch predic-
tor
to avoid any performance penalty.
Figure 3 shows a simple example in which
the NMBP can achieve only 33 percent
branch prediction accuracy. This is mainly
due to the existence of a tight inner loop
with a low trip count for which switching
to the original branch predictor can enhance
the branch prediction accuracy.

To reduce the queue size, we must limit
communication traffic to only the most
beneficial hints. Consequently, in the hint-
gathering unit, we use
two content-
addressable memories (CAMs) with several
entries to discard recently sent instruction-
cache and data-cache hints. Eliminating

redundant hints also minimizes resource con-
tention on the animator core. Furthermore,
to save on transmission bits, we send the
block only related bits of the address for
cache hints,
ignore hints on speculative
paths, and, for branch prediction hints,
send only lower-order bits of the program
counter that are used for updating the
NMBP’s branch history table.

Why disable hints?

We can disable hints when they’re no lon-
ger beneficial for the animator core—for
example, because the program execution on
the undead core diverges from the correct
execution path, animator core performance
is already near its ideal case, or the undead
core can’t get ahead of the animator core.
In all these scenarios, hint disabling helps
in four ways:

(cid:3) It avoids occupying the animator core’s

resources with ineffective hints.

(cid:3) The queue fills up less often, which
means fewer stalls for the undead core.
(cid:3) Disabling hint gathering and distribu-

tion saves power.

(cid:3) It indicates when the undead core has
strayed far from the correct execution
path and requires resynchronization.

....................................................................

40

IEEE MICRO

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 41

Core1

Core2

Core1

Core2

Core1

Core2
ore2

Hint gathering

Hint gathering

Cache fingerprint

Cache fingerprint

Cluster1

Cluster2
er2

Animator

core

Core3

Core4

Core3

Core4
Core4

Hint gathering

Hint gathering

Cache fingerprint

Cache fingerprint

L2 cache banks

2 cache banks
L2 cache banks

Core3

Core4

L2

cache
banks

Data
switch

L2

cache
banks

Core1

Core2

Core1
Core1

Core2
Core2

Cluster3

Cluster4
Cluster
er4

Core3

Core4

Core3
Core
e3

Core4
Core4

Figure 4. The high-level Necromancer design for a large chip multiprocessor (CMP) system
with 16 cores, modeled after the Sun Rock processor.

Hint-disabling mechanisms

The hint-disabling unit is responsible for
realizing when to disable each type of hint.
To disable cache hints, the cache fingerprint
unit generates high-level cache access infor-
mation based on the committed instructions
in the last time interval (for example, the last
1,000 committed instructions). The system
sends these fingerprints through the queue
and compares
them with the animator
core’s cache access pattern. Based on a pre-
specified threshold value for the similarity
between access patterns, the animator core
decides whether to disable the cache hint.
In addition, when a hint is disabled, the
hint remains disabled throughout a signifi-
cant time period (called the back-off period).
Apart from prioritizing the animator core’s
original branch predictor for a subset of PCs,
we can also use the NMBP for global dis-
abling of the branch prediction hints. To do
so, we use a score-based scheme with a single
counter. For every branch that the original
branch predictor and NMBP agree on, no
action is required. Nonetheless,
the
branches that the NMBP correctly predicts
and the original branch predictor does not,
the score counter is incremented by one. Sim-
ilarly,
for the branches that the NMBP

for

mispredicts but the original branch predictor
correctly predicts, the score counter is decre-
mented by one. Finally, at the end of each dis-
abling time interval, if the counter is below a
certain threshold, the branch prediction hints
will be disabled for the back-off period.

Resynchronization

Because the undead core can stray from
the correct execution path and no longer
provide useful hints, we need a mechanism
to restore it to a valid state. To accomplish
this, we occasionally resynchronize the two
cores, as we mentioned earlier. According
to Powell et al., for a modern processor,
this process takes on the order of 100 cycles.2
Moreover, we squash all instructions in the
undead core’s pipeline, resets the rename
table, and invalidates the data-cache content.
We take advantage of the hint-disabling in-
formation to identify when resynchroniza-
tion is needed. For instance, a potential
resynchronization policy is to resynchronize
when at least two hints are disabled.

Necromancer design for CMP systems

We can apply Necromancer to CMP sys-
tems with more cores. Figure 4 illustrates the
Necromancer design for a 16-core system

....................................................................

NOVEMBER/DECEMBER 2010

41

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 42

...............................................................................................................................................................................................
CHIP MULTIPROCESSORS

m grid

a p plu

m esa

art

e q uake

m p

a m

gzip

vpr

g cc

crafty

p arser

bzip 2

tw olf

A vera g e

Fault location

Program counter
Branch target buffer
Instruction fetch queue
Input latch of decoder
Rename address table
Integer register file
Floating point register file
Reorder buffer
Integer ALU
Integer multiplier
Integer divider
Floating point ALU
Floating point multiplier
Floating point divider
Load/store queue

1.7

1.0

0.4

Figure 5. Variation in the speed-up of the animator core for different hard fault locations across SPEC-CPU-2K benchmarks.
To isolate the hard fault locations’ impact, we normalized the results in each column to the average speed-up that the
Necromancer coupled cores could achieve for that particular benchmark.

with four clusters modeled after the Sun
Rock processor. Each cluster contains four
cores that share a single animator core,
shown in the call-out. To maintain the Nec-
romancer design’s scalability, we employ the
aforementioned 4-core cluster design as the
basic building block. Because many dies
are fault free, to avoid disabling the animator
cores, we can leverage these cores for acceler-
ating the operation of live cores by exploiting
speculative method-level parallelism. How-
ever, evaluation of the latter is beyond the
scope of this article.

For a heterogeneous CMP system, shar-
ing an animator core between multiple
cores might not be possible because cores
have different computational capabilities. A
potential solution is to partition the original
set of cores into groups such that each group
has several large cores and a small core. In
each group, the smaller core should be able
to operate as a conventional core or an ani-
mator core when there is a defect in one of
the larger cores within its group. These
dual-purpose cores are a suitable fit for
many heterogeneous CMP systems that are
designed with many simple cores such as
the IBM Cell processor.

Evaluation methodology

We heavily modified SimAlpha to model
our coupled core execution.10 We used
interprocess communication to model the

information flow between our 6-issue out-
of-order Alpha 21264 and 2-issue out-of-
order core with the same resources as Alpha
21064 while simulating SPEC-CPU-2K.
To study how manufacturing defects affect
the Necromancer system, we developed an
area-weighted, Monte Carlo fault-injection
engine. During each iteration of the Monte
Carlo simulation, we selected a microarchi-
tectural structure and injected a single,
random stuck-at fault into the timing simu-
lator. We inject these hard faults in 15 differ-
ent microarchitectural structures, as Figure 5
shows. We evaluated dynamic and static
using Wattch,11
power
HotLeakage,12 and Cacti.13 We used the
Synopsys
toolchain,
with a TSMC 90-nm technology library, to
evaluate the overheads of the remaining mis-
cellaneous logic (such as shift registers).

standard industrial

consumption

Results
As we described earlier, the Necromancer
design includes parameters such as cache
hint CAM sizes, release window sizes, and
hint-disabling thresholds. To fix these archi-
tectural parameters, we performed an exten-
sive design space exploration. Given these
parameter values, on average, Necromancer
achieves 39.5 percent speed-up over the
baseline animator core.

To highlight the impact of hard fault
achievable

locations on Necromancer’s

....................................................................

42

IEEE MICRO

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 43

Table 1. Summary of performance benefit, area overhead, and power overhead of our scheme for CMP

systems with different numbers of cores. Performance of Necromancer coupled cores is normalized

to a baseline animator core’s average performance and to a live core’s average performance.

Necromancer’s performance

normalized to a...

Target system

animator core

live core

overhead (%)

overhead (%)

baseline

Area

Power

Single-core (2-Mbyte L2)

2-core CMP (2-Mbyte shared L2)

4-core CMP (4-Mbyte shared L2)
8-core CMP (8-Mbyte shared L2)

16-core CMP (16-Mbyte shared L2)

1.39

1.59

1.79
1.94

2.02

0.68

0.78

0.87
0.95

0.99

13.6

10.1

5.3
5.3

5.3

15.6

11.8

8.5
5.1

2.7

speed-up, Figure 5 depicts the performance
breakdown for 15 fault locations in the
Alpha 21264 pipeline. Results in each col-
umn are normalized to the average speed-
up that Necromancer could achieve for that
particular benchmark. We did this to elimi-
nate the advantage and disadvantage that
comes from the inherent benchmark suitabil-
ity for core coupling. As the figure shows,
hard faults in some locations—namely, the
program counter, integer ALU, and instruc-
tion fetch queue—are more harmful than
others. Also, reaction to defects can differ
significantly between benchmarks (such as
parser). We conclude two main points from
this plot. First, on average, only a few fault
locations exist that can drastically impact
Necromancer speed-up gains. Second, for a
given fault location, different benchmarks
show various degrees of susceptibility; thus,
heterogeneity across a CMP system’s bench-
marks helps Necromancer achieve a higher
speed-up by allowing more suitable work-
loads to be assigned to the coupled cores.

Table 1 demonstrates the amount of
speed-up that the Necromancer coupled
cores can achieve. We achieve a higher over-
all speed-up as the number of cores increases
because Necromancer achieves different
speed-ups based on the defect type, location,
and the workload running on the system. For
a 16-core system, on average, the coupled
cores can achieve a live-core performance, es-
sentially providing the appearance of a fully-
functional 6-issue baseline core with a 2-issue
animator core. Here, we assume full use,
which means there is always one job per

core. So, for larger CMPs with more hetero-
geneity across the benchmarks running on
the system, there’s more opportunity for
Necromancer to exploit. This table also
shows our scheme’s area and power over-
heads. As the table shows, the area overhead
gradually shrinks as the number of cores
increases because the animator core’s cost is
amortized among more cores. Nevertheless,
because we simply replicate the 4-core build-
ing block to construct CMPs with more than
4 cores, the area overhead remains the same.
In terms of power overhead, as the speed-up
results show, for CMPs with fewer than 8
cores, the undead core remains ahead of
the animator core and must stall when the
queue is full. During stalls, the undead core
doesn’t consume dynamic power.

Necromancer’s main objective is to im-
prove the average system throughput of a
population of manufactured chips. For this
purpose, we model 1,000 manufactured
chips with randomly distributed defects
based on our target defect rate. In the case
of a defect in an original core, we apply
our scheme. On the other hand, if any of
the animator cores, communication queues,
or Necromancer-specific modules (such as
the hint-gathering unit) are faulty, we simply
disable the animator core and the rest of the
system can continue its normal operation.
Figure 6 depicts the throughput enhance-
ment results
(shaded regions) based on
throughput binning for a 4-core CMP. Nec-
romancer significantly enhances the overall
system throughput. The horizontal axis
shows the system throughput, normalized to

....................................................................

NOVEMBER/DECEMBER 2010

43

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 44

...............................................................................................................................................................................................
CHIP MULTIPROCESSORS

Enhancement when caches are protected

100

95

90

85

)

%

l

(
 
d
e
Y

i

Enhancement when caches are not protected

80

3.0

3.1

3.2

3.3

3.4

3.5

3.6

3.7

3.8

3.9

4.0

4-core CMP
4-core CMP + cache protection

4-core CMP + NM
4-core CMP + NM + cache protection

Figure 6. Achievable yield for a 4-core CMP, given an expected level of
system throughput. Here, we consider two baselines—a CMP system
with and without proper protection for on-chip caches—to show the yield
improvement when Necromancer is applied.

the throughput of a single baseline core. In
this plot, we illustrate the throughput binning
results for throughput values between 3 and
4. Because we assume, on average, one defect
per 5 chips, yield is always above 80 percent.
However, there’s a small chance that multiple
defects hit the same chip, which precludes a
yield of 100 percent at a throughput of 3,
even when protecting the on-chip caches.
Cache protection is a necessity and, fortu-
nately, can be provided easily (for example,
column redundancy).

A s technology scaling continues, high-

performance microprocessors become
more susceptible to defects. Necromancer
enhances system throughput by exploiting
faulty cores to perform useful computation.
By providing intrinsically robust hints,
effective hint disabling, and dynamic inter-
core state resynchronization, Necromancer
significantly improves the distribution of
manufactured chips when throughput bin-
ning is performed. Moreover, if a wearout
failure happens during system operation,
Necromancer provides more graceful degra-
dation of performance than simply dis-
abling the broken cores.

For future work, we plan to enhance Nec-
romancer to adapt to larger numbers of fail-
ures. This increases both the number of

faulty cores and the number of faults per
core. With multiple faults per core, the qual-
ity of the hints a faulty core generates is likely
to deteriorate. However, we can aggregate
faulty cores together to jointly provide a set
of robust hints. Faulty cores are partitioned
into groups, where the faulty modules’
union is nonoverlapping. As a result, each
group has a complete set of working modules
and is thus capable of providing correct hints
by intelligently dividing the work across the
MICRO
faulty cores.

....................................................................
References
1. S. Borkar, ‘‘Designing Reliable Systems from
Unreliable Components: The Challenges of

Transistor Variability and Degradation,’’

IEEE Micro, vol. 25, no. 6, 2005, pp. 10-16.

2. M.D. Powell et al., ‘‘Architectural Core Sal-

vaging in a Multicore Processor for Hard-

Error Tolerance,’’ Proc. 36th Ann.

Int’l

Symp. Computer Architecture (ISCA 09),

ACM Press, 2009, pp. 93-104.

3. N. Aggarwal et al., ‘‘Configurable Isolation:

Building High Availability Systems with Com-

modity Multi-core Processors,’’ Proc. 34th

Ann.

Int’l Symp. Computer Architecture,

(ISCA 07) ACM Press, 2007, pp. 470-481.

4. J. Srinivasan et al., ‘‘Exploiting Structural

Duplication for Lifetime Reliability Enhance-

ment,’’ Proc. 32nd Int’l Symp. Computer
Architecture (ISCA 05),
IEEE CS Press,

2005, pp. 520-531.

5. S. Gupta et al., ‘‘The StageNet Fabric for

Constructing Resilient Multicore Systems,’’

Proc. 41st Ann.

IEEE/ACM Int’l Symp.

Microarchitecture (Micro 08),

IEEE CS

Press, 2008, pp. 141-151.

6. T. Austin, ‘‘DIVA: A Reliable Substrate for
Deep Submicron Microarchitecture De-

sign,’’ Proc. 32nd Ann.

IEEE/ACM Int’l

Symp. Microarchitecture (Micro 99), IEEE

CS Press, 1999, pp. 196-207.

7. B. Greskamp and J. Torrellas, ‘‘Paceline:

Improving Single-thread Performance in

Nanoscale CMPs Through Core Overclock-

ing,’’ Proc. 16th Int’l Conf. Parallel Architec-
tures and Compilation Techniques, IEEE CS

Press, 2007, pp. 213-224.

8. Z. Purser, K. Sundaramoorthy, and E. Roten-

berg, ‘‘A Study of Slipstream Processors,’’

Proc. 33rd Ann. ACM/IEEE Int’l Symp.

....................................................................

44

IEEE MICRO

department at the University of Michigan,
Ann Arbor. His research interests include
compilers and architectures, with a focus on
fault tolerance, power efficiency, and single-
thread performance. Gupta has an MSE in
computer engineering from the University of
Michigan.

Scott Mahlke is an associate professor in the
electrical engineering and computer science
department at the University of Michigan,
Ann Arbor, where he leads the Compilers
Creating Custom Processors research group.
His research interests include compilers for
multicore processors, application-specific
processors for mobile computing, and reli-
able system design. Mahlke has a PhD in
electrical engineering from the University of
Illinois at Urbana-Champaign.

Direct questions and comments to Amin
Ansari, 4861 CSE Bldg., Univ. of Michigan,
2260 Hayward St., Ann Arbor, MI 48109;
ansary@umich.edu.

[3B2-14] mmi2010060036.3d

8/12/010

16:20 Page 45

Microarchitecture (Micro 00), ACM Press,

2000, pp. 269-280.

9. R.D. Barnes et al., ‘‘Beating In-order Stalls

with ‘Flea-Flicker’ Two-pass Pipelining,’’

Proc. 36th Ann. Int’l Symp. Microarchitec-

ture (Micro 03),

IEEE CS Press, 2003,

p. 387-398.

10. T. Austin, E. Larson, and D. Ernst, ‘‘Simples-
calar: An Infrastructure for Computer Sys-

tem Modeling,’’ Computer, vol. 35, no. 2,

2002, pp. 59-67.

11. D. Brooks, V. Tiwari, and M. Martonosi,

‘‘A Framework

for Architectural-level

Power Analysis and Optimizations,’’ Proc.

27th Ann. IEEE/ACM Int’l Symp. Computer

Architecture (ISCA 00), ACM Press, 2000,
pp. 83-94.

12. Y. Zhang et al., Hotleakage: A Temperature-

Aware Model of Subthreshold and Gate

Leakage for Architects, tech. report, Com-

puter Science Dept., Univ. of Virginia, 2003.

13. N. Muralimanohar, R. Balasubramonian, and

N.P. Jouppi, ‘‘Optimizing NUCA Organiza-

tions and Wiring Alternatives for Large
Caches with Cacti 6.0,’’ Proc. 40th Ann.

IEEE/ACM Int’l Symp. Microarchitecture

(Micro 07), IEEE CS Press, 2007, pp. 3-14.

Amin Ansari is a PhD candidate in the
electrical engineering and computer science
department at the University of Michigan,
Ann Arbor. His research interests include
designing architectural and microarchitectur-
al
techniques for enhancing reliability of
high-performance microprocessors in deep
submicron technologies. Ansari has an MSE
in computer science and engineering from
the University of Michigan. He’s a student
member of IEEE and the ACM.

Shuguang Feng is a PhD candidate in the
electrical engineering and computer science
department at the University of Michigan,
Ann Arbor. His research interests include
fault-tolerant, reconfigurable computer ar-
chitectures and techniques for exploiting the
interaction between software and hardware
to enhance system reliability. Feng has an
MSE in computer engineering from the
University of Michigan, Ann Arbor.

Shantanu Gupta is a PhD candidate in the
electrical engineering and computer science

....................................................................

NOVEMBER/DECEMBER 2010

45

