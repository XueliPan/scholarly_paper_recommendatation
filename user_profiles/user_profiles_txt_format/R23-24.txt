Flash Photography Enhancement via Intrinsic Relighting

Elmar Eisemann∗

MIT / ARTIS†-GRAVIR/IMAG-INRIA

Fr´edo Durand

MIT

(a)

(b)

(c)

Figure 1: (a) Top: Photograph taken in a dark environment, the image is noisy and/or blurry. Bottom: Flash photography provides a sharp but
ﬂat image with distracting shadows at the silhouette of objects. (b) Inset showing the noise of the available-light image. (c) Our technique
merges the two images to transfer the ambiance of the available lighting. Note the shadow of the candle on the table.

Abstract

We enhance photographs shot in dark environments by combining
a picture taken with the available light and one taken with the ﬂash.
We preserve the ambiance of the original lighting and insert the
sharpness from the ﬂash image. We use the bilateral ﬁlter to de-
compose the images into detail and large scale. We reconstruct the
image using the large scale of the available lighting and the detail
of the ﬂash. We detect and correct ﬂash shadows. This combines
the advantages of available illumination and ﬂash photography.

Keywords: Computational photography, ﬂash photography, re-
lighting, tone mapping, bilateral ﬁltering, image fusion

1 Introduction

Under dark illumination, a photographer is usually faced with a
frustrating dilemma:
to use the ﬂash or not. A picture relying

∗e-mail: eisemann@graphics.csail.mit.edu, fredo@mit.edu
†ARTIS is a research project in the GRAVIR/IMAG laboratory, a joint

unit of CNRS, INPG, INRIA and UJF.

Permission  to  make  digital  or  hard  copies  of  part  or  all  of  this  work  for  personal  or
classroom use is granted without fee provided that copies are not made or distributed for
profit or direct commercial advantage and that copies show this notice on the first page or
initial  screen  of  a  display  along  with  the  full  citation.  Copyrights  for  components  of  this
work owned by others than ACM must be honored. Abstracting with credit is permitted. To
copy  otherwise,  to  republish,  to  post  on  servers,  to  redistribute  to  lists,  or  to  use  any
component  of  this  work  in  other  works  requires  prior  specific  permission  and/or  a  fee.
Permissions may be requested from Publications Dept., ACM, Inc., 1515 Broadway, New
York, NY 10036 USA, fax +1 (212) 869-0481, or permissions@acm.org.
© 2004 ACM 0730-0301/04/0800-0673 $5.00

on the available light usually has a warm atmosphere, but suffers
from noise and blur (Fig. 1(a) top and (b)). On the other hand,
ﬂash photography causes three unacceptable artifacts: red eyes,
ﬂat and harsh lighting, and distracting sharp shadows at silhouettes
(Fig. 1(a) bottom). While much work has addressed red-eye re-
moval [Zhang and Lenders 2000; Gaubatz and Ulichney 2002], the
harsh lighting and shadows remain a major impediment.

We propose to combine the best of the two lightings by taking
two successive photographs: one with the available lighting only,
and one with the ﬂash. We then recombine the two pictures and
take advantage of the main qualities of each one (Fig. 1(c)). Our
central tool is a decomposition of an image into a large-scale layer
that is assumed to contain the variation due to illumination, and a
small-scale layer containing albedo variations.

Related work Most work on ﬂash photography has focused on
red-eye removal [Zhang and Lenders 2000; Gaubatz and Ulichney
2002]. Many cameras use a pre-ﬂash to prevent red eyes. Profes-
sional photographers rely on off-centered ﬂash and indirect lighting
to prevent harsh lighting and silhouette shadows.

Our work is related to the continuous ﬂash by Hoppe and Toyama
[2003]. They use a ﬂash and a no-ﬂash picture and combine them
linearly. The image-stack interface by Cohen et al.
[2003] pro-
vides additional control and the user can spatially vary the blending.
Raskar et al. [2004] and Akers et al. [2003] fuse images taken with
different illuminations to enhance context and legibility. DiCarlo et
al. [2001] use a ﬂash and a no-ﬂash photograph for white balance.
Multiple-exposure photography allows for high-dynamic-range
images [Mann and Picard 1995; Debevec and Malik 1997]. New
techniques also compensate for motion between frames [Kang et al.
2003; Ward 2004]. Note that multiple-exposure techniques are dif-
ferent from our ﬂash-photography approach. They operate on the
same lighting in all pictures and invert a non-linear and clamped

673

2 Image decoupling for ﬂash relighting

Our approach is summarized in Fig. 2. We take two photos, with
and without the ﬂash. We align the two images to compensate for
camera motion between the snapshots. We detect the shadows cast
by the ﬂash and correct color using local white balance. We ﬁnally
perform a non-linear decomposition of the two images into large-
scale and detail layers, and we recombine them appropriately.

We ﬁrst present our basic technique before discussing shadow
correction in Section 3. We then introduce more advanced recon-
struction options in Section 4 and present our results in Section 5.

Taking the photographs The two photographs with and with-
out the ﬂash should be taken as rapidly as possible to avoid motion
of either the photographer or subject. The response curve between
the two exposures should ideally be known for better relative radio-
metric calibration, but this is not a strict requirement. Similarly, we
obtain better results when the white balance can be set to manual.
In the future, we foresee that taking the two images in a row will be
implemented in the ﬁrmware of the camera. To perform our exper-
iments, we have used a tripod and a remote control (Fig. 1 and 8)
and hand-held shots (Fig. 2, 5, 7). The latter in particular requires
good image alignment. In the rest of this paper, we assume that the
images are normalized so that the ﬂash image is in [0, 1].

The registration of the two images is not trivial because the light-
ing conditions are dramatically different. Following Kang et al.
[2003], we compare the image gradients rather than the pixel val-
ues. We use a low-pass ﬁlter with a small variance (2 pixels) to
smooth-out the noise. We keep only the 5% highest gradients and
we reject gradients in regions that are too dark and where informa-
tion is not reliable. We use a pyramidal reﬁnement strategy similar
to Ward [2004] to ﬁnd the transformation that minimizes the gra-
dients that were kept. More advanced approaches could be used to
compensate for subject motion, e.g. [Kang et al. 2003].

Bilateral decoupling We ﬁrst decouple the images into inten-
sity and color (Fig. 2). Assume we use standard formulas, although
we show in the appendix that they can be improved in our context.
The color layer simply corresponds to the original pixel values di-
vided by the intensity. In the rest of the paper, we use I f and In f for
the intensity of the ﬂash and no-ﬂash images.

We then want to decompose each image into layers correspond-
ing to the illumination and the sharp detail respectively. We use
the bilateral ﬁlter [Tomasi and Manduchi 1998; Smith and Brady
1997] that smoothes an image but respects sharp features, thereby
avoiding halos around strong edges [Durand and Dorsey 2002].

The bilateral ﬁlter is deﬁned as a weighted average where the
weights depend on a Gaussian f on the spatial location, but also
on a weight g on the pixel difference. Given an input image I, The
output of the bilateral ﬁlter for a pixel s is:

Js = 1

k(s) ∑

p∈Ω

f (p − s) g(Ip − Is) Ip,

(1)

where k(s) is a normalization: k(s) = ∑p∈Ω f (p − s) g(Ip − Is). In
practice, g is a Gaussian that penalizes pixels across edges that have
large intensity differences. This ﬁlter was used by Oh et al. [2001]
for image editing and by Durand et al. for tone mapping [2002].

We use the fast bilateral ﬁlter where the non-linear ﬁlter is ap-
proximated by a set of convolutions [Durand and Dorsey 2002].
We perform computation in the log10 domain to respect intensity
ratios. The output of the ﬁlter provides the log of the large-scale
layer. The detail layer is deduced by a division of the intensity by
the large-scale layer (subtraction in the log domain). We use a spa-
tial variance σ f of 1.5% of the images diagonal. For the intensity
inﬂuence g, we use σg = 0.4, following Durand and Dorsey [2002].

Figure 2: We take two images with the available light and the ﬂash
respectively. We decouple their color, large-scale and detail inten-
sity. We correct ﬂash shadows. We re-combine the appropriate
layers to preserve the available lighting but gain the sharpness and
detail from the ﬂash image.

response. In contrast, we have a quite-different lighting in the two
images and try to extract the lighting ambiance from the no-ﬂash
picture and combine it with the ﬁne detail of the ﬂash picture.

We build on local tone-mapping techniques that decompose an
image into two or more layers that correspond to small- and large-
scale variations, e.g. [Chiu et al. 1993; Jobson et al. 1997; Tumblin
and Turk 1999; DiCarlo and Wandell 2000; Durand and Dorsey
2002; Reinhard et al. 2002; Ashikhmin 2002; Choudhury and Tum-
blin 2003]. Only the contrast of the large scales is reduced, thereby
preserving detail.

These methods can be interpreted in terms of intrinsic images
[Tumblin et al. 1999; Barrow and Tenenbaum 1978]. The large
scale can be seen as an estimate of illumination, while the detail
corresponds to albedo [Oh et al. 2001]. Although this type of de-
coupling is hard [Barrow and Tenenbaum 1978; Weiss 2001; Tap-
pen et al. 2003], tone mapping can get away with a coarse approx-
imation because the layers are eventually recombined. We exploit
the same approach to decompose our ﬂash and no-ﬂash images.

A wealth of efforts has been dedicated to relighting, e.g.
[Marschner and Greenberg 1997; Sato et al. 1999; Yu et al. 1999].
Most methods use acquired geometry or a large set of input images.
In contrast, we perform lighting transfer from only two images.

In this volume, Petschnigg et al. [2004] present a set of tech-
niques based on ﬂash/no-ﬂash image pairs. Their decoupling ap-
proach shares many similarities with our work, in particular the
use of the bilateral ﬁlter. The main difference between the two ap-
proaches lies in the treatment of ﬂash shadows.

674

Figure 3: Basic reconstruction and shadow correction. The ﬂash shadow on the right of the face and below the ear need correction. In the
na¨ıve correction, note the yellowish halo on the right of the character and the red cast below its ear. See Fig. 4 for a close up.

Figure 4: Enlargement of Fig. 3. Correction of smooth shadows. From left to right: no ﬂash, ﬂash, na¨ıve white balance, our color correction

Reconstruction Ignoring the issue of shadows for now, we can
recombine the image (Fig. 2). We use the detail and color layer of
the ﬂash image because it is sharper and because white balance is
more reliable. We use the large-scale layer of the no-ﬂash picture
in order to preserve the mood and tonal modeling of the original
lighting situation. The layers are simply added in the log domain.
Fig. 3 illustrates the results from our basic approach. The output
combines the sharpness of the ﬂash image with the tonal modeling
of the no-ﬂash image.

For dark scenes, the contrast of the large scale needs to be en-
hanced. This is the opposite of contrast reduction [Durand and
Dorsey 2002]. We set a target contrast for the large-scale layer
and scale the range of log values accordingly. The low quantiza-
tion from the original image does not create artifacts because the
bilateral ﬁlter results in a piecewise-smooth large-scale layer.

In addition, we compute the white balance between the two im-
ages by computing the weighted average of the three channels with
stronger weights for bright pixels with a white color in the ﬂash
image. We then take the ratios wr, wg, wb as white-balance coefﬁ-
cients. This white balance can be used to preserve the warm tones
of the available light. In practice, the color cast of the no-ﬂash im-
age is usually too strong and we only apply it partially using wt
where t is usually 0.2.

We must still improve the output in the ﬂash shadow. While
their intensity is increased to match the large scale of the no-ﬂash
image, there is a distinct color cast and noise. This is because, by
deﬁnition, these areas did not receive light from the ﬂash and inherit
from the artifacts of the no-ﬂash image. A ring ﬂash might reduce
these artifacts, but for most cameras, we must perform additional
processing to alleviate them.

3 Shadow treatment

In order to correct the aforementioned artifacts, we must detect the
pixels that lie in shadow. Pixels in the umbra and penumbra have
different characteristics and require different treatments. After de-
tection, we correct color and noise in the shadows. The correction
applied in shadow is robust to false positives; Potential detection
errors at shadow boundaries do not create visible artifacts.

Umbra detection We expect the difference image ∆I between
ﬂash and no-ﬂash to tell how much additional light was received
from the ﬂash. When the images are radiometrically calibrated,

∆I

histogram of ∆I

detected umbra with penumbra

Figure 5: Shadow detection

∆I is exactly the light received from the ﬂash. However, shadows
do not always correspond to ∆I = 0 because of indirect lighting.
While shadow pixels always correspond to the lowest values of ∆I,
the exact cutoff is scene-dependent.

We use histogram analysis to compute a threshold t∆I that deter-
mines umbra pixels. Shadows correspond to a well-marked mode
in the histogram of ∆I. While the additional light received by parts
of the scene lit by the ﬂash varies with albedo, distance and normal,
the parts in shadow are only indirectly illuminated and receive a
more uniform and very low amount of light.

We compute the histogram of pixels ∆I. We use 128 bins and
smooth it with a Gaussian blur of variance two bins. We start with a
coarse threshold of 0.2 and discard all pixels where ∆I is above this
value. We then use the ﬁrst local minimum of the histogram before
0.2 as our threshold for shadows detection (Fig. 5). This success-
fully detects pixels in the umbra. However, pixels in the penumbra
correspond to a smoother gradation and cannot be detected with our
histogram technique. This is why we use a complementary detec-
tion based on the gradient at shadow boundaries.

Penumbra detection Shadow boundaries create strong gradi-
ents in the ﬂash image that do not correspond to gradients in the
no-ﬂash image. We detect these pixels using two criteria: the gra-
dients difference, and connectedness to umbra pixels.

We compute the magnitude of the gradient ∇I f and ∇In f and
smooth it with a Gaussian of variance 2 pixels to remove noise.
We identify candidate penumbra pixels as pixels where the gradient
is stronger in the ﬂash image. We then keep only pixels that are
“close” to umbra pixels, that is, such that at least one of their neigh-
bors is in umbra. In practice, we use a square neighborhood of size
1% of the photo’s diagonal. This computation can be performed
efﬁciently by convolving the binary umbra map with a box ﬁlter.

We also must account for shadows cast by tiny objects such as

675

pieces of fur, since these might have a pure penumbra without um-
bra. We use a similar strategy and consider as shadow pixels that
have a large number of neighbors with higher gradient in the ﬂash
image. We use a threshold of 80% on a square neighborhood of size
0.7% of the photo’s diagonal.

We have observed that the parameters concerning the penumbra
are robust with respect to the scene. The image-space size of the
penumbra does not vary much in the case of ﬂash photography be-
cause the distance to the light is the same as the distance to the im-
age plane. The variation of penumbra size (ratio of blocker-receiver
distances) and perspective projection mostly cancel each other.

Flash detail computation Now that we have detected shad-
ows, we can reﬁne the decoupling of the ﬂash image. We exploit
the shadow mask to exclude shadow pixels from the bilateral ﬁlter-
ing. This results in a higher-quality detail layer for the ﬂash image
because it is not affected by shadow variation.

Color and noise correction Color in the shadow cannot sim-
ply be corrected using white balance [DiCarlo et al. 2001] for two
reasons. First, shadow areas receive different amounts of indirect
light from the ﬂash, which results in hybrid color cast affected by
the ambient lighting and color bleeding from objects. Second, the
no-ﬂash image often lacks information in the blue channel due to
the yellowish lighting and poor sensitivity of sensors in the small
wavelengths. Fig. 3 illustrates the artifacts caused by a global white
balance of the shadow pixels.

In order to address these issues, we use a local color correction
that copies colors from illuminated regions in the ﬂash image. For
example, in Fig. 3, a shadow falls on the wall, sofa frame and jacket.
For all these objects, we have pixels with the same intrinsic color in
the shadow and in the illuminated region.

Inspired by the bilateral ﬁlter, we compute the color of a shadow
pixel as a weighted average of its neighbors in the ﬂash image I f
(with full color information). The weight depends on three terms:
a spatial Gaussian, a Gaussian on the color similarity in In f , and
a binary term that excludes pixels in shadow (Fig. 6). We perform
computation only on the color layer (see Fig. 2) in Luv. We use
σ f of 2.5% of the photo’s diagonal for the spatial Gaussian and
σg = 0.01 for the color similarity. As described by Durand and
Dorsey [2002] we use the sum of the weights k as a measure of pixel
uncertainty. We discard color correction if k is below a threshold.
In practice, we use a smooth feathering between 0.02 and 0.002 to
avoid discontinuities.

Recall that the large-scale layer of intensity is obtained from the
no-ﬂash image and is not affected by shadows. In the shadow, we
do not use the detail layer of the ﬂash image because it could be
affected by high-frequencies due to shadow boundary. Instead, we
copy the detail layer of the no-ﬂash image, but we correct its noise
level. For this we scale the no-ﬂash detail to match the variance of
the ﬂash detail outside shadow regions.

In order to ensure continuity of the shadow correction, we use
feathering at the boundary of the detected shadow: We follow a
linear ramp and update pixels as a linear combination of the original
and shadow-corrected value. Fig. 3 and 4 show the results of our
shadow correction. It is robust to false shadow positives because it
simply copies colors from the image. If a pixel is wrongly classiﬁed
in shadow, its color and noise are preserved as long as there are
other pixels with similar color that were not classiﬁed in shadow.

Distance

No-flash photo

Color similarity in no-flash 

Final weight = distance * 
color similarity * shadow mask, 

Flash photo

outside shadow inside shadow

Shadow mask

outside shadow inside shadow

Reconstructed colors

Figure 6: For a pixel in the ﬂash shadow, the color layer is com-
puted as a weighted average of non-shadow colors. The weights
depend on three terms: distance, similarity in the no-ﬂash image
and a binary shadow mask.

When the no-ﬂash picture is too dark, the edge-preserving prop-
erty of the bilateral ﬁlter is not reliable, because noise level is in the
range of the signal level. Similar to the technique we use for color
correction, we can use the ﬂash image as a similarity measure be-
tween pixels. We propose a cross-bilateral ﬁlter1 where we modify
Eq. 1 for the no-ﬂash image and compute the edge-preserving term
g as a function of the ﬂash-image values:

s = 1
Jn f

k(s) ∑

p∈Ω

f (p − s) g(I f

p − I f

s ) In f
p ,

(2)

This preserves edges although they are not really present in the
no-ﬂash image. Shadow correction can however not be performed
because the shadow edges of the ﬂash picture are transferred by the
g term. Fig. 1 exploits cross-bilateral decomposition.

The large-scale layer of the ﬂash image can also be exploited to
drive the reconstruction. The distance falloff makes objects closer
to the camera brighter. We use this pseudo-distance to emphasize
the main object. We use a shadow-corrected version of ∆I as our
pseudo-distance. Pixels in shadow are assigned a pseudo-distance
using a bilateral-weighted average of their neighbors where simi-
larity is deﬁned in the no-ﬂash image. The principle is to multiply
the large scale of the no-ﬂash image by the pseudo-distance. This
can be performed using a user-provided parameter. Pseudo-distance
was used in Fig. 8.

5 Results and discussion

Our technique takes about 50 seconds on a 866 MHz Pentium 3
for a 1280x960 image. The majority of the time is spent in the
color correction, because this bilateral ﬁlter cannot be efﬁciently
piecewise-linearized [Durand and Dorsey 2002] since it operates
on the three channels. Images such as Fig. 8 that do not include
shadow correction take about 10 seconds.

Fig 1, 3, 7 and 8 illustrate our results. The ambiance of the avail-
able light is preserved and the color, sharpness and detail of the
ﬂash picture is gained. In our experience, the main cause of failure
of our technique is poor quality (not quantity) of available lighting.
For example, if the light is behind the subject, the relighting results
in an under-exposed subject. We found, however, that it is not hard
to outperform the poor lighting of the ﬂash. It is well known that
lighting along the optical axis does not result in good tonal model-
ing. In contrast, Fig. 2 and 8 present a nice 3/4 side lighting. We

1Petschnigg et al. [2004] propose a similar approach that they call joint

4 Advanced decoupling

The wealth of information provided by the pair of images can be
further exploited to enhance results for very dark situations and
more advanced lighting transfer.

bilateral ﬁlter.

676

no-ﬂash

ﬂash

result

Figure 7: The ﬂash lighting results in a ﬂat image. In our result, light seems to be coming from the window to the right.

received conﬂicting feedback on Fig. 7, which shows that image
quality is a subjective question. In this image, the light is coming
from the 3/4 back, which is an unusual lighting for a photograph.
Some viewers appreciate the strong sense of light it provides, while
others object to the lack of tonal modeling.

Another cause of failure is overexposure of the ﬂash, leading to a
ﬂat detail layer. In this situation, the detail information is neither in
the no-ﬂash (due to noise) nor in the ﬂash photo (due to saturation).
Shadow detection works best when the depth range is limited.
Distant objects do not receive light from the ﬂash and are detected
in shadow. While this is technically correct, this kind of shadow due
to falloff does not necessitate the same treatment as cast shadow.
Fortunately, our color correction is robust to false positives and de-
grades to identity in these cases (although transition areas could po-
tentially create problems). Similarly, black objects can be detected
as shadows, but this does not affect quality since they are black in
the two images and remain black in the output. Light ﬂares can
cause artifacts by brightening shadow pixels. The method by Ward
[2004] could alleviate this problem.

We have used our algorithms with images from a variety of
cameras including a Sony Mavica MVC-CD400 (Fig. 1), a Nikon
Coolpix 4500 (all other images), a Nikon D1 and a Kodak DC4800
(not shown in the paper). The choice of the camera was usually
dictated by availability at the time of the shot. The speciﬁcations
that affected our approach are the noise level, the ﬂexibility of con-
trol, the accuracy of ﬂash white balance, and compression quality.
For example, the Kodak DC4800 exhibited strong JPEG artifacts
for dark images, which required the use of the cross-bilateral ﬁlter.
The need for the cross-bilateral ﬁlter was primarily driven by the
SNR in the no-ﬂash picture. The Kodak DC4800 has higher noise
levels because it is old. Despite its age, the size of its photosites
allows the Nikon D1 to take images in dark conditions. In addition,
the use of the RAW format with 12 bits/channel allows for higher
precision in the ﬂash image (the lower bits of the no-ﬂash image are
dominated by noise). However, with the sensitivity at 1600 equiva-
lent ISO, structured noise makes cross-bilateral ﬁltering necessary.

6 Conclusions and future work

We have presented a method that improves the lighting and am-
biance of ﬂash photography by combining a picture taken with
the ﬂash and one using the available lighting. Using a feature-
preserving ﬁlter, we estimate what can be seen as intrinsic layers
of the image and use them to transfer the available illumination to
the ﬂash picture. We detect shadows cast by the ﬂash and correct
their color balance and noise level. Even when the no-ﬂash picture
is extremely noisy, our method successfully transfers lighting due
to the use of the ﬂash image to perform edge-preserving ﬁltering.

The method could be tailored to particular cameras by ﬁne-
tuning parameters such as σg based on a sensor-noise model. Tra-

Figure 8: The tonal modeling on the cloth and face are accurately
transferred from the available lighting. The main subject is more
visible in the result than he was in the original image.

ditional red-eye removal could beneﬁt from the additional infor-
mation provided by the pair of images. Texture synthesis and in-
painting could be used to further improve shadow correction. Ide-
ally, we want to alleviate the disturbance of the ﬂash and we are
considering the use of infrared illumination. This is however chal-
lenging because it requires different sensors and these wavelengths
provide limited resolution and color information.

The difference of the ﬂash and no-ﬂash images contains much
information about the 3D scene. Although a fundamental ambi-
guity remains between albedo, distance and normal direction, this
additional information could greatly expand the range and power of
picture enhancement such as tone mapping, super-resolution, photo
editing, and image based-modeling.

Acknowledgments We acknowledge support from an NSF
CISE Research Infrastructure Award (EIA-9802220) and a Desh-

677

pande Center grant. Elmar Eisemann’s stay at MIT was supported
by MIT-France and ENS Paris. Many thanks to the reviewers, Jo¨elle
Thollot, Marc Lapierre, Ray Jones, Eric Chan, Martin Eisemann,
Almuth Biard, Shelly Levy-Tzedek, Andrea Pater and Adel Hanna.

Appendix:
Intensity-Color decoupling Traditional ap-
proaches rely on linear weighted combinations of R, G, and B for
intensity estimation. While these formulae are valid from a color-
theory point of view, they can be improved for illumination-albedo
decoupling. Under the same illumination, a linear intensity compu-
tation results in lower values for primary-color albedo (in particular
blue) than for white objects. As a result, the intensity transfer might
overcompensate as shown in Fig. 9(left) where the red fur becomes
too bright. To alleviate this, we use the channels themselves as
weights in the linear combination:
I = R

R+G+B R + G
In practice, we use the channels of the ﬂash image as weight for
both pictures to ensure consistency between the two decoupling op-
erations. The formula can also be used with tone mapping operators
for higher color ﬁdelity.

R+G+B G + B

R+G+B B.

DURAND, AND DORSEY. 2002. Fast bilateral ﬁltering for the
display of high-dynamic-range images. ACM Trans. on Graphics
21, 3.

GAUBATZ, AND ULICHNEY. 2002. Automatic red-eye detection

and correction. In IEEE Int. Conf. on Image Processing.

HOPPE, AND TOYAMA. 2003. Continuous ﬂash. Tech. Rep. 63,

MSR.

JOBSON, RAHMAN, AND WOODELL. 1997. A multi-scale retinex
for bridging the gap between color images and the human obser-
vation of scenes. IEEE Trans. on Image Processing 6, 965–976.

KANG, UYTTENDAELE, WINDER, AND SZELISKI. 2003. High

dynamic range video. ACM Trans. on Graphics 22, 3.

MANN, AND PICARD. 1995. Being ’undigital’ with digital cam-
eras: Extending dynamic range by combining differently ex-
posed pictures. In Proc. IS&T 46th ann. conference.

MARSCHNER, AND GREENBERG. 1997. Inverse lighting for pho-

tography. In Proc. IS&T/SID 5th Color Imaging Conference.

OH, CHEN, DORSEY, AND DURAND. 2001. Image-based model-

ing and photo editing. In Proc. SIGGRAPH.

PETSCHNIGG, AGRAWALA, HOPPE, SZELISKI, COHEN, AND
TOYAMA. 2004. Digital photography with ﬂash and no-ﬂash
image pairs. ACM Trans. on Graphics in this volume.

RASKAR, ILIE, AND YU. 2004. Image fusion for context enhance-

ment. In Proc. NPAR.

Figure 9: The computation of intensity from RGB can greatly affect
the ﬁnal image. Left: with linear weights, the red pixels of the fur
become too bright. Right: using our non-linear formula.

REINHARD, STARK, SHIRLEY, AND FERWERDA. 2002. Photo-
graphic tone reproduction for digital images. ACM Trans. on
Graphics 21, 3.

References

AKERS, LOSASSO, KLINGNER, AGRAWALA, RICK, AND HAN-
RAHAN. 2003. Conveying shape and features with image-based
relighting. In Visualization.

ASHIKHMIN. 2002. A tone mapping algorithm for high contrast

images. In Eurographics Workshop on Rendering, 145–156.

BARROW, AND TENENBAUM. 1978. Recovering intrinsic scene
characteristics from images. In Computer Vision Systems. Aca-
demic Press.

CHIU, HERF, SHIRLEY, SWAMY, WANG, AND ZIMMERMAN.
1993. Spatially nonuniform scaling functions for high contrast
images. In Graphics Interface.

SATO, SATO, AND IKEUCHI. 1999. Illumination distribution from
brightness in shadows: Adaptive estimation of illumination dis-
tribution with unknown reﬂectance properties in shadow regions.
In ICCV.

SMITH, S. M., AND BRADY, J. M. 1997. SUSAN - a new ap-

proach to low level image processing. IJCV 23, 45–78.

TAPPEN, M. F., FREEMAN, W. T., AND ADELSON, E. H. 2003.

Recovering intrinsic images from a single image. In NIPS.

TOMASI, C., AND MANDUCHI, R. 1998. Bilateral ﬁltering for

gray and color images. In ICCV, 836–846.

TUMBLIN, AND TURK. 1999. Lcis: A boundary hierarchy for

detail-preserving contrast reduction. In Proc. SIGGRAPH.

TUMBLIN, HODGINS, AND GUENTER. 1999. Two methods for
display of high contrast images. ACM Trans. on Graphics 18, 1.

CHOUDHURY, AND TUMBLIN. 2003. The trilateral ﬁlter for high
In Eurographics Symposium on

contrast images and meshes.
Rendering.

WARD. 2004. Fast, robust image registration for compositing
high dynamic range photographs from handheld exposures. J.
of Graphics Tools 8, 2.

COHEN, COLBURN, AND DRUCKER. 2003. Image stacks. Tech.

WEISS. 2001. Deriving intrinsic images from image sequences. In

Rep. 40, MSR.

ICCV.

DEBEVEC, AND MALIK. 1997. Recovering high dynamic range

radiance maps from photographs. In Proc. SIGGRAPH.

DICARLO, J., AND WANDELL, B. 2000. Rendering high dynamic

range images. Proc. SPIE: Image Sensors 3965, 392–401.

DICARLO, J. M., XIAO, F., AND WANDELL, B. A. 2001. Illumi-

nating illumination. In 9th Color Imaging Conference, 27–34.

YU, DEBEVEC, MALIK, AND HAWKINS. 1999.

Inverse global
illumination: Recovering reﬂectance models of real scenes from
photographs from. In Proc. SIGGRAPH.

ZHANG, AND LENDERS. 2000. Knowledge-based eye detection
for human face recognition,. In Conf. on Knowledge-based In-
telligent Systems and Allied Technologies,.

678

