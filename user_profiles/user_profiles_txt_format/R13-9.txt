Learning Partially Observable Action Models: Efﬁcient Algorithms

Dafna Shahaf Allen Chang Eyal Amir

Computer Science Department

University of Illinois, Urbana-Champaign

Urbana, IL 61801, USA

dshahaf2,achang6,eyal
}

{

@uiuc.edu

Abstract

We present tractable, exact algorithms for learning actions’
effects and preconditions in partially observable domains.
Our algorithms maintain a propositional logical representa-
tion of the set of possible action models after each obser-
vation and action execution. The algorithms perform exact
learning of preconditions and effects in any deterministic ac-
tion domain. This includes STRIPS actions and actions with
conditional effects. In contrast, previous algorithms rely on
approximations to achieve tractability, and do not supply ap-
proximation guarantees. Our algorithms take time and space
that are polynomial in the number of domain features, and
can maintain a representation that stays compact indeﬁnitely.
Our experimental results show that we can learn efﬁciently
and practically in domains that contain over 1000’s of fea-
tures (more than 21000 states).

1 Introduction

Complex, real-world environments afford only partial obser-
vations and limited apriori information about their dynam-
ics. Acting in such domains is difﬁcult for that reason, and
it is very important to try to acquire as much information
about the environment dynamics as possible. Most current
planning approaches assume that the action model is spec-
iﬁed fully in a formalism such as STRIPS (Fikes & Nils-
son 1971) or PDDL (Ghallab et al. 1998). This holds even
for algorithms for planning in partially observable domains
(Cimatti & Roveri 2000; Bertoli et al. 2001).

We present efﬁcient algorithms for learning general deter-
ministic action models in partially observable domains. Un-
like previous approaches, our algorithms do not approximate
the learned model. Our algorithms take as input a proposi-
tional logical formula that describes partial knowledge about
the initial state of the world and the action model. The al-
gorithms also take as input a sequence of actions that were
taken and observations that were received by an agent act-
ing in the world. The algorithms work by computing the set
of all hypothetical action models that are consistent with the
actions taken and observations received.

There have been previous approaches that attempt to learn
deterministic action models automatically, but only recently
have approaches which learn action models in the presence

Copyright c
(cid:13)
gence (www.aaai.org). All rights reserved.

2006, American Association for Artiﬁcial Intelli-

of partial observability appeared. Of these previous ap-
proaches, our work is closest to (Amir 2005). In this pre-
vious work an algorithm tracks a logical representation that
encodes the set of possible action models. The approach
assumes that actions never fail (in which case, no precondi-
tions are learned), or that actions map states 1:1 (in which
case, the size of the logical representation may grow ex-
ponentially with the number of steps). In addition, the ap-
proach cannot learn preconditions of actions efﬁciently.

We relax these assumptions using several technical ad-
vances, and provide algorithms that are exact and tractable
(linear in time steps and domain size) for all determinis-
tic domains. Our algorithms maintain an alternative logical
representation of the partially known action model. They
present two signiﬁcant technical insights and advances over
(Amir 2005):

Our most general algorithm (Section 3) represents logi-
cal formulas as directed acyclic graphs (DAGs, versus ”ﬂat”
formulas) in which leaves are propositional symbols and in-
ternal nodes are logical connectives. This way, subformu-
las may be re-used or shared among parent nodes multiple
times. This data structure prevents exponential growth of the
belief state, guarantees that the representation always uses a
ﬁxed number of propositional symbol nodes, and guarantees
that the number of connective nodes grows only polynomi-
ally with the domain size and number of time steps (compare
this representation with Binary Decision Diagrams (BDDs),
which can explode exponentially in size).

We also present efﬁcient algorithms that represent formu-
las in conjunctive normal form (CNF) (Section 4). These
algorithms consider tractable subcases for which we can
polynomially bound the size of the resulting CNF formu-
las. They are applicable to non-failing STRIPS actions and
possibly failing STRIPS actions for which action precondi-
tions are known. That is, they trade off some expressivity
handled by the ﬁrst algorithm in order to maintain the belief
state compactly in a CNF representation. Such representa-
tions are advantageous because they can be readily used in
conjunction with modern inference tools, which generally
expect input formulas to be in CNF.

Related Work A number of previous approaches to learn-
ing action models automatically have been studied in addi-
tion to aforementioned work (Amir 2005). Approaches such

920as (Wang 1995; Gil 1994; Pasula, Zettlemoyer, & Kaelbling
2004) are successful for fully observable domains, but do
not handle partial observability. In partially observable do-
mains, the state of the world is not fully known, so assigning
effects and preconditions to actions becomes more compli-
cated.

One previous approach in addition to (Amir 2005) that
handles partial observability is (Qiang Yang & Jiang 2005).
In this approach, example plan traces are encoded as a
weighted maximum satisﬁability problem, from which a
candidate STRIPS action model is extracted. A data-mining
style algorithm is used in order to examine only a subset of
the data given to the learner, so the approach is approximate
by nature.

Hidden Markov Models can be used to estimate a stochas-
tic transition model from observations. However, the more
complex nature of the problem prevents scaling up to large
domains. These approaches represent the state transition
matrix explicitly, and can only handle relatively small state
spaces. Likewise, structure learning approaches for Dy-
namic Bayesian Networks are limited to small domains (e.g.,
10 features (Ghahramani & Jordan 1997; Boyen, Friedman,
& Koller 1999)) or apply multiple levels of approximation.
Importantly, DBN based approaches have unbounded error
in deterministic settings. In contrast, we take advantage of
the determinism in our domain, and can handle signiﬁcantly
larger domains containing over 1000 features (i.e., approxi-
mately 21000 states).

2 A Transition Learning Problem

In this section we describe the combined problem of learning
the transition model and tracking the world. First, we give
an example of the problem; we will return to this example
after introducing some formal machinery.

EXAMPLE

Consider a Briefcase domain, consisting of
several rooms and objects. The agent can lock and unlock
his briefcase, put objects inside it, get them out and carry the
briefcase from room to room. The objects in the briefcase
move with it, but the agent does not know it.

The agent performs a sequence of actions, namely putting
book1 in the briefcase and taking it to room3. His goal is to
determine the effects of these actions (to the extent he can,
theoretically), while also tracking the world.

We now deﬁne the problem formally (borrowed from

(Amir 2005)).

P,S,A,R
h
i

Deﬁnition 2.1 A transition system is a tuple

•
•

•
•

⊆

Pow(P) is the set of world states; a state s

P is a ﬁnite set of ﬂuents.
S
subset of P containing exactly the ﬂuents true in s.
A is a ﬁnite set of actions.
R

A

∈

S

S is the (deterministic) transition relation.

×
R means that state s0 is the result of perform-

S is the

⊆

×

s, a, s0
h

i ∈

ing action a in state s.

Our Briefcase world

the
locked, is-at(room), at(object,room), in-BC(object),
and
changeRoom(from,to), press-BC-Lock.

putIn(object),

has ﬂuents

actions

such

of

as

getOut(object),

form

This domain includes actions with conditional effects:
Pressing the lock causes the briefcase to lock if it was un-
locked, and vice versa. Also, if the agent is in room1, the re-
sult of performing changeRoom(room1,room3) is always is-
at(room3). Other outcomes are conditional– if in-BC(apple)
holds, then the apple will move to room3 as well. Otherwise,
its location will not change. This is not a STRIPS domain.

Our agent cannot observe the state of the world com-
pletely, and he does not know how his actions change it;
in order to solve it, he can maintain a set of possible world
states and transition relations that might govern the world.

Deﬁnition 2.2 A transition belief state Let
all possible transition relations on S, A. Every ρ
is a transition belief state.

R

be the set of

S

⊆

× R

i

Informally, a transition belief state ρ is the set of pairs
that the agent considers possible. The agent updates
s, R
h
his belief state as he performs actions and receives observa-
tions. We now deﬁne semantics for Simultaneous Learning
and Filtering (tracking the world state).

ρ

S

⊆

R,

i ∈

× R

a transition belief state, ai are actions. We

Deﬁnition 2.3 (Simultaneous Learning and Filtering)
ρ
assume that observations oi are logical sentences over P .
1. SLAF[(cid:15)](ρ) = ρ ((cid:15): an empty sequence)
2. SLAF[a](ρ) =
s0, R
{h
3. SLAF[o](ρ) =
s, R
{h
4. SLAF[
aj , ojii
j
h
≤
SLAF[
aj, ojii<j
h

s, a, s0
i | h
ρ
i ∈
t](ρ) =
≤
t] (SLAF[oi](SLAF[ai](ρ)))

s, R
h
o is true in s
}

We call step 2 progression with a and step 3 ﬁltering with o.
In short, we maintain a set of pairs that we consider possible;
the intuition behind this deﬁnition is that every pair
s0, R
i
h
becomes a new pair
as the result of an action. If an
observation discards a state ˜s, then all pairs involving ˜s are
removed from the set. We conclude that R is not possible
when all pairs including it have been removed.

˜s, R
h

i ∈

}

≤

i

|

s, R
h

EXAMPLE (CONT.) Our agent put book1 in the briefcase
and took it to room3. Assume that one of the pairs in his be-
lief state is
: s is the actual state before the actions, and
i
R is a transition relation which assumes that both actions do
not affect at(book1,room3). That is, after progressing with
both actions, at(book1,room3) should not change, according
at(book1,room3),
to R. When receiving the observation
the agent will eliminate this pair from his belief state.

¬

3 Update of Possible Transition Models

In this section, we present an algorithm for updating the be-
lief state. The na¨ıve approach (enumerating) is clearly in-
tractable; we apply logic to keep the representation compact
and the learning tractable. Later on, we show how to solve
SLAF as a logical inference problem.

We deﬁne a vocabulary of action propositions which can
be used to represent transition relations as propositional for-
mulas. Let L0 =
A, F a literal, G a
conjunction of literals (a term). We call aF
G a transition rule,
G its precondition and F its effect.

, where a

aF
G}

aF
G means ”if G holds, executing a causes F to hold”.
Any deterministic transition relation can be described by a
ﬁnite set of such propositions (if G is a formula, we can take

∈

{

921expl(t+1)

expl(t)

.....

¬

expl(t)

¬

.....

tr1

expl(0)

tr2

init

locked

PressB  
causes
locked 

if ¬locked

PressB  
causes
¬locked 
if locked

tr1

tr2

init

locked

PressB  
causes
locked 

if ¬locked

PressB  

PressB  
causes
¬locked 
if locked

 

tr1

tr2

PressB  causes
locked if ¬locked

init

locked

PressB  causes
¬locked if locked

Figure 2: The DAG at time 0

Figure 3: The DAG at time t

Figure 4: Building the DAG at time t+1, using time t

ai, oi

PROCEDURE DAG-SLAF(
input: an action-observation sequence and a formula over P
1: for f
2:
3: kb := ϕ , where each f

explf := a new proposition initf

P is replaced with initf

P do

t, ϕ)

0<i<

∈

≤

i

h

Lines 1-3: Preprocess ϕ

∈

{

{

4: for i=1...t do DAG-SLAF-STEP(ai, oi)

Process the Sequence

5: return Vf

P (f

explf )

∈

↔

}
kb
∧

∧

base

}

P do

PROCEDURE DAG-SLAF-STEP(a, o)
input: o an observation (a logical formula over P )
1: for f
2:
3:
4:
5: ReplaceFluents(kb)

kb := kb
{
expl0f := SuccState(a,f)
ReplaceFluents(expl0f )

}
see eq. (2)

see eq. (1)

Poss(a,f)

∧

∈

}

{

6: for f

Lines 1-5: maintain kb, and keep the
ﬂuents’ new status in expl0

}

{

P do explf := expl0f

∈
Lines 1-6: Progress with action a
{

}

7: kb := kb
8: ReplaceFluents(kb)

∧

o

{

PROCEDURE ReplaceFluents(ψ)
input: ψ a formula
1: for f

P do replace f with explf in ψ

∈

Figure 1: DAG-SLAF

Lines 7-8: Filter with observation o

}

its DNF form and split to rules with term preconditions. F
must be a term, so again we can split the rule into several
rules with a literal as their effect).
Deﬁnition 3.1 (Transition Rules Semantics) Given s
∈
A and R, a transition relation represented as a set of
S, a
transition rules, we deﬁne s0, the result of performing action
a in state s: if aF
= F . The rest of the
|
literals do not change. If there such s0, we say that action a
is possible in s.

= G, s0
|

R, and s

G ∈

∈

We can now use logical formulas to encode belief states,

using L = L0
s
belief states will be used interchangeably.

= ϕ
|
}

P . A belief state ϕ is equivalent to

i |
. Logical formulas and set-theoretic notions of

s, R

{h

R

∪

∧

Algorithm Overview (see Figure 1): We are given ϕ, a
formula over L. First, we convert it to a formula of the form
kb, where kb and explf do not include

explf )

P (f

f

∈

↔

∧

V

any ﬂuents. We do it by adding new propositions to the lan-
guage (see below). ϕ will remain in this form throughout
the algorithm, and we will update explf and kb.

We iterate through the action-observation sequence (pro-
cedure DAG-SLAF-STEP), while maintaining several for-
mulas in a directed acyclic graph (DAG) structure. (1) For
any ﬂuent f , we maintain the logical formula, explf . This
formula represents the explanations of why f is true. Every
time step, we update explf , such that it is true if and only
if f currently holds. The formula is updated according to
successor-state axioms (line 3, and see details below). (2)
We also maintain the formula kb, which stores additional
knowledge that we gain. When we perform an action, kb as-
serts that the action was possible (line 2); when we receive
an observation, we add it to kb (line 7).

We make sure that both formulas do not involve any ﬂu-
ent, but only propositions from L0
IP . Updating the formu-
las inserts ﬂuents into them, so we need to call ReplaceFlu-
ents. This procedure replaces the ﬂuents with their (equiv-
alent) explanations. We use a DAG implementation, so we
add an edge to the relevant node– no need to copy the expla-
nation formula; This allows us recursive sharing of formu-
las, and helps us maintain compactness.

∪

base := [

After processing the sequence, we return the updated ϕ.
In order to make sure that the models of ϕ are indeed valid
models, we conjoin it with another formula, base:
(aF

aF
G )]
Note that we assume the actions in the sequence were pos-
sible; we can also handle the general case, if the agent can
observe which actions were possible.

a,F,G ¬

F
a¬
G )]

G0 →

G ∧

(aF

a,F,G

V

V

G0

∧

→

[

EXAMPLE– BUILDING THE DAG: In Figures 2-4 we see
how the DAG is constructed. We focus on the way expllocked
is updated throughout the sequence. A node marked expl(i)
is the root of the formula that represents expllocked at time i.
At time 0 the explanation is just a leaf, initlocked. There are
L0. At time t, expllocked is
leafs for every proposition in IP ∪
a DAG with the same leafs: we show how to progress with
the action press-BC-Lock to the DAG of time t + 1 using the
graph of time t. expllocked of time t + 1 is the top node in
Figure 4. It is true iff one of its children is true. The left one
(
node) represents the case that a transition rule that causes
∨
locked was activated (its precondition held in time t). We
show one transition rule, with precondition
The right child (

node) corresponds to the case that locked

locked.

¬

∧

922(cid:155)
(cid:155)
(cid:154)
(cid:154)
(cid:154)
locked was activated.
held in time t, and no rule that causes
Note that we use the previous explanation of locked when-
ever we refer to its status at time t (in this ﬁgure, we reuse it
three times).

¬

Correctness of the Algorithm:

Theorem 3.2 DAG-SLAF is correct. For any formula ϕ and
a sequence of actions and observations

t,

ai, oii0<i<
h
≤
t, ϕ)
}
).

≤

=

{h

s, R

that satisfy DAG-SLAF(
ai, oii0<i<
h
that satisfy ϕ
s, R
i
}

SLAF[
ai, oii0<i<
h

t](

{h

≤

i

INTUITION: we deﬁne an effect model for action a at
time t, Teff(a, t), which is a logical formula consisting of
Situation-Calculus-like axioms (Reiter 2001). It describes
the ways in which performing the action at time t affects the
world. We then show that SLAF[a](ϕ) is equivalent to con-
sequence ﬁnding (in a restricted language) of ϕ
Teff(a, t).
Finally, we show that DAG-SLAF calculates those conse-
quences.

∧

A, deﬁne the effect model of a at time t to be:

Deﬁnition 3.3 Effect Model:
For a
Teff(a, t) = at →

∈

P Poss(a, t, f )

f

∈

Poss(a, t, f ) =
G (af

V
[(

¬

[ft+1 ↔

∧

SuccState(a, t, f )]

(1)

(2)

G ∧

Gt))

(

∧

G0

(a¬

f
G0 ∧

G0t)) ]

SuccState(a, t, f ) =

W
G (af

[(

G ∧

Gt) )

∨

W
(ft ∧

(

(a¬

f
G0 ∧

G0 ¬

G0t)))]

at asserts that action a occurred at time t, and ft+1 means

V

W

that f held after performing that action.

The effect model corresponds to effect axioms and expla-
nation closure axioms from Situation Calculus: a ﬂuent f
holds at time t iff either (a) there exists a transition rule af
G
which is true, and was activated (G held in time t). (b) f
f
held at time t, and there is no rule a¬
that is true and was
G0
activated. In other words, f was true and no transition rule
changed it. Also, it asserts that the action was possible: there
cannot be two activated rules with contradicting effects.

o

∧

•
•

at ∧

Deﬁnition 3.4 We reuse the notation of SLAF:
Teff(a, t))

SLAF[a](ϕ) = CnLt+1(ϕt ∧
SLAF[o](ϕ) = ϕ
CnLt+1(ψ) denotes the consequences of ψ in the vo-
cabulary that includes only ﬂuents of time t + 1 and L0;
Lt+1 = L0
Lemma 3.5 If ϕ is a belief state formula, and a
SLAF[a](

Pt+1, where Pt+1 =

f
ft+1|

∈
) =

s, R

A

∪

∈

P

}

{

.

s, R
{h
S

i ∈
× R | h

S
s, R

i

satisﬁes ϕ
}
satisﬁes SLAF[a](ϕ)
}

i

× R | h

Therefore, applying SLAF to a transition belief formula
is equivalent to applying SLAF to a transition belief state
(Proof idea: we show that the two sets have the same el-
ements, using Craig’s Interpolation Theorem for Proposi-
tional Logic). We now show that DAG-SLAF computes ex-
actly the consequences in the restricted language. We denote
by ϕ[ψ/f ] the formula ϕ, where every occurrence of ﬂuent
f is replaced with the formula ψ.

s, R

i ∈

{h

We need to compute the consequences of ϕt ∧
↔

at ∧
Teff(a, t). W.l.g., ϕ is of the form kb
explf ,
where kb and explf do not involve ﬂuents. To do this, we
add new propositions to our language, initf for every ﬂuent
f . Every ϕ is equivalent to

P f

V

∧

∈

f

ϕ[initf /f

f

|

∈

P ]

∧

P (f

f

∈

↔

initf ).

V

We replace any ﬂuent g in Teff(a, t) with its explanation.
ϕt ∧
kb
kb
P ].
Consequence ﬁnding for Lt+1 is the same as performing

Teff(a, t)
≡
explf )
P (ft ↔
explf )
P (ft ↔

Teff(a, t)[explg/gt |

at ∧
∧
at ∧

at ∧
∧
∧

Teff(a, t)

≡

∧

∈

g

∈

∈

f

f
V
V

resolution on Pt and at.
CnLt+1(ϕt ∧

Teff(a, t)) =

at ∧
P Poss(a, t, f )[explg/gt |
P (ft+1 ↔

f

∈

∈

kb

∧

P ])

f
V
V

handling observations is similar.

g

P ]

∈

∧

SuccState(a, t, f )[explg/gt

g

|

∈

This is exactly what DAG-SLAF computes. The proof for

COMPLEXITY

In order to keep the representation
compact and the algorithm tractable, we maintain a DAG
instead of a formula. That is, when ReplaceFluents replaces
f with explf , we only need to update a pointer, rather than
copying the expression again.
The space and time complexities of the algorithm are
)k+1), where ϕ0 is the initial belief
Obs
+
O(
ϕ0|
|
|
|
|
Obs
state,
is the total length of the observations throughout
|
|
the sequence (can be omitted if observations are always con-
junctions of literals), t is the length of the sequence, and k is
a parameter of the domain- the minimum number such that
preconditions are k-DNF.

+ tk(2

P
|

+

+ t

P
|

).
|

If there are no preconditions (always executable actions),
we can maintain a ”ﬂat” formula (instead of a DAG) with
complexity O(
ϕ0|
|

Obs
|
|

Inference on DAGs The DAG-SLAF algorithm returns a
formula represented as a DAG. We would like to perform
inference on this formula. We can always ”ﬂatten” the DAG
and use a SAT solver, but then we will lose our compact rep-
resentation. Instead, we used an algorithm which is a gener-
alization of the PDDL algorithm. It is a recursive algorithm:
in each iteration, it chooses an uninstantiated variable p. It
updates the graph for the case of p = TRUE (propagating
this assignment recursively throughout the DAG). We get a
smaller graph, and call the function recursively on it. If it
ﬁnds a satisfying assignment, we return TRUE. Otherwise,
we try p = FALSE. If both did not succeed, we backtrack.

For a DAG of size m, with n leafs, the algorithm takes

space O(m), and time exponential in n and linear in m.

4 CNF-based Algorithms

The algorithm presented in the previous section encodes
logical formulas using arbitrary directed-acyclic graphs. In
some cases, however, it can be convenient to directly main-
tain transition belief states in CNF. Many powerful algo-
rithms which we would like to be able to leverage for per-
forming logical inference assume that the input formula is in
CNF. Included in particular are modern satisﬁability check-

923ing algorithms such as zChaff, which can handle some for-
mulas containing as many as one million clauses and ten
million variables (Moskewicz et al. 2001).

which a[f ] and a[
case that exactly one of af , a¬

f ] both hold, or models where it is not the
◦ hold are disallowed.
The algorithm CNF-SLAF maintains transition belief for-

f , or af

¬

h

¬

do

, ϕ)

∈ P

a, o
i

PROCEDURE CNF-SLAF(
input: successful action a, observation term o, ﬂuent-factored
transition belief formula ϕ (see Deﬁnition 4.1)
1: for f
2:
3:
4:
5:
6:
7:
8:
9: Eliminate subsumed clauses in ϕ
10: return ϕ

a[f ]
kbf := (
explf := (af
expl
if o

f
)
¬
∨ >
f then
¬
set ϕf := (

∧ ¬
¬
= f (f is observed) then
|

∨
∨
f
f := (a¬

ϕf := (
=
|

(
∧
a[
∧ ¬
◦

a[
¬
f ]
¬
∧
a[f ]

expl
explf ))

explf )
(af

∧
kbf

∧
kbf

else if o

◦
(af

explf

f
¬

expl

expl

∨ ⊥

∨ ⊥

∨ >

kbf

f ))

f )

(f

(f

∧

∧

∧

∧

∧

∧

∨

∨

f ]

)

)

)

¬

¬

¬

¬

f

h

, ϕ)

a, o
i

PROCEDURE CNF-FSLAF(
input: action a with known precondition term p, observation term
o, ϕ a transition belief formula with the following factored form:
ϕ = Vi Wj ϕi,j, where each ϕi,j is a ﬂuent-factored formula.
1: if action a did not fail then
2:

li) where li are the literals appearing in
ϕ := ϕ
p, and F (l) is the ﬂuent-factored formula equivalent to l
(i.e., F (l) = ((l
⇒ >
)
∧ >

)
∧
)).

Wi F (

⇒ ⊥

⇒ >

f
¬

∧ >

l
¬

Vf

((f

⇒

∈P

∧

∧

¬

∧

)

(

(

)

)

>
3:
ϕi,j := AE-STRIPS-SLAF[o](ϕi,j)
4: else
5:
6:
7: Each ϕi,j is factored into Ai,j

ϕi,j := AE-STRIPS-SLAF[p,

for ϕi,j in ϕ do

all (and only) clauses containing a ﬂuent from
such that there exists B such that for all j, Bi,j
Wj ϕi,j with B

Wj Ai,j

P
≡

8: Eliminate subsumed clauses in ϕ
9: return ϕ

∧

∧

](ϕi,j)

a, o
i

h
Bi,j where Bi,j contains
. For any i
B, replace

Figure 5: Single-step CNF SLAF algorithms.

4.1 STRIPS Actions
In this section, we present an efﬁcient algorithm, CNF-
SLAF (Figure 5), for progressing CNF transition belief for-
mulas. Because CNF is an inherently less ﬂexible knowl-
edge representation language, these algorithms assume that
the action model to be learned is unconditional STRIPS in
order to still provide compactness guarantees. In particu-
lar, under certain conditions we show that transition belief
formulas stay indeﬁnitely compact.

¬

f ]

∈ P

◦, a¬

a
∈A{

af , af

f , a[f ], a[

For STRIPS actions, we consider a simpler propositional
vocabulary than the one deﬁned in Section 3. Deﬁne action
propositions Lf =
for ev-
. Let the vocabulary for the formulas representing
ery f
S
transition belief states be deﬁned as L =
Lf .
f ) is true if and only if action a in the
Intuitively: af (a¬
transition relation causes f (
f ) to hold after a is executed.
◦ is true if and only if action a does not affect ﬂuent f . a[f ]
af
(a[
f ) is in the precondition of
¬
a. Also, let the formula baseCN F encode the axioms that
“inconsistent” models are not possible. That is, models in

¬
f ]) is true if and only if f (

P ∪

S

∈P

¬

}

f

mulas in ﬂuent-factored form:

f

¬

∨

∧

∨

∈P

(f

f )

expl

f
¬

explf )

formula ϕ is ﬂuent-
ϕf with
kbf where explf ,
f , and kbf contain only propositions from Lf and
= CnLf (explf ∨

Deﬁnition 4.1 A transition belief
factored if it is in the form ϕ = baseCN F ∧
ϕf = (
expl
Af |
¬
The subformula explf (expl
f ) represents knowledge that
¬
is learned when the ﬂuent f (
f ) is observed. Note that the
¬
ﬂuent-factored form for the transition belief state containing
no prior knowledge about the state of the world or its action
model is given by

expl

f ).

V

∧

¬

.

(f

)

)

∨ >

∧

∨ >

∧ >

V

The following theorem shows the correctness of CNF-
SLAF. It also gives time and space complexity results. In
particular, note that in the case that every ﬂuent is observed
every at most k steps, the transition belief formula stays in
k-CNF (i.e., indeﬁnitely compact).

f

∈P

(
f
¬

Theorem 4.2 SLAF[
CNF-SLAF[
](ϕ) for
a, o
a, o
](ϕ)
i
h
i
h
any ﬂuent-factored formula ϕ, successfully executed action
a, and observation term o. Moreover:
1. The procedure takes time linear in the size of the formula.
2. If each ﬂuent is observed every at most k steps and the

≡

input formula is in k-CNF, the formula stays in k-CNF.

4.2 Action Failures
In this section we will explicitly consider the notion of ac-
tion failure. When an action fails, all worlds where the the
preconditions were not met are to be removed from the be-
lief state. That is, for a failed action a, Deﬁnition 2.3 be-
comes SLAF [a](ϕ) =
.
ϕ
}
Note that in the general case of learning any deterministic
action model (Section 3), action failures can be handled im-
plicitly by adding deterministic rules which cause a state to
map to itself if the preconditions of the taken action do not
hold. However, such transition rules cannot be compactly
expressed in the restricted vocabulary from Section 3.

s, R
h

s, a,

s, R

·i 6∈

i}|h

i ∈

R,

{h

The algorithm CNF-FSLAF (Figure 5) handles the case
of failed actions. It assumes that such failures are observ-
able, and it also assumes that the preconditions of the failed
actions are known beforehand. That is, the algorithm learns
actions’ effects but not actions’ preconditions. The algo-
rithm utilizes another algorithm, AE-STRIPS-SLAF (Amir
2005), as a subroutine. The following theorem shows that
CNF-FSLAF produces a safe approximation of SLAF (that
is, the correct action model is never lost) and gives condi-
tions under which CNF-FSLAF produces an exact result.
Additionally, time and space complexity are given.

and if any of

= CNF-FSLAF[
Theorem 4.3 SLAF [
a, o
](ϕ)
a, o
](ϕ)
i
h
|
i
h
,
the conditions below hold then
CNF-FSLAF[
SLAF[
](ϕ).
a, o
a, o
](ϕ)
i
h
i
h
1. For every transition relation in ϕ, a maps states 1:1.
2. ϕ contains all its prime implicates.
3. There is at most one state s where

ϕ for any R.

≡

s, R
h

i ∈

Theorem 4.4 (Time and space complexity of CNF-FSLAF)

924DAG-SLAF: Time

DAG-SLAF: Space

DAG-SLAF: Finding a Model

 

19 Fl. 

55 Fl. 

109 Fl. 

1000

/

)
4
^
0
1
s
e
d
o
n
(
 
e
z
i
S

800

600

400

200

 

l

a
u
m
r
o
F

 
 
,
)
c
e
s
m

 

(
 
s
p
e
t
S
0
0
5
 
r
o
f
 
e
m
T

i

0

0

19 Fl.

55 Fl. 

109 Fl. 

 
,
)
c
e
s
m

 

(
 
s
p
e
t
S
0
0
5
 
r
o
f
 
e
m
T

i

/

)
4
^
0
1
s
e
d
o
n
(
 
e
z
i
S
a
u
m
r
o
F

l

 

1000

800

600

400

200

0

)
c
e
s
m

(
 
e
m
T

i

 
l

a
t
o
T

900

750

600

450

300

150

0

1000

2000

3000

4000

5000

0

1000

2000

3000

4000

5000

0

1000

2000

3000

4000

5000

6000

Number of Steps

Number of Steps

Number of Steps

Figure 6: Experimental results for the DAG SLAF algorithm showing time required to process actions, space required to process actions,
and time required to extract an action model.

1. CNF-FSLAF takes time linear in the size of the formula

(CHANGE) causes (RIGHT COM2)

for a single action, observation pair.

2. If every ﬂuent is observed every at most k steps and the
input formula is in km-CNF where m is the maximum size
of any precondition, then the formula stays in km-CNF.

5 Experimental Results

We evaluated our DAG-SLAF and CNF-SLAF algorithms
on several domains,
including Blocks world, Briefcase
world, a variation of the Safe world, Bomb-in-toilet world,
and the Driverlog domain from the 2002 International Plan-
ning Competition 1. In the variation of the Safe world do-
main tested, the agent must try a number of different possi-
ble combinations in order to open a safe. In addition, there
is an action CHANGE which permutes the right combination.
Note that many of these domains (Briefcase world, Safe
world, Bomb-in- toilet world) feature conditional actions,
which cannot be handled by previous learning algorithms.

Figure 6 shows the time (always in milliseconds) and
space results for instances of Block World of various sizes.
The time plots show the time required to process every 500
actions. Note, that DAG-SLAF requires less than 2 mil-
liseconds per action. The space plots show the total space
needed in bytes. The rightmost graph shows the time taken
by our inference procedure to ﬁnd an action model of the
Blocksworld domain (19 ﬂuents) from the formula produced
by DAG-SLAF. Finding an action model is accomplished by
simply ﬁnding a model of the logical formula produced by
the algorithm using our inference procedure; inference times
are generally longer for more complex queries.

The middle graph in Figure 7 shows time taken to process
actions for the CNF-SLAF algorithm compared to the AE-
STRIPS-SLAF (series labeled (AE)) algorithm from (Amir
2005). The algorithms were tested on instances of various
size of the Driverlog domain. The CNF-SLAF algorithm is
comparable in speed, but slower by a constant factor. Note
that CNF-SLAF solves a more difﬁcult problem because
AE-STRIPS-SLAF does not learn action preconditions.

Figure 8 shows part of an action model learned after run-
ning the DAG-SLAF algorithm on the Safe world domain

1http://planning.cis.strath.ac.uk/competition/

if (RIGHT COM1)

(CHANGE) causes (RIGHT COM3)

if (RIGHT COM2)

(CHANGE) causes (NOT (RIGHT COM2))

if (RIGHT COM2)

(CLOSE) causes (NOT (SAFE-OPEN))

if (TRUE)

(TRY COM1) causes (SAFE-OPEN)

if (RIGHT COM1)

Figure 8: A Model of the Safe World (after 20 steps)

for 20 steps. The model shown was the ﬁrst model extracted
by our inference procedure on the logical formula returned
by the algorithm. In this particular model, the algorithm was
able to partially learn how the correct combination is per-
muted after the CHANGE action executes.

The learning rate graphs in Figure 7 show learning rates
for the DAG-SLAF and CNF-SLAF algorithms. The graphs
show the number of transition rules (corresponding to action
propositions) successfully learned out of 50 randomly se-
lected rules from the domain. Initially, the algorithms have
no knowledge of the state of the world or the action model.
The ﬁrst graph shows results for the Safe world domain con-
taining 20 ﬂuents and the latter graph shows results for the
Driverlog domain containing 15 ﬂuents. Note that in the do-
mains tested, it is not possible for any learning algorithm to
completely learn how every action affects every ﬂuent. This
is because in these domains certain ﬂuents never change, and
thus it is impossible to learn how these ﬂuents are affected
by any action. Nevertheless, our algorithms demonstrate rel-
atively fast learning rate under different degrees of partial
observability. Unsurprisingly, the results show that learning
rate increased as the degree of observability increased. It is
also worth noting that inference time generally decreased as
the degree of observability increased.

We also ran our CNF-SLAF algorithm on an instance of
the Driverlog domain containing 50 objects and nearly 2000
ﬂuents. 200 plans that solve 200 randomly created prob-
lems for this domain were generated. Each plan contained
on average 21 actions. Both CNF-SLAF and the algorithm
of (Qiang Yang & Jiang 2005) were run on these plans. The

925DAG-SLAF:Learning Rate

CNF-SLAF: Filtering Time

CNF-SLAF: Learning Rate

40

30

20

10

 

)
0
5
m
o
d
n
a
R

 
f
o
 
t
u
O

(

 

 

d
e
n
r
a
e
L
s
e
u
R
n
o
i
t
i

l

 

s
n
a
r
T

0

0

41 fluents
98
161
226
41 (AE)
98 (AE)
161 (AE)
226 (AE)

)
s
(
 
e
m
T

i

 
l

a
t
o
T

10

8

6

4

2

0

Full Observability

60% Observability

Full Obs.
60% Obs.
40% Obs.
20% Obs.

50

40

30

20

10

)
0
5

 
f
o
(
 
d
e
n
r
a
e
L
s
e
u
R

l

 

0

0

10

20

Number of Steps

0

500

1000

1500

2000

Number of Steps

10

20

30

40

50

Number of Steps

Figure 7: (Left, right) Experimental results showing learning rates for the DAG-SLAF and CNF-SLAF algorithms under different degrees of
observability (the percentage of randomly selected ﬂuents that are observed at each step). (Middle) Results showing time to process actions
for the CNF-SLAF algorithm and AE-STRIPS-SLAF algorithms (series labeled (AE)).

algorithms were given only the initial state of each plan, the
actions taken in the plans, and the goal condition of each
problem. A simple extension to the CNF-SLAF algorithm
was made to handle schematized actions. The CNF-SLAF
algorithm took 61 seconds to process the plans, and using
the SAT solver zChaff, 25 seconds were required to ﬁnd a
model. On the other hand, the algorithm of (Qiang Yang &
Jiang 2005) required 83 seconds to ﬁnd a model. Thus, the
running times of the algorithms are comparable.

Our algorithm ﬁnds exactly all action models that are con-
sistent with the actions and observations, while the algo-
rithm of (Qiang Yang & Jiang 2005) only guesses a (pos-
sibly incorrect) action model heuristically. The goal of this
paper is to perform exact learning where there is no assumed
probabilistic prior on the set of action models. In this case,
there is no principled way to favor one possible action model
over another. However, introducing bias by preference be-
tween models was studied by the nonmonotonic reasoning
community, and can be applied here as well.

6 Conclusion

We have presented new algorithms for learning determinis-
tic action models in partially observable domains. Unlike
previous algorithms, our algorithms handle the fully gen-
eral case of arbitrary deterministic action models, perform
exact learning, and are tractable. We have given theoreti-
cal analyses for our algorithms as well as empirical results
which demonstrate effectiveness. Moreover, manual con-
struction of action models for domain descriptions is tedious
and painstaking, even for experts. Thus, our algorithms may
help alleviate such knowledge-engineering bottlenecks. We
plan to extend our work to stochastic domains.

The algorithms presented deal with purely propositional
domains. Therefore in relational domains that contain many
ground actions, these algorithms, applied naively, do not
scale well. In such cases, algorithms that build upon the pre-
sented algorithms that take advantage of relational structure
are better suited (Shahaf & Amir 2006).

Acknowlegement This work was supported by a De-
fense Advanced Research Projects Agency (DARPA) grant
HR0011-05-1-0040, and by a DAF Air Force Research

Laboratory Award FA8750-04-2-0222 (DARPA REAL pro-
gram). The second author was supported by a fellowship
from the University of Illinois Graduate College.

References

Amir, E. 2005. Learning partially observable deterministic action
models. In IJCAI ’05. MK.
Bertoli, P.; Cimatti, A.; Roveri, M.; and Traverso, P. 2001. Plan-
ning in nondeterministic domains under partial observability via
symbolic model checking. In IJCAI ’01, 473–478. MK.
Boyen, X.; Friedman, N.; and Koller, D. 1999. Discovering the
hidden structure of complex dynamic systems. In Proc. UAI ’99.
Cimatti, A., and Roveri, M. 2000. Conformant planning via sym-
bolic model checking. JAIR 13:305–338.
Fikes, R., and Nilsson, N. J. 1971. Strips: A new approach to the
application of theorem proving to problem solving. Artif. Intell.
Ghahramani, Z., and Jordan, M. I. 1997. Factorial hidden markov
models. Machine Learning 29:245–275.
Ghallab, M.; Howe, A.; Knoblock, C.; McDermott, D.; Ram,
A.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. PDDL – The
Planning Domain Deﬁnition Language, version 1.2. Technical
report, Yale center for computational vision and control.
Gil, Y. 1994. Learning by experimentation: Incremental reﬁne-
ment of incomplete planning domains. In Proc. ICML-94.
Moskewicz, M. W.; Madigan, C. F.; Zhao, Y.; Zhang, L.; and Ma-
lik, S. 2001. Chaff: Engineering an Efﬁcient SAT Solver. In Pro-
ceedings of the 38th Design Automation Conference (DAC’01).
Pasula, H. M.; Zettlemoyer, L. S.; and Kaelbling, L. P.
2004. Learning probabilistic relational planning rules. In Proc.
ICAPS’04.
Qiang Yang, K. W., and Jiang, Y. 2005. Learning action mod-
In Proc.
els from plan examples with incomplete knowledge.
ICAPS’05. AAAI Press.
Reiter, R. 2001. Knowledge In Action: Logical Foundations for
Describing and Implementing Dynamical Systems. MIT Press.
Shahaf, D., and Amir, E. 2006. Learning partially observable
action schemas. In Proc. AAAI ’06.
Wang, X. 1995. Learning by observation and practice: an in-
In Proc.
cremental approach for planning operator acquisition.
ICML-95, 549–557. MK.

926