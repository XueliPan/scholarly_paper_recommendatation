A Run-time System for Efﬁcient Execution of Scientiﬁc Workﬂows on

Distributed Environments

George Teodoro

Tulio Tavares

Wagner Meira Jr.

Dorgival Guedes

Renato Ferreira
Tony Pan

Tahsin Kurc
Joel Saltz

Department of Computer Science

Universidade Federal de Minas Gerais, Brazil

{george,ttavares,renato,meira,dorgival}@dcc.ufmg.br

Department of Biomedical Informatics

The Ohio State University, USA
{kurc,tpan,jsaltz}@bmi.osu.edu

Abstract

Scientiﬁc workﬂow systems have been introduced in re-
sponse to the demand of researchers from several domains
of science who need to process and analyze increasingly
larger datasets. The design of these systems is largely
based on the observation that data analysis applications
can be composed as pipelines or networks of computations
on data. In this work we present a run-time support sys-
tem that is designed to facilitate this type of computation
in distributed computing environments. Our system is opti-
mized for data-intensive workﬂows, in which efﬁcient man-
agement and retrieval of data, coordination of data pro-
cessing and data movement, and check-pointing of inter-
mediate results are critical and challenging issues. Experi-
mental evaluation of our system shows that linear speedups
can be achieved for sophisticated applications, which are
implemented as a network of multiple data processing com-
ponents.

1 Introduction

Data analysis is a signiﬁcant activity in almost every
scientiﬁc research project. Challenges in designing and
implementing support for efﬁcient data analysis are many,
mainly due to characteristics of scientiﬁc applications that
generate and reference very large datasets. Large datasets
are oftentimes generated by large scale experiments or long
running simulations. One example is the Large Hadron
Collider project at CERN. Starting this year, this project is

This research was supported in part by the National Science Founda-
tion under Grants #ACI-0203846, #ACI-0130437, #ANI-0330612, #ACI-
9982087, #CCF-0342615, #CNS-0406386, #CNS-0403342, #CNS-
0426241, NIH NIBIB BISTI #P20EB000591, Ohio Board of Regents
BRTTC #BRTT02-0003.

expected to generate raw data on petabyte scale from four
large underground particle detectors each year (6). Projects
like the Grid Datafarm (17) are being implemented to be
able to process these datasets.

To help the researchers in their experiments and analy-
sis, scientiﬁc workﬂow systems (13; 12; 3; 15) have been
introduced. In most scientiﬁc applications, analysis work-
ﬂows are data-centric and can be modeled as dataﬂow pro-
cess networks (14). That is, a data analysis workﬂow can
be described as a directed graph, in which the nodes rep-
resent application processing components and the directed
edges represent the ﬂow of data exchanged between these
components.

Distributed environments, like a PC cluster or collec-
tion of PC clusters, provide viable platforms to efﬁciently
store large datasets and execute data processing operations.
In a scientiﬁc workﬂow system, the user should be able
to describe and create components based on the tasks they
want to execute, arrange these components into a network
of operations on data based on the application data pro-
cessing semantics, and run the network of components on
very large data collections on clusters of storage and com-
putation nodes. Scientiﬁc workﬂow systems should also
support component reuse. In other words, a components
may be part of a speciﬁc workﬂow, but also can be reused
in another application workﬂow. An example data analysis
workﬂow in an image analysis application is shown in Fig-
ure 1. This example involves analysis of digital microscopy
slides to study the phenotype changes induced by some ge-
netic manipulations. In the ﬁgure, we can see four different
tasks (image analysis operations) that should be applied in
sequence to the slides. In summary, some of the challenges
in designing workﬂow systems that support processing of
large datasets are 1) to store, query and manage large dis-
tributed databases, 2) to manage the input and output data

Proceedings of the 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)0-7695-2704-3 /06 $20.00  © 2006and the scheduling and monitoring of these workﬂows ex-
ecution in the distributed environment, and 3) to optimize
the reuse of components in different workﬂows.

We proposed and developed the Anthill system (8),
a system based on the ﬁlter-stream programming model
that was originally proposed for Active Disks (2), to ad-
dress some of the issues in execution of scientiﬁc data-
intensive workﬂows.
In Anthill, ﬁlters represent differ-
ent data processing components of the data analysis struc-
ture and streams are an abstraction for communication be-
tween ﬁlters. Using this framework, applications are im-
plemented as a set of ﬁlters over the network connected
using streams, creating task parallelism as in a pipeline.
During execution, multiple copies of each ﬁlter can be in-
stantiated, allowing every stage of the pipeline to be repli-
cated, resulting in data parallelism. In an earlier work (8),
we demonstrated the efﬁcacy and efﬁciency of Anthill for
data mining tasks.

In this paper we report on the results of an effort to ex-
tend the functionality of Anthill. These extensions include
1) a program maker component, which builds workﬂow ex-
ecutables from dynamically loadable shared libraries and
workﬂow description ﬁles, 2) a persistent storage layer,
which provides support for management of meta-data asso-
ciated with workﬂow components, storage and querying of
input, intermediate, and output datasets in workﬂows, and
3) in-memory storage (cache) layer, which is designed to
improve performance when data is check-pointed or stored
in and retrieved from the persistent storage layer. The per-
sistent storage layer builds on Mobius (11), which is a
framework for distributed and coordinated storage, man-
agement, and querying of data element deﬁnitions, meta-
data, and data instances. Mobius is designed as a set of
loosely coupled services with well-deﬁned protocols. Data
elements/objects are modeled as XML schemas and data
instances as XML documents, enabling use of well-deﬁned
protocols for storing and querying data in heterogeneous
systems.

The extensions presented in this paper are generic in the
sense that they can be applied in a range of situations, and
our experiments have shown that we incur low overhead
during execution.

2 Related Work

The Chimera (9) project has developed a virtual data
system, which represents data derivation procedures and
derived data for explicit data provenance. This information
can be used for re-executing an application and regenerat-
ing the derived data. Our approach focuses on storing the
partial data results; we do not store a large amount of infor-
mation about data derivation, but we are able to efﬁciently
store datasets generated between each pipeline stage. The
Pegasus (7) can create a virtual data system that saves
the information about data derivation procedures and de-
rived data using Chimera. It also maps Chimera’s abstract
workﬂow into a concrete workﬂow DAG that the DAG-

Man (10) meta-scheduler executes. The Kepler (3; 15)
system provides support for Web Service based workﬂows.
The authors show the composition of workﬂows based on
the notion of actor oriented modeling, ﬁrst presented in
PTOLEMY II (1). Pegasus and Kepler systems have in-
teresting solutions to the workﬂow management problem.
However, they do not directly address the problem of in-
tegrating workﬂow execution with data management and
retrieval. Our system is constructed to support efﬁcient ac-
cess to data stored in distributed databases and scalable ex-
ecution of workﬂows in an integrated manner. NetSolve (5)
provides access to computational software and hardware
resources, distributed across a wide-area network. To sup-
port sharing of software resources available in the network,
NetSolve creates an infrastructure to call shared libraries
that implement the available functionalities. The other fea-
tures of NetSolve include support for fault-tolerance and
load balancing across computational resources.

3 Extended Anthill Framework

The architecture of the extended Anthill system, as
shown in Figure 2, is composed of two main parts:
the
program maker and the run-time environment. The ﬁrst
part allows users to store and share data processing compo-
nents in a repository and provides a toolkit for generating
workﬂows based on shared components from the repos-
itory. The run-time environment is designed to support
analysis workﬂows in data intensive applications. The run-
time environment is further divided into a distributed work-
ﬂow meta-data manager, a distributed in-memory data stor-
age, and a persistent storage system. The workﬂow meta-
data manager (WFMDM) works as a data manager for the
workﬂow execution. It stores information for datasets read
or written by the application on the ﬂy. It is also responsi-
ble for deciding on demand which portions of the input data
are processed by each ﬁlter. Note that the WFMDM can be
executed in distributed fashion across multiple machines
or as a centralized entity. The in-memory data storage
(IMDS) subsystem works as an intermediary between the
application and the Persistent Storage Management System
(PSM). Based on the meta-data provided by the WFMDM,
the IDMS basically reads the necessary data from the PSM
and stores the outputs of each component in the PSM. The
PSM uses the Mobius framework (11) to expose and virtu-
alize data resources as XML databases and to allow for ad
hoc instantiation of data stores and federated management
of existing, distributed databases. The system also provides
mechanisms for efﬁciently saving partial results without in-
troducing synchronization between the application and the
run-time environment.

We now proceed to detailing the implementation of each
of these components. They are designed to achieve scalable
and efﬁcient execution in distributed and heterogeneous en-
vironments.

Proceedings of the 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)0-7695-2704-3 /06 $20.00  © 2006Figure 1. Example application workﬂow.

Shared Library Repository

Get Libraries

Program Maker

Filter Maker

Program Descriptor

Application Filters

    Runtime Environment 

Data-Intensive Workflow Execution Support

In Memory Data Storage

Workflow Meta-data Manager

Workflow Management System

XML Data Storage MiddleWare

Figure 2. Framework Components

This component is a tool for allowing users to incor-
porate existing program components and libraries in the
workﬂow system. To accomplish this task, it creates ad-
ditional code in each stage of the workﬂow pipeline to
support execution of program executables and dynamically
loadable shared libraries. The Program Maker is divided
into three parts: Shared Libraries and Executables Reposi-
tory, Program Descriptor, and Filter Maker.

3.1.1 Shared Libraries and Executables Repository
In our framework data analysis workﬂows can be created
from dynamically loadable libraries and program executa-
bles. We have developed a repository component to en-
able management of function libraries and executables so
that users can store and search for application processing
components and use them in workﬂows. To support efﬁ-
cient management and querying of the repository, we im-
plemented it using Mobius (11). The user can interact with
the repository via three basic operations: upload, search
for, and download programs and libraries.

The ﬁrst operation, upload, requires the creation of
meta-data that describes the compiled code being up-
loaded. This meta-data, which is implemented as an XML
document, contains all the information necessary to iden-
tify the type of data the program is able to work with, the
data structures used by each of its arguments, and the de-
serialization and serialization functions that need to be ap-

plied to the input and output datasets of the program. It also
includes additional information about the system require-
ments of the particular program or library (e.g., hardware
platform requirements, dependencies on other libraries).
The second operation, search, is used to perform queries
to search for stored libraries and program executables and
to access the meta-data related to each stored element. The
last operation, download, receives a reference to a com-
piled code or library, downloads it from the repository, and
stores it in a local directory.

3.1.2 Program Descriptor
This is the conﬁguration ﬁle (represented as an XML doc-
ument) of the entire data processing pipeline of an applica-
tion. It is divided into four sections: hostDec, placement,
layout, and compiledFilters.

hostDec is used to describe all machines available in
the environment. It is used to determine the resources
for each of the application components.

placement is used to declare the components compris-
ing a particular workﬂow application, the library in
which they are located, and the number of instances
that should be created for each component.

layout deﬁnes the connections between the compo-
nents, the policies associated with each connection
(e.g., each data buffer exchanged between two com-
ponents over the connection can be check-pointed),
and the direction of communication.

compiledFilters is used to provide information that the
framework needs to be able to execute a given com-
ponent.
Information here is used to ﬁnd out which
library the component code comes from, the number
and types of parameters that should be passed or re-
turned to/from the component, and the data transfor-
mation functions that need to be called to serialize/de-
serialize the input and outpur data of the component.

3.1.3 Filter Maker
This component receives a Program Descriptor conﬁgura-
tion ﬁle and executes the workﬂow described in that ﬁle. It
generates the source code of the connection ﬁlter for each
application component declared in the conﬁguration ﬁle,

Proceedings of the 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)0-7695-2704-3 /06 $20.00  © 2006as well as the Makeﬁle required for compiling and linking
the entire application workﬂow. The user should deﬁne an
environment variable pointing to the directory where it is
stored so that the ﬁlter maker subsystem can determine the
location of the application-speciﬁc libraries to be linked to
the workﬂow. The connection ﬁlter wraps the application
speciﬁc data processing component so that it can be exe-
cuted properly. A high level deﬁnition of the connection
ﬁlter is given in Algorithm 1. It executes a loop that reads
data from the input stream, de-serializes and passes it to the
application component code, which is invoked in the pro-
cess method. It then proceeds to serializing any output that
is generated by the application component and sending it
out the next ﬁlter in the workﬂow.

Algorithm 1 Application Filter

while there is data to be processed do

read(data)
inputData = de-serialize(data)
outPutData = process(inputData)
if there is any outPutData to be written then

outPut = serialize(outPutData)
write(outPut)

end if

end while

The run-time environment, shown in Figure 2, is divided
into three main components: the Data-Intensive Workﬂow
Execution Support System, which is responsible for instan-
tiating the workﬂow program, the Workﬂow Management
System, which is responsible for managing the entire work-
ﬂow execution, and the Persistent Storage System.

3.2.1 Data-Intensive Workﬂow Execution Support

System

This component is implemented on top of Anthill (8),
which is responsible for instantiating the components on
distributed platforms and managing the communication be-
tween them. Anthill is based on the ﬁlter-stream program-
ming model, which means that in this environment appli-
cations are decomposed into a set of ﬁlters that communi-
cate through streams. At execution time, multiple instances
of each ﬁlter can be spawned on different nodes on a dis-
tributed environment, achieving data parallelism as well as
pipelined task parallelism.

We have extended the Anthill run-time to provide trans-
parent communication between the application and the
Workﬂow Management System (WMS). These modiﬁca-
tions provide support for exchanging information across
application components and the WMS. This information
includes, for instance, which ﬁlters are available for data
processing, which documents have been processed, and so
on.

3.2.2 Workﬂow Management System
This component is divided into two subcomponents: the
Workﬂow Meta-Data Manager (WFMDM) and the In-
Memory Data Storage (IMDS). The WFDMD works as the
data manager of the entire workﬂow execution. It main-
tains information about all the data involved in the appli-
cation execution, either read or written. When the work-
ﬂow execution is initiated, the WFMDM receives a XPath
query (4) that speciﬁes the input dataset. It then relays the
query to all instances of the Persistent Storage Manager
(PSM) and builds a list of all matching documents with
the associated meta-data. Each document of the list goes
through three different states as the execution progresses:

Not processed: This state applies to all documents that
compose the input dataset in the beginning of the ex-
ecution. It means that they are available to be pro-
cessed.

Being processed: input documents sent to ﬁlters are in
this status as well as documents sent across ﬁlters, be-
cause they have been created and are being processed
by one or more ﬁlters.

Processed: a documents is marked processed when it has
been processed by a ﬁlter and the result has been
stored in the IMDS.

During the workﬂow execution, the WFMDM is respon-
sible for assigning documents to ﬁlters. This data partition
is done on demand as each time a ﬁlter reads input data, a
request is received by the WFMDM. The goal is to always
assign a local document to the ﬁlter.

The IMDS works as an intermediary between applica-
tion ﬁlters and the persistent storage manager (PSM). It is
implemented as a ﬁlter, which is instantiated on multiple
machines based on user conﬁguration. The system always
tries to have ﬁlter requests for data answered by a local
IMDS. When there is no local IMDS for a given ﬁlter, an-
other one is assigned to the ﬁlter by the run-time system.

As ﬁlters request data during execution, these requests
are passed down to the local IMDS (or to the assigned
one). The IMDS acts pretty much as a caching system,
only relaying requests to unavailable data to the WFMDM.
Several instances of the IMDS can be distributed across
available machines and work independently, meaning that
multiple instances can be reading different portions of the
data simultaneously. This is similar to a classic parallel
I/O approach, except that it is on top of a distributed XML
database.

The task of saving intermediate results is also executed
by the IMDS. It can save all data sent through the stream.
During execution, the IMDS creates, on the ﬂy, distributed
databases for each stream and stores all the data exchanged
over a given stream as documents in Mobius. It behaves
like a write-back caching mechanism, releasing the appli-
cation code from having to wait for the I/O operation to
complete. As in the case of reads, multiple write opera-
tions can be executed in parallel.

Proceedings of the 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)0-7695-2704-3 /06 $20.00  © 20063.2.3 Persistent Storage Manager

We use Mobius (11) as our persistent data storage manager.
We employ the Mobius Mako services to store all data used
in workﬂows. The Mobius Mako provides a platform for
distributed storage of data as XML documents. Databases
of data elements can be created on-demand. The data is
stored and indexed so that it can be queried efﬁciently us-
ing XPath. Data resources are exposed to the environment
as XML data services with well-deﬁned interfaces. Using
these interfaces, clients can access a Mako instance over
the network and carry out data storage, query, and retrieval
operations.

3.2.4 Communication Protocol

Application components (implemented as ﬁlters) commu-
nicate with the rest of the run-time support transparently.
Each application component is just concerned with receiv-
ing its own input data, processing it, and generating its out-
put. In Figure 3 we illustrate the internal communication
structure across the several components of the run-time in-
frastructure. We use a stage in the pipeline of an image pro-
cessing application (described later, see Figure 5) as an ex-
ample. As seen in the ﬁgure, there are two ﬁlters involved
in that stage: color classiﬁcation and tissue segmentation,
both being fairly standard image analysis algorithms.

In Figure 3(a), we detail the communication within the
run-time components for the case of a read operation (as-
sume the ﬁrst ﬁlter, color classiﬁcation, is reading its next
image). The process starts with a message from the ap-
plication’s ﬁlter requesting the next document from the
IMDS. This operation will then invoke a request to the lo-
cal WFMDM instance for an available document for pro-
cessing, and will receive an ID of some document, poten-
tially available locally. With that information, the IMDS
can serve the original requester with the data. It may need
to query the PSM, if the data is not available in the IMDS
already.

Figure 3(b) illustrates the communication protocol for
write operations. It is a slightly more complicated proto-
col. As the color classiﬁcation code outputs its data, the
ﬁlter has to ﬁrst create the dependencies (i.e., the docu-
ments used to create other documents) on the local IMDS
instance. After that, the data is sent from one ﬁlter to the
next, using the streams infrastructure within Anthill. Once
the ﬁlter on the receiving end gets the data, it creates a local
copy of the data before passing it to the application code.
As this copy of the data is stored locally, the IMDS notiﬁes
the local WFMDM instance about the local data copy and
the sender’s WFMDM instance that the data was success-
fully received. This will prompt a change the change in the
state (to processed) of the input document that generated
that particular output document. The IMDS will eventually
move the data from its memory to the PSM. It happens in
the background so that the application is not penalized.

Application Filters

Color
Classif.

Tissue
Seg.

6. Process document

1. READ_DOC

5. AVAILABEL_DOC

In-Memory Data Storage
 (transparent copies)

IMDS 1

IMDS 2

3. FREE_DOC_ID

2. RET_FREE_DOC

4. Retrive doc from Mobius

WFMDM 1

WFMDM 2

Workflow Meta-data Manager
 (transparent copies)

Persistent Storage

Mobius

Mobius

Application Filters

 Color
Classif.

1. GET_DEPS

2. RET_DOC_DEPS

Tissue
Seg.

3. WRITE_DOC

In-Memory Data Storage

 (transparent copies)

IMDS 1

6. DOC_PROCESSED

IMDS 2

5. NEW_DOC_ID

Workflow Meta-data Manager

 (transparent copies)

WFMDM 1

7. DOC_PROCESSED_ACK

4. BEGIN_NEW_DOC_IN_CACHE

If necessary, 
write some documents 
to Mobius

WFMDM 2

Persistent Storage

Mobius

Mobius

(a)

(b)

Figure 3. Communication protocol inside the
run-time support components.

4 Application Example

In this section we brieﬂy describe an example applica-
tion and how it is mapped into a workﬂow using the tools
available in our framework.

The example application uses high-resolution digitized
microscopic imaging to study phenotype changes in mouse
placenta induced by genetic manipulations. It handles the
segmentation of images that compose the 3D mouse pla-
centa into regions corresponding to the three tissue lay-
ers:
the labyrinth, spongiotrophoblast, and glycogen, as
described in (16).

We have divided this application into six stages, as seen
in Figure 4, and mapped four of the most expensive stages
as the components of the workﬂow. The basic description
of each of the four stages are:

Foreground/Background Separation (FG/BG): Im-
ages are converted from the RGB color space to the CMYK
color space and a combination of the color channels are
thresholded to get the foreground tissue.

Histogram Normalization: Images are corrected for
This process consists of three sub-
color variations.
operations: computing the average colors for the images;
selecting one image as the color normalization target; and

Proceedings of the 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)0-7695-2704-3 /06 $20.00  © 2006generating a histogram for each of the red, blue and green
channels.

Color Classiﬁcation: Pixels in an image are classiﬁed
using a Bayesian classiﬁer. The classiﬁcation of a pixel
puts it in one of 8 different categories: dark nuclei, medium
intensity nuclei, light nuclei, extra light nuclei, red blood
cell, light cytoplasm, dark cytoplasm, and background.

Tissue Segmentation: In this step, using a Bayesian
classiﬁer, each tissue is classiﬁed into one of the three tis-
sue types: Labyrinth, Spongiotrophoblast, and Glycogen.

Stage 3

FG/BG

ITK

Mask
PNG

Placenta

PNG

Stage 1

Color Test

MatLab

(R, G, B)

CSV

Reference

Image

Human

Interaction

Stage 2

Reference

 Image

Stage 4

Histogram

Normalization

MatLab

Color

Corrected

PNG

Stage 6

Color Classification

ITK

Mapped
Image
PNG

Stage 5

Training Phase

Human

Interaction

Bayes

Classfication

ITK

Training

Information

PNG

Tissue

Segmentation

MatLab

Seg. Map

PNG

Figure 4. Mouse Placenta Application

In the rest of this section we describe how a developer

can implement this application using our system.

The main work to integrate an application into our sys-
tem consists in constructing the compiledFilter section of
the Program Descriptor conﬁguration ﬁle, describing the
entire data analysis pipeline of the application (see Sec-
tion 3.1.2). In the compiledFilter section, the user describes
details about each ﬁlter that perform application speciﬁc
data processing functions. Due to space limitations, we do
not elaborate on the format of the conﬁguration ﬁle other
than to say that it is a XML document containing a detailed
description of each of the application components (ﬁlters),
with all the information required for automatically gener-
ating ﬁlters.

In the workﬂow composition phase, the user needs to
specify what ﬁlters are in the workﬂow and the connection
between them. This information is part of the placement
and layout sections of the Program Descriptor ﬁle. After
this information is speciﬁed, the user can call a script with
the program parameters and a XML query that identiﬁes
the data elements, which are stored in and managed by the
PSM, that should be processed.

Workflow Management and Storage System

Writer

Stage 3

Stage 4

Stage 6

FG/BG

ITK

Histogram

Normalization

MatLab

Color Classification

ITK

Tissue

Segmentation

MatLab

.
..

FG/BG

Workflow Management and Storage Sytem

...

.
..

.
..

Color

Tissue

Classification

Segmentation

Histogram

Normalization

. . .

Writers

Figure 5. Mouse Placenta Application Work-
Flow

In the example application, inputs needed by stage 3 are
the outputs of stage 2, so we have a clear data dependency
between them. During the execution of stage 2, our frame-
work creates new data collections on the ﬂy across avail-
able machines and stores the output data elements and the
related meta-data to be used as stage 3 input. Once stages
2 and 3 have completed execution, stage 4 can be exe-
cuted. Again the the framework takes care of storing the
output from this stage. Stage 6 has a data stream between
two application ﬁlters. Figure 5 shows this stream and a
dotted arrow from it to the WMS. This arrow represents
an optional efﬁcient stream storage mechanism. This fea-
ture allows storage of partial results during execution. This
check-pointing facility can be used to re-start the execution
from the last set of stored partial results.

5 Experimental Results

In this section we evaluate the implementation of the
example image analysis application developed using the
framework described in this paper. The experiments were
run on a cluster with 7 PCs connected using a Fast Ether-
net switch. Each node has two 1.4GHz AMD Opteron(tm)
processors, running Linux 2.4.

To evaluate our implementation, we used a dataset of
866 images, that had been created by digitizing sections
from a mouse placenta, as described in (16). The size of
the whole dataset is 23.49 GB. The dataset has been stored
in the PSM; we ran one Mobius Mako service instance on
each storage node and distributed the images in the dataset
across multiple Mako nodes in round-robin fashion. Dur-
ing the experiments, we instantiated one IMDS on each
machine and one WFMDM instance on one of the ma-
chines. To better use multiple processors on each machine,

Proceedings of the 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)0-7695-2704-3 /06 $20.00  © 2006we also created two transparent copies of the application
ﬁlters per machine.

Table 1. Avg. Input Dataset Deviation

Number of Machines

2
3
4
5
6
7

Value
8.75749
9.35287
7.44299
4.26831
3.97418
3.56119

machines is about 6000 seconds and the speed-up is al-
most linear. Figure 7(b) shows the speed-up of the last
and most expensive stage, the “Color Classiﬁcation” and
“Tissue Segmentation”. For this stage, the execution time
using 2 machines is about 30000 seconds and the speed-up
is almost linear.

Speed-up (866 Placenta Images (23.49GB) - 866 Masks (488MB))

Speed-up
Speed-linear

Number of Machines vs. Execution Time (866 Placenta Images (23.49GB))

Test Time
deviation
Processing Functon
deviation
Read Document
deviation
(des)serialize Functions
deviation
Write to Stream
deviation
Read from Stream
deviation

 2

 3

 4

 5

 6

 7

Number of Machines

(a)

Speed-up (866 Placenta Images (23.49GB))

Speed-up
Speed-linear

)
s
d
n
o
c
e
s
(
 

i

e
m
T
n
o

 

i
t

u
c
e
x
E

l

e
u
a
V

 4000

 3500

 3000

 2500

 2000

 1500

 1000

 500

 0

 7

 6

 5

 4

 3

 2

 2

 3

 4

 6

 7

 5
Machines

(b)

Figure 6. (a) The dissection of the execution
time of the FB/BG stage.
(b) the speed-up
values.

Figure 6 shows an experimental evaluation of the Fore-
ground/Background Separation (FG/BG) stage. The num-
bers illustrate the good scalability of our system, which
achieves almost linear speed-up as seen in Figure 6(b). In
Figure 6(a), the details of the execution time are shown.
The results show that the execution time is dominated by
the time spent in the process function. The “Read Docu-
ment” time represents the time spent by our system to re-
trieve the data required by the application. The time spent
using two machines is smaller than that spent using three
machines. To explain this, we measured the average devi-
ation of the data size stored in each machine (see Table 1).
This measures load imbalance between the machines. We
observe that there is more load imbalance on three nodes
than on two nodes, resulting in the increase in data retrieval
time.

Figure 7(a) shows the speed-up results of the Histogram
Normalization stage. This stage uses images and associ-
ated image masks as input. The execution time using 2

 2

 2

 3

 4

 6

 7

 8

Speed-up (866 Placenta Images (23.49GB)- 866 Masks (488MB))

Speed-up
Speed-linear

 5

Machines

(a)

 5
Machines

(b)

 2

 2

 3

 4

 6

 7

Figure 7. Speed-up: Histogram and Color
Classiﬁcation stages

Figure 8 shows the performance of the system when par-
tial results from a stage is saved in the system. In the ﬁg-
ure, the execution time of the last stage of the application is
shown, when the partial results from the Color Classiﬁca-
tion ﬁlter are saved or not saved in the system. As is seen
from the ﬁgure, the overhead is very small and less than

 8

 7

 6

 5

 4

 3

 7

 6

 5

 4

 3

l

e
u
a
V

l

e
u
a
V

Proceedings of the 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)0-7695-2704-3 /06 $20.00  © 2006Number of Machines vs. Execution Time (866 Placenta Images (23.49GB)- 866 Masks (488MB))

No CheckPoint
CheckPoint

 30000

 25000

 20000

 15000

 10000

)
s
d
n
o
c
e
s
(
 

i

e
m
T
n
o

 

i
t

u
c
e
x
E

 5000

 2

 3

 4

 6

 7

 5
Machines

Figure 8. Color Classiﬁcation and Tissue
Segmentation Test: doing and not doing
checkpoint

5% on average.

6 Conclusion and Future Work

In this paper we have presented extensions to a run-time
system, Anthill, for efﬁcient execution of scientiﬁc work-
ﬂows on distributed environments. The new components
can create a stub Anthill ﬁlter (also called the connection
ﬁlter) automatically from a high level description of a given
application component. These ﬁlters can run user code
with a simple interface.

In order to provide data management with low overhead,
we use the Mobius infrastructure. The modules of the run-
time support are also built as a set of Anthill ﬁlters which
communicate among themselves and with Mobius trans-
parently to the user code. Our experiments have shown
that our implementation can be used to execute sophisti-
cated applications, with multiple components, with almost
linear speedups. This means that our system imposes very
little overhead.

Our next step is to work toward building a robust, de-
pendable workﬂow system. Fault tolerance is important in
any environment with a large number of machines and pro-
cesses running for non-trivial periods of time. We plan to
use the efﬁcient data management mechanisms presented
in this paper to store data checkpoints and allow applica-
tions to resume execution from check-pointed data.

References

[1] PTOLEMYII project, Department of EECS, US Berkeley.

http://ptolemy.eecs.berkeley.edu/ptolemyII/, 2004.

[2] A. Acharya, M. Uysal, and J. Saltz. Active disks: Program-
ming model, algorithms and evaluation. In Eighth Interna-
tional Conference on Architectural Support for Program-
ming Languages and Operations Systems (ASPLOS VIII),
pages 81–91, Oct 1998.

[3] I. Altintas, C. Berkley, E. Jaeger, M. Jones, B. Ludscher,
and S. Mock. Kepler: An Extensible System for Design
and Execution of Scientiﬁc Workﬂows. In the 16th Intl.
Conference on Scientiﬁc and Statistical Database Manage-
ment(SSDBM), Santorini Island, Greece, June 2004.

[4] A. Berglund, S. Boag, D. Chamberlim, M. F. Fernández,
M. Kay, J. Robie, and J. Siméon. Xml path language
In World Wide Web Consortium (W3C), August
(xpath).
2003.

[5] H. Casanova and J. Dongarra. Netsolve: A network enabled
server for solving computational science problems. In Inter-
national Journal of Supercomputer, pages 212–223, 1997.

[6] CERN.

Large

hadron

collider

(lhc)

-

http://www.interactions.org/lhc/.

[7] E. Deelman, J. Blythe, Y. Gil, C. Kesselman, G. Mehta,
K. Vahi, A. Lazzarini, A. Arbree, R. Cavanaugh, and S. Ko-
randa. Mapping abstract complex workﬂows onto grid en-
vironments. In Journal of Grid Computing, pages 25–39,
2003.

[8] R. Ferreira, W. Meira Jr., D. Guedes, L. Drummond,
B. Coutinho, G. Teodoro, T. Tavares, R. Araujo, and G. Fer-
reira. Anthill: A scalable run-time environment for data
mining applications. In Symposium on Computer Architec-
ture and High-Performance Computing (SBAC-PAD), 2005.
[9] I. Foster, J. Voeckler, M. Wilde, and Y. Zhao. Chimera:
A virtual data system for representing, querying, and au-
In The 14th International Con-
tomating data derivation.
ference on Scientiﬁc and Statistical Database Management
(SSDBM’02), 2002.

[10] J. Frey, T. Tannenbaum, I. Foster, M. Livny, and S. Tuecke.
Condor-G: A computation management agent for multi-
In Proceedings of the Tenth IEEE
institutional grids.
Symposium on High Performance Distributed Computing
(HPDC10). IEEE Press, Aug 2001.

[11] S. Hastings, S. Langella, S. Oster, and J. Saltz. Distributed
data management and integration framework: The mobius
project. In Global Grid Forum 11 (GGF11) Semantic Grid
Applications Workshop, pages 20 – 38. IEEE Computer So-
ciety, 2004.

[12] S. Hastings, M. Ribeiro, S. Langella, S. Oster,
U. Catalyurek, T. Pan, K. Huang, R. Ferreira, J. Saltz, and
T. Kurc. Xml database support for distributed execution of
data-intensive scientiﬁc workﬂows. SIGMOD Record, 34,
2005.

[13] G. Kola, T. Kosar, J. Frey, M. Livny, R. J. Brunner, and
M. Remijan. Disc: A system for distributed data inten-
sive scientiﬁc computing. In Proceeding of the First Work-
shop on Real, Large Distributed Systems (WORLDS’04),
San Francisco, CA, December 2004.

[14] E. A. Lee and T. M. Parks. Dataﬂow process networks. In

Proceedings of the IEEE, pages 773–799, may 1995.

[15] B. Ludascher, I. Altintas, C. Berkley, D. Higgins, E. Jaeger-
Frank, M. Jones, E. Lee, J. Tao, and Y. Zhao. Scientiﬁc
workﬂow management and the kepler system. Concurrency
and Computation: Practice & Experience, Special Issue on
Scientiﬁc Workﬂows, 2005.

[16] T. C. Pan and K. Huang. Virtual mouse placenta: Tissue
layer segmentation. Proceedings of the 27th Annual Inter-
national Conference of the IEEE Engineering in Medicine
and Biology Society (EMBC2005), Sep 2005.

[17] O. Tatebe, Y. Morita, S. Matsuoka, N. Soda, and
S. Sekiguchi. Grid datafarm architecture for petascale
data intensive computing. In 2nd IEEE/ACM International
Symposium on Cluster Computing and the Grid (CCGrid),
2002.

Proceedings of the 18th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'06)0-7695-2704-3 /06 $20.00  © 2006