UA12002 

STECK & JAAKKOLA 

469 

Unsupervised Active Learning m  Large Domains 

Artificial 

Harald Steck 
Intelligence 

Laboratory 
of Technology 

Massachusetts 
Cambridge, 

Institute 
MA 02139 

Tornrni S. Jaakkola 

Artificial  Intelligence 

Laboratory 
of Technology 

Massachusetts 
Cambridge, 

Institute 
MA 02139 

harald@ai.mit.edu 

tommi@ai.mit.edu 

Abstract 

an­

depends 

crucially 

approach to 

learning 
is a powerful 
data effectively. 

We show that the 
of active learning 

to 
The 
gain, for example, 
does 
evaluation 
subset of 

Active 
alyzing 
feasibility 
on the choice of measure  with  respect 
which the query is being optimized. 
information 
standard 
not permit an accurate 
small committee, 
the model space. We propose 
measure 
and discuss 
sure. We devise, 
approach 
vantages 
the context 
work models. 

a surrogate 
a small committee 

for committee 
of this approach 

a representative 

requiring  only 

are illustrated 

the properties 

in addition, 

selection. 

a bootstrap 

of this new mea­

in 
net­

The ad­

with a 

of recovering  (regulatory) 

on the cur­

often represented 

of the model 

the overall 

the number of queries) 

predictions.' 

have been de­
and regression. 

unsupervised 

the most recent 

ones 

such approaches 

computational 
to domains 

with a fairly 

resources, 

depends 

to ensure 

learning 

enormous 

of the active 

have addressed 

in the above sense; 

for supervised 

few approaches 

their applicability 

learning 
learning 

[14, 15, 10]. While relevant, 

a representative 
objective 
is to minimize 
the data (e.g., 
reliable 
to active 

This expectation 
learner, 
subset 

from the system. 
rent knowledge 
by a committee, 
space used.  The 
cost of acquiring 
necessary 
approaches 
Various 
veloped, 
mostly 
Relatively 
active 
include 
typically  require 
limiting 
small number of variables. 
In this paper we develop 
active 
show that the computational 
cially 
of the measure 
optimal 
which  the 
ticular 
( cf. Section 
turns out to require 
nentially  with 
random variabl
we term "average 
emerges 
mittee approach 
eral motivation 
unsupervised 
active 
bootstrap 
approach 
bers ( cf.  Section 
Section 

query is chosen, 
5). The standard 
a committee 

[12, 4]. In Section 
for this approach, 

learning. 
for selecting 

6) and an empirical 

on the properties 

KL divergence 

which is tailored 

an approach 

as a natural 

extension 

efficiency 

learning 

of pairs" 

to unsupervised 

to large domains. 

We 
cru­
depends 
to 
in par­

with respect 
additivity 
information 

(KL2). It 

to the query by com­
3, we give a gen­
it to 
by a 
mem­

This is followed 
the committee 

and generalize 

gain 
expo­
the size of the domain (the number of 
es). We  propose 

size that scales 

a new measure,  which 

evaluation  ( cf. 

INTRODUCTION 

of 

involve 

networks 

of interest 

for example, 

in functional 

interventions 

need to be estimated 

models have to be inferred 

Ensuring reliable 

number and type of observations 

or deliberate 
the setting 

where models of genetic 
regu­
on the basis of 

a large collection 
X, where the asso­
under severe 
sam­
predictions 
of out­
is 

1 
Many domains 
inter-dependent  random  variables 
ciated 
pling constraints. 
comes of perturbations 
challenging. 
This is precisely 
genomics, 
latory 
a very limited 
the variables 
to intervene 
than to rely on purely 
deed necessary 
ventions. 
tions might mean "knocking 
a gene (rarely 
genes). 
The overall 
set certain 
and observe 
this setting 
can be expected 

problem 
variables 
a sample from the rest. Active learning 
seeks to determine 

more informative 
a response 
data. This is in­

of inter­
2 
Before 
learning, 
ventionallearning, 
on a subset 
We denote 
instantiation 

can be formalized 
Q c X to their desired 

(genes). 
in the system to elicit 

to yield the most informative response 

to be able to predict outcomes 

out" or "over-expressing" 

the intervention 

observational 

as follows. We 

It is frequently 

In functional 

the interven­

genomics 

multiple 

context, 

1 We include 

rather 

about 

which 

value q 

9). 

in 

ACTIVE LEARNING 

we develop an 

approach 

let us first formalize 

to unsupervised 
active 
the basic idea of inter­

where the learner 

can impose values 

of the variables 
an intervention 
of the remaining 

and observe 

the response. 

by do(q) [11]. The observed 

variables 

X \ Q is then 

the model structure 

here as  a "prediction". 

470 

STECK & JAAKKOLA 

UA12002 

MEASURES  OF DISAGREEMENT 

P(XIdo(q)).2 Note that 
4 
may differ from the condi­
as the query variables 

are 

set to some value by the learner. 

For instance, 

model is used to describe 

over X, the distribution 

P(XIdo(q)

) 

P(XIdo(q)) 

distribution 
P(XIq), 

drawn from the distribution 
the distribution 
tional 
actively 
when a Bayesian 
the distribution 
is given by the mutilated 
by setting 
pointing 
we will loosely 
mainder 

towards the variables 

Q =  q and removing 

of the paper. 

network 

network, 

which is obtained 

all the directed 
in Q [11]. For brevity, 

edges 

denote an intervention 

by q in the re­

active learning, 

a distribution 
the disagreement 

each model m in 
over X. 

C describes 
to quantify 
members m1, m

In unsupervised 
the committee 
This suggests 
any two committee 
tance or divergence 
applied 
probability 
D(2l(P(XIq,mJ)IIP(XIq,m
)).3 To capture 
2
all disagreement 
committee 
impose the following 

E C by a dis­
measure D(2), where D(2) is 

we 
for the measure 

to the associated 

desiderata 

among  the 

members, 

the over­

among 

D: 

2 

distributions, 

capable 

is actually 

in all approaches 

to active 
is that the model class used by the active 

A common assumption 
learning 
learner 
tribution 
the data. Concerning 
networks, 
to the Causal Markov as­
[13]. More­
sumption 
over, we assume that there are no variables 
to the learner, 

underlying 
this corresponds 
and the Faithfulness 

and that the variables 

of describing 

assumption 

are discrete. 

unknown 

the true dis­

Bayesian 

1. symmetry: 
of experts 

D depends 

only on the unordered 

set 

2. weights: 
assigned 

D admits different 
to the various 
mi 

experts 

weights 

such as P(mi) 

3. additivity: 

the measure 

of disagreement 

D in a 

domain is the sum of the measures 
subdomains. 

in independent 

QUERY  BY COMMITTEE 

3 

formulation 

label if there is a high degree of disagreement 

assuming no noise concern­

approach 

of experts, 

The original 

uses a (small) 
com­
the current 
knowl­

which represents 
learner. 
learning, 

The Query by Committee 
mittee 
edge of the active 
is for supervised 
ing the labels [12, 4]. The basic idea is to query a 
missing 
among predictions 
ing of a binary classifier, 
ment among a committee 
perts predict 
the one label, 
the opposite. 
The degree of disagreement 
serves 
as an 
estimate 
gain in this ap­
proach.  While maximizing 
information 
optimize 
tion accuracy, 
important 

of the committee. 
for example, 
is maximal if half the ex­

gain does not necessarily 
co-vary 

information 
the expected 

the two objectives 

while the other half does 

classes of models [4]. 

for the (myopic) 

In active 

learn­

predic­

for several 

the disagree­

instantaneous 

divergence 
measures 

D(2), one 
with the 

complying 

measure 

Based on any pairwise 
can obtain generalized 
first two properties 
averages, 
possible 
ferent 
of K =  ICI experts: 

generalized 

measures 

denoted 

ways of averaging, 

by taking weighted 

(arithmetic) 

by Om-P(M). As there are three 

this gives rise to three dif­
concerning 

a committee 

C 

• ( D(2l (P(XIq, m)II(P(XIq, m'))m'-P(Ml)) m-P(Ml 
• I D(2) ((P(XIq,m)) P(M)IIP(XIq,m'))) 

m'-P(M) 

\ 

m-

.1/n(2l(P(XIq,m
\\ 

)!!P(XIq,m'))) ) 

m-P(M) m'-P(M) 
behave in different 

These three generalized 
ways concerning 
next section, 
the committee 

the additivity. 
where we also discuss 
size. 

measures 

This is outlined 
in the 

lower bounds  on 

prior to the observation 

from first prin­
The information 
its outcome is 
5 

obtained 

The degree 

after observing 

to the uncertainty 

can be motivated 
theory: 

Query by committee 
ciples 
of information 
from an experiment 
equivalent 
of its outcome. 
mittee can serve as an estimate 
contrast 
ing we have to define a measure 
tify the degree of disagreement 
members. 
ble measures 
measure 
tion gain, which is the only consistent 
the sense of Shannon 

In the following, 

to supervised 

the standard 

learning, 

we examine 

entropy. 

besides 

of disagreement 
of the com­
In 

of this uncertainty. 
in unsupervised 
learn­
of variability 
among the committee 

to quan­

such measure 

in 

several 

possi­

of informa­

2We loosely 

use P(XIdo(q)) 

in order to denote the dis­

tribution 
over X\ 

Q. 

ADDITIVITY PROPERTY  AND 

COMMITTEE  SIZE 

that 

ensures 

property 

of the measure 

in the right way with the size of the do­

The additivity 
it "scales" 
main. This is an important  property 
see, e.g., [8]. If a domain X can be decomposed 
into 
Xi (X =  Ui Xi) such that 
independent 
P(X) =  fL P(Xi), and given that the model class M 
under consideration 
independences, 

is capable 
P(XIM) =  Tii P(XiiM), 

of representing 

subdomains 

these 

i.e., 

then an 

of measures, 

3For brevity 

of notation, 

we write P(·l·) 

P(-1·, D) in the remainder 
every term. 

of this paper, 

instead 
as  D appears 

of 
in 

UA12002 

STECK & JAAKKOLA 

471 

First, 

in large domains, 

accordingly, 
namely 

computations 

approximately, 

= I:; D(P(X;IM)). 
property 

is also crucial 
This is primar­

has to be evaluated 

to sum explicitly 

of a measure 
in practice. 

the mea­
as it 
over all con­

intractable 
of X. It is hence desirable 

measure D decomposes 
additive 
like D(P(XIM)) 
The additivity 
for feasible 
ily for three reasons. 
sure D(XIq) 
is typically 
figurations 
that the scores 
D( .. lq) and D( .. lq') of queries q and q' differ by a large 
margin: 
errors  while 
the query 
with the largest 
evaluations 
the measure 
as the size of the domain increases. 
margin of a subadditive 
creasing 
ditivity 
A measure 
is computationally 

domain size. Finally
determines 
that retains 

the 
diminishes 
below, ad­

a large margin permits 
it is still possible 

are possible. 
ensures 

more errors, 
of 

the additivity 
that the margin does not decrease 

size of the committee. 
for small committees 

large approximation 

, as explained 

more efficient. 

In contrast, 

additivity 

By allowing 

to identify 

Moreover, 

the minimal 

measure 

score. 

faster 

with in­

the JS divergence 

holds,4 
terms of the distributions 
before and after the next observation 
query q: 

can also be understood 
in 
over the model space M 
of X given the 

D��P(M)(P(XIq,m)) 
(nKL(P(Mix,q)IIP(Ml)) P(X)q) 
-I H(P(Mix,q))) 
H(P(M))

\ 

P(Xjq) 

in terms of the JS divergence, 

among the com­

the expected 

when measured 

to minimizing 

the disagreement 

Thus maximizing 
mittee, 
is equivalent 
tropy of the search space. The latter 
sentially 
as a measure 
understood 
search space  given 
disagreement 
effective  size 

corresponds 
of the search space. 

of the effective 
the data.  Hence, 

was taken in [10, 14]. The entropy 

to maximally 

en­
es­
can be 

size of the 
the 
the 

maximizing 
shrinking 

posterior 
approach 

5.1 

PAIRWISE MEASURE 

The additivity 

of the pairwise 

sure D(2l(PIIQ) is a necessary 

concerning 

of the measures 

the additivity 
perts. This suggests 
Leibler 
I:i DKL(P(X;)IIQ(X;)) 
Q(X) = 11; Q(X;). 

as DKL(P(X)II
Q(X)) 
if P(X) = rri P(X;) 

that we use the Kullback­

(KL) divergence, 

K ex­

and 

= 

of the JS divergence 

is 

mea­

property 

well-known 

Another 
that it is bounded 
pends on the number of experts. 
divergence 
mittee size, JS divergence 
requirement  for 
trarily 
Indeed, 
experts m represents 
the correct 
P(XIq, 
ditive. 
committee 

members also factor 

large domains. 

only if 

from above. This upper bound de­
Given a fixed com­
cannot be additive 

for arbi­
even when each of the 

independences, 
m) = 11; P(Xdq, m), JS is not necessarily 
It is additive 

say, 
ad­

the weights 

assigned 
to the 

accordingly: 
= II P(M;), (3) 

P(M) 

5.2 

GENERALIZED MEASURES 

measures 

as the pairwise 

We focus here on the generalized 
the KL divergence 
three generalized 
come the  Jensen-Shannon 
ward JS divergence 
"average 

introduced 
divergence 

KL divergence 

Then the 
in Section 
(JS), the back­

(BJS), and the measure 

measure. 

measures 

of pairs" 

(KL2). 

4 be­

we call 

that employ 

5.2.1 

Jensen-Shannon Divergence 

The JS divergence 
gain, and relates 

to the entropy 

in a standard 
way: 

is equivalent 

to the information 

to 

pertaining 

way, for JS divergence 

where M; are the sets of subgraphs 
the subdomain X;, i.e., P(X;IM) =  P(X;IM;). 
Put another 
(11;P(X;Iq,m))m�P(M) 
must hold. 
The fact that P(M) has to factor 
leads to a (pessimistic) 
size ICI. Given L independent 
gence can be additive  only 

=  f}; (P(X;Iq,m))m�P(M) 

lower bound on the committee 

subdomains, 

to be additive 

to Eq. 3 
according 

JS diver­

if 

L 

�II IM;I � 2£ 
ICJSI 

(4) 

D��P(M)(P(XIq,

m)) 
II(P(XIq,m')) P(M))) P(M) 

i=l 
where IM;I denotes 
graphs due to model uncertainty. 

-( DKL (P(XIq,m)
-I H(P(XIq

, m))) 
H(P(XIq))

rule holds, of course, 

4The Bayesian 

(1) 

the number of alternative 

sub­
The committee 
size 

when the distri­

\ 

P(M) 

If the Bayesian 
rule 

P(XIq, M) = ���) P(MIX, 
q) (2) 

by the intervention 

are conditioned on the query q. However, 
when 
do(q), the Bayesian 
rule 

butions 
q is replaced 
need not hold in general.  Concerning 
do(q) given parameter 
Eq. 2 holds for  interventions 
[15]. 
ularity 
independence 
Also note that 
P(Miq) =  P(M). 

and parameter 

Bayesian 

mod­

networks, 

472 

STECK & JAAKKOLA 

UAI2002 

has to grow exponentially 

therefore 
independent 
feasible 
achieve 

committee 
(full) additivity 

subdomains 

involving 
size is typically 

with the number of 
uncertainty. 
Any 
much too small to 

of the JS divergence. 

size of the committee 

of 
The minimal 
the domain size. This leads to a desirable 
of 
the committee 
computational 

size and can be expected 
advantage 

to provide a 
over the other two measures. 

is independent 

scaling 

Note that, analogously 
variability, 
However, 

but it does not measure 
holds 

the following 

relation 

to BJS, KL2 is a measure 

of 

information 

gain. 

D��P(M) 
D�-P(M) 

(P(XIq
, m)) 
(7) 
(P(XIq
, m)) 
(P(XIq
, m)) + D��P(M) 
together 
is given in the Appendix, 
with 
of the KL2 divergence. 
is an upper bound 

properties 
that the KL2 divergence 

The derivation 
additional 
also implies 
on the JS divergence, 
negative. 

since all the measures 

are non­

Eq. 7 

size necessary 
for the ad­

the fact that the KL2 

can be evaluated 

the small committee 
of the KL2 divergence, 

Besides 
ditivity 
divergence 
leads to efficient 
number of pairwise 
with the number of experts, 
ticularly 

computations 

efficient. 

evaluations 

a small committee 
is par­

in practice. 

Since the 

based on pairs of experts 

grows quadratically 

5.2.2 

Backward JS Divergence 

JS divergence 

The backward 
the JS divergence. 
ments in the KL divergence 

For BJS divergence 
are swapped, 

(BJS) is very similar 

the two argu­

to 

(5) 

D�/�P(M) (P(XIq
, m)) 
=  ( DKL ( (P(XIq, 
m)) P(M) IIP(XIq, 
=  DKL ( (P(XIq, 
m)) P(M) II(P(XIq, m'));:'t;)) 
( DKL (P(M)IIP(Mix,q))) P(XIq) 

m'))) P(M) 

to model space. 

extends 

information 

of variance, 
it does 
gain. Note that the above 

that the similarity 
is a measure 

illustrating 
While BJS divergence 
not measure 
derivation 
shows that BJS divergence 
as a KL divergence between 
the weighted 
weighted 

average ,(P(XIm))P(M)' and  the 
P(m) (where 
average, (P(XIm));:'t;) =  Tim P(XIm)
measures). 

we use KL divergence 

for unnormalized 

can be viewed 

arithmetic 
geometric 

6 

BJS divergence 

Like JS divergence, 
only if the committee 
given in Eq. 4. This follows 
metic averages 

taken inside 

size exceeds 

essentially 
the logarithm. 

can be additive 
the lower bound 
from the arith­

CHOOSING A COMMITTEE 

entire 

as the committee 

based on each measure 

If the class of models is tractable, 

we gave lower bounds on the 
of dis­
e.g., in 

or trees [9], one may use the 
In general, 
one can 
process 
of the learning 
A disadvan­

section, 
In the previous 
size of the committee, 
agreement. 
the case of a forest 
model space as the committee. 
expect that the stochasticity 
increases 
tage of increased 
in 
finding the unknown true model underlying 
may be decreased. 
stochasticity 
(myopic) learner 
model space is explored. 
portant 
at the beginning 
skewed initial 

may render the 
part of the 

This may be particularly 
of the learning 

On the other hand, an increased 

stochasticity 

is that the progress 

data may easily 

size shrinks. 

concerning 

the learner. 

more robust, 

the queries 

as a larger 

process 
when 

confuse 

the data 

im­

There are two obvious 
members: 

ways of selecting 

the committee 

• Markov Chain Monte Carlo (MCMC): find a com­

mittee based on the K highest-scoring 
[15, 10]. 

models 

• bootstrap 

j bagging: 

The K models comprising 

are drawn independently 

the committee 
other from the model space (with replacement
given the data 
seen so far. This can be achieved 

of each 

), 

D 

5.2.3 KL2 

Divergence 

represents 

KL divergence 

the disagreement 

a measure we refer to as KL2. When we 

The third measure quantifies 
terms of the  average 
experts, 
have independent 
KL2 divergence 
committee 

subdomains, 
is additive 

i.e., P(X) 
if each expert m on the 
this independence: 
to the previous 

in 
of all pairs of 
=  Ili P(Xi), 
P(XIm) 
measures, 
the distribution 

Ili P(Xdm). In contrast 
there is no constraint 
P(M) over experts. 
has to be large enough so that it can represent 
model uncertainty 
independently 
all uncertainty 
in the committee. 
size is hence given by the subdomain 
uncertainty: 

of each other. In particular, 
the over­
in the  domain  need 

A lower bound on the committee 
with the largest 

only 
the committee 
the 

not be represented 

Consequently, 

subdomains, 

of the different 

concerning 

in each 

= 

ICKL212: maxiMii:O:: 

(6) 

2. 

' 

UAI2002 

STECK & JAAKKOLA 

473 

by generating 
data 
V, 
from each sample 

K bootstrap 

samples 
learning 
and by subsequently 

1Ji from the 
a model mi 

(i =  1, ... , K). 

J)i 

to the bootstrap approach 
in finding a (small) 

We resort 
efficiently 
sents the search space. In the context 
Bayesian  networks, 
used successfully 
dence intervals. 

has been 
in [5] along with estimates 
of confi­

as it can be used 
that repre­
of learning 

the bootstrap  approach 

committee 

networks(e.g., 
presence 

of edges). 

reasonable 

when the posterior dis­

over the model space does not change much 
successive 
Such 

time steps of active 

learning. 

learned 

of previously 
This is particularly 
tribution 
between 
an improvement 
the noise introduced 
also considerably 
of the algorithm. 

enhance 

may not only lead to a reduction 

in 
but may 
by the search strategy, 
efficiency 

the computational 

confers 

is carried 

Sparsity 

learning 

are feasible. 

out on small data sets, 
models to be parsimo­

several 
an easy interpretation 

among the variables. 
In partic­
of the KL2 divergence 

net­
of Bayesian 
advantages. 
First, a 
of the 
Sec­

sim­
as for each pair of models 
DKL (P(XIq, 

When active 
one can expect the resulting 
nious to avoid over-fitting. 
work structures 
sparse graph facilitates 
dependencies 
qualitative 
ond, learning 
and inference 
ular, the computation 
plifies considerably, 
and m' the expression 
m)IIP(XIq
, m')) = 
P(x; I"J ,q,m)  b 
"'  "'  P(  1 I ) 
L-1. L...x "  ,•  Xj, 1rj,'lr1· q, m og P( ·I ,  ') can e 
x1 1rj,q,m 
I 
computed 
efficiently 
all instantiations 
small). 
can be approximated 
obtained 
graph). 
immediate 
experiments 
mately by importance 

one can explicitly 
sum over 
as the number of joint parents 
the marginals 
efficiently 

is 
P(xj, 1rj, 1rjldo(q), m) 
by a small sample size 
(given the mutilated 
does not lead to any 

of the JS divergence. 
In our 
the JS divergence 
approxi­

sparsity 
simplifications 

by forward-sampling 

Unfortunately, 

we evaluate 

sampling. 

Moreover, 

(i.e., 

1, 1, 3 

m 

SEARCH FOR OPTIMAL QUERY 

7 

has been determined, 
of 
learner 

the  aim 
is to find the query that 

In large do­
of disagreement. 
intractable 
to compute 
query. This is because 
of (query)  variables 

there 

in a do­

and each variable 
strat­

variables, 
Hence, some heuristic 

may 
search 
in the space of queries. 
For com­
we use a greedy approach 

in this 

N 

active 

subsets 

the measure 

it is typically 

Once a committee 
the (myopic) 
maximizes 
mains,  however, 
the score of every possible 
are 2N different 
main comprising 
have several 
states. 
egy has to be devised 
putational 
paper. This scheme proceeds 
number of query variables. 
scoring 
The greedy procedure 
not be increased 
current 
required 
vise this threshold 
in the scores, 

query. Moreover, 
to exceed a (small) 

which are computed 

terminates 
by adding another 

(and its state) 

efficiency, 

variable 

when the score can­
variable 
to the 
in each round is 

the increase 

threshold 

value. We de­

in rounds of ascending 

In each round the highest­
is added to the query. 

only approximately. 

of many small, 

subdomains, 

In large  domains  consisting 
dependent 
mately determined 
to the sub domains. 
may actually 

the optimal 
by the optimal 
In this context 

query is approxi­
queries 
the greedy strategy 

find close-to-optimum 
queries. 

nearly 

pertaining 

in­

BAYESIAN NETWORKS 

8 

value in order to account 

for noise 
9 

PRELIMINARY EXPERIMENTS 

of this paper, we study active 
networks. 

learn­
We assume that the vari­
distribution, 
and treat the 

In the remainder 
ing in Bayesian 
ables have a multinomial 
learning 
the posterior 
probability 
as described 
local search, 
In [3],  this 
approach 
observational 

task as an optimization 

problem: 

we optimize 
structure 

by 

of the network 
for  observational 

data in [7]. 

was extended 

to allow for both 

and experimental data. 

additional 

Its effect on the progress 

may introduce 

Local  search 
learning 
process. 
ing, however, 
finding the optimal 
lem [2]. It is conceivable 
be improved 

is hard to assess 

by enabling 

network 

of learn­
in large domains, 
since 
prob­
is an NP-complete 
strategy 
may 

that the search 
it to reuse various 

noise in the 

properties 

the Bayesian 

network 

given interventions. 

with the widely-used 

[1] and a highest-scoring 

from gene expression 

regulatory 
data along 
(factor-gene 
binding 

in­

was to recover 

The objective 
from which the data was sampled, 
We carried 
out experiments 
alarm-network 
network 
recovered 
with other information 
formation) 
crete variables 
edges. The regulatory 
56 edges, 
In our experiments 
ried out at most 200 learning 

[6]. The alarm network 

and each variable 

network 

sources 

with 2, 3, or 4 states 

comprises 
37 dis­
46 

and includes 

involves 

33 variables, 

was discretized 
we car­

with the alarm network, 

to 4 states. 

steps. The regulatory 

we compared 

from randomly 

only two models. 

To facilitate 

speedy evaluations, 

KL2 and JS divergences 

the active 
to passive 

learner 
learning 
chosen interven­
the committee 

In our experiments, 
employing 
as well as to learning 
tions. 
included 
time step in active 
minute on a 1.7 GHz P4 (with the simplest 
tation). 
the KL2 measure increases quadratically with 
mit tee size ( cf. Section 
may become prohibitive 

5. 2.3), large committee 
in large domains. 

Since the computational 

effort for evaluating 

In our experiments 

typically 

took about one 

learning 

implemen­

sizes 

here, a 

the com­

474 

STECK & JAAKKOLA 

UAI2002 

alga edge edge predictive 

accuracy: 

alga q.v. edg. edg. predictive 

accuracy: 

error entr. o I 1  5 10 

74 199 1.5 2.4 6.0 7.9 
p 
r: 2 var. 59 183 l.l 1.3 1.9 2.4 
r: 5 var. 52 167 1.2 1.2 1.4 1.5 
a: JS  56 174 1.2 1.2 1.3 1.5 
a: KL2  46 153 1.3 1.2 l.l l.l 
2 I 6 11 0.1 1 o.2 1 o.2 1 o.2 1 

I std 

of the learned model  structure 

given 0, 2, 5 and 10 interventions. 

the 

experiment: 

We assessed 
and the predic­

Table 1: Alarm network 
quality 
tive accuracy 
models were learned 
random queries 
ing using JS and KL2 divergences. 
obtained 
shows the standard 

ways: (p) passive, 
(r) 
learn­
All values were 
the bottom row 

by averaging over 
5 trials; 

of 2 or 5 variables 

deviation. 

and (a) active 

in different 

The 

network 

Table 2: The quality 
ulatory 
tions, 
restrictions 
learning 
process. 

err. entr. 0  1  5  10 
p 
37 65 1.8 2.4 3.8 4.2 
1 30 54 1.7 1.9 2.5 2.7 
r 
5  21  33 1.5 1.6 1.7 1.7 
r 
a: JS �1  24 43 1.4 1.5 1.8 2.0 
a:KL2 < 1  27 42 1.6  1.6  1.8 
1.9 
a: JS 
21 37 1.4 1.4 1.5 1.5 
a:KL2  16 26 1.6 1.6 1.5 1.3 
1 I 2 11 o.2 1 o.2 1 o.2 1 o.2 1 
of the models learned 
in the reg­
regarding 

experiment; 

the abbrevia­

I std 

see Table 1; the column (q.v.) indicates 

on the number of query variables 

the 
in the 

over all possible  interventions  in­

network, 
surements, 

derived 
up to 300 queries. 

originally 

involved 

from 320 expression 

mea­

by the edge 

network 

approach 

measures 

accuracy 

the learned 

by the active 

are actually 

We also assessed 

learners 
aimed at optimiz­

not directly 
quantities. 
of the learned 
to predict 

structures 
and the edge error as defined in [15]; we com­
based on a bootstrap 
(cf. also [5]). Note that the di­
in 
employed 

We assessed 
entropy 
puted these quantities 
of 200 subsamples 
vergence 
this paper 
ing these edge-wise 
predictive 
[10]. Besides 
bution over the variables, 
ability 
tified this in terms of the KL divergence 
original 
the KL divergence 
volving 
domly chosen interventions 
spectively. 

distri­
a model's 
we also assessed 
We quan­
of interventions. 
between 
the 
model and the  learned  model. 
We averaged 

the 
models, 
to 
the marginal 

as well as over 100 ran­
re­

one or two variables, 

on 5 and 10 variables, 

similarly 

the ability 

to predict 

the effects 

to be "nearly" 

additive 
when 

contain 
several 
In contrast, 

"nearly" 

indepen­

the small committee 

gence can be expected 
the sparse networks 
dent components. 
size causes 
experiments, 
tween the scores 
Due to the relatively 
the active 
the smallest 
periments. 

leading 

the JS divergence 

to be subadditive 

to relatively 

of different 

queries 

in our 
small margins 
be­
5). 

(cf. Section 

large number of query variables, 

learner employing 
edge error and edge entropy 

the KL2 divergence 

yields 

in both ex­

the reliability 

of individual 

edges in 
we took a 

regulatory  network.  Again, 

KL2 divergence 

based on 200 subsamples. 
resulted 

The ac­
in 31 
13 edges with P > 50% 
these 48 

4 edges with P  > 30%.  All 

approach 

employing 

We also examined 
the recovered 
bootstrap 
tive learner 
edges with P > 90%, another 
and additional 
edges are indeed present 
ture. We examined 
missing 
that their absence 
(posterior 
because 
in the posterior 
fitting 

is favored 
probability) 

when the training 

of the penalty 

in the learned 

probabilit

networks 

set is small. 

on data sets of size 300; this is 
for model complexity 
y, which helps avoid over­

inherent 

(P < 10%), and found 

by the scoring 

function 

in the original network 
the 8 edges that were erroneously 

struc­

in the Tables 1 and 2. It is 

as well as the edge  en­

considerably 

by active 

learn­

the number of vari­

simultaneously 

appears 
of the learned 

to have a crucial 
models. 
chosen queries, 

but also to 

This ap­

in the alarm network 
employing 

the JS divergence 
inter­

experiments, 

in large networks 

are depicted 

that the edge error 

The results 
obvious 
tropy could be reduced 
ing. Moreover, 
ables queried 
impact on the quality 
plies not only to randomly 
the active 
the active 
vened on 1.6 variables 
steps, 
The average 
ables on average. 
ables in the regulatory 
network 
and 7.2, respectively. 
in terms of the additivity 
even with only two committee 

learner: 
learner 

while KL2 divergence 

on average 

involved 
numbers 

over the 200 time 
9.2 query vari­
of query vari­
experiments 
were 2.1 

This difference 

property 

can be explained 
(cf. Section 
5): 
KL2 diver-

members, 

learning 

obviously 

accuracy 
when it comes to predicting 

to pas­
compared 

interventions. 
if the num­

chosen queries: 

This also applies 

(not exceeding 

some 

active 

the graph structure, 

of several 
from randomly 

the predictive 
in particular 

Besides 
also improves 
sive learning, 
the effects 
to learning 
ber of query variables 
optimal 
that, compared 
The results  suggest 
some predictive 
KL2 favors 
few simultaneous 
interventions 
racy concerning 
several 
this behavior 

is increased 

becomes 

value), 

trading 

for an improved 
accu­
interventions. 
We found that 

more pronounced 

with smaller 

to JS divergence, 
accuracy 

from a 

also improves. 
the predictive  accuracy 

UAI2002 

STECK & JAAKKOLA 

475 

These differences 

Acknowledgements 

Moreover, 

since KL2 

this 

structure 

the model structure, 
is particu­
of numerous 
this low er­

the effects 

low errors  concerning 

network 

however. 

that the "right" 

for prediction 

after 100 time steps. 

data sets, e.g., 
are not significant, 
yields 
suggests 
larly important 
interventions. 
ror in model structure 
during the learning 
able for parameter 
slightly 
increased 
number of interventions, 
seems to be less important. 

prediction 

process, 

estimation. 

by querying 

there is less data avail­
This may explain 
error concerning 
a small 

the 

where the model structure 

several 

variables 

As KL2 divergence achieves 

from gene expression 

for 
Hartemink 
us with the highest-scoring 
Bayesian 
net­
data [6], and the 
comments. 

We would like to thank Alexander 
providing 
work obtained 
anonymous  reviewers 
for valuable 
ald Steck acknowledges 
support 
search 
Tommi Jaakkola 
Telegraph 
ITR grant IIS-0085836. 

supported 
Corporation 

Har­
from the German Re­

acknowledges 

Foundation 

and Telephone 

and from NSF 

from Nippon 

(DFG) under grant STE 1045/1-1. 

to the number of query variables, 
and which of their states 

which of the variables 

it also 

In addition 
matters 
are chosen. 
ables randomly 
active 
latter 

learner 
queries 

leads to about the same  results 

In our experiments, 

picking 

5 query vari­
as the 
the 
fewer variables. 

JS divergence, 

employing 
considerably 

although 

References 

[1] I. A. Beinlich, 

H. J. Suermondt, 

R. M. Chavez, 
and G. F. Cooper. The alarm monitoring 
tem: a case study with two probabilistic 
techniques 
In Proceedings 
Intel­
the 
on Artificial 
1989. 
ligence 

European 
in Medicine, 

pages 247-56. 

networks. 

Conference 

for belief 

London, 

sys­

2nd 

of 

inference 

experiments 

to be able to 

genes simultaneously, 

we also 

with queries 

learner 
In this case we can easily 

restricted 

to at 

find the 

the active 

on numerous 

query with respect 
is relevant 

As it is rare in biological 
intervene 
studied 
most one variable. 
optimal 
(sub )additivity 
queries. 
JS divergence, 
performs 
slightly 
sure. The quality 
relevance 
several 

better 
of the learned 

It is not surprising 
as a valid measure 

[2] D. M. Chickering. 

Learning 

Bayesian networks is 

to the JS divergence 
since 
only with multiple-variable 

In Proceedings 

NP-complete. 
Workshop 
pages 121-30, 1996. 

on Artificial 

of the International 
gence and Statistics, 

Intelli

that in this  context 

the 

of information 

gain, 

[3] G. F.  Cooper 

than the surrogate 
KL2 mea­

models along with the 

of KL2 could be improved 

by intervening 

on 

genes simultaneously. 

and C. S. Yoo. Causal discovery 
of experimental 

and observational 

of Uncertainty 

in Artificial 

from a mixture 
data. In Proceedings 
Intelligence, 
1999. 

pages 116-25.  Morgan Kaufmann, 

CONCLUSIONS 

10 

setting 

the Query by Com­

to unsupervised 

among the committee 

to computational 

efficiency. 
to quantify 

that additivity 
computations, 
size. The standard 
requires 

In this paper we have generalized 
with spe­
mittee approach 
We exam­
cial attention 
ined three divergence measures 
the degree 
members and 
of disagreement 
demonstrated 
is cru­
cial for feasible 
in terms of 
the committee 
( JS divergence) 
exponentially 
surrogate 
fore can be evaluated 
a small committee. 
a bootstrap 
approach 
selecting 
independently 

with the size of the domain, 
additive 
-even when based on 

of the measure 
particularly 
gain 
size that grows 

are drawn 
from the model space given the data. 

Apart from that, we showed that 
tool for 

can serve as an efficient 

a small committee, 

KL2  measure 

a committee 

as the members 

efficiently 

-and there­

remains 

information 

while  our 

involve 

are possible. 
only a very limited 

our 
of 
size and its effect on the quality 
of the 

A number of extensions 
experiments 
the committee 
queries. 
guarantees 

developing 
measures. 

for  additive 

We are  also 

stronger 

For example, 

exploration 

[4] Y. Freund, 

H. S. Seung, 

E. Shamir, 

and N. Tishby. 

Selective 
algorithm. 

sampling 

using  the 

query by committee 

Machine Learning, 

28:133-68, 
1997. 

and A. Wyner. 

M. Goldszmidt, 
with Bayesian 

[5] N. Friedman, 
Data analysis 
strap approach. 
itors, 
of the 
certainty 
Morgan Kaufmann, 

a boot­
networks: 
In K. Laskey and H. Prade, ed­
Conference 

Proceedings 

1999. 

on Un­

15th 

in Artificial  Intelligence, 

pages 196-205. 

[6] A. J. Hartemink, 

D. K. Gifford, 

T. S. Jaakkola, 

and R. A. Young.  Combining 
location 
and ex­
discovery 
pression 
regulatory 
on 
Biocomputing, 

data for principled 

networks. 

In Pacific Symposium 

2002. 

of genetic 

[7] D. Heckerman. 

Bayesian 
ing in Graphical 
Academic 

A tutorial 
In M.l. Jordan, 

on learning 
with 
Learn­
Kluwer 
The Netherlands, 

Models, pages 301-54. 

editor, 

Publishers, 

networks. 

1996. 

theoretical 

[8] N. F.G. Martin and J. W .  England. 
Wesley, 

Theory of Entropy. 

Addison-

1981. 

Mathematical 

476 

STECK & JAAKKOLA 

UAI2002 

[9] M. Meila and T. Jaakkola. 

Tractable 

Bayesian 

learning 
and M. Goldszmidt, 

networks. 
editors, 

of tree belief 

Proceedings 

of the 

In C. Boutilier 

Conference 

16th 
telligence, 

pages 380-8. 

on Uncertainty 

Morgan Kaufmann, 

in Artificial 
2000. 

In­

To make the discussion 
more complete, 
rewriting 

let us briefly 

this measure: 
D��2P(M)(P(X!m)) 

=  (L [P(xlm)-(P(x!m')JP(MJ 

X 

concerning 

KL2 divergence 
another 
way of 

mention 

(9) 

!m)J P(M) 
·logP(x

=  ( L [P(x!m)-(P(x!m'))P(M)] 
1  P(x!m) J 
. og (P(x!m'))P(M) P(M) 

X 

is taken concerning 

average 

to the JS di­
each of the ex­

here the difference  between 

of the log-term, P(x!m), rather 
to the log-terms. 
This is obtained 
the expression  which 

The second line shows that, in contrast 
vergence, 
perts and the arithmetic 
the weights 
respect 
panding 
and then renaming 
9 follows 
trates 
Eq. 7 in the text). 

immediately 

from the second line, and illus­
again that KL2 is the sum of JS and BJS (cf. 

The last line in Eq. 

the indices. 

than with 

by first ex­

defines KL2 divergence, 

[10] K. Murphy. Active  learning 

of causal 

Bayes net 
University 
of Califor­

report, 

structure. 
nia, Berkeley, 
2001. 

Technical 

[11] J. Pearl. Causality: 

Cambridge 

Models, 
University 

Press, 

Reasoning, 

New York, 

and In­

ference. 
2000. 

[12] H. S. Seung, M. Opper, and H. Sompolinsky. 

Query by committee. 
Computational 
1992. 

Proc. fifth Workshop 

on 

Learning Theory, pages 287-94, 

[13] P. Spirtes, 

C. Glymour, 

and R. Scheines. 

Cau­
Springer 

Lecture 

Prediction, 

sation, 
Notes in Statistics 

and Search. 
81, 1993. 

[14] S. Tong and D. Koller. 

Active learning 

rameter 
in Bayesian 
vances in Neural Information 

estimation 

Processing 

networks. 

for pa­
In Ad­
Systems 

(NIPS), 2001. 

13 

[15] S. Tong and D. Koller. 

Active 
networks. 

for struc­
In Proceedings 

learning 

ture in Bayesian 

Joint Conference 

of the 
on Artifi­

International 
17th 
cial Intelligence, 
2001. 

pages 863-9. 

Morgan Kaufmann, 

APPENDIX 

The KL2 
including: 

divergence 

can be rewritten 

in various 

ways, 

D��2P(M)(P(X!m)) 
( (nKL(P(XIm
(nKL(P(XIq

)IIP(XIm'))) P(M) P(M) 
,m)II<P(XIq,m'))�t=))) P(M) 

(8) 

= 

= 

disagreement 

to BJS divergence 

can 

5.2.2). 

similarly 

of the average 

each of the experts 

It shows that KL2 divergence 

The last line is obtained 
(cf. Section 
be viewed as a measure 
between 
metric average.  Qualitatively, 
vergence 
a geometric 
Eq. 7 in the text now follows 
and 5, where the arithmetic 
divergences 

geo­
from JS di­
average 
with 

from summing 
averages 
inside 

up Eqs. 1 
of the KL 

only by replacing 

and their weighted 

cancel out. 

an arithmetic 

average. 

this differs 

Based on the above derivation, 

