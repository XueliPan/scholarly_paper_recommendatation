434

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 17, NO. 5, MAY 2006

Providing Service Guarantees in

High-Speed Switching Systems with

Feedback Output Queuing

Victor Firoiu, Member, IEEE, Xiaohui Zhang,

Emre Gu¨ ndu¨ zhan, Member, IEEE, and Nicolas Christin, Member, IEEE

Abstract—We consider the problem of providing per-customer service guarantees in a high-speed packet switch typically situated at
the edge between a set of customers and a service provider network. As basic requirements, the switch should be scalable to high
speeds per port, a large number of ports, and a large number of customers (macroflows) with independent guarantees. Existing
scalable solutions are based on virtual output queuing, which is computationally complex when required to provide service guarantees
for a large number of macroflows. We present a novel architecture for packet switching that provides support for such service
guarantees. A cost-effective fabric with small external speedup is combined with a feedback mechanism that enables the fabric to be
virtually lossless, thus avoiding packet drops indiscriminate of macroflows’ behavior. Through analysis and simulation, we show that
this architecture provides accurate support for service guarantees, has low computational complexity, and is scalable to very high port
speeds.

Index Terms—Computer networks, packet switching, quality of service, feedback control, congestion control.

(cid:2)

1 INTRODUCTION

HIGH-SPEED communication between businesses has been

a large share of the telecommunications market in
recent years. This communication needs to be of high
quality, secure, and reliable. Traditionally, these services
were provided using ATM and frame-relay technologies,
but at a premium cost. Recent advances in traffic engineer-
ing and the advent of voice-over-IP (VoIP) technologies
provide an opportunity to carry all enterprise traffic (voice,
streaming, and non-real-time data) at a lower cost. Virtual
private networks (VPNs)
[1] and virtual private LAN
services (VPLS) [2] are two examples of such network
services. A main requirement for such services is to provide
quality of service (QoS) guarantees. Interactive media such
as VoIP needs low delay and low loss; other traffic needs
minimum throughput guarantees.

In this paper, we consider the problem of providing
such guarantees in a high-speed, cost-effective switch at
the interface (edge) between enterprise and service
provider networks. At a minimum, the switch is required
to provide three types of service: Premium, Assured, and

. V. Firoiu is with BAE Systems, Advanced Information Technologies, 6 New

England Executive Park, Burlington, MA 01803.
E-mail: victor.firoiu@baesystems.com.

. X. Zhang is with Airvana, 19 Alpha Road, Chelmsford, MA 01824.

E-mail: xzhang@airvananet.com.

. E. Gu¨ndu¨zhan is with Nortel. E-mail: egunduzh@nortel.com.
. N. Christin is with Carnegie Mellon University, Information Networking
Institute, CyLab Japan, Kobe Harbor-land Center Building 17F, 1-3-3
Higashikawasaki-cho, Chuo-ku, Kobe 650-0044, Japan.
E-mail: nicolasc@andrew.cmu.edu.

Manuscript received 11 June 2004; revised 2 March 2005; accepted 20 June
2005; published online 24 March 2006.
Recommended for acceptance by W. Zhao.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number TPDS-0143-0604.

Best Effort [3], [4]. Premium service provides low loss and
small delay for a flow sending within a predetermined rate
limit (anything above the limit
is discarded). Assured
service guarantees delivery for traffic within a limit, but
allows and forwards extra traffic within a higher limit if
transmit opportunities are available.

A provider edge switch is required to differentiate
between different customers and provide separate guaran-
tees to each customer. We call the totality of traffic of a
given customer subject to a specific service level agreement
(SLA) a macroflow. A macroflow is, therefore, an aggregate
of possibly many TCP connections and/or UDP application
streams, which we will call microflows. While there are far
fewer macroflows than there are microflows, the switch
must be able to accommodate a relatively large number (in
the order of hundreds or even thousands) of macroflow
guarantees per port, where each port must support speeds
on the order of several Gbps. Traffic from one customer can
enter through one or multiple ingress ports and exit
through one or multiple ports. On the other hand, to come
up with practical solutions, we assume that the provided
service guarantees only need to be enforced over timescales
in the order of a few milliseconds, which is within the
requirements of most applications, thereby alleviating the
traditional requirement that service guarantees have to be
enforced over timescales as small as a single packet
transmission time. We consider the problem of providing
1-to-1 and N-to-1 services, which correspond to traffic
entering through one ingress port and exiting through one
egress port, and to traffic entering through N ingress ports
and exiting through one egress port, respectively.1 Indeed,
1-to-N (entering through one ingress port, exiting through

1. 1-to-1 and N-to-1 services can also be referred to as “pipe” and “funnel

scope,” as defined in [5].

1045-9219/06/$20.00 (cid:2) 2006 IEEE

Published by the IEEE Computer Society

FIROIU ET AL.: PROVIDING SERVICE GUARANTEES IN HIGH-SPEED SWITCHING SYSTEMS WITH FEEDBACK OUTPUT QUEUING

435

N egress ports) and N-to-N (entering through N ingress
ports, exiting through N egress ports) can be provided as
combinations of 1-to-1 and N-to-1 services. In the case of
Assured N-to-1 service, it is also desirable to provide a fair
distribution of service among the N components of the
macroflow.

Current state-of-the-art switch architectures are based on
virtual output queuing (VOQ), which requires a fabric
speedup s (cid:2) 2 and a matching algorithm to find which
packets are sent
into the fabric at each fabric cycle.
However, realizing a speedup of s (cid:2) 2 may be impractical
at very high line speeds (> 10 Gbps) given the limitations
on memory access speeds. Furthermore, even though some
of the VOQ architectures can support service guarantees, a
major problem is that the matching algorithms have high
complexity, are run at each fabric cycle, and all virtual
output queues at all input lines in the system need to
participate in a centralized algorithm [6].

To provide a low-complexity switch architecture that
fulfills the above requirements, we observe that the main
cause for high complexity in current architecture resides in
the necessity of addressing congestion at an output line.
Short-term congestion can be absorbed by buffers, whereas
long-term congestion results in packet loss. We also observe
that many measurement studies (for example, [7]) have
shown that traffic on the Internet is dominated by the TCP
protocol, which accounts for about 90 percent of all traffic.
A salient feature of TCP is that packet transmission is
controlled by a congestion avoidance algorithm [8]. As an
effect, the average sending rate of a TCP microflow is a
decreasing function of drop probability and of round-trip
time. (See [9] for a quantitative evaluation of this function.)
In practice, TCP flows have a stable (long-term) operation
when the drop probability is between 0 and 0:1, corre-
sponding to loss rates less than 10 percent, and very rarely
operate above 0:2 [9]. Heavy long-term congestion that
results in a drop probability above 0:2 can be produced by
non-TCP (and more generally, non-congestion-controlled)
traffic such as multimedia traffic over UDP.

Our proposed architecture, named “Feedback Output
Queuing” (FOQ), exploits these observations by efficiently
supporting fast fabrics with relatively slow output memory
interfaces and, hence, a small effective speedup. For
example, a speedup of 1:25 at the fabric-to-line interface is
sufficient to maintain an output drop probability up to 0:2
for traffic flows fully utilizing this interface. For higher
levels of long-term congestion (e.g., drop probability above
0:2), the FOQ architecture uses a feedback mechanism to
reduce the traffic volume before it enters the switch fabric.
Specifically, FOQ monitors the output link usage of each
macroflow, and, at the input links, proactively discards
packets from macroflows that are exceeding their allocated
the FOQ
bandwidth at
links. That way,
mechanism limits the buildup and delay at
the fabric
buffers. In addition, the FOQ mechanism provides support
for the Assured service 1-to-1 and N-to-1 scope.

the output

As far as Premium traffic is concerned, given that rate
guarantees are ensured to be within switch capacity by
some admission control procedure, policing Premium
traffic at its guaranteed rate at the ingress guarantees that
Premium traffic cannot create congestion in the absence of

other types of
traffic. Thus, Premium service can be
provided through a simple priority scheduling in OUT
ports and fabric, bypassing the FOQ mechanism.

In the following, we show through analysis and
simulation studies that the proposed FOQ architecture can
alleviate congestion at the output lines of an output queued
switch with slow output memory interface and can thus
provide deterministic QoS guarantees. FOQ requires only a
modest speedup (e.g., 1.3) at the output interface of the
switch. The congestion-control algorithm in the FOQ
architecture is fully parallelized at the input and output
lines, requiring Oð1Þ complexity at each input and output
line. This low complexity enables implementation of the
FOQ architecture at very high line rates (> 10 Gbps).

The rest of the paper is organized as follows: In the next
section, we discuss the related work in more detail. Then,
we give a detailed description of the FOQ architecture in
Section 3. In Section 4, we develop an analytical model for
FOQ, based on a proportional-integral (PI) controller, and
analyze its performance under step-shaped traffic bursts,
before introducing a quantized version of a PI controller.
We present our simulation results in Section 5 and conclude
the paper with a comparison between FOQ and VOQ in
Section 6.

2 RELATED WORK

Several switch architectures with QoS capabilities have
been proposed in the literature, with particular advantages
and shortcomings.

An early architecture is output queuing (OQ). An OQ
switch having N inputs and N outputs with each line of
speed c bits/second requires a switching fabric of speed Nc,
i.e., a speedup s ¼ N. In this case, no congestion occurs at
the inputs or at the fabric, only at the output lines. To
manage congestion and provide QoS support, a set of
queues and a scheduling mechanism are implemented at
each output. The main advantage of this architecture is that
it can provide QoS support with simple mechanisms of
queuing and scheduling, but the main problem is that the
fabric speedup of N can be impractical. In fact, current
technology enables fast interconnection networks operating
at current high-speed line rates and with typical number of
lines (for example, c ¼ 10 Gbps and N ¼ 16), but writing the
packets coming out of the interconnection network into
output buffers at high speeds remains a problem. In other
words, although the fabric may have an internal speedup of
N, the effective speedup seen at an output buffer is limited
by the memory write speed, which is usually much less.

An alternative to OQ is virtual output queuing (VOQ)
[10], [11], which requires a smaller fabric speedup, such as
s in the range between 2 and 4. Unlike OQ, VOQ requires a
matching algorithm to find which packets will be sent into
the fabric at each fabric cycle. There are quite a few such
algorithms proposed in the literature, which are based on
parallel iterative matching, time slot assignment, maximal
matching, or stable matching (see [6] and references
therein). Some of these algorithms can also support service
guarantees. The advantage of VOQ is its ability to switch
high-speed lines with low fabric speedup. However, its
main problems are that
the matching algorithms are

436

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 17, NO. 5, MAY 2006

complex (OðM 2N 2Þ, where M is the number of independent
service guarantees per port and N is the number of ports),
have to be run at each fabric cycle, and all VOQs at all input
lines in the system need to participate in a centralized
algorithm. We note that output queued switches can also be
perfectly emulated by combined input-output queued
(CIOQ) switches with a speedup s (cid:2) 2 [12]. Unfortunately,
the arbitration algorithm has a computational complexity of
OðN 2Þ, which can be reduced to OðNÞ, but in that case, the
space complexity becomes linear in the number of cells in
the switch. Therefore, emulating an OQ switch by a CIOQ
switch or a VOQ switch appears to have limited scalability.
In recent years, these potential scalability concerns have
been addressed by implementing a very small number of
independent service guarantees. Under the differentiated
services framework [13], microflows are aggregated in
M ¼ 6 traffic classes, and service guarantees are offered
for classes. The downside is that the realized QoS per
microflow has a lower level of assurance (higher probability
of violating the desired service level) than the QoS per
aggregate [14], [15]. Moreover, recently proposed VPN and
VLAN services [16], [17] require per-VPN or VLAN QoS
guarantees. All
the above are arguments in favor of
implementing a number of independent service guarantees
per port much larger than six.

More recent proposals [18] decrease the time interval
between two runs of the matching algorithm, but with a
trade-off in increased burstiness and additional scheduling
algorithms for mitigating unbounded delays. Moreover, the
service presented in [18] is of type premium 1-to-1, but
cannot provide assured N-to-1 service.

Last, similar to the FOQ architecture proposed in this
paper, the IBM Prizma switch architecture [19] uses a
shared memory and no centralized arbitration algorithm.
However, Prizma relies on on-off flow control, while the
feedback scheme proposed in the present paper dynami-
cally controls the amount of traffic admitted into the fabric,
and FOQ feedback is based on the state of the output
queues, while Prizma relies on the state of internal switch
queues. Both the origin of the information and the dynamic
control of the drop level lead us to believe that FOQ can use
the capacity available in the switch more efficiently.

3 FEEDBACK OUTPUT QUEUING ARCHITECTURE

We consider a switch as in Fig. 1 with a fabric having
internal speedup of N and an internal buffer capability.2 We
also assume that the fabric has one or a very small number
of queues per port.
In the following, we present an
architecture for providing per-macroflow service guaran-
tees where the number of macroflows per port M is large,
that is, M (cid:3) 1.

Packets enter through a set of N input ports of speed c.
As a packet is received at port i, a destination port j is
determined by a routing module.

Given the stated objective of providing per-customer
the switch needs classifiers to separate between

SLA,

Fig. 1. Detailed FOQ switch architecture.

customer macroflows. Note that the presence of classifiers
is required as soon as per-customer differentiation is
desired, independently of FOQ or any other differentiation
mechanism. Fast classifiers are readily available in the
literature (e.g., [20], [21], [22]), and can be used to determine
the QoS macroflow k of an incoming packet. An internal
identifier denoting the macroflow k is included in the
internal packet header used in the switch, so that classifica-
tion primitives used in later stages of the switch can be
performed through a single lookup operation.

the packet

is discarded.

If not discarded,

After classification, an IN dropper determines if the
is
packet
transmitted to the fabric through a line of speed sc. We
assume a fabric with internal speed of Nsc, i.e., at each
fabric cycle one packet from each IN line can be moved to
an OUT line while sustaining speeds of sc from all IN lines.
Multiple (up to N) packets can be received at an OUT line in
one cycle, and in that case the packets are placed in a fabric
queue F Qj corresponding to the destination line j.

Packets are forwarded by the OUT line j at speed sc,
separated into OUT queues fOQj;kgk based on their QoS
flow, and scheduled for transmission to OUT port j of
speed c. The OUT scheduling implements various service
guarantees such as priority, minimum rate guarantee,
maximum rate limit, and maximum delay guarantee. This
OUT scheduling results in a certain service rate (in general,
variable in time) for each OUT queue.

If traffic to OQj;k has a rate higher than the current
service rate of macroflow k, packets accumulate in this
queue and some of them may be dropped by a queue
management mechanism such as drop-tail or RED (see
[23] for details). If the traffic to all queues at OUT line j
amounts to an aggregate rate above sc,
then packets
accumulate at
If
this situation
persists, F Qj fills and packets get dropped in the fabric.
In this case, QoS guarantees for some macroflow k may
be violated since fabric drops do not discriminate
between different macroflows.

the fabric queue F Qj.

We define the relative congestion at a queue

C ¼ 1 (cid:4)

rO
rI

;

ð1Þ

2. This fabric has a cost-effective implementation using shared memory
technology. The case of zero/small memory fabric with no/small internal
speedup is a separate problem, and we report our study elsewhere.

where rI and rO are traffic rates input to and output from
the queue respectively. It is easy to see that, as long as the
traffic coming out of OUT line j is such that the relative

FIROIU ET AL.: PROVIDING SERVICE GUARANTEES IN HIGH-SPEED SWITCHING SYSTEMS WITH FEEDBACK OUTPUT QUEUING

437

Another measure is the relative congestion during the
interval T , similar to (1):

RelCongðT Þ ¼ 1 (cid:4) OutP ktsðT Þ=InP ktsðT Þ:

Observe that, unlike the drop probability,
the relative
congestion takes into account the variation of the queue
size during T . Since the FOQ objective is to keep the traffic
rate at the fabric interface below a critical
it is
apparent that the relative congestion is more effective in
controlling that traffic rate. This is confirmed by the model
in Section 4 and the simulation in Section 5.

level,

We consider a discrete proportional-integrator (PI) [25]
for the feedback control algorithm. In Section 4, we derive
its configuration from stability conditions. The PI algorithm
outputs a value of drop probability between 0 and 1
transmitted to the IN droppers every interval.

An implementation issue is the data rate of feedback
transmission. Considering K macroflows at each of the
N OUT ports and that the drop information is coded in
F bits, the total feedback data rate is KNF =T . For example,
for K ¼ 1; 000, N ¼ 32, F ¼ 8, T ¼ 1 ms, the feedback data
rate is 256 Mb/s. It is possible to reduce this rate by
reducing the precision of the feedback data and, thus, its
encoding. In an extreme case, the feedback has three values:
increase, decrease, or keep same drop level. All IN modules
use this indication in conjunction with a predefined table of
drop levels. We call this the “Gear-Box algorithm” (GB),
model it in Section 4 and show its performance in Section 5.

4 A CONTROL THEORETICAL MODEL FOR

FEEDBACK OUTPUT QUEUING

theoretical approach.

In this section, we develop an analytical model for the FOQ
architecture by a control
In our
analysis, we use a classical discrete PI controller to adjust
the drop rate of each macroflow. We simplify our analysis
by assuming only a single macroflow at first, and later
discuss how and under what conditions our results may
apply to the general multiple macroflow case. We also
assume in our analysis that there is no limitation to the
capacity of the feedback channel in the system. We then
show that an efficient algorithm for limited-capacity feed-
back channels can be obtained by quantizing the control
decisions of the PI controller, which we call the Gear Box
algorithm.

The basic control structure at a particular OUT port j and
for a particular macroflow k is shown in Fig. 3. If there are a
total of K macroflows in each OUT port, then each OUT
port has K such controllers. All variables we use in this
section are for the aggregate traffic in macroflow k originat-
ing from all IN ports and destined to OUT port j, unless we
note otherwise (i.e., we do not use the subscript ðj; kÞ for
notational convenience). (cid:2) is the total arrival rate for traffic
destined for the OUT queue OQj;k. A total portion of the
arriving traffic is dropped at the IN droppers at a rate (cid:3), so
that the surviving portion goes into the fabric queue F Qj at
a rate u ¼ (cid:2) (cid:4) (cid:3). This traffic shares the fabric queue with
other traffic destined to OUT line j, and then it is delivered
to OUT dropper ðj; kÞ at a rate r. In the analysis, we assume

Fig. 2. Illustration of the evolution of the relative congestion C as a
function of rI , for rO fixed (rO (cid:4) 1).

congestion Cj;k at each queue fOQj;kgk is below a threshold
dmax < 1 (cid:4) 1=s, and the OUT port j is utilized at its full
capacity c, then the traffic throughput at the interface of
fabric to OUT line j is below sc and, thus, there is no
congestion at that interface and no fabric drop.

We plot the evolution of C as a function of rI in Fig. 2, for
rO fixed at rO ¼ 1. When rI < rO, C < 0, and the output
queue is being drained. When rI > rO the relative conges-
tion increases, with C ! 1 as rI ! 1. Our goal
is to
maintain the relative congestion at a “moderate level” when
the output queue is congested. In the figure, moderate
congestion is represented as the area in which 1 < rI < 1:2,
roughly corresponding to 0 < C < 0:17.

In the FOQ architecture, a feedback mechanism is
introduced to control the relative congestion at each OUT
queue below a threshold. When the relative congestion at
an OUT queue increases, the feedback mechanism instructs
the input modules to drop a part of the traffic destined to
this queue. By keeping the traffic below a congestion
threshold, the fabric drop is avoided. Thus, packets are
dropped only from those macroflows that create congestion,
and the QoS guarantees are provided to all macroflows as
configured.

It is worth noting that the macroflows having packets
dropped at ingress by FOQ would have packets dropped in
the same amount at egress in the case of an ideal output
queuing with speedup of N. Thus, FOQ reduces the
demand of fabric throughput by eliminating the need for
forwarding packets that are later discarded.

3.1 Realizations of FOQ
We next consider options for a practical realization of the
FOQ architecture. More precisely, we consider implementa-
tions of FOQ as a discrete feedback control system. A
certain measure of congestion is sampled at intervals of
duration T at each OUT queue. A control algorithm
computes a drop indication based on the last sample and
an internal state and transmits it to all IN modules. There,
packets of the indicated macroflow are randomly dropped
with a probability that is a function of the drop indication.
We have several ways to measure the congestion at a
queue. A simple method is to compute the average drop
probability at the queue during the sampling interval:

DropP robðT Þ ¼ DroppedP ktsðT Þ=InP ktsðT Þ:

438

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 17, NO. 5, MAY 2006

e½n(cid:5) ¼ r½n(cid:5) (cid:4) ropt½n(cid:5) between the average fabric output rate,
r½n(cid:5), and the desired fabric output rate, ropt½n(cid:5),

(cid:3)½n(cid:5) ¼Ke½n(cid:5) þ KI

e½m(cid:5);

n

X

m¼0
¼Kðr½n(cid:5) (cid:4) ropt½n(cid:5)Þ

þ KI

r½m(cid:5) (cid:4)

ropt½m(cid:5)

 

n

X

m¼0

n

X

m¼0

!
:

K and KI are constants that are used to tune the impact of
the proportional and integral factors in the error correction.
For instance, KI ¼ 0 would mean that the drop rate is
chosen proportionally to the error e½n(cid:5), while K ¼ 0 would
mean the the drop rate is chosen proportionally to the
integral of the error since the beginning of time. We ensure
that the closed-loop system is stable, that is, in steady-state
e½n(cid:5) ! 0 as n ! 1, by picking specific values for K and KI
that satisfy a stability condition. The stability condition is
itself derived by examining the step response of the system,
which we discuss next.

We investigate the step response of the system by
presenting two step inputs to the system: (cid:2)½n(cid:5) ¼ (cid:2) and
ropt½n(cid:5) ¼ Ropt, for n (cid:2) 0. The magnitude of the arrival rate
can in general be larger than the maximum fabric output
rate, i.e., (cid:2) > sc. In this case, the fabric output will be
constant at r½n(cid:5) ¼ sc for an initial period 0 (cid:7) n < N0. During
this period the fabric queue will always be nonempty and
the controller cannot sense the actual magnitude of the
arrival rate. Therefore, the controller output will increase
linearly:

(cid:2)

(cid:3)½n(cid:5) ¼

0
Kðsc (cid:4) RoptÞ þ ðn þ 1ÞKIðsc (cid:4) RoptÞ otherwise:

if n < 0

The fabric queue size, measured at the end of each period,
will increase until the drop rate reaches (cid:2) (cid:4) sc and will then
decrease back to zero:

qn ¼ T

ð(cid:2) (cid:4) sc (cid:4) (cid:3)½m (cid:4) 1(cid:5)Þ;

¼ T

ð(cid:2) (cid:4) scÞ (cid:4)

(cid:3)½m (cid:4) 1(cid:5);

n

X

m¼1

¼ T

ð(cid:2) (cid:4) scÞ

n

X

m¼0

n

X

m¼0

n

X

m¼0

(cid:4)

n

X

m¼1

(cid:3)
Kðsc (cid:4) RoptÞ þ mKIðsc (cid:4) RoptÞ

(cid:4);

¼ T ½ðn þ 1Þð(cid:2) (cid:4) scÞ

(cid:4) nKðsc (cid:4) RoptÞ (cid:4)

KIðsc (cid:4) RoptÞ(cid:5):

nðn þ 1Þ

2

ð3Þ

ð2Þ

The duration of this initial period, N0, and the maximum
queue size can easily be calculated from this quadratic
equation setting qN0(cid:4)1 ¼ 0. To find the behavior of the
system for n (cid:2) N0 we use a new time axis, n0 ¼ n (cid:4) N0, with
an initial condition for the accumulator memory

(cid:3)½n0(cid:5) ¼Kðr½n0(cid:5) (cid:4) ropt½n0(cid:5)Þ

þ KI

r½m(cid:5) (cid:4)

ropt½m(cid:5)

þ SN0 ;

 

n0
X

m¼0

n0
X

m¼0

!

ð4Þ

Fig. 3. FOQ architecture.

the fabric queue is sufficiently large, so that there are no
drops due to queue overflow.

The total drop rate, (cid:3), is adjusted by a controller (how (cid:3) is
distributed among the N IN droppers is not relevant for this
analysis; we explain how we implement the actual drop
mechanism in the next section). The purpose of
the
controller is to keep the fabric output rate for packets
destined to OQj;k at a desired level, ropt. The desired rate
can be chosen according to the current rate out of OQj;k

ropt ¼ (cid:4)srOðj;kÞ;

where (cid:4) is a constant smaller than but close to 1. In this
way, the desired rate will be close to the capacity, sc, of the
fabric output line when the OUT queue OQj;k is the only
busy queue and utilizing the entire speed of port j.
Furthermore, it will be reduced in proportion to the service
rate of OQj;k when multiple OUT queues are contending for
the OUT port. The two nonlinearities in the figure simply
state that the drop rate cannot be negative or greater than
the arrival rate (cid:2). In our analysis, we assume that the
controller is operating in the linear region and ignore the
nonlinearities.

The delay T between the output of the controller and the
arrival rate models a zero-order hold at the controller
output. The controller operates on time-average of the error
signal taken over an interval T , rather than the signal itself,
and modifies its output only at intervals of T . In the rest of
this section, we denote the time-average of a signal xðtÞ over
the period T by the discrete notation x½n(cid:5). For example the
time-average of the fabric output rate is given by

r½n(cid:5) ¼

rðtÞdt:

Z ðnþ1ÞT

1
T

nT

When the system is in steady state, the amount of traffic, q,
in the fabric queue destined to OQj;k does not change
significantly during the interval T . Therefore, we can
approximate the average fabric output rate by

r½n(cid:5) (cid:6)

uðtÞdt;

Z ðnþ1ÞT

1
T

nT

¼ (cid:2)½n(cid:5) (cid:4) (cid:3)½n (cid:4) 1(cid:5):

the average fabric output rate r½n(cid:5)

In other words,
is
obtained by subtracting the drop rate (cid:3)½n (cid:4) 1(cid:5) that was
selected based on the information available at time ðn (cid:4) 1Þ
from the current arrival rate (cid:2)½n(cid:5).

For a discrete PI controller, the drop rate selected at
time n, (cid:3)½n(cid:5) (and applied to incoming traffic in the next
interval ½ðn þ 1ÞT ; ðn þ 2ÞT (cid:5)) is calculated using the error

FIROIU ET AL.: PROVIDING SERVICE GUARANTEES IN HIGH-SPEED SWITCHING SYSTEMS WITH FEEDBACK OUTPUT QUEUING

439

SN0 ¼ KI N0ðsc (cid:4) RoptÞ:

Equations (2) and (4) describe a closed-loop control
system. We show in Appendix A that the two poles of this
system are at

K þ KI (cid:4) 1

z1 ¼ (cid:4)

q

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðK þ KI (cid:4) 1Þ2 þ 4K

where

and

þ

1
2

(cid:4)

1
2

2

2

4.1 Multiple Macroflows
When there are multiple macroflows, the analysis for the
initial period ðn < N0Þ needs to be updated. Let v be the
total rate of the traffic that does not belong to macroflow k
but is destined to port j. If the step size for macroflow k is
such that (cid:2) þ v > sc, then, for an initial period, the average
fabric output rate for macroflow k is approximately

r½n(cid:5) ¼ sc

u½n(cid:5)

v½n(cid:5) þ u½n(cid:5)

:

K þ KI (cid:4) 1

z2 ¼ (cid:4)

q

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðK þ KI (cid:4) 1Þ2 þ 4K

:

It follows that we have the stability condition given by the
proposition below:

Proposition 1. For K (cid:2) 0, the closed-loop system described by

(2) and (4) is stable if and only if

0 < KI < 2ð1 (cid:4) KÞ:

ð5Þ

Proof. If K þ KI > 1, then jz2j > jz1j, and both poles are

inside the unit circle if and only if

K þ KI (cid:4) 1 þ

q

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðK þ KI (cid:4) 1Þ2 þ 4K

< 2;

Since r is not constant anymore, the previous results for the
initial period do not apply in general. However, once the
is over and u and v are adjusted so that
transient
u½n(cid:5) þ v½n(cid:5) (cid:7) sc, the approximation (2) holds, and the results
for the single macroflow case can be used replacing SN0 by a
new initial condition. We defer a detailed analysis of the
initial transient period for the multiple macroflow case to a
in two cases, when u or v is
future study. However,
negligible compared to the other, the results for the single
macroflow case can be used with some changes. If u (cid:3) v,
then r½n(cid:5) (cid:6) sc and we can approximate the multiple
macroflow case by the single macroflow case. On the other
hand, if u (cid:8) v, then we can assume that v is constant since
the effect of the new traffic, u, will be negligible. Therefore,

which yields

K þ

< 1:

KI
2

On the other hand, if K þ KI < 1, then jz2j < jz1j and

both poles are inside the unit circle if and only if

(cid:4) ðK þ KI (cid:4) 1Þ þ

q

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðK þ KI (cid:4) 1Þ2 þ 4K

< 2;

which yields

KI > 0:

Combining the two cases gives the condition for
tu
stability.

In Appendix A, we solve the system with the stability
condition (5) and show that the controller output is given by

(cid:2)

(cid:3)½n(cid:5) ¼

½K þ ðn þ 1ÞKI(cid:5)ðsc (cid:4) RoptÞ; n < N0
Dð1 (cid:4) A1zn(cid:4)N0
n (cid:2) N0;

þ A2zn(cid:4)N0

;

2

1

ð6Þ

where

and

A1 ¼

SN0
D z1

z2
1 (cid:4)
z1 (cid:4) z2

; A2 ¼

SN0
D z2

z2
2 (cid:4)
z1 (cid:4) z2

;

D ¼ (cid:2) (cid:4) Ropt

r½n(cid:5) (cid:6) sc

¼ (cid:5)u½n(cid:5);

u½n(cid:5)

v

with (cid:5) ¼ sc=v during the initial period n < N0. In this case,
N0 is defined by

(cid:2) (cid:4) (cid:3)½N0 (cid:4) 1(cid:5) þ v ¼ sc:

For n < N0, the drop rate can be calculated by replacing (2)
with

r½n(cid:5) (cid:6) (cid:5)ð(cid:2)½n(cid:5) (cid:4) (cid:3)½n (cid:4) 1(cid:5)Þ:

The response for n (cid:2) N0 is still given by (6), but with a new
initial condition replacing SN0 .

4.2 Quantized PI—The Gear Box Algorithm
A practical implementation of the discrete-time PI control
described above requires a few modifications to the control
loop. The first modification is related to how the bytes will
actually be dropped at the desired drop rate calculated by
the controller. The drop rate has to be divided fairly among
the N IN droppers. Furthermore, it is well-known that
dropping consecutive packets may result in poor perfor-
mance in the affected flows. Therefore, it is desirable to
spread the drop rate to an interval and to introduce some
randomness into the drop process. For these reasons, we
introduce a packet drop probability, p½n(cid:5), which is updated
at intervals of T according to the desired drop rate and the
estimated average arrival rate,

is the difference between the arrival and the desired rates.
We observe that after the initial linear increase, the drop
rate approaches exponentially to the difference between the
arrival and the desired rates. Furthermore, since the
absolute value of the negative pole is relatively larger for
KI > 1 (cid:4) K, the system will show more oscillatory behavior
in this case compared to the KI < 1 (cid:4) K case.

(cid:3)½n(cid:5)

ð1 (cid:4) p½n (cid:4) 1(cid:5)Þ(cid:3)½n(cid:5)

p½n(cid:5) ¼

¼

^(cid:2)(cid:2)½n þ 1(cid:5)

r½n(cid:5)

:

ð7Þ

Note that here we used the fabric output rate divided by
the admit probability (i.e., 1 (cid:4) p½n (cid:4) 1(cid:5)) as an estimate of the
next average arrival rate. This is justified for the cases where
the average arrival rate is a slowly varying function relative
to interval T and the delay.

440

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 17, NO. 5, MAY 2006

The second modification to the feedback structure is
related to the constraint on the size of the feedback channel,
which becomes a limiting factor on the precision of the
feedback signal at high speeds. Our goal is to use only a
finite number of drop probability values and to derive a
controller that will have a similar performance with the PI
controller. For this purpose, we expand (7) as

and

1

1

p½n(cid:5) ¼

^(cid:2)(cid:2)½n þ 1(cid:5)

(cid:6)

Ke½n(cid:5) þ KI

(cid:7)

;

e½m(cid:5)

n

X

m¼1

¼

^(cid:2)(cid:2)½n þ 1(cid:5)

ðKe½n (cid:4) 1(cid:5) þ KI

e½m(cid:5)

þ Ke½n(cid:5) þ KIe½n(cid:5) (cid:4) Ke½n (cid:4) 1(cid:5)Þ:

n(cid:4)1
X

m¼1

Using again the assumption ^(cid:2)(cid:2)½n þ 1(cid:5) (cid:6) ^(cid:2)(cid:2)½n(cid:5), we can rewrite
the above equation as

p½n(cid:5) (cid:6) p½n (cid:4) 1(cid:5) þ

ðKe½n(cid:5)

1

^(cid:2)(cid:2)½n þ 1(cid:5)

þ KIe½n(cid:5) (cid:4) Ke½n (cid:4) 1(cid:5)Þ;
p½n (cid:4) 1(cid:5) þ ð1 (cid:4) p½n (cid:4) 1(cid:5)Þ

¼

r½n(cid:5)ðKe½n(cid:5)

þ KIe½n(cid:5) (cid:4) Ke½n (cid:4) 1(cid:5)Þ;

(cid:8)

¼ 1 (cid:4)

ðK þ KIÞe½n(cid:5) (cid:4) Ke½n (cid:4) 1(cid:5)

(cid:9)

p½n (cid:4) 1(cid:5)

ðK þ KIÞe½n(cid:5) (cid:4) Ke½n (cid:4) 1(cid:5)

:

þ

r½n(cid:5)

r½n(cid:5)

Now, if we define

(cid:6)½n(cid:5) ¼

ðK þ KIÞe½n(cid:5) (cid:4) Ke½n (cid:4) 1(cid:5)

;

r½n(cid:5)

then the update for the drop probability simply becomes

p½n(cid:5) ¼ ð1 (cid:4) (cid:6)½n(cid:5)Þp½n (cid:4) 1(cid:5) þ (cid:6)½n(cid:5):

ð8Þ

In order to use finite values of p½n(cid:5), we quantize (cid:6)½n(cid:5) to three
levels:

(cid:6)q½n(cid:5) ¼

8
<

:

(cid:7);
0;
(cid:7)
(cid:7)(cid:4)1 ;

if (cid:6)½n(cid:5) > (cid:3)max;
if (cid:4) (cid:3)min (cid:7) (cid:6)½n(cid:5) (cid:7) (cid:3)max;
if (cid:6)½n(cid:5) < (cid:4)(cid:3)min:

ð9Þ

Then, the update for discrete probability values becomes

pq½n(cid:5) ¼ ð1 (cid:4) (cid:6)q½n(cid:5)Þpq½n (cid:4) 1(cid:5) þ (cid:6)q½n(cid:5);

which can also be written as an update of admit
probabilities as

1 (cid:4) pq½n(cid:5) ¼ ð1 (cid:4) (cid:6)q½n(cid:5)Þð1 (cid:4) pq½n (cid:4) 1(cid:5)Þ:

If we set K ¼ 0, then (9) can also be expressed in terms of
the relative congestion C½n(cid:5) ¼ 1 (cid:4) rO½n(cid:5)=r½n(cid:5) as

where

(cid:6)q½n(cid:5) ¼

8
<

:

(cid:7);
(cid:7)
(cid:7)(cid:4)1 ;
0

if C½n(cid:5) > dmax;
if C½n(cid:5) < dmin;
otherwise;

dmax ¼ 1 (cid:4)

1
(cid:4)s

þ

(cid:3)max
(cid:4)sKI

dmin ¼ 1 (cid:4)

1
(cid:4)s

(cid:4)

(cid:3)min
(cid:4)sKI

:

We call the quantized mechanism with K ¼ 0 the Gear
Box (GB) controller, since there are only three possible
actions: increase the drop probability, decrease the drop
probability, and no change. With the GB controller, it is
sufficient to have a 2-bit feedback signal every T seconds.
Furthermore, the different levels of the admit probabilities
are the different powers of ð1 (cid:4) (cid:7)Þ. Therefore, the calcula-
tion at the IN droppers can be implemented by storing

Pk ¼ 1 (cid:4) ð1 (cid:4) (cid:7)Þk

for k 2 f0; 1; . . .g as a table in the memory and just updating
a pointer to this table based on the feedback signal. In other
words, k corresponds to the chosen “drop step”; for k ¼ 0,
all traffic is admitted into the fabric, whereas, when k ! 1,
all traffic is discarded at the input link(s).

In terms of physical interpretation, dmax corresponds to
the maximum amount of drops that one wants to tolerate at
the output queue, expressed as a fraction of the total
number of packets. When the congestion exceeds dmax at the
output queue, the GB controller increases the drop rate at
the input queue, by increasing k. As an example, in the
context of TCP flows, we would generally expect a network
operator to choose dmax (cid:6) 0:2. On the other hand, dmin
translates the minimal amount of drops, expressed as a
fraction of the total number of packets, that characterizes
congestion at the output queue. When the relative conges-
tion goes below dmin, the GB controller decreases k. Thus,
we would expect network operators to pick relatively small
values for dmin, e.g., dmin (cid:6) 0:02.

the GB algorithm is OðKNÞ

4.3 Complexity Analysis
Using a predefined drop table as described above, the
complexity of
for
K macroflows and N ports. Indeed, for each macroflow,
GB has to compute the input and output rates, which is
an Oð1Þ operation at each of the N output ports. Looking
up the corresponding value of
the drop rate in a
predefined drop table and sending the value back to
the input droppers are also Oð1Þ operations. Note that, in
addition, the drop rate computation is only performed
every FOQ cycle (T seconds).

Note that

the computational complexity of

the
PI algorithm is also OðNMÞ. The key differences between
PI and GB lie in 1) the complexity of the arithmetic
operations (floating-point computations for PI versus table
lookups for GB), and 2) the need for an arbitrary precise
feedback channel in PI.

the control

4.4 Hysteresis Control
To increase the stability of
in our
implementation of the GB algorithm, we choose the value
for (cid:7) such that the relative congestion after a step increase
or decrease in IN drop probability be equal. To find the
value for (cid:7) that has this property, note that when the
relative congestion C reaches dmax,
the drop step is
increased and the relative congestion immediately changes
to a different value Cnew;1. More precisely, if we have

loop,

FIROIU ET AL.: PROVIDING SERVICE GUARANTEES IN HIGH-SPEED SWITCHING SYSTEMS WITH FEEDBACK OUTPUT QUEUING

441

as the value for (cid:7) such that the relative congestion after a
step increase or decrease in IN drop probability be equal.

p

(10)

We illustrate the behavior of the system when subject to
in Fig. 4, where dmid ¼ 1
the configuration of
(cid:4) ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1 (cid:4) dminÞð1 (cid:4) dmaxÞ
. When the input rate increases such
that the output relative congestion goes from dmin to dmax,
the input drop probability remains at the same level, and
jumps to P1 when the output relative congestion reaches
dmax. This jump in the input drop probability has the
immediate effect of causing the output relative congestion
to decrease to a value dmid. Then, if the output relative
congestion increases again to dmax, the input drop prob-
ability remains at P1 before jumping to P2 when the output
relative congestion reaches dmax. Now, if the input drop
probability is at P2 and the relative congestion decreases
from dmid to dmin, the input drop probability remains at P2
and jumps down to P1 as soon as the relative congestion
reaches dmin. The decrease in the input drop probability
from P2 to P1 immediately increases the output relative
congestion to dmid.

As shown in Fig. 4, this configuration has the key
advantage of providing hysteresis to the GB control, by
always trying to have the relative congestion come back to
dmid, thereby providing stability against small perturba-
tions. We will use this configuration in our simulations
presented in the following section.

4.5 GB Stability
Although the GB algorithm is based on a PI control, the
quantization and removal of the proportional term (K ¼ 0)
could impact the stability of the control. We provide here a
step response analysis of the GB algorithm and show that
every bounded step input ((cid:2)½n(cid:5) ¼ (cid:2) > sc, ropt½n(cid:5) ¼ Ropt for
n (cid:2) 0) results in a bounded output (i.e., the drop step
remains finite). In other words, we show that GB is stable in
a BIBO sense.

Denote by k½n(cid:5) the drop step at time n. Initially, we have
k½0(cid:5) ¼ 0. Because (cid:2) > sc, there is a backlog in the fabric
queue, so that C½n(cid:5) > dmax and the drop step first con-
tinuously increases, reaching k½nE(cid:5) ¼ KE at a given time nE,
with the rate of traffic admitted into the fabric such that:

Ropt

1 (cid:4) dmin

(cid:7) u½nE(cid:5) ¼ (cid:2)ð1 (cid:4) (cid:7)ÞKE (cid:7)

Ropt

1 (cid:4) dmax

< sc:

After n ¼ nE, because there is still a backlog of traffic in
the fabric, the drop step continues to increase for a while.
Meanwhile,
the fabric backlog is decreasing because
u½n(cid:5) < sc. The backlog in the fabric is eventually completely
cleared, at time n ¼ nZ, for which k½nZ(cid:5) ¼ KZ (cid:2) KE.

Immediately after time n ¼ nZ, we have u½n(cid:5) (cid:7) u½nE(cid:5)
(cid:7) Ropt=ð1 (cid:4) dmaxÞ, and no backlog of traffic in the fabric.
Thus, the drop step does not increase. Depending on the
values of dmin, dmax and the fabric buffer size, either the drop
step remains constant, or it decreases. In any case, for any n,
k½n(cid:5) (cid:7) KZ. Thus, a bounded input results in a bounded
output, and the GB controller is BIBO-stable.

We illustrate the step response of the GB controller in
Fig. 5, where we compare the responses of both the PI and
GB controller to a step input (cid:2) ¼ 2 Gbps, Ropt ¼ 1 Gbps.
The parameters are set to dmax ¼ 0:17, dmin ¼ 0:02, K ¼ 0:12,
KI ¼ 1:1, and s ¼ 1:28, c ¼ 10, T ¼ 1 ms. We plot the loss
rate due to FOQ in function of the time for both the PI and

Likewise, when C reaches dmin, the drop step is decreased
and the relative congestion immediately changes to a
different value Cnew;2. That is,

has the effect of changing rI to rI;new ¼ rI

ð1(cid:4)(cid:7)Þ , yielding

Fig. 4. FOQ dynamics and stability.

then rI changes to rI;new ¼ rIð1 (cid:4) (cid:7)Þ, so that

which can be rewritten as

C ¼ 1 (cid:4)

¼ dmax;

rO
rI

Cnew;1 ¼ 1 (cid:4)

rO

rIð1 (cid:4) (cid:7)Þ

Cnew;1 ¼ 1 (cid:4)

1 (cid:4) dmax

1 (cid:4) (cid:7)

;

:

C ¼ 1 (cid:4)

¼ dmin;

rO
rI

Cnew;2 ¼ 1 (cid:4)

rOð1 (cid:4) (cid:7)Þ

;

rI

that is,

Cnew;2 ¼ 1 (cid:4) ð1 (cid:4) dminÞð1 (cid:4) (cid:7)Þ;

and we want to have Cnew;1 ¼ Cnew;2. Hence,

1 (cid:4)

1 (cid:4) dmax

1 (cid:4) (cid:7)

¼ 1 (cid:4) ð1 (cid:4) dminÞð1 (cid:4) (cid:7)Þ;

which reduces to

giving, finally,

1 (cid:4) dmax
1 (cid:4) dmin

¼ ð1 (cid:4) (cid:7)Þ2;

(cid:7) ¼ 1 (cid:4)

r

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 (cid:4) dmax
1 (cid:4) dmin

ð10Þ

442

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 17, NO. 5, MAY 2006

Queuing discipline with 1 : 7 : 2 weights, corresponding to
the required rate guarantees.

In Fig. 6, we plot the evolution in time of the service rate
for the three flows, without and with FOQ, respectively. In
Fig. 7, we show the dynamics of drop rate for the same
scenarios. In all plots, each data point corresponds to an
average over a sliding window of size 1 ms. In the non-FOQ
case, flow 0 is served slightly under its guaranteed rate (at
approximately 0:85 Gbps, having about 10 perecent drop
rate) due to congestion created by the two other flows in the
fabric. Moreover, the rates received by flows 1 and 2 are
largely violating the service guarantee that flow 2 should
receive 3.5 times more capacity than flow 3; in fact, as
evidenced in Fig. 6a, flow 1 actually gets a service rate lower
than flow 2. This is due to the drops in the fabric queue
(Fig. 7c), which occur without discrimination between any
of the flows.3 In particular, flow 1 is heavily dropped in the
fabric and never reaches its allocated share in the output
queue. When using FOQ (Fig. 6b), flow 1 receives 7 Gbps
and flow 2 gets 2 Gbps, thus both achieving their minimum
rate guarantees. Furthermore, flow 0 receives a service rate
equal to its sending rate (0.952 Gbps) with no drops, except
for the initial transient period. This is explained by the FOQ
action reflected in Fig. 7b, where we see an increase of input
drop for flows 1 and 2 as a reaction to output congestion. As
a consequence, the fabric drop is zero almost all the time in
the FOQ case, in contrast with the high drop rate in the base
case. The spike in fabric drops is due to the transient state,
where ingress drop is increasing but not yet sufficient for
eliminating fabric congestion. With FOQ, fabric drop occurs
only at bursts with high rate and long duration. It can be
mitigated by larger fabric memory or higher frequency of
feedback.

In Fig. 8, we show the dynamics of packet transit delay
through the whole switch. In the case without FOQ, there is
little delay differentiation (flows 0 and 1 experience, in fact,
identical delays), whereas in the FOQ case, flow 0 observes
negligible delay ((cid:8) 1(cid:8)s), flows 1 and 2 experience delays
that are proportional to their respective service rates (their
OUT queues are close to full in the steady state due to the
drop-tail queue management).

5.2 FOQ Dynamics with TCP Traffic
Next, we examine the interaction of FOQ-GB with TCP
traffic. To that effect, we run a simulation where 4,500 TCP
sources send traffic through a switch. In this experiment, we
only consider one macroflow (containing many micro-
flows). Four subnets containing 1,000 TCP sources each and
one subnet containing 500 TCP sources are connected to the
switch by five independent 1 Gbps links. All sources send
traffic to the same destination subnet, which is also
connected to the switch by a 1 Gbps link, with a one-way
propagation delay of 20 ms. We have the number of active
TCP microflows increase over time as follows. Each source
in the first subnet starts sending traffic between t ¼ 0 s and
t ¼ 1 s, according to a uniform random variable. Then, each
source in the second subnet starts sending traffic between

3. Fabric drops follow a very regular pattern in this example because all

flows are CBR and synchronized.

Fig. 5. Step response of
for
(cid:2) ¼ 2 Gbps, dmax ¼ 0:17, dmin ¼ 0:02, K ¼ 0:12, KI ¼ 1:1, s ¼ 1:28,
Ropt ¼ c ¼ 1 Gbps, and T ¼ 1 ms.

the PI, hybrid, and GB controllers,

GB controllers, as well as for the hybrid controller
characterized by (8), which is essentially GB without
quantization. For PI, we directly obtain the loss rate from
(6) by dividing (cid:3) by (cid:2), while we numerically simulate the
behavior of the GB and hybrid controllers in Matlab. In the
figure, we see that, for the GB controller, nE ¼ 8 ms,
nZ ¼ 14 ms and KZ > KE. The GB controller eventually
settles after 20 ms close to the objective value of
(cid:2)(cid:4)Ropt
(cid:2) ¼ 50%. On the other hand, the PI controller has a
significantly shorter settling time, of about 8 ms. The hybrid
controller shows a behavior close to that of PI, which
indicates that quantization is the main factor leading to the
degradation in settling time. All three controllers are stable
in the proposed configuration.

5 SIMULATION EXPERIMENTS

The objective of this section is to expand the numerical
analysis discussed above, by presenting a set of experimental
results that illustrate the salient properties of FOQ. First, we
describe a relatively simple experiment with three macro-
flows, each consisting of a single microflow (hereafter, micro-
and macroflows are simply called “flows”) and constant-bit-
rate (CBR) traffic, before presenting experimental results
gathered for a more realistic situation where traffic consists of
a large number of nonsynchronized TCP sources.

5.1 FOQ and Service Guarantees
We simulate a 16 (cid:9) 10 Gbps-port switch with a 5 MB shared
memory fabric having external speedup s ¼ 1:28, 2 MB
drop-tail OUT queues per flow, and no ingress queues. The
FOQ-GB mechanism has a sampling rate T ¼ 1 ms and
feedback thresholds dmax ¼ 0:17, dmin ¼ 0:02. We run each
simulation for 200 ms.

The offered load is composed of three flows sending at
constant rates starting at t ¼ 0: flow 0: 0:952 Gbps, flow 1 and
2: 9:52 Gbps each, all entering the switch at separate ingress
ports and exiting through the same output port. Given that
the total offered load is 20 Gbps, the OUT port has a potential
200 percent overload. The flows are guaranteed minimum
service rates of 1 Gbps, 7 Gbps, and 2 Gbps, respectively. In
particular, in such a configuration, ideally, flow 0 should not
experience any losses. All flows share the same fabric queue.
At the OUT scheduler, each flow is assigned a separate queue.
These output queues are scheduled in a Weighted Fair

FIROIU ET AL.: PROVIDING SERVICE GUARANTEES IN HIGH-SPEED SWITCHING SYSTEMS WITH FEEDBACK OUTPUT QUEUING

443

Fig. 6. Throughput plots (a) without FOQ and (b) with FOQ.

Fig. 7. Drop rate plots. (a) Input drop without FOQ. (b) Fabric drop without FOQ. (c) Output drop rate without FOQ. (d) Input drop with FOQ.
(e) Output drop rate without FOQ. (f) Output drop rate with FOQ.

Fig. 8. Delay plots (a) without FOQ and (b) with FOQ.

t ¼ 2 s and t ¼ 3 s. Subsequently, every two seconds,
sources in an additional subnet start transmitting. Hence,
we have no overload between t ¼ 0 s and t ¼ 2 s, a potential

2 : 1 overload in the fabric between t ¼ 2 s and t ¼ 4 s, a
3 : 1 overload between t ¼ 4 s and t ¼ 6 s, a 4 : 1 overload
between t ¼ 6 s and t ¼ 8 s, and a 5 : 1 overload then on.

444

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 17, NO. 5, MAY 2006

Fig. 9. Ingress drops and fabric queue. FOQ manages to maintain a low fabric queue by dropping packets at the input links. When FOQ is not
present, there are no input drops. (a) Input drops. (b) Fabric queue length.

There is a potential s : 1 bottleneck at the output port of the
switch governing the 1 Gbps link to the destination subnet
after t ¼ 2 s. All TCP sources send 1,040-byte packets.

The FOQ parameters are chosen as in the previous
experiment, i.e., s ¼ 1:28, dmax ¼ 0:17 and dmin ¼ 0:02. The
fabric queue now has a size of 500 KB and the output queue
has a size of 400 KB. The output queue runs RED, with
maxP ¼ 0:5, maxT H ¼ 300 KB, minT H ¼ 100 KB, a sampling
time of 1 ms, and a weight wq ¼ 0:1. We compare the
performance of the switch with and without FOQ.

We first observe in Fig. 9b, where each datapoint
represents a moving average over a sliding window of size
50 ms that, regardless of the potential overload, FOQ
consistently manages to maintain the fabric backlog ex-
tremely close to zero by dropping packets at the input links.
As illustrated in Fig. 9a, input drops increase with the
overload. Conversely, without FOQ and, therefore, in the
absence of input drops, the fabric buffer is filling up with
the number of active TCP sources and is eventually
completely full once all sources have started transmitting.
Ultimately, as illustrated in Fig. 10a, traffic is dropped in the
fabric. There are no fabric drops when FOQ is used.

Last, we observe in Fig. 10b that the output loss rate is
limited by 1 (cid:4) 1=s (cid:6) 21:8 percent when FOQ is disabled. On
the other hand, FOQ maintains the egress relative congestion

close to dmid ¼ 0:098, as shown in Fig. 11a and, consequently,
the output loss rate remains close to 9.8 percent. When the loss
rates become roughly constant, the output queue length,
represented in Fig. 11b, also becomes constant by virtue of a
stable RED control [25].

As a conclusion to this second experiment, we have
shown that FOQ’s objectives of preventing fabric drops and
regulating the traffic that arrives at the output link were met
in the case of an experiment with a large number of TCP
sources. The results were even more positive than those
obtained with constant-rate sources, as FOQ does not
exhibit transient behaviors in this scenario. This can be
justified by the fact that FOQ feedback is run at a much
higher frequency (every T ¼ 1 ms) than the TCP congestion
control algorithms, which are run with an approximately
40 ms delay here.

6 DISCUSSION AND CONCLUSIONS

In this paper, we presented the Feedback Output Queuing
architecture for packet switching. FOQ provides support for
service guarantees when the switching speed is limited by
the memory read and write speeds. Using a fast switching
fabric in this case leads to a buildup in fabric buffers and,
eventually, either to buffer overflow and packet discarding

Fig. 10. (a) Fabric and (b) output losses. FOQ manages to completely avoid fabric losses, and also significantly reduces the amount of traffic
dropped at the output link.

FIROIU ET AL.: PROVIDING SERVICE GUARANTEES IN HIGH-SPEED SWITCHING SYSTEMS WITH FEEDBACK OUTPUT QUEUING

445

Fig. 11. (a) Relative congestion and (b) output queue. FOQ maintains the relative congestion between dmin and dmax.

or to unbounded delays at the fabric inputs due to back
pressure. FOQ solves this problem by triggering packet
discard only from flows that exceed their allocated
bandwidth and, therefore, limiting the buildup and delay
at the fabric buffers. In the worst case, the arrival rate will
be (cid:2)max, the total input capacity of the fabric. For the PI
controller the maximum fabric queue size and the max-
imum delay in the fabric can be calculated from (3) by
inserting (cid:2) ¼ (cid:2)max. Any delay value above this number can
be deterministically guaranteed to a flow by using a proper
scheduler (e.g., WFQ-based) at the output queues after the
fabric.

An alternative approach to solve the same problem is to
use VOQ at fabric inputs. Relatively recent studies show that
VOQ can also provide deterministic delay bounds [26]. This
is, however, at the expense of computational complexity.
VOQ algorithms require OðN 2Þ computations per packet slot
to determine which packets will be sent to their destinations.
This high computational complexity makes the VOQ ap-
proach less feasible for high bit-rate switches. In contrast,
FOQ requires a total of OðNÞ computations per packet slot
and OðKNÞ computations per feedback interval, where K is
the number of supported macroflows. Since the feedback
interval is much larger than a packet slot, computations for
the feedback are actually negligible. Furthermore, the
computations are distributed to the inputs and outputs, so
that each input and output performs Oð1Þ computations. In
other words, FOQ’s computational complexity is much lower
than VOQ, the current state of the art.

We applied discrete feedback control theory to derive a
stable configuration for FOQ. Through analysis and simula-
tions, we showed that a quantized version of a PI controller
named “Gear-Box control” is stable, responds quickly to
traffic bursts, and provides highly accurate QoS guarantees.
We believe that this work has sparked many venues for
future research. There is a range of control algorithms to be
investigated besides those presented here. The interaction
between the TCP congestion control algorithm and FOQ
(and RED queue management) is an interesting control
problem. The FOQ architecture can be extended with a set

and

and

of input queues in order to provide zero loss for a wider
range of bursty traffic, given a limited fabric memory size.

APPENDIX A

In this appendix, we give a detailed derivation of some of
the equations.

Taking the z-transforms of (2) and (4), we get

(cid:4)
(cid:3)ðzÞ ¼ K rðzÞ (cid:4) roptðzÞ

(cid:3)

z

þ KI

z (cid:4) 1
þ rSN0 ðzÞ

(cid:3)
(cid:4)
rðzÞ (cid:4) roptðzÞ

ð11Þ

rðzÞ ¼ (cid:2)ðzÞ (cid:4) z(cid:4)1(cid:3)ðzÞ:

ð12Þ

Transfer functions of this system between the output rate, r,
and the two inputs and initial state, (cid:2), ropt, and SN0 , are
given by

rðzÞ
(cid:2)ðzÞ
rðzÞ
roptðzÞ

¼

¼

zðz (cid:4) 1Þ

z2 þ ðK þ KI (cid:4) 1Þz (cid:4) K

ðK þ KIÞz (cid:4) K

z2 þ ðK þ KI (cid:4) 1Þz (cid:4) K

rðzÞ
SN0 ðzÞ

¼

1 (cid:4) z

z2 þ ðK þ KI (cid:4) 1Þz (cid:4) K

;

;

:

Let z1 and z2 be two roots of the system characteristic
equation, i.e.,

z2
1;2 þ ðK þ KI (cid:4) 1Þz1;2 (cid:4) K ¼ 0:

Then, without loss of generality,

z1 ¼ (cid:4)

z2 ¼ (cid:4)

K þ KI (cid:4) 1

K þ KI (cid:4) 1

2

2

þ

(cid:4)

1
2
1
2

q

q

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðK þ KI (cid:4) 1Þ2 þ 4K
;
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðK þ KI (cid:4) 1Þ2 þ 4K

:

We showed in Proposition 1 that the system is stable if

0 < KI < 2ð1 (cid:4) KÞ:

446

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 17, NO. 5, MAY 2006

[9]

J. Padhye, V. Firoiu, D. Towsley, and J. Kurose, “Modeling TCP
Reno Performance: A Simple Model and Its Empirical Validation,”
IEEE/ACM Trans. Networking, vol. 8, no. 2, pp. 133-145, Apr. 2000.
[10] T. Anderson, S. Owicki, J. Saxe, and C. Thacker, “High Speed
Switch Scheduling for Local Area Networks,” ACM Trans.
Computer Systems, vol. 11, no. 4, pp. 319-352, Nov. 1993.

[11] N. McKeown and T. Anderson, “A Quantitative Comparison of
Iterative Scheduling Algorithms for Input-Queued Switches,”
Computer Networks and ISDN Systems, vol. 30, no. 24, pp. 2309-
2326, Dec. 1998.

[12] S.-T. Chuang, A. Goel, N. McKeown, and B. Prabhakar, “Matching
Output Queueing with a Combined Input-Output Queued
Switch,” Proc. IEEE Conf. Computer Comm. (Infocom ’99), vol. 3,
pp. 1169-1178, Mar. 1999.

[13] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, and W. Weiss,
“An Architecture for Differentiated Services,” Internet Eng. Task
Force RFC 2475, Dec. 1998.

[14] R. Gue´rin and V. Pla, “Aggregation and Conformance in
Differentiated Service Networks: A Case Study,” Proc. ACM
SIGCOMM Computer Comm. Rev., vol. 31, no. 1, pp. 21-32, Jan.
2001.

[15] Y. Xu and R. Gue´rin, “Individual QoS versus Aggregate QoS: A
IEEE Conf. Computer Comm.

Loss Performance Study,” Proc.
(Infocom ’02), vol. 3, pp. 1170-1179, June 2002.

[16] E. Rosen, C. Filsfils, G. Heron, A. Malis, L. Martini, and S.
Vogelsang, “An Architecture for L2VPNs,” Internet Eng. Task
Force draft, draft-ietf-ppvpn-l2vpn-00.txt, July 2001.

[17] M. Carugi, D. McDysan, L. Fang, F. Johansson, A. Nagarajan, J.
Sumimoto, and R. Wilder, “Service requirements for Layer 3
Provider Provisioned Virtual Private Networks,” Internet Eng.
Task Force draft, draft-ietf-ppvpn-requirements-04.txt, Mar. 2002.
[18] K. Kar, T.V. Lakshman, D. Stiliadis, and L. Tassiulas, “Reduced
Complexity Input Buffered Switches,” Proc. Conf. Hot Interconnects
VIII, Aug. 2000.

[19] C. Minkenberg and T. Engbersen, “A Combined Input and Output
Queued Packet-Switched System Based on Prizma Switch-on-a-
Chip Technology,” IEEE Comm. Magazine, vol. 38, no. 12, pp. 70-
77, Dec. 2000.

[20] P. Gupta and N. McKeown, “Packet Classification on Multiple
Interest Group Data Comm. Conf.

Fields,” Proc. ACM Special
(SIGCOMM ’99), pp. 147-160, Aug. 1999.

[21] V. Srinivasan, S. Suri, and G. Varghese, “Packet Classification
Using Tuple Space Search,” Proc. ACM Special Interest Group Data
Comm. Conf. (SIGCOMM ’99), pp. 135-146, Aug. 1999.

[22] A. Soule, K. Salamatian, N. Taft, R. Emilion, and K. Papagiannaki,
“Flow Classification by Histograms: Or How to Go on Safari in the
Internet,” Proc. ACM SIGMETRICS ’04, pp. 49-60, June 2004.

[23] S. Floyd and V. Jacobson, “Random Early Detection for Conges-
tion Avoidance,” IEEE/ACM Trans. Networking, vol. 1, no. 4,
pp. 397-413, July 1993.

[24] G. Franklin, J. Powell, and M. Workman, Digital Control of Dynamic

Systems, third ed. Menlo Park, Calif.: Addison-Wesley, 1998.

[25] V. Firoiu and M. Borden, “A Study of Active Queue Management
IEEE Conf. Computer Comm.

for Congestion Control,” Proc.
(Infocom ’00), vol. 3, pp. 1435-1444, Apr. 2000.

[26] G. Nong and M. Hamdi, “Providing QoS Guarantees for Unicast/
Multicast Traffic with Fixed and Variable-Length Packets in
Multiple Input-Queued Switches,” Proc. IEEE Int’l Symp. Compu-
ters and Comm. (ISCC ’01), pp. 166-171, July 2001.

[27] V. Firoiu, X. Zhang, and E. Gu¨ ndu¨ zhan, “Feedback Output
Queuing: A Novel Architecture for Efficient Switching Systems,”
Proc. Conf. Hot Interconnects X, pp. 15-20, Aug. 2002.

We next find the solution for the drop rate (cid:3) assuming this
stability condition is satisfied. For step inputs and initial
condition (cid:2)ðzÞ ¼ z(cid:2)=ðz (cid:4) 1Þ, roptðzÞ ¼ zRopt=ðz (cid:4) 1Þ, SN0 ðzÞ
¼ zSN0 =ðz (cid:4) 1Þ, and defining D ¼ (cid:2) (cid:4) Ropt as the difference
between the arrival and the desired rates, we have from (11)
and (12):

(cid:3)ðzÞ ¼

KD z

z(cid:4)1 þ KID z2
ðz(cid:4)1Þ2 þ SN0
z þ KI

1 þ K

z(cid:4)1

z

z(cid:4)1

;

¼z2 ½ðK þ KIÞD þ SN0 (cid:5)z (cid:4) KD (cid:4) SN0
ðz (cid:4) 1Þðz2 þ ðK þ KI (cid:4) 1Þz (cid:4) KÞ

:

This can be written as a partial fraction expansion as

(cid:3)ðzÞ ¼ D

(cid:8)

z

z (cid:4) 1

(cid:4)

A1z
z (cid:4) z1

þ

A2z
z (cid:4) z2

(cid:9)
;

where

A1 ¼

SN0
D z1

z2
1 (cid:4)
z1 (cid:4) z2

;

and A2 ¼

SN0
D z2

z2
2 (cid:4)
z1 (cid:4) z2

;

which can be solved easily. Finally, recall that this system
was obtained initially by defining a new time axis for
n (cid:2) N0. Therefore, after taking the inverse z-transform, we
combine the result with n < N0 case to get

(cid:2)

(cid:3)½n(cid:5) ¼

½K þ ðn þ 1ÞKI(cid:5)ðsc (cid:4) RoptÞ;
Dð1 (cid:4) A1zn(cid:4)N0
Þ;

þ A2zn(cid:4)N0

1

2

if n < N0;
if n (cid:2) N0:

ACKNOWLEDGMENTS

This paper is a revised and extended version of [27]. Most of
this work was done while all the authors were with Nortel.
The authors would like to thank Eric Haversat, Tom Holtey,
and Franco Travostino of Nortel Networks for many useful
discussions.

REFERENCES
[1] B. Gleeson, A. Lin, J. Heinanen, G. Armitage, and A. Malis, “A
Framework for IP Based Virtual Private Networks,” Internet Eng.
Task Force RFC 2764, Feb. 2000.

[2] W. Augustyn, G. Heron, V. Kompella, M. Lassere, P. Menezes, H.
Ould-Brahim, and T. Senevirathne, “Requirements for Virtual
Private LAN Services (VPLS),” Internet Eng. Task Force draft,
draft-ietf-l2vpn-vpls-requirements-00.txt, Oct. 2002.

[3] B. Davie, A. Charny, J. Bennett, K. Benson, J.-Y. Le Boudec, W.
Courtney, S. Davari, V. Firoiu, and D. Stiliadis, “An Expedited
Forwarding PHB,” Internet Eng. Task Force RFC 3246, Mar. 2002.
J. Heinanen, F. Baker, W. Weiss, and J. Wroclawski, “Assured
Forwarding PHB Group,” Internet Eng. Task Force RFC 2597, June
1999.

[4]

[5] D. Goderis, S. Van Den Bosch, Y. T’joens, O. Poupel, C. Jacquenet,
G. Memenios, G. Pavlou, R. Egan, D. Griffin, P. Georgatsos, L.
Georgiadis, and P. Van Heuven, “Service Level Specification
Semantics and Parameters,” Internet Eng. Task Force draft, draft-
tequila-sls-02.txt, Feb. 2002.

[6] G. Nong and M. Hamdi, “On the Provisioning of Quality of
Service Guarantees for Input Queued Switches,” IEEE Comm.
Magazine, vol. 38, no. 12, pp. 62-69, Dec. 2000.
S. McCreary and K.C. Claffy, “Trends in Wide Area IP Traffic
Patterns: A View from Ames Internet Exchange,” Proc. Int’l Test
Conf. (ITC ’00) pp. 1-11, Sept. 2000.

[7]

[8] M. Allman, V. Paxson, and W. Stevens, “TCP Congestion

Control,” Internet Eng. Task Force RFC 2581, Apr. 1999.

FIROIU ET AL.: PROVIDING SERVICE GUARANTEES IN HIGH-SPEED SWITCHING SYSTEMS WITH FEEDBACK OUTPUT QUEUING

447

Victor Firoiu received the Diploma of Engineer-
ing degree from the Department of Computer
Science at the Polytechnic Institute of Bucuresti,
Romania, in 1987, and the MS and PhD degrees
from the Department of Computer Science at the
University of Massachusetts, Amherst, in 1995
and 1998, respectively. He is currently lead
scientist in Advanced Information Technologies,
BAE Systems in Burlington, Mass., where he
performs research in adaptive communication in
dynamic environments. From 1998 and 2004, he led the Performance
Engineering Center within Advanced Technology, Nortel Networks,
pioneering novel product and service architectures in the areas of
network edge and QoS services, switch architecture, and adaptive and
resilient networks. He cochaired the IRTF “Building Differentiated
Services” working group. He was program cochair of the SPIE ITCOM
2002 conference on Scalability and Traffic Control in IP Networks, and
served on the executive committees for IEEE Infocom 2002 and 2003
and on several technical program committees for IEEE Infocom and
Globecom conferences. He is a member of the IEEE.

Xiaohui Zhang received the BS degree in
computer science from Peking University and
the MA degree in computer science from Boston
University. He is a senior system architect at
Airvana, a leading vendor
for 1xEV-DO 3G
wireless radio network. Previously, he worked
as technology architect at Nortel, where he was
responsible for designing networking software
and hardware architecture, modeling switches
system performance, and solving critical perfor-

Emre Gu¨ ndu¨ zhan received the BSc and the
MSc degrees in electrical and electronics en-
gineering from Bilkent University, Ankara, Tur-
key, in 1992 and 1994, respectively, and the
PhD degree in electrical engineering from the
University of Maryland, College Park, in 1998.
He has been with the Advanced Technology
Research group of Nortel since 1998. His
research interests include speech and video
processing with emphasis on transmission over
IP networks, quality of service in fast switches, and application quality in
broadband access networks. He is a member of the IEEE.

Nicolas Christin received the engineering
degree from (cid:4)EEcole Centrale Lille, France,
in
1999, and the Master’s and PhD degrees in
computer science from the University of
Virginia in 2000 and 2003, respectively. From
2002 to 2003, he worked in Nortel’s Ad-
vanced Technology group. From 2003 to
2005, he was a postdoctoral
fellow in the
School of
Information Management and Sys-
tems at the University of California, Berkeley.
He is now a faculty member of Carnegie Mellon University’s
Information Networking Institute, on international assignment
in
CyLab Japan. His research interests are in computer networks,
network security, and network economics, and range from designing
and evaluating formal models and algorithms to implementation
aspects and measurements. He is a member of the IEEE.

mance issues. He holds two patents and four pending patents.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

