Collective Media Annotation using Undirected Random Field Models

International Conference on Semantic Computing
International Conference on Semantic Computing
International Conference on Semantic Computing

Matthew Cooper

FX Palo Alto Laboratory

Palo Alto, CA USA
cooper@fxpal.com

Abstract

We present methods for semantic annotation of multime-
dia data. The goal is to detect semantic attributes (also
referred to as concepts) in clips of video via analysis of a
single keyframe or set of frames. The proposed methods in-
tegrate high performance discriminative single concept de-
tectors in a random ﬁeld model for collective multiple con-
cept detection. Furthermore, we describe a generic frame-
work for semantic media classiﬁcation capable of captur-
ing arbitrary complex dependencies between the semantic
concepts. Finally, we present initial experimental results
comparing the proposed approach to existing methods.

1. Introduction

Substantial current multimedia analysis research focuses
on information retrieval for video content. As media col-
lections move onto the internet, web search companies are
extending their text-based search capabilities to video data.
These systems typically rely on the link structure and text
on the web pages containing the videos to index content.
Video search and retrieval has also been the focus of the
highly successful TRECVID workshops [10]. Although the
use of visual information is emphasized in the TRECVID
evaluations, extracting semantics from visual data in the ab-
sence of textual descriptors remains a major challenge.

Recent work to address this semantic gap has concen-
trated on ontology-based approaches to semantic feature ex-
traction [3, 13]. In this work, a “basis” set of binary classi-
ﬁers are built to determine if a video shot exhibits a speciﬁc
semantic feature. These classiﬁcation outputs are combined
statistically to provide higher-level analysis or enhance in-
dexing and retrieval. Many of these approaches operate at
the shot-level following an initial segmentation. This is de-
sirable for computational efﬁciency, dynamic analysis of lo-
cal sets of frames, and for extraction of semantic features
that exhibit some temporal duration.

Simultaneously, manual tags are now proliferating on

various shared video and image sites such as Flickr and
YouTube. While this information is of tremendous poten-
tial value for video indexing, such as for reﬁning and train-
ing automatic systems, it also presents challenges. Lengthy
videos can have tags that apply only to a small (unidentiﬁed)
portion of the video. Also, the classic problems of polysemy
and synonymy in text categorization (e.g. [1]) are inherited
in aggregating tag data for multimedia categorization.

To supply consistent and reliable annotations for index-
ing or metadata creation, a semi-automatic approach is re-
quired. Manual annotation is not feasible on the scale of
legacy assets, let alone for the ever increasing amounts of
newly produced content. Automatic techniques that scale
can accommodate the quantity of the data, but the quality
of automatic annotations is not sufﬁcient. We can identify
several requirements for the analysis components of an ideal
media annotation system:

• The ability to integrate heterogenous modalities in a

common framework.

• A generic architecture suitable for a wide array of
types of annotations which vary considerably in their
sparseness in data sets.

• The ability to supply a conﬁdence measure or ranking
associated with annotations to support manually revis-
ing results as needed. It should be possible to provide
such measures in time-varying forms at various tem-
poral resolutions.

• A fully automatic mode for annotating either archival
data, or data for which manually supplied tag infor-
mation, text transcripts, or web page link structure is
absent.

We review related work and propose a framework below to
work towards these aims. While we focus on annotating
video, the system is broadly applicable to digital media col-
lections of various modalities.

0-7695-2997-6/07 $25.00 © 2007 IEEE
0-7695-2997-6/07 $25.00 © 2007 IEEE
0-7695-2997-6/07 $25.00 © 2007 IEEE
DOI 10.1109/ICSC.2007.57
DOI 10.1109/ICSC.2007.57
DOI 10.1109/ICSC.2007.57

337
337
337

2. Independent concept detection

3. Collective concept detection

The problem addressed here is the automatic annotation
of temporally segmented video using a set of binary at-
tributes. We refer to these attributes as concepts, and the
general problem as concept detection. We describe a sys-
tem to jointly detect the presence/absence of a set of con-
cepts by exploiting concept interdependence. General sets
of such attributes include those used in [15, 9]. We now
describe the integration of discriminative single label clas-
siﬁers in a framework for collective multimedia annotation
in several computational phases.

2.1. Feature extraction

In the ﬁrst processing step, low-level feature data must
be extracted. In our case, the source video is segmented ac-
cording to shots, and keyframes are identiﬁed within each
shot. The keyframes are processed to extract low-level fea-
ture data. This data may include visual features such as
color histograms, texture or edge features, motion analysis
or face detection output. If time-aligned text data is present,
we can also include standard low-level text features such
as word counts or tf/idf features [12]. The speciﬁc set of
features used is not critical. In the multi-modal case, early
or late fusion approaches can be used to construct the single
concept classiﬁers [14]. We assume their availability for the
construction of the probabilistic model detailed below.

2.2. Classiﬁcation

In the second step, we train a discriminative classiﬁer for
each concept using a labeled training set of low-level fea-
tures. For several years, support vector machines (SVMs)
have been the preferred classiﬁer at TRECVID, and we use
them here. The output of each SVM is transformed to a
probability using a logistic mapping to provide a system for
independent concept detection. In the experiments below,
we use the publicly available low-level features (described
in [16]) and SVM outputs provided by the MediaMill team
[15]. These SVM outputs are highly optimized using a com-
putationally intensive grid search for classiﬁcation param-
eters on a per concept basis. These outputs represent an
extremely competitive baseline annotation system.

The graphical model corresponding to this approach ap-
pears in Figure 1(a). Probabilistically, if the ith concept in
the concept set C is denoted by the binary random variable
Yi, then

P (Y|X) =

P (Yi|X) .

(1)

(cid:89)

i∈C

for low level feature data X.

We now build collective annotation models using the sin-
gle concept discriminative models described in the previ-
ous section. Our ﬁrst approach builds on the discrimina-
tive random ﬁeld (DRF) model of Kumar and Hebert [6].
This model combines discriminative single concept classi-
ﬁers with pairwise concept co-occurrence features repre-
senting contextual information. Their goal is to perform
collective binary classiﬁcation of pixel blocks in images as
either “natural” or “man-made.” For per-block classiﬁca-
tion, they use logistic regression. The random ﬁeld model
incorporates spatial dependencies. More speciﬁcally, they
model the probability of the (collective) vector of binary la-
bels Y given the low-level image block data X as

P (Y|X) =

Ai(Yi, X)+

1

Z(X)

exp

(cid:32)

(cid:88)

i∈S



Iij(Yi, Yj, X)

 .

(2)

(cid:88)

(cid:88)

i∈S

j∈Ni

The terms Ai are the association potentials (or unary or
node potentials) which are local discriminative classiﬁers
for each spatial location i in the set S. Iij is the term rep-
resenting the interaction between the spatial locations i, j.
Ni is the set of neighbors of node i in the graph. [7] de-
tails learning and inference methods for the broad class of
conditional random ﬁelds, including DRFs.

We adapt this approach to collective semantic concept
detection in a video clip. Speciﬁcally, we use the SVM clas-
siﬁers of Section 2 for the association terms in (2). For this,
we map each SVM output to the corresponding probability
PD(Yi = 1|X) following [11]. The subscript D denotes
the single concept (discriminative) classiﬁers output proba-
bilities1. We then set the association term for concept Yi to
be:

Ai(Yi, X) = νi log(PD(Yi = 1|X))

(3)

so that in the absence of interaction terms (Iij = 0), (2)
reverts to the per-concept SVM models. The scalar weights
νi are determined in training as described below.

Next we must identify which concepts are related, i.e.
which concepts are connected by an edge in our graph. For
this, [17] performs a chi-squared test using the ground truth
labeling of the training set. They connect each concept
to the other concepts to which it has the most statistically
signiﬁcant relationships. The resulting graph deﬁnes the
neighborhoods Ni for each concept Yi. This is surely not
optimal. However, learning optimal unstructured graphs
in the general case is NP-hard. Nonetheless, we feel that

1PD(Yi|X) does not equal the marginal probability P (Yi|X) com-

puted from P (Y|X) from (2).

338338338

(a)

(b)

Figure 1. An example of graphical models used for detecting four concepts. Panel (a) shows the
model for independent classiﬁcation, while panel (b) depicts collective classiﬁcation.

evaluating other heuristics and approximation schemes for
graph induction is both critical to the success of these meth-
ods and a promising area for future work.

The interaction potentials in [6] were inspired by long-
standing work in image analysis using Markov random
ﬁelds. Here, we deﬁne interaction potentials building on
recent work in text categorization [2]. First, we rewrite (2)
to clarify our notation:

P (Y|X) =

exp

νi log(PD(Yi|X))

1

Z(X)

(cid:32)

(cid:88)

i∈C

(cid:88)

(cid:88)

(cid:88)

+

i∈C

j∈Ni

k∈K

λ(k)
ij f (k)

ij (Yi, Yj, X)

.

(4)

(cid:33)

We use the notation PD to distinguish the probability
mapped from the single concept SVM output from the cor-
responding marginal probability of the joint model. Com-
paring this equation to (2), we see that our interaction term
is the linear form:

I(Yi, Yj, X) =

(cid:88)

λ(k)
ij f (k)

ij (Yi, Yj, X)

k∈K
= Λij

T Fij(Yi, Yj, X) .

(5)

We ﬁnally note that we can rewrite (4) compactly as:

P (Y|X) =

exp

VT PD(X)+

(cid:32)

1

Z(X)

Λij

T Fij(Yi, Yj, X)

.

(6)

(cid:33)

(cid:88)

(cid:88)

i∈C

j∈Ni

Here the ith elements of V and PD(X) are νi and
log(P (Yi|X)), respectively. Likewise, the kth elements of
Λij and Fij(Yi, Yj, X) are λ(k)
ij (Yi, Yj, X), respec-
tively. This form shows that the random ﬁeld model is sim-
ply a log-linear classiﬁer. For maximum likelihood model

ij and f (k)

training, the gradients of the log-likelihood thus take a stan-
dard form (e.g. [6]).

3.1. The CML+I model

We now detail the interaction potential functions for two
models. The ﬁrst model, denoted collective multi-label in-
teraction (CML+I), captures inter-concept co-occurrence.
These features are deﬁned for each pair of concepts Yi, Yj
that are connected in our graph (i.e. not for all pairs). Thus,
we have the indexed family of interaction potential func-
tions:

f (0)
ij (Yi, Yj, X) =

f (1)
ij (Yi, Yj, X) =

f (2)
ij (Yi, Yj, X) =

f (3)
ij (Yi, Yj, X) =

(cid:40)

1 Yi = Yj = 0
0
(cid:40)

otherwise

1 Yi = 1, Yj = 0
0

otherwise

(cid:40)

(cid:40)

1 Yi = 0, Yj = 1
0

otherwise

1 Yi = Yj = 1
0

otherwise

(7)

By modeling the four possible combinations separately, we
hope to capture all types of pairwise co-occurrence within
the model. For example, the concept “urban” can be ex-
pected to occur when “outdoor” is one (true), however,
“outdoor” may occur when “urban” is zero (false). The
above features allow the model to distinguish the three cases
in which either “urban” or “outdoor” or both are false. For
this model the index set for the interaction potentials is
simply K = {0, 1, 2, 3}. This interaction model was pro-
posed in [2] for text categorization without discriminatively
trained association potentials.

339339339

3.2. The CMLF+I model

We deﬁne a second model to capture concept-feature
co-occurrence, combining ideas from [2] and [4] to deﬁne
the collective multi-label-feature interaction (CMLF+I). We
ﬁrst quantize the low level visual features X across the
training set using k-means (or any other vector quantiza-
tion technique). Denote this discrete representation for the
low-level features as Q(X) ∈ {0, · · · , Q}. For this model,
we deﬁne interaction potentials:

f (q,0)
ij

(Yi, Yj, X) =

1 Yi = Yj = 0, Q(X) = q
0

otherwise

f (q,1)
ij

(Yi, Yj, X) =

f (q,2)
ij

(Yi, Yj, X) =

1 Yi = 1, Yj = 0, Q(X) = q
0

otherwise

1 Yi = 0, Yj = 1, Q(X) = q
0

otherwise

f (q,3)
ij

(Yi, Yj, X) =

1 Yi = Yj = 1, Q(X) = q
0

otherwise

(8)

(cid:40)

(cid:40)

(cid:40)

(cid:40)

The index set for the interaction potentials is K = {(q, i) :
0 ≤ i ≤ 3, 0 ≤ q ≤ Q}. Like CML+I, this model dis-
tinguishes among the four possible combinations of each
pair of labels. It extends CML+I to distinguish among each
label combination in conjunction with a low-level feature
observation. For example, if we observe the quantized low-
level feature value q, which represents most observations
with large green regions in the frame (i.e. vegetation), we
may expect the model weights for features with the “out-
door” concept one (true) and the “urban” concept to be zero
(false). In this manner, content can be used to more ﬁnely
model the inter-concept relationships.

4. Experiments

Here we summarize initial experiments comparing var-
ious proposed systems using conditional random ﬁelds us-
ing the TRECVID 2005 development data for the high-level
concept detection task. The training and test sets each in-
clude more than 6000 video shots from various broadcast
news sources collected in 2004. We also use the graphs in
[17] for direct comparison:

5 concept graph : car, face, person, text, walking/running

11 concept graph : building, car, face, maps, outdoor, per-

son, sports, studio, text, urban, walking/running

The experimental results are summarized in Figure 2 for the
5 concept graph (panel (a)) and the 11 concept graph (panel
(b)). The performance measure is mean average precision

(MAP) which averages precision at each level of recall for
each concept, and then computes the mean (of the average
precisions) over the set of concepts [10].

4.1. Model training and inference

We train the systems to maximize the likelihood of the

training set to ﬁt parameters. The log-likelihood is

L(D) =

νi log(PD(Y(d)|X(d)))+

(cid:32)

(cid:88)

(cid:88)

d∈D

i∈C

(cid:88)

(cid:88)

(cid:88)

i∈C

j∈Ni

k∈K

λ(k)
ij f (k)

ij (Y(d), X(d)) − log(Z(X(d)))

.

(cid:33)

(9)

This gives the gradient equations:

(cid:32)

dL
dνi

=

(cid:88)

d∈D

log(PD(Y (d)

i

|X(d)))−

log(PD(Yi|X(d)))P (Yi|X(d))

,

(10)

(cid:88)

Yi

(cid:32)

dL
dλ(k)
ij

=

(cid:88)

d∈D

(cid:88)

Yi,Yj

f (k)
ij (Y (d)

i

, Y (d)

j

, X(d))−

f (k)
ij (Yi, Yj, X(d))P (Yi, Yj|X(d))

.

(11)

i

j

, Y (d)

Here we use Y(d) to denote the ground truth for train-
ing sample X(d). Also, Y (d)
denote the true values
for concepts i, j ∈ C for sample X(d) while Yi, Yj de-
note binary variables of integration. As demonstrated in
[8] limited memory conjugate gradient methods greatly ac-
celerate training maximum entropy and log-linear classi-
ﬁers. We employ the Broyden-Fletcher-Goldfarb-Shanno
(BFGS) minimization routine for maximum likelihood
model training.

We presently use exhaustive inference, which entails
evaluating P (Y|X) for each observed Y(d), d ∈ D in train-
ing, and marginalizing to compute P (Yi|X) ∀i ∈ C. This
is generally prohibitive due to the exponential growth in
concept combinations with the number of concepts. In the
present context, the number of observed combinations is
much smaller. For example, we observe fewer than 200
combinations in the experiments using the 11 concept graph
(as opposed to the 2048 possible combinations).

(cid:33)

(cid:33)

340340340

(a)

(b)

Figure 2. Experimental results comparing various system variants. Panel (a) shows results for the 5
concept graph, while panel (b) shows results for the 11 concept graph.

4.2. Additional system descriptions

We have two experimental baselines. The ﬁrst, denoted
CMU, shows the results from [17] which proposed the fol-
lowing conditional random ﬁeld model:

P (Y|X) =

(αi + Vi

T X)Yi+

1

Z(X)

exp

(cid:32)

(cid:88)

i∈C



λijYiYj

 .

(12)

(cid:88)

(cid:88)

i∈C

j∈Ni

The key differences between our approach and theirs are

1. We use a probabilistic mapping of an SVM output in

place of their linear association term.

2. Their interaction term does not distinguish between the

three cases in which Yi · Yj = 0.

3. Their interaction term does not account for feature-

concept interaction as in the CMLF+I systems.

It should be noted we did not use the same training/test data
split as [17]. The second, denoted SVM, shows the results
of using the discriminative SVM outputs for independent
concept detection as provided by [15].

To isolate the contributions of the discriminative per-
label classiﬁers, we use the following interaction potential
as used in the CMU system, to build the collective multi-
label (CML) system with (4):

fij(Yi, Yj, X) =

(13)

(cid:40)

1 Yi = Yj = 1
0 otherwise

For this case there is a single feature for each edge
(Yi, Yj), and the summation over k degenerates to a sin-
gle term (so we drop the index k above). This system dif-
fers from the CMU system only in its use of the SVMs
for the discriminative classiﬁer terms. The CML sys-
tem (0.6089) does better than SVM system (0.5977) and
the CMU system (0.5882) for the 5 concept graph.
It
also outperforms both systems on the 11 concept graph
(CMU=0.5211,SVM=0.4975,CML=0.5262).

Next, we add the more complete interaction model of (7)
to (4) to form the system denoted CML+I. This system ac-
counts for different types of inter-concept relationships and
performs at the highest level of all systems in both cases,
with MAP of 0.6228 and 0.5307 for the 5 concept and 11
concept graphs, respectively.

[2] includes another closely related model that we in-
clude in our experimental comparison. Their system is de-
noted CMLF, and is similar to CMLF+I. It uses feature-
concept interaction potentials of the form:

(cid:40)

f (q)
ij (Yi, Yj, X) =

1 Yi = Yj = 1, Q(X) = q
0

otherwise

(14)

As included in our experiments, we use the SVMs to pro-
vide the per-concept association potentials.
[2] used log-
linear association potentials. The main differences between
CMLF and CMLF+I are:

1. We quantize the low-level features extracted from the
video shot or keyframe, so that the resulting interaction
potentials are indexed by the concept pair (i, j) and the
quantized value Q(X). Their context is text catego-
rization, so the potentials are indexed by the concept

341341341

Table 1. The table shows per-concept average precision results using the CML and CML+I models
on the 5 concept graph.

Per concept results 5-concept graph

WALKING/RUNNING

CAR

PEOPLE

TEXT
FACE
MAP

SVM

CML

0.252123
0.352651
0.830886
0.669119
0.894924
0.599941

0.229518
0.34861
0.927372
0.650824
0.888202
0.608905

CML+I
0.260092
0.355762
0.935306
0.670135
0.89243
0.622765

pair (i, j) and the term (word) w corresponding to the
element in the term vector X(w). So there is no quan-
tization step, although it’s straightforward to use term
clustering to effect quantization in the text domain.

2. We consider features corresponding to all four possible

combinations for Yi and Yj.

The CMLF outperforms the SVM baseline for both
graphs, but the CMU baseline does better for the 11 con-
cept graph. The CMLF system has MAP of 0.6009 and
0.5156 for the 5 and 11 concept graphs, respectively. The
CMLF+I system uses the more complete interaction poten-
tial functions of (8). This system outperforms CMLF, but in
both cases is worse than CML+I. The CMLF+I system has
MAP of 0.6122 and 0.5184 for the 5 and 11 concept graphs,
respectively. We believe that this is due to the coarse quanti-
zation and relatively uninformative low-level features used
(k-means to 126 classes), and expect that more sophisticated
quantization will yield further improvements.

Table 1 shows the average precision results for each con-
cept in the 5 concept graph. “People” shows the biggest
increase in both the CML and CML+I cases, we speculate,
due to joint inference with the “face” concept, and possibly
“walking/running” as well. The reverse effect is not evi-
dent, as “people” can be observed without viewing a “face”
(or “walking/running”). This fact could cause the slightly
worse performance for “face” and “walking/running” in the
CML case, relative to the independent SVMs. However for
each concept, CML+I performs essentially as well or better
than the SVMs. We feel these limited results demonstrate
the potential performance gains associated with collective
annotation using full pairwise concept interaction models
and a properly constructed graph.

5. Related work

There is substantial amount of related work, the most
closely related in both technique and application domain

is [17]. As noted above, our random ﬁeld model has sev-
eral components differing from that system that we feel
enhance the model. However, these components are in-
spired by several other related systems. The basic CML and
CMLF models were proposed for multi-category text doc-
ument classiﬁcation in [2]. The main difference here is the
use of independently trained discriminative classiﬁers for
the association terms. Also, they did not use quantization
to build the features analogous to (14). The use of quan-
tization to extend maximum entropy text categorization to
image categorization was proposed by [4]. However, this
work concerned independent per-category rather than col-
lective categorization. Again, we mention the original work
on the DRF model in [6, 5], which integrated discrimina-
tive local classiﬁers for collective classiﬁcation. However,
this work was targeted at classiﬁcation of multiple regions
within a single still image, and used spatially-based interac-
tion terms to deﬁne the graphical model. It also did not in-
clude concept-feature interaction. Finally, the original work
on conditional random ﬁelds in [7] lays much of the theo-
retical groundwork for the various extensions in [6, 17, 2]
and the work herein.

6. Conclusion

In this paper, we have presented early work on further
extending conditional random ﬁeld models for generic me-
dia annotation and semantic concept detection. The goal
of these models is to incorporate contextual information by
modeling complex pairwise concept interaction, and pair-
wise concept and feature interactions. Our initial experi-
ments have shown moderately positive results, but the re-
sults using the CMLF+I model suggest that we need to fur-
ther explore the use of quantization to determine its impact
on performance.

Signiﬁcant future research remains to be performed. The
biggest single unresolved issue is graph induction. Per-
formance will depend critically on a graph that includes

342342342

cept ontology for multimedia. IEEE Multimedia Magazine,
13(3), 2006.

[10] P. Over, T. Ianeva, W. Kraiij, and A. Smeaton. Trecvid 2006
an overview. In Proceedings of the TRECVID 2006 Work-
shop, Nov. 2006.

[11] J. Platt. Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods. In Ad-
vances in Large Margin Classiﬁers. MIT Press, 1999.

[12] S. E. Robertson and K. Sparck-Jones. Simple, proven ap-
proches to text retrieval. Technical Report TR356, Univer-
sity of Cambridge, Computer Laboratory, 1997.

[13] C. G. Snoek, M. Worring, D. C. Koelma, and A. W.
Smeulders. A learned lexicon-driven paradigm for inter-
IEEE Transactions on Multimedia,
active video retrieval.
9(2):280–292, 2007.

[14] C. G. M. Snoek, M. Worring, and A. W. M. Smeulders. Early
versus late fusion in semantic video analysis. In MULTIME-
DIA ’05: Proceedings of the 13th annual ACM international
conference on Multimedia, pages 399–402, New York, NY,
USA, 2005. ACM Press.

[15] C. G. M. Snoek, M. Worring, J. C. van Gemert, J.-M. Geuse-
broek, and A. W. M. Smeulders. The challenge problem for
automated detection of 101 semantic concepts in multime-
dia. In MULTIMEDIA ’06: Proceedings of the 14th annual
ACM international conference on Multimedia, pages 421–
430, New York, NY, USA, 2006. ACM Press.

[16] J. C. van Gemert, J.-M. Geusebroek, C. J. Veenman,
C. G. M. Snoek, and A. W. M. Smeulders. Robust scene
categorization by learning image statistics in context.
In
CVPRW ’06: Proceedings of the 2006 Conference on Com-
puter Vision and Pattern Recognition Workshop, page 105,
Washington, DC, USA, 2006. IEEE Computer Society.

[17] R. Yan, M.-Y. Chen, and A. Hauptmann. Mining relation-
ships between concepts using probabalistic graphical mod-
els. In Proc. IEEE ICME, 2006.

[18] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing
free-energy approximations and generalized belief propaga-
tion algorithms. IEEE Transactions on Information Theory,
51(7):2282–2312, 2005.

important inter-concept dependencies. At the same time,
the number of edges in the graph directly impacts compu-
tational complexity, indicating an important tradeoff.
In-
cluding extraneous edges can also be expected to degrade
performance. Additionally, scalability of the approach will
need to be a major focus. In particular, our current reliance
on exact inference can’t scale to hundreds or thousands of
concepts, even if their co-occurrence is highly sparse. There
is a burgeoning literature on approximate inference tech-
niques for graphical models. [2] suggests two approaches:
binary pruned inference and supported inference, which are
based on label co-occurrence in the training set. Belief
propagation is also becoming a fairly standard approximate
inference technique [18].
[5] also includes extensive ex-
periments with approximate inference techniques for DRFs
which are also applicable to the models described here.

References

[1] M. W. Berry, S. T. Dumais, and G. W. O’Brien. Using lin-
ear algebra for intelligent information retrieval. SIAM Rev.,
37(4):573–595, 1995.

[2] N. Ghamrawi and A. McCallum. Collective multi-label clas-
siﬁcation. In CIKM ’05: Proceedings of the 14th ACM in-
ternational conference on Information and knowledge man-
agement, pages 195–200, New York, NY, USA, 2005. ACM
Press.

[3] L. Hollink and M. Worring. Building a visual ontology for
video retrieval. In MULTIMEDIA ’05: Proceedings of the
13th annual ACM international conference on Multimedia,
pages 479–482, New York, NY, USA, 2005. ACM Press.

[4] J. Jeon, V. Lavrenko, and R. Manmatha. Automatic image
annotation and retrieval using cross-media relevance mod-
In SIGIR ’03: Proceedings of the 26th annual inter-
els.
national ACM SIGIR conference on Research and develop-
ment in informaion retrieval, pages 119–126, New York,
NY, USA, 2003. ACM Press.

[5] S. Kumar. Models for learning spatial interactions in nat-
ural images for context-based classiﬁcation. PhD thesis,
Carnegie Melon Univeristy, Pittsburgh, PA, USA, 2005.
Chair-Martial Hebert.

[6] S. Kumar and M. Hebert. Discriminative random ﬁelds. Int.

J. Comput. Vision, 68(2):179–201, 2006.

[7] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Condi-
tional random ﬁelds: Probabilistic models for segmenting
and labeling sequence data. In ICML ’01: Proceedings of
the Eighteenth International Conference on Machine Learn-
ing, pages 282–289, San Francisco, CA, USA, 2001. Mor-
gan Kaufmann Publishers Inc.

[8] R. Malouf. A comparison of algorithms for maximum en-
tropy parameter estimation. In COLING-02: proceeding of
the 6th conference on Natural language learning, pages 1–7,
Morristown, NJ, USA, 2002. Association for Computational
Linguistics.

[9] M. Naphade, J. R. Smith, J. Tesic, S.-F. Chang, W. Hsu,
L. Kennedy, A. Hauptmann, and J. Curtis. Large-scale con-

343343343

