Semi-Automatic Extraction and Exploitation of Hierarchical

Pipeline Parallelism Using Proﬁling Information

Georgios Tournavitis and Björn Franke
Institute for Computing Systems Architecture

School of Informatics
University of Edinburgh

Scotland, United Kingdom

{gtournav,bfranke}@inf.ed.ac.uk

ABSTRACT
In recent years multi-core computer systems have left the
realm of high-performance computing and virtually all of
today’s desktop computers and embedded computing sys-
tems are equipped with several processing cores. Still, no
single parallel programming model has found widespread
support and parallel programming remains an art for the
majority of application programmers. In addition, there ex-
ists a plethora of sequential legacy applications for which
automatic parallelization is the only hope to beneﬁt from
the increased processing power of modern multi-core sys-
tems. In the past automatic parallelization largely focused
on data parallelism. In this paper we present a novel ap-
proach to extracting and exploiting pipeline parallelism from
sequential applications. We use proﬁling to overcome the
limitations of static data and control ﬂow analysis enabling
more aggressive parallelization. Our approach is orthogonal
to existing automatic parallelization approaches and addi-
tional data parallelism may be exploited in the individual
pipeline stages. The key contribution of this paper is a
whole-program representation that supports proﬁling, paral-
lelism extraction and exploitation. We demonstrate how this
enhances conventional pipeline parallelization by incorporat-
ing support for multi-level loops and pipeline stage replica-
tion in a uniform and automatic way. We have evaluated
our methodology on a set of multimedia and stream pro-
cessing benchmarks and demonstrate speedups of up to 4.7
on a eight-core Intel Xeon machine.

Categories and Subject Descriptors
D.3.4 [Programming Languages]: Processors—Compil-
ers; D.1.3 [Programming Techniques]: Concurrent Pro-
gramming—Parallel Programming

General Terms
Experimentation, Languages, Measurement, Performance

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
PACT’10, September 11–15, 2010, Vienna, Austria.
Copyright 2010 ACM 978-1-4503-0178-7/10/09 ...$10.00.

Keywords
Parallelization, Pipeline Parallelism, Streaming Applications,
Program Dependence Graph

1.

INTRODUCTION

Multi-core computing systems have become ubiquitous
and are the technological drivers not only for general-purpose
desktop computers, but provide unprecedented computing
power to virtually all games consoles and even mobile phones.
However, to realize the full potential the applications have
to be well parallelized. Unfortunately, eﬃcient paralleliza-
tion of a sequential program is a challenging and error-prone
task. It is generally agreed that manual code parallelization
by expert programmers results in the most streamlined par-
allel implementation, but at the same time this is the most
costly and time-consuming approach. Parallelizing compiler
technology, on the other hand, has the potential to greatly
reduce cost and time-to-market while ensuring formal cor-
rectness of the resulting parallel code.

Automatic parallelism extraction is certainly not a new
research area [18]. Progress was achieved in 1980s to 1990s
on restricted Doall and Doacross loops [3, 1, 19].
In
fact, this research has resulted in a whole range of paral-
lelizing research compilers, e.g. Polaris [21], Suif-1 [10]
and, more recently, Open64 [5]. Complementary to the
on-going work in auto-parallelization many high-level paral-
lel programming languages – such as Cilk-5 [7], OpenMp,
StreamIt [8], Upc [11] and X10 [27] – and programming
models – such as Galois [17], Stapl [25] and Hta [9] – have
been proposed. Interactive parallelization tools [12, 16, 2,
13] provide a way to actively involve the programmer in the
detection and mapping of application parallelism, but still
demand great eﬀort from the user. While these approaches
make parallelism expression easier than in the past, the ef-
fort involved in discovering and mapping parallelism is still
far greater than that of writing an equivalent sequential pro-
gram.

In the past parallelizing compiler technology targeted ma-
inly scientiﬁc applications with an abundance of data level
parallelism and, hence, the scope was largely restricted to
the detection and mapping of this particular kind of paral-
lelism. On the other hand, popular multimedia – and many
other streaming applications – typically comprise multiple
levels of parallelism. The advent of general-purpose chip
multi-processors (Cmps) as well as the proliferation of mul-
timedia consumer electronics has underlined the importance
of tools that take a broader approach and target higher-level

3771

2

3

4

5

7

8

9

10

11

12

14

16

17

18

20

21

22

24

25

26

27

28

29

30

31

32

33

34

36

37

while (end) {

/∗ . . . input . . . ∗/
decode info(&bs , &fr ps ) ;
/∗ . . . input . . . ∗/
I I I g e t s i d e i n f o(&bs , &I I I s i d e i n f o , &fr ps ) ;

level 1

for ( gr = 0; gr < max gr ; gr++) {

for ( ch = 0; ch < stereo ; ch++) {

I I I g e t s c a l e f a c t o r s ( gr , ch , . . . ) ;
III hufman decode ( gr , ch , . . . ) ;
III dequantize sample ( gr , ch , . . . ) ;

} /∗ ch ∗/

I I I s t e r e o ( gr , . . . ) ;

for ( ch = 0; ch < stereo ; ch++) {

I I I r e o r d e r (ch , gr , . . . ) ;
I I I a n t i a l i a s (ch , gr , . . . ) ;

for ( sb = 0; sb < SBLIMIT; sb++) {

III hybrid ( sb , ch , . . . ) ;

} /∗ ss ∗/

level 3

level 2

for ( ss = 0; ss < SSLIMIT; ss++) {

for ( sb = 0; sb < SBLIMIT; sb++) {

level 3

i f

( ( ss % 2) && ( sb % 2))
polyPhaseIn [ sb ] = −hybridOut [ sb ] [ ss ] ;

else

polyPhaseIn [ sb ] = hybridOut [ sb ] [ ss ] ;

} /∗ sb ∗/
clip += SubBandSynthesis (ch , ss , . . . ) ;

} /∗ ss ∗/

} /∗ ch ∗/

} /∗ gr ∗/

o u t f i f o (∗pcm sample , . . . ) ;

} /∗ while ∗/

level 1

Single-level
approaches

input,
decoding

98%

Multi-level

+

Replication

header info,
huffman dec.

7%

speedup
potential
1.02x

output

2%

dequantize
dequantiz.

22%

x2

speedup
potential
4.16x

stereo, reorder,
antialias, hybrid23%

subband
synthesis

24%

output

2%

: bottleneck stage

: replicable stage

Figure 1: Multimedia applications typically consist of computation pipelines which operate on a stream of
data of diﬀerent granularity. This ﬁgure shows a code excerpt from the EEMBC 2.0 MP3 decoding application.

parallelism. In this paper we are concerned with the extrac-
tion and exploitation of pipeline parallelism which is central
to most multimedia and stream processing applications [22].
This does not exclude the exploitation of other levels of par-
allelism and, in fact, the approach presented in this paper is
orthogonal to e.g. data level parallelization within individ-
ual pipeline stages.

Parallelism detection is tightly coupled to static data and
control ﬂow analyzes that provide essential information about
dependence relationships that a parallelization scheme must
obey in order to guarantee correctness. Unfortunately, de-
pendence analysis is often statically undecidable and conser-
vative approximations need to be made which in turn limit
the success of automatic parallelization. Recent advances in
proﬁling based parallelization, however, have demonstrated
that many potential dependences do not materialize in real-
world applications and more aggressive approaches can de-
liver performance improvements matching those of manual
parallelization while only requiring minimal user interac-
tion [31]. We build on top of this work and present an
optimistic pipeline extraction methodology based on inter-
mediate representation proﬁling (Ir Profiling) and a hi-
erarchical whole program representation. Because proﬁling
for data and control dependences is inherently unsafe our
approach does not guarantee correctness. We therefore pro-
vide the user with a graphical representation of the extracted
pipeline and highlight the critical items for manual veriﬁca-
tion. Traditionally, there has been an information gap be-
tween proﬁling and compilation. This is due to binary pro-
ﬁling tools that track dependences on machine instruction
level, but are unable to back annotate this information and
make it usable within the parallelizing compiler. We bridge
this gap and show how instrumentation of the compiler in-
termediate representation (Ir) enables us to make the pro-
ﬁling data readily available in the compiler. Furthermore,
we develop a uniform program representation based on the
Program Dependence Graph (Pdg) that is used throughout

the entire parallelization process including proﬁling, trans-
formation, partitioning, mapping and eventually code gen-
eration. We introduce a hierarchical top-down pipeline ex-
traction methodology that splits functions and multi-level
loops on demand, for example, to achieve a better balance
between pipeline stages. Individual pipeline stages that form
a performance bottleneck, but can be shown to operate on
independent data items, are replicated and, eﬀectively, cre-
ate the opportunity for Out-of-Order (OoO) execution.

Our approach is complete and comprises a code-generation
stage that automatically forms processing pipelines and bui-
lds on top of a lightweight, retargetable runtime system. Our
current code generator targets readily available commodity
systems and does not require special hardware (Hw) or oper-
ating system (Os) support. We have evaluated our approach
on a number of popular multimedia and stream processing
applications taken from the Eembc and Spec benchmark
suites. These applications contain complex, idiosyncratic
programming constructs that are typical of many real-world
applications. We demonstrate how we perform pipeline ex-
traction in the presence of multi-level loops, dynamic mem-
ory management, and interleaved I/O and achieve speedups
of up to 4.7 on a eight-core Intel Xeon machine.

1.1 Motivating Example

Multimedia applications typically comprise pipelined com-
putations and operate on a stream of data of diﬀerent gran-
ularity. This is illustrated in the code excerpt in ﬁgure 1
representing a simpliﬁed Mp3 decoder. The outer-most loop
operates on audio frames, III stereo operates on both stereo
channels, and III hufman decode and III antialias operate
on a single audio channel at each invocation.
It is clear
that any approach that forms pipelines by partitioning the
code on a single level (e.g.
[28, 4, 30]) is always bound by
the execution time of the slowest pipeline stage. For our
example, partitioning on level 1 can only form a pipeline
of two highly imbalanced stages, input & decoding (98%

378of total execution time) and output (2%), which in theory
can lead to a minuscule 2% improvement over the sequen-
tial execution but in practice will lead to slowdown due to
communication overheads. Similar to top-level partitioning
neither does partitioning at levels 2 or 3 alone result in a
balanced pipeline. Decoupled Software Pipelining (DSWP)
[20], for example, operates on a representation too low level
to perform the necessary data privatization required for its
parallel implementation. Our approach, on the other hand,
is not restricted to a single loop level from which to extract
pipeline stages and constructs the more balanced pipeline
on the right side of ﬁgure 1. Using a hierarchical whole-
program representation we identify the pipeline stage that
represents the bottleneck (input & decoding) and selectively
expose inner levels, creating a pipeline that spans multiple
levels of the loop nest. The more equally balanced pipeline
has a speedup potential of 4.16.

In general, multi-level loop partitioning can expose more
parallelization opportunities and, thus, provide more ﬂexi-
bility to the partitioner. For instance, the III hufman decode
function that is inherently sequential can be decoupled from
III dequantize sample. Then the latter, which is in fact a
parallel stage and can operate on two audio channels in-
dependently, can be replicated as shown in the rightmost
pipeline in ﬁgure 1.

In summary, our approach extracts a pipeline structure
that a programmer would construct using a data ﬂow or
streaming programming paradigm. For instance, replication
of pipeline stages corresponds to stateless ﬁlters and split-
join operations in the StreamIt programming language.
Similarly, multi-level loop partitioning corresponds to multi-
rate signal processing where ﬁlters operate at diﬀerent input
and output rates.

1.2 Contributions
Among the contributions of this paper are:

• We demonstrate that Ir-proﬁling is a powerful tool
for the extraction and exploitation of parallelization
Ir-proﬁling bridges the informa-
beyond loop level.
tion gap between the low-level execution proﬁle and
the IR maintained within the parallelizing compiler.
This simple, yet eﬀective back annotation capability
distinguishes our approach from most other proﬁling
methodologies.

• We develop a top-down approach for the extraction of
processing pipelines from sequential applications. We
exploit the power of a whole-program Ir, but avoid ex-
posing overly detailed (and possibly redundant) depen-
dence information to the pipeline extraction pass. Us-
ing an iterative, selective unfolding strategy we specif-
ically target the levels of computationally intensive
code regions that will yield exploitable parallelism.

• We extend conventional linear-pipeline parallelization
with two concepts borrowed from streaming languages,
namely multi-level pipelines and stage replication. Ad-
ditionally, we demonstrate how their combination can
uncover further parallelization opportunities in a do-
main existing approaches have failed to explore.

The remainder of this paper is structured as follows. We
introduce our methodology for pipeline extraction in section
2. This is followed by the presentation of empirical results

sequential

 source

C

CoSy

compiler

CF G

partitioning

instr/nted 

source

C

s

p

p

i

e

p

c
i

ﬁ

e

li

c

n

e

a

t
i

o

n

exec

trace
ﬁles

input

pipelinelib

runtime

Proﬁtability
estimation

Hierachical

Explorer

whole

program

PDG

Dependence

Analyzer

Loop Analysis

Partitioner

Trace Analyzer

GCC

Figure 2: Overview of the parallelization work-ﬂow.

in section 3. In section 4 we discuss related work before we
summarize and conclude in section 5.

2. METHODOLOGY

The parallelization work-ﬂow is illustrated in ﬁgure 2. The
sequential source program written in C is processed by the
CoSy C compiler and its IR is instrumented. The resulting
executable is executed with one or more representative input
ﬁles. This results in a set of trace ﬁles that is presented to
our trace analyzer for dependence analysis. The generated
program dependence graph is passed on to the partitioner
that will produce a pipeline speciﬁcation with annotations
relating to the original Ir. The CoSy compiler then pro-
cesses the source program a second time. It generates par-
allel C code with calls to our runtime system according to
the pipeline speciﬁcation. Eventually this code is compiled
for the target platform using the Gcc compiler and linked
against the pipeline runtime library.

In subsection 2.1 we explain how we generate the program
dependence graph from the trace ﬁles. The hierarchical top-
down partitioning algorithm is presented in paragraph 2.2.
Code generation is covered in paragraph 2.3. Pipeline stage
replication and the generation of multi-level pipelines are
part of this algorithm and are introduced in paragraphs 2.4
and 2.5, respectively.

2.1 Program Dependence Graph

The Ir which is used to perform the hierarchical paral-
lelism extraction is based on the Program Dependence Graph
(Pdg) [6] with its explicit representation of both control and
data-dependencies. In our case the Pdg is derived from a
middle-level control-ﬂow representation which is augmented
with data-dependence edges as computed by the Depen-
dence Analyzer (ﬁgure 2) based on proﬁling information.
Our choice to operate on a middle-level representation is
motivated by the following factors: (i) it provides rich high-
level information about the loop constructs and the data
organization, facilitating aggressive code and data transfor-
mations, (ii) code partitioning transformations can remain
simple since we can still rely on the full set of compiler op-
timization passes and (iii) the existing basic-block structure
avoids overly aggressive coarsening during partitioning with-
out compromising load-balancing.

Each node of the Pdg represents a single-entry-multiple-
exit (Seme) list of Ir statements. In addition, using a pre-
processing pass we isolate function calls and place them in
separate nodes. We deﬁne compound nodes to encapsulate
the hierarchical structure of a program as any of the follow-
ing: (i) the basic block (Bb) containing a function call site

379Algorithm 1: Top-down parallelism selection.

Input
· L, F : loop and function set respectively
· CT REE: tree of compound nodes
· L0: virtual top-most loop
· Ldoall: proﬁtable DOALL loops
· Wi : ∀i ∈ CT REE, proﬁled weight of i
· np: # of available cores
Result
· Pdoall: selected DOALL loops
· Ppipe: selected pipelined loops
Data
· Q: work queue

in addition to the maximum recursively deﬁned set of Bbs
that the speciﬁc function call includes, (ii) the set of Bbs
belonging to a loop (loop-structure information is based on
a variant of standard control ﬂow-graph analysis performed
before the instrumentation pass) (iii) a Strongly Connected
Component (Scc) containing other compound nodes or sin-
gle Bbs.

The Pdg is initially computed for the whole program and
then compound nodes are formed in a postorder fashion
based on a whole program function/loop tree. At this point
compound nodes consist of either functions or loops. The
current implementation is based on the simple iterative al-
gorithm from [1]. Each data-dependence edge in the Pdg
is annotated with the following ﬁelds: (i) loop-carried bit
mask that designates the loop-levels which carry the rele-
vant dependencies, (ii) intra-iteration dependency bit mask,
and (iii) mean size of the data communicated between the
adjacent nodes per iteration at each level.

2.2 Top-Down Hierarchical Pipeline Stage

Partitioning

Most applications that exhibit pipeline parallelism will
only have a small number of dominating stages. When tar-
geting Cmps with high a number of cores this factor will
eventually limit the maximum attainable speedup and lead
to poor utilization of the available resources. This neces-
sitates the exploitation of multiple domains of parallelism.
More speciﬁcally, we consider (i) data-level parallelism as
well as (ii) pipelines that span multiple loop-levels and (iii)
replication of stateless stages.

The parallelism selection algorithm operates on a partially
folded Pdg.
Initially, the whole program is folded into a
single function node. Algorithm 1 describes the iterative
traversal of the Ir and unfolding of compound nodes which
results in the selection of the loops that contain proﬁtable
parallelism. At this stage, the algorithm unfolds only the
compound nodes (loops or function calls) that are likely to
expose additional exploitable parallelism. At current, we
use a simple heuristic based on ﬁxed thresholds for data
and pipeline parallelism (lines 5 and 7, respectively), but
will consider a more advanced and possibly machine learning
based strategy at a later stage.

For sequential loops we use pipelining (line 8). The ef-
fectiveness of pipeline parallelization is mainly determined
by the execution time of the longest stage. Therefore, the
primary objective of a pipeline partitioning algorithm is to

1 Procedure top_down_parallelize
2 Q ← {L0} ;
3 while Q (cid:5)= ∅ do

c ← Q.poll();
if (c ∈ Ldoall) ∧ (Wc > thresholddoall) then

add c in Pdoall;

else if (c ∈ L) ∧ (Wc > thresholdpipe) then

(P, Wpipe) ←ﬁnd_pipeline(c, np);
if P (cid:5)= ∅ ∧ Wc/Wpipe > thresholdspeedup then

add c in Ppipe;

else

Q.add(children of c in CT REE);
else if (c ∈ F ) ∧ (Wc > threshold) then

Q.add(children of c in CT REE);

4

5

6

7

8

9

10

11

12

13

14
15 end

balance the load across the available cores. Our key obser-
vation is that balancing can be eﬀective only if the granu-
larity of each node is small relatively to the stage size of a
theoretically optimally balanced pipeline that uses the max-
imum of the available cores, i.e., Wloop ÷ #P . Unfortu-
nately, the size of the schedulable nodes is inherently lim-
ited by the code structure (e.g.
function, loop nests) and
the dependencies among them. We address this problem
through (i) partitioning of multi-level loops, (ii) stage repli-
cation and (iii) function-splitting. Therefore, partioning is
performed in two steps. First, we preprocess the PDG per-
forming the aformentioned transformations when applicable
(preprocess PDG() in algorithm 2). Next, we partition the
resulting DAG using a heuristic that eﬀectively tries to bal-
ance the load given the number of available resources (par-
tition loop()).

The preprocessing step iterates over the Pdg of the loop
attempting to break the slowest compound node in ﬁner-
grain blocks of computation. At the beginning of each it-
eration, Sccs are collapsed, resulting in a Directed Acyclic
Graph (Dag). This ensures that the algorithm processes
chunks of computation that can be actually executed in par-
allel. In the case of a dominating stage that is replicable we
eagerly augment it with nodes that preserve its replication
property, i.e., without introducing new cycles in the Pdg.
This augmented stage is replicated multiple times till it is no
longer the dominating stage of the pipeline (lines 8-10). Al-
ternatively, if a loop or function dominates the loop body we
opt to unfold it (lines 11-12) in order to generate ﬁner-grain
components using multi-level loop partitioning and function-
splitting. Finally, in the case of a Strongly Connected Com-
ponents (Scc) we seek to uncover a computation which is
not part of the dependence cycle. Again, we select to unfold
the compound node of the Scc with the highest W (lines
13-15). This iterative process is repeated until the dominat-
ing stage is under the threshold wloop ÷ #P (line 17), since
further partitioning will not increase the attainable speedup.
In addition, we bound the unfolding of loops up to a depth
thresholddepth since partitioning deeply nested loops is lim-
ited by the synchronization/communication overhead. For
the applications we considered a loop depth of 5 to be ade-
quate to uncover all available exploitable parallelism.

In the second step, partition loop partitions the Pdg of
a given loop in successive stages (P0..Pnp−1) by eﬀectively
following a topological ordering of the graph. Consequently,
any inter-stage data dependencies will be pointing to a suc-

380P

{P Wi}

Algorithm 2: Pipelined-loop partitioning.
Input
· l: the target loop
· np: # of available cores
Result
· P : the set of partitions
· Wl : max
Data
· Q: sorted descending list, sorted by Wv, v ∈ P DG
· Pi: set of nodes in P DG assigned to partition i
· P Wi: aggregate weight of partition Pi
· RS[v]: v → [1, np], v ∈ P DG, > 1 if v is replicated
1 Procedure partition_loop(l, np)
2 (DAG, Wmax) ← preprocess_PDG();
3 k ← 0;
4 Q.add(roots of DAG);
5 while (Q (cid:5)= ∅) ∧ (k < np) do

v ←ﬁrst in Q : P Wk + Wv < Wmax + θ;
if v == ∅ then
k ← k + 1;
P Wk ← 0;
continue ;

else if RS(v) > 1 then

add v in Pk+1 .. Pk+RS(v);
k ← k + RS(v) + 1;

else

add v in Pk;
P Wk ← P Wk + Wv;

Q.add(ready children of v in DAG);

17
18 end
19 add remaining nodes of P DG in Pnp−1;
20 return (

i∈[0,k−1] Pi, max{P W }) ;

S

6

7

8

9

10

11

12

13

14

15

16

1 Procedure preprocess_P DG(l)
2 set RS(u) ← 1, ∀u ∈ l;
3 repeat
4

DAG ← compute_SCCs(P DGl);
augment_replicable_stages();
peek v ∈ DAG : Wv is max;
update Wv, ∀v ∈ DAG;
if v is replicable ∧ RS(u) < np − 2 then

RS(v) ← RS(v) + 1;
Wv ← Wv · RS(v)−1
;
RS(v)
L then
P DGl.unfold(v);
else if v is SCC then

else if v ∈ F

S

peek w ∈ SCC : Ww is max;
P DGl.unfold(w);

else break ;

5

6

7

8

9

10

11

12

13

14

15

16

17 until Wv ≤ (Wl ÷ np) + θ ∨ ldepth(u) > thresholddepth;
18 return (DAG, Wv);

cessive stage, preserving the sequential semantics of loop
execution. Nodes are processed using a descending sorted
list, sorted by the proﬁle weight of each node. In case of a
tie, the node that minimizes communication with the next
stage is selected. A node is included in the current stage
only if its addition will not result in exceeding the threshold
given as input (line 6). The threshold is not wloop ÷ #P as
before, but it is given as a parameter that equals the maxi-
mum node weight that derived from preprocessing. Thus, we
avoid eager partitioning -which subsequently leads to higher
communication and context switching cost- when it does not
decrease the execution time of the dominating stage. In ad-
dition, we use an slack factor θ to avoid excessive number of
under-ﬁlled partitions.

Loop partitioning, along with the set of partitions P , also
returns the weight of the dominating stage, which assists
hierarchical parallelization to determine whether it should
add the current loop to the set of loops that can be eﬀectively
parallelized Ppipe (line 9 in algorithm 1) or proceed to inner-
levels of the component hierarchy (line 12).

2.3 Parallel Code Generation and Runtime

System

The code generation process (ﬁgure 3) takes as input the
partitioned pipeline speciﬁcation and the original procedure-
level Cfgs. Parallel code generation is then performed on
the Cfg for each procedure at the middle-level Ir in the
CoSy compiler.

The description of the code generation process that fol-
lows is based on the example code in ﬁgure 4. On the left
hand, the annotations refer to the contribution (in %) of
each statement in relation to the whole program execution

PDG

Partinioner

pipeline

speciﬁcation

mutliple
CFGs

Code

generation

sequential

CFG

Figure 3: Overview of the code generation process.

time. The initial Cfg which is used for the proﬁling stage as
well as parallelization is shown in ﬁgure 5(a). Basic blocks
with just one unconditional control statement (e.g. bb4, bb5 )
are place holders for loop control structures which are not
present (e.g. test and increment block respectively) and are
inserted by the initial control ﬂow analysis pass.

Control-Flow.

First, algorithm 3 computes the set of Bbs (V a

s ) of the
original Cfg which include the control instructions that de-
termine the execution of the Bbs in the current pipeline
stage s (line 3). This control-replication set of Bbs can
be computed as the union of the vertices in Pdg that are
backwards-reachable from any vertex in s using control-
dependence edges only. In ﬁgure 5, for stage 2 this is V r
2 {bb1,
bb8}. Then, we create a new function with a subgraph of
the original Cfg that includes both the blocks assigned to
stage s by the partitioner V a
s and the control-replication
blocks V r
s (line 10). We also replicate the preheader, loop-
bb7, bb0 and
header and the loop-increment blocks (e.g.
bb5 respectively) which are later used to inject the pipeline
communication code. Initially, replicated Bbs contain only
the control instruction of the original Bb. The subgraph in-

381Algorithm 3: Pipeline parallel code generation.
Input
· CF G: original control-ﬂow graph
· P DOM T : post-dominance tree of CF G
· P DG: program dependence graph
· N : number of pipeline stages
· Pdesc[N ]: pipeline stage descriptors
· V a
Result
· CF G(cid:2)[N ]: a (Vs, Es) graph ∀ stage s ∈ Pdesc
Data
· V r
· outctrls: predicate variables assigned in stage s

s : control-replication BBs of stage s

s : set of BBs assigned to stage s

1 Procedure pipeline_loop:
2 foreach s ∈ Pdesc[N ] do

3

4

5

6

7

8

9

10

11

12

13

14

15

S

V r
s = compute_control_replication_BB(P DG, s);
Vs = V r
Es = {e(u, v) : u, v ∈ Vs};
foreach e(u, v) : u ∈ Vs ∧ v /∈ Vs do

V a
s ;

s

= deepest ancestor of v in P DOM T : v(cid:3) ∈ Vs;

v(cid:3)
Es = Es

S

{e(cid:3)

(u, v(cid:3)

)};

end
CF G(cid:2)[s] = outline_CFG_subgraph(CF G, Vs, Es);
outctrls = capture_outgoing_control_ﬂow(CF G(cid:2)[s]);
ﬁx_local_data_references(CF G(cid:2)[s], s);
ﬁx_global_data_references(CF G(cid:2)[s], s);
insert_pipeline_dataﬂow(CF G(cid:2)[s], outctrls, s);
init_copyin_data(CF G(cid:2)[s], outctrls, s);
init_copyout_data(CF G(cid:2)[s], outctrls, s);

16
17 end
18 insert_pipeline_initialization_ﬁnalization(CF G);

cludes every edge connecting vertices in Vs, but also edges
that substitute dangling outgoing edges. We redirect these
edges to the deepest ancestor of the missing vertex in the
post-dominance tree of the original graph (lines 6-8). For in-
stance, in stage 3 of the previous example the edge (bb1, bb7)
is replaced by edge (bb1, bb2) as shown in ﬁgure 5(c). The
ﬁnal step is to add Ir statements that capture the outcome
of the conditional-control statements of Bbs in V a
s which
are control-replication Bbs for any of the subsequent stages
(line 11). For each such conditional-control statement we
deﬁne a new local variable, predicate, which is assigned the
value of the conditional expression. This transformation en-
ables us to communicate predicate values to all the equiv-
alent control-replication Bbs by utilizing a single, uniform
pipeline data-ﬂow mechanism (line 14).

5%

20%
50%

20%
5%

1

2

3

4

5

6

7

8

while ( ( n = r e a d f i l e ( inf , data ) ) != EOF) {

for ( blk=0; blk<n ; blk++) {

coef [ blk ] = decode ( data , blk ) ;
raw data [ blk ] = inv transform ( coef , blk ) ;

}
out data = enhance filter ( raw data ) ;
w r i t e f i l e ( outf , out data ) ;

} /∗ while ∗/

Figure 4: Source code for the example in ﬁgure 5

Privatization.

1

In line 12 any references to local

variables or arguments
of the original function are patched to refer to thread private
data. For every such variable referenced in the body of an
outlined stage function a new local, stack-allocated variable
is deﬁned. Aggregate-typed variables (structures or arrays)
might be changed to pointer-to-original-type variables to fa-
cilitate copy-by-reference initialization for shared “copy in”
objects.

References to global variables (line 13) are more compli-
cated to handle since all threads share the same global scope.
Aggregate types are all substituted from pointer-to-original-
type variables to avoid allocating and initializing memory in
1

Terminology regarding variable or object visibility, storage,
etc.
is based on the language-portable IR of the compiler
and not the one of ISO C. Nevertheless, the usage of terms
should be clear from the context.

Bss for threads that do not access a speciﬁc variable. At ﬁrst
we followed the common parallel programming technique of
using an array-of-original-type, indexed by the thread ID. A
second option is to use Thread Local Storage variables that
implement a platform-speciﬁc and optimized mechanism to
access thread private data. Evaluation of these two options
led to the conclusion to adopt the latter, although not purely
portable, because thread ID indexing results in signiﬁcant
performance degradation. This was particularly signiﬁcant
in applications like bzip2 (see table 2.3) that access global
variables repetitively in computationally intensive code.

Function name
qSort3
sortIt
simpleSort
fullGtu
Total for
compression thread

Reduction of memory accesses (in %)

loads

stores

20.82
50.41
15.91
1.21

24.35

3.59
20.88
4.77
3.59

2.52

Table 1: Comparison of using thread-local-storage for
privatizing global and static variables, compared to a
thread-id indexed array scheme. The table shows the
reduction of dynamic memory accesses for the dom-
inating functions of bzip2 compression algorithm.

Further investigation revealed that this was due to a com-
bination of poor common subexpression elimination and in-
creased register pressure. In addition, this eﬀect is overem-
phasized in architectures like x86 64 which have a limited
amount of architectural registers. Thread local storage, on
the other hand, does not suﬀer from these side-eﬀects be-
cause it utilizes a free segment register and ﬁxed oﬀsets to
perform each access to private data. Nevertheless, we re-
tained thread-id indexed arrays in order to provide a mecha-
nism to access private data before/after thread creation/ter-
mination. Finally, any local variables of static storage du-
ration are handled similarly to globals.

Data-ﬂow.

Pipeline data-ﬂow is based on the communication prim-
itives of the runtime system (libpipeline). Each pipeline
thread is connected to each immediate predecessor and suc-

382bb0
n (cid:2) read_ﬁle()
goto bb1

bb1
if n!=-1 bb7 : bb6

bb6
 

bb7
blk := 0
goto bb8

bb8
if blk<n bb9 : bb2

(cid:4)

bb2
call enhance_ﬁlter()
goto bb3

bb9
call decode()
goto bb10

bb3
call write_ﬁle()
goto bb4

(cid:3)

bb10
call inv_transform()
goto bb11

bb4
goto bb5

bb11
blk := blk + 1
goto bb8

bb5
goto bb0

(cid:2)

(cid:2)

[0, 1]

L[CD]

4

5

 (data)

(n)

7

(blk)

[8, 11]

L[-, CD]

(blk)

9

(blk)

L[-, D]

 (coef)

(cid:2)

(blk)

(raw_data)

10

(cid:3)

2

3

(out_data)

L

(cid:4)

control dep.
data dep.
control & data dep.
L[] : loop-carried dep. vector
C: loop-carried control dep.
D: loop-carried data dep.

Partitioning

: 0, 1, 4, 5, 6, 7, 8, 9, 11

(cid:2)
(cid:3)
(cid:4)

: 10

: 2, 3

Stage _(cid:2)

Stage _(cid:3)

Stage _(cid:4)

0

1

6

4

5

7

8

9

11

0

1

7

8

5

10

11

0

1

2

3

5

: replicated BB
: normal BB

(a) sequential CFG

(b) PDG with folded SCCs

(c) partitioned CFGs

Figure 5: A simpliﬁed example based on the source of ﬁgure 4, demonstrating the intermediate representations
used for partitioning and parallel code generation.

cessor using two single-producer -single-consumer (Spsc)
queues, one outgoing for produced data and one incoming
for recycling consumed buﬀers. Queue operations (push(),
pop()) communicate ﬁxed-sized pointers, which eﬀectively
point to structures that contain the communicating data.

Inter-stage dependencies are satisﬁed at the loop itera-
tion boundaries. Stages with a predecessor pop a context
buﬀer and restore its contents at the beginning of the loop-
header (e.g. bb0 for the outer loop in ﬁg. 5(a)). Similarly,
stages with a successor store the outgoing data in a buﬀer
structure and push them in the outgoing queue at the end
bb11). Scalar variables
of the loop-increment Sese (e.g.
are copied-by-value in the context buﬀer. Aggregate-typed
and dynamically allocated objects are copied by value at
the producer side, unless our analysis shows that there is no
intra-stage loop-carried dependence for this variable. In the
latter case we can copy-by-reference. However, in the case
that data are pushed further to subsequent stages we have to
establish additional pointer recycling queues between non-
adjacent stages.

“Copy in” and “Copy out” Data.

In addition to data communicated through inter-stage de-
pendencies, code generation has to handle the initializa-
tion/ﬁnalization of “copy in” and “copy out” variables, re-
spectively (lines 15-16). We qualify as “copy in” (similar in
semantics to OpenMp’s copyin clause) any thread-private
variable which needs to be initialized with the value of the
sequential thread. “Copy in” variables are identiﬁed by in-
coming data-dependence edges that have their source out-
side the loop body. A variable that is “copy in” in multiple
stages, but it is also part of an inter-stage dependency is ini-
tialized only in the earliest stage. Pipeline data-ﬂow ensures
that later stages will receive the initialized copies. On the
other hand, thread-private variables are qualiﬁed as “copy

out” if there is a deﬁnition in the loop body that reaches an
external use. Handling “copy out” variables is more com-
plicated, though. If a loop has multiple stages with a Bb
pointing to a loop-exit, the last modifying stage has to be
identiﬁed at runtime. Variables that have been initialized
with the copy of the sequential thread by reference are im-
plicitly copied out and no further action is necessary. Vari-
ables which are private to more than one stage are ﬁnalized
by the sequential thread using the array of private copies
where each thread has stored a reference to the buﬀer it
used before terminating. In order to determine the loop-exit
that terminated the loop we use a special variable private to
each thread and a mechanism analogous to the one described
earlier for control-replication blocks.

Pointer Disambiguation.

The allocation of thread-private pointer variables is han-
dled just like any other variable, scalar or aggregate. Ini-
tialization of pointers that might point to thread-private
data, though, is more complicated. Each thread is consum-
ing multiple incoming buﬀers which in turn contain priva-
tized copies of data. Pointers to these data must be initial-
ized appropriately. In principle, in the presence of recursive
data-structures (e.g.
linked lists) this requires either scan-
ning and updating pointers through the whole structure or
patching each dynamic pointer access. Copy-or-discard pro-
posed by Tian et al.
[30] is following the latter approach
which introduces a signiﬁcant overhead at every pointer ac-
. Fortunately, in practice the majority of streaming
cess
applications use simpler structures. Therefore, we opted for
a simple, yet eﬀective mechanism that dynamically disam-

2

2

In C programs arrays are passed-by-reference, i.e., trans-
lated to pointers. This results to most of the accesses in
nested functions to be pointer rather than array accesses.

3833

biguates and translates pointers to thread-private-data. For
each privatized object we register its original address in the
. In
context of the sequential thread in a translation table
the case of statically allocated objects we emit explicit calls
at the pipeline initialization section. For heap-allocated ob-
jects, we intercept any memory allocation calls (malloc(),
etc.). At inter-stage buﬀer allocation each buﬀer is assigned
a sequence number, context-id, and each object copy is asso-
ciated with this ID and its (dynamic or static) size in bytes
(ﬁgure 6). Eﬀectively, this establishes a notion of alterna-
tive thread-private contexts. Each object as accessed by the
initial context of the sequential thread is associated with the
special zero context-id. Then, at pipeline runtime when a
buﬀer is popped out of the queue every “copy in” or thread-
private pointer is updated based on its current value and
the sequence number of the incoming buﬀer.

Translation table

(addr base, size)

address base
0xaaaa0000

size

context id
0

(addr, id)

.
.
.

.
.
.

.
.
.

0xbbbb0000

(addr base, size)

(base+offset)

allocation

private-copy

creation

d
i
 
t
x
e
t
n
o
c

k

.
.
.

.
.
.

0

1

k

(addr, id)

translate to 
private-copy

Figure 6: Runtime pointer disambiguation.

2.4 Stage Replication

Pipeline stages that have no loop-carried intra-stage de-
pendencies are stateless and, thus, can be replicated in or-
der to further reduce the critical path of the pipeline. This
kind of parallelism is highly beneﬁcial for applications that
contain a single dominating stage that forms a performance
bottleneck.

In order to process multiple incoming buﬀers in parallel
our framework spawns multiple threads. Data that carry
any loop-carried false dependencies within the replicated
stage have to be privatized to facilitate independent exe-
cution.
Incoming buﬀers can be processed Out-of-Order
(OoO) by the thread pool since its output is either com-
pletely independent or it will be explicitly pushed to the
next stage through the outgoing buﬀers. It is important to
stress at this point that the OoO property creates further
opportunities regarding the work distribution policy, i.e., the
policy that speciﬁes which thread will consume each incom-
ing buﬀer. Thus, in the case that diﬀerent iterations of a
stage show great variation in processing time we have the
ﬂexibility to employ a load balancing scheme. In this study
we employ a simple round-robin policy that has the beneﬁt
of lock-free single-producer-single-consumer queues.

Detecting parallel stages using the Pdg is relatively strai-
ghtforward. Using the loop-carried dependence bit sets pre-
computed in the proﬁling stage we can mask out data depen-
dencies that manifest only across inner loop levels. Comput-
ing the additional data that have to be privatized to elimi-
nate any false dependencies can be deferred until the code-
3

The translation table is practically implemented using a

search tree rather than a table.

generation phase. Stage replication is invalid in the case of
stages with loop exits that modify global data which are later
used outside the loop (“copy out” data). Otherwise, replica-
tion is permitted but without the option of OoO processing
due to the induced intra-stage control dependency.

Although stage replication essentially diverges from a pure-
ly linear pipeline paradigm it is straightforward to extend
our code generation and runtime strategy to handle it. We
extended libpipeline runtime system with the following prim-
itives: (i) push a buﬀer to one of the multiple out-going
queues, the selection is based on a callback (current imple-
mentation supports only static balancing), (ii) pop a buﬀer
from on of the multiple queues of the previous stage, (iii)
notify siblings threads by inserting a special ﬁnalisation to-
ken in the tail of their incoming queue. The latter is only
necessary if the replicated stage contains loop exits blocks,
otherwise it is the responsibility of the preceding thread to
replicate the ﬁnalization across all its successors.

2.5 Multi-Level Pipelines

We have extended our analyzes to handle partitions that
span more that one loop-level. Eﬀectively, this coalesces
multiple pipelines that operate at diﬀerent rates into a sin-
gle linear pipeline. It is important to note that the choice
of a Pdg based representation, as opposed to a control-
ﬂow based one, enables us to handle the rather complicated
control of nested loops in a uniform and transparent way.
Therefore, the partitioning algorithm can proceed and dis-
tribute the internal nodes of an unfolded inner loop level to
diﬀerent partitions and largely disregards the code genera-
tion intricacies. Then, as part of the P DG → CF G trans-
formation, the unfolded loops are processed in a bottom up
order. The replication of the additional control dependencies
that determine the execution of the nodes of nested loops re-
sult in the replication of the control-ﬂow of the inner-loop to
multiple threads, similar to single-level partitioning. Data
communiation is handled in the same fashion too. For in-
stance, in ﬁgure 5 we process the inner For loop ﬁrst. In
this case bb9, bb10 are control dependent to the header of the
nested loop bb8. This Bb is included in the resulting par-
titioned CGFs of both stage 1 and 2, ﬁgure 5(c). Finally,
data-communication code which is relevant to the inner-loop
only (e.g. blk and coef ) is injected in bb11 for stage 1 (out-
going) and bb8 for stage 2 (incoming).

2.6 Safety

The use of proﬁling for dependence analysis cannot guar-
antee safety. Unlike static analysis that reasons about all
possible program execution paths proﬁle-driven analysis is
limited to a small number of paths and, hence, might miss
data or control dependences. We therefore expect the user
to perform the ﬁnal veriﬁcation of the suggested partitioning
scheme.

In this work we have taken the following steps to verify
the correctness of the generated parallel code. First, we
have run a number of tests with both the sequential and
the parallel versions of the programs on diﬀerent input data
sets and compared their outputs. Second, we have manually
inspected the generated code. In this second step we have
been guided by our tools that generate an additional, graph-
ical pipeline diagram highlighting the data items and code
regions where static and dynamic dependence information
diﬀers.

384Intel Xeon Server

Hardware

Dual Socket, Intel Xeon X5450 @ 3.00GHz

2 Quad-cores, 8 cores in total

32KB I-cache & 32KB write-back D-cache
6MB L2-cache shared/2 cores (12MB/chip)

16GB DDR2 SDRAM

64-bit Scientiﬁc Linux with kernel 2.6.18 x86 64

GNU gcc 4.4.1

-O3 -march=core2

O.S
Compiler

Table 2: Hardware and software conﬁguration de-
tails of the evaluation platform.

We are fully aware that such a manual veriﬁcation pro-
cess is not scalable and envisage a scenario whereby dynamic
checks for dependence violations are inserted into the gen-
erated code. These checks may be implemented in the un-
derlying pipeline library or be based on additional hardware
[32]. This is, however, beyond the scope of this paper.

3. EMPIRICAL EVALUATION

We evaluated our methodology on a shared memory sys-
tem comprising dual quad-core Intel Xeon processors. The
conﬁguration of the target platform is given in table 2.

3.1 Performance

For our evaluation we have chosen four non-trivial bench-
mark applications from the Eembc and Spec benchmark
suites with up to 23,000 lines of code. The details of these
benchmarks are shown in table 3. While there are more
programs in the two benchmark suites that are amendable
to pipeline extraction we are mainly interested in multime-
dia and stream processing and, hence, we have restricted
ourselves to those programs which are most representative
of this application domain. In the following paragraphs we
provide an in-depth discussion of the four benchmark appli-
cations, their parallelization and the resulting performance.

Mp3 Decoding (Eembc 2.0).

This benchmark implements a decoder for the de-facto
standard for digital music compression, Mpeg-1 Audio Layer
3. As shown in the motivating example in ﬁgure 1, the de-
coding pipeline comprises multiple kernels that process the
encoded data stream at various levels of granularity, ranging
from whole audio frames down to frequency sub-bands.

The key challenge in parallelizing this application is in
exposing suﬃcient work spread over multiple loop levels to
the Pdg partitioner in order to facilitate the extraction of
a well-balanced pipeline. The Mp3 decoder makes use of
idiosyncratic programming idioms that typically evade static
analysis such as returning function values through buﬀers
passed into functions by pointers, deeply nested function
calls and the extensive use of dynamically allocated buﬀers.
Existing approaches either do not address this issue at all
[4], or rely on manual code transformations (e.g.
function
inlining, full loop unrolling or loop distribution) [28]. The
latter is both an error-prone process, but most importantly,
if it is not guided and selective, it can lead to suboptimal
results.

Using multi-level loop distribution and stage replication
we achieve a speedup of 3.52 on 6 cores over the sequential
baseline. The extracted pipeline structure for this bench-
mark is shown in ﬁgure 7(a) and, in fact, this structure re-
sembles the one contained in the explicitly parallel streaming
StreamIt benchmarks [29]. This result is encouraging and

suggests that our pipeline extraction methodology is capa-
ble of “imitating” the coarse-grain algorithmic parallelization
that previously was alone in the hand of the expert program-
mer.

Bzip2 Compression (Spec2000).

This benchmark implements a lossless, block-sorting data
compressor.
It exhibits a typical pipeline structure which
operates on constant size data blocks. It consists roughly
of the following stages: (i) input and Run-Length Encoding
(Rle) which are both inherently sequential, (ii) indepen-
dently compression of each block by performing a Burrows-
Wheeler transform, (iii) Mtf transform and (iv) output.

Figure 7(b) depicts the extracted pipeline structure for
the bzip2 benchmark. Processing of constant sized blocks
can be accomplished in parallel in a replicated stage. The
extensive use of dynamic memory allocation, pointer arith-
metic and pointer aliasing (e.g. pointers of diﬀerent types
are used to access the same allocated buﬀer) have been a
particular challenge to our tools, still an overall speedup of
4.7 on eight processors could be achieved. This is an example
where a purely static approach would have failed completely
to detect the parallelization opportunities, but proﬁling ac-
curately and correctly identiﬁes the independence of indi-
vidual blocks, even if this has to be ultimately veriﬁed by
the user.

Selective unfolding and splitting of function nodes suc-
cessfully decoupled interleaved I/O operations and compu-
tation nested in lower-level functions. An alternative to this
approach would have been to use special link-time hooks
that bypass output calls and save the output in intermedi-
ate buﬀers which are later ﬂushed in-order [23, 30]. This,
however, would assume the use of speciﬁc I/O libraries and
bzip2 is using memory mapped I/O instead of the stan-
dard library. Therefore, these approaches presume manually
modiﬁed sources.

Mpeg-2 Video Decoding (Eembc 2.0).

This benchmark implements the widely used international
standard for video compression. At an algorithmic level,
Mpeg-2 decoding features multiple processing stages (e.g.
coeﬃcient decoding, saturation control, motion compensa-
tion) which successively operate on the encoded input stream
of frames on diﬀerent levels of granularity (e.g.
frames,
slices, components, macro-blocks).

Using our tools we extract the multi-level pipeline struc-
ture shown in ﬁgure 7(c) that results in a speedup of 2.68
utilizing three processors. Given that we solely rely on
pipeline parallelism and do not attempt to exploit further
ﬁne-grained Ilp or Simd-style short vector data parallelism
this result is impressive. For this application the speedup is
eventually restricted by an unbalanced distribution of work
between stages. This can be seen in ﬁgure 7(c) where next
to the extracted pipeline and the relative time spent in each
pipeline stage is shown.
Integrated approaches targeting
Ilp and short vector instructions inside pipeline stages may
eventually contribute to further speedups, but are outside
the scope of this paper.

Jpeg Compression (Eembc 2.0).

This benchmark implements the Jpeg image compression
algorithm, the dominant standard in digital image photogra-

385Applications source
MP3
bzip2
MPEG-2
JPEG

EEMBC 2.0
SPEC2000
EEMBC 2.0
EEMBC 2.0

LOC
20K
5K

128Kb/s cbr st.
64MB program

23K 375 frames 704x576
22K 4096x4096x24bit bmp

(cid:2)
(cid:2)
−
−

(cid:2)
−
(cid:2)
(cid:2)

−
(cid:2)
(cid:2)
(cid:2)

6
8
3
2

3.52x
4.70x
2.68x
1.47x

dataset

replication multi-loop func. split # cores speedup

Parallelization mechanisms

Table 3: Benchmarks used for evaluation.

header info,
huffman dec.

7%

intput,
RLE

15%

input,
decode MB

37%
27%
36%

input, color convert,
downsample     40%

dequantize
dequantiz.

22%

x2

BW transform,
generate MTF,
generate tables

dequantiz.

13.5%

x6

motion 
compentation

36%

fwd DCT, quantize, 
encode, output  59%

output

4%

output

27%

stereo, reorder,
antialias, hybrid23%

subband
synthesis

24%

output

2%

: bottleneck stage

: replicable stage

(a) MP3 decoding

(b) bzip2 compression

(c) MPEG-2 decoding

(d) JPEG compression

Figure 7: Flow graph of the extracted pipeline for each application in table 3.

phy and the Www. Using our tools we managed to extract
a two-stage pipeline (see ﬁgure 7(d)) which resulted to a
moderate speedup of 47%. This is mainly due to the fact
that although Jpeg features abundant ﬁne-grain parallelism,
even on an algorithmic level most of each components are
inter-dependent. In order to parallelize this application we
annotated the memory allocation routines before analyzing
the application with the IR-proﬁler. This was necessary be-
cause the reference implementation is using a custom mem-
ory manager that utilizes OS features like memory mapped
I/O. Nevertheless, this is a relatively straightforward process
compared to process-based systems like ([23, 28]) that claim
transparent privatization but in this case would most proba-
bly require major modiﬁcation to the source of the memory
manager.

3.2 Safety

We have tested each of the benchmarks using an extensive
set of additional, previously unseen input data sets. For all
benchmarks and inputs this has resulted in identical outputs
for the sequential and the parallelized codes. In addition,
manual code inspection did not reveal any violation of de-
pendencies. This result is initially surprising. However, for
the static pipelines we are targeting in this work complex,
but regular dependence patterns can be expected.

4. RELATED WORK

Early work on extracting pipeline parallelism from em-
bedded applications [14] introduced a number of theoretical

formalisms, but failed to provide a practical and scalable
methodology for detecting and exploiting this kind of par-
allelism. The same authors demonstrate in [15] that using
dynamic loop analysis techniques helps overcome the deﬁ-
ciencies of static data dependency analyses and can lead to
the detection of more parallelizable loops in typical applica-
tions from the embedded domain.

The work presented by Thies et al. [28] is most relevant
to our work. The authors propose a proﬁle-based paral-
lelization methodology for applications that exhibit stream-
ing computation patterns. They use a library of macros
to manually annotate the boundaries of the pipeline stages
at the source-level.
In a subsequent step the correctness
of the user-deﬁned partitioning is veriﬁed using a binary-
instrumentation tool. A runtime systems manages multi-
ple processes communicating through pipes. This simpliﬁes
code generation as each pipeline stage (=process) has its
own private address space and no explicit privatization is
necessary. However, unnecessary overhead for communica-
tion and process management is introduced. We consider it
a major disadvantage that the user of this methodology is
required to both have a good knowledge of the algorithm to
parallelize and its actual sequential implementation. Man-
ually determining proﬁtable partitionings is generally hin-
dered by deeply nested functions or loops and idiosyncratic
programming styles. In addition, the use of macros and the
manual “matching” of the communication operations, espe-
cially in the case of variably-sized objects further compli-
cates the work ﬂow. This is actually demonstrated by the
fact that most of the proposed partitioning in [28] are highly

386unbalanced or fail to unwind sequential (e.g. I/O) and par-
allel operations.

In [4] the Maps methodology for extracting task paral-
lelism from sequential C applications and mapping onto Mp-
SoC platforms is presented. By clustering individual state-
ments into larger clusters a bottom-up approach is taken,
resulting in relatively ﬁne-grained threads that for their ef-
ﬁcient execution require a tightly-coupled architecture with
special thread and communication support. Maps assumes
cleaned up code and only operates on single-level loops. It
has been shown to deliver good results on small embedded
benchmark kernels and the proprietary, low communication
latency Tct platform, but it is unclear if the Maps approach
scales to larger applications or more generic hardware plat-
forms.

Tian et al. [30] focus on the eﬃcient exploitation of pipeline
parallelism using a data speculation runtime system which
creates copies of static as well as dynamically allocated data
on-demand. Similar to [23], this study handles only single-
level loops and using a ﬁxed pipeline skeleton of three stages.
Eﬀectively, this is only applicable to loops with only a few
statements with loop-carried dependences (e.g.
induction
variable increment) in the beginning or end of the loop and
a dominating middle stage. In that sense the scope of our
approach is broader and it can be applied in more complex
cases which as we show in section 3 are predominant in mul-
timedia applications. In addition, the authors do not discuss
or give any information about the problem of correlating
the proﬁling information which is based on debugging in-
formation and binary instrumentation. This information is
necessary to perform parallelism extraction and thread par-
titioning automatically. Their results show excellent scaling
but do not report whole application speedups and in some
cases presume that applications process multiple input ﬁles
in parallel.

Decoupled Software Pipelining (Dswp) is a recent devel-
opment and works by statically splitting programs into crit-
ical path and oﬀ-critical path threads that run concurrently
on thread-parallel architectures. Unlike prior attempts at
automatically multi-threading sequential programs, Dswp
does not try to partition programs into totally independent
threads. Instead, it pipelines programs into dependent com-
municating threads. Ottoni et al.
in [20] ﬁrst proposed
Dswp for parallelizing sequential general-purpose applica-
tions using fully-automatic analysis and code generation.
The main diﬀerence to the approach presented in this paper
is that Dswp operates on a low-level instruction based rep-
resentation and despite the detailed memory dependence in-
formation it cannot apply aggressive privatization. This has
an impact on the scope of the resulting thread-level paral-
lelism. Dswp requires ﬁne-grain communication primitives
to communicate register values, and relies on the cache hi-
erarchy to communicate memory dependencies. In contrast,
we target coarser-grain parallelism and memory-based com-
munication which gives rise to pipelines broadly resembling
the algorithmic stages of the application. This enables us
to use commodity hardware and longer communication free
intervals and unlike Dswp we do not rely on special micro-
architectural support to unfold the full performance poten-
tial.

Vachharajani et al.

[33] extended Dswp with support
for speculation over rarely occurring control or memory de-
pendencies, thus extending its applicability and facilitat-

ing better balanced partitionings to more than one threads.
[24] presented Ps-Dswp, a
Subsequently, E. Raman et al.
variant of Dswp which exploits data-parallelism in stages
with no loop-carried dependencies achieving increased scal-
ability for some applications. Nevertheless, both extensions
still rely the dedicated hardware support. Recent work [23]
on Ps-Dswp includes a speculation system which if used
for parallelization can achieve good performance and scal-
ability on commodity hardware for applications with sim-
ple, single-level loop partitionings and a single, dominating
data-parallel stage. Parallelism extraction and the necessary
code transformations were performed manually but system-
atically in this case.

Rul et al. in [26] demonstrate the signiﬁcance of proﬁling
information in uncovering pipeline parallelism in general-
purpose applications. They manually parallelize two appli-
cations using Pthreads, but their approach lacks an au-
tomated code generation methodology.
In addition, they
follow a simulation-based proﬁling approach which incurs a
very high runtime overhead and lacks any facility to back
annotate proﬁling information to higher level code in order
to drive the code transformation and parallelization process.
Tournavitis et al. [31] use a similar proﬁle-driven approach
which is based on a Control Flow-Graph (Cfg) Ir that is
annotated with explicit data-dependence edges to automat-
ically parallelize scientiﬁc applications. However, their ap-
proach is restricted to Doall loops and, thus, its applicabil-
ity to multimedia applications where coarse loops typically
include I/O or inherently sequential phases (e.g. Huﬀman
or run-length encoding) is limited. In addition, their inter-
mediate representation fails to expose control dependencies
which prohibit the parallelization of more general or nested
loops. Finally, the authors use supervised machine-learning
to train a statistical model that determines the proﬁtabil-
ity of Doall loops based on static and dynamic program
features. This approach, although promising, cannot easily
be extended to handle more coarse-grained parallelization
schemes where proﬁtability depends on the overall structure
rather than local features.

5. SUMMARY, CONCLUSIONS AND

FUTURE WORK

In this paper we have developed a semi-automatic, proﬁle-
driven methodology for the extraction of pipeline parallelism
from sequential codes. We improve on existing work in that
we do not rely on manual code annotation, but only in-
volve the user for ﬁnal approval. We consider our work to
be a feasibility study into the limits of parallelism detection
using proﬁle information. Our approach covers multi-level
loops, hierarchical pipelines and pipeline stage replication
in a uniform framework. It, thus, avoids the performance
bottlenecks resulting from imbalanced pipeline stages that
existing pipeline parallelization approaches suﬀer from. We
have demonstrated that our methodology can successfully
exploit pipeline parallelism in real-world multimedia and
streaming applications featuring idiosyncratic programming
constructs. Speedups of up to 4.7 for an eight-core Intel
Xeon machine are promising and demonstrate the potential
of proﬁle-driven, semi-automatic parallelization approaches
that target parallelism beyond the loop level. Future work
will focus on the extraction and exploitation of dynamic
parallelism, target heterogeneous architectures, improve our

387safety mechanisms and consider the use of machine learning
for the construction of improved partitioning heuristics.

6. REFERENCES

[1] J. Allen and K. Kennedy. Optimizing compilers for modern

architectures: a dependence-based approach. Morgan
Kaufmann Publishers Inc. San Francisco, CA, USA, 2001.

[2] T. Brandes, S. Chaumette, M. C. Counilh, J. Roman, A. Darte,

F. Desprez, and J. C. Mignot. Hpﬁt: A set of integrated tools
for the parallelization of applications using high performance
fortran. part i: Hpﬁt and the transtool environment. Parallel
Computing, 23(1-2):71 – 87, 1997. Environment and tools for
parallel scientiﬁc computing.

[3] M. G. Burke and R. K. Cytron. Interprocedural dependence
analysis and parallelization. SIGPLAN Not., 39(4):139–154,
2004.

[4] J. Ceng, J. Castrillon, W. Sheng, H. Scharwachter, R. Leupers,

G. Ascheid, H. Meyr, T. Isshiki, and H. Kunieda. MAPS: an
integrated framework for MPSoC application parallelization. In
DAC 2008: Proceedings of the 45th Annual Design
Automation Conference. ACM/IEEE, pages 754–759, 2008.

[5] S. C. Chan, G. R. Gao, B. Chapman, T. Linthicum, and

A. Dasgupta. Open64 compiler infrastructure for emerging
multicore/manycore architecture all symposium tutorial. In
IPDPS 2008: 22nd IEEE International Symposium on
Parallel and Distributed Processing, Miami, FL, USA, 2008.
[6] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program

dependence graph and its use in optimization. ACM Trans.
Program. Lang. Syst., 9(3):319–349, 1987.

[7] M. Frigo, C. E. Leiserson, and K. H. Randall. The

implementation of the Cilk-5 multithreaded language.
volume 33, pages 212–223, New York, NY, USA, 1998. ACM.

[8] M. I. Gordon, W. Thies, M. Karczmarek, J. Lin, A. S. Meli,
A. A. Lamb, C. Leger, J. Wong, H. Hoﬀmann, D. Maze, and
S. Amarasinghe. A stream compiler for communication-exposed
architectures. In ASPLOS-X: Proceedings of the 10th
international conference on Architectural support for
programming languages and operating systems, pages 291–303,
New York, NY, USA, 2002. ACM.

[9] J. Guo, G. Bikshandi, D. Hoeﬂinger, G. Almasi, B. Fraguela,

M. Garzaran, D. Padua, and C. von Praun. Hierarchically tiled
arrays for parallelism and locality. volume 0, page 316, Los
Alamitos, CA, USA, 2006. IEEE Computer Society.

[10] M. W. Hall, J. M. Anderson, S. P. Amarasinghe, B. R. Murphy,

S.-W. Liao, E. Bugnion, and M. S. Lam. Maximizing
multiprocessor performance with the SUIF compiler.
Computer, 29(12):84–89, 1996.

[11] P. Husbands, C. Iancu, and K. Yelick. A performance analysis

of the berkeley upc compiler. In ICS ’03: Proceedings of the
17th annual international conference on Supercomputing,
pages 63–73, New York, NY, USA, 2003. ACM.

[12] F. Irigoin, P. Jouvelot, and R. Triolet. Semantical

interprocedural parallelization: an overview of the PIPS
project. In ICS ’91: Proceedings of the 5th international
conference on Supercomputing, pages 244–251, New York, NY,
USA, 1991. ACM.

[13] M. Ishihara, H. Honda, and M. Sato. Development and

implementation of an interactive parallelization assistance tool
for OpenMP: iPat/OMP. IEICE - Trans. Inf. Syst.,
E89-D(2):399–407, 2006.

[14] I. Karkowski and H. Corporaal. Exploiting ﬁne- and

coarse-grain parallelism in embedded programs. In PACT ’98:
Proceedings of the 1998 International Conference on Parallel
Architectures and Compilation Techniques, pages 60–67, 1998.

[15] I. Karkowski and H. Corporaal. Overcoming the limitations of

the traditional loop parallelization. High-Performance
Computing and Networking, Jan 1998.

[16] K. Kennedy, K. S. McKinley, and C. W. Tseng. Interactive

parallel programming using the ParaScope editor. IEEE Trans.
Parallel Distrib. Syst., 2(3):329–341, 1991.

[17] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan,

K. Bala, and L. Chew. Optimistic parallelism requires
abstractions. In PLDI ’07: Proceedings of the 2007 ACM
SIGPLAN conference on Programming language design and
implementation, pages 211–222, New York, USA, 2007. ACM.

[18] L. Lamport. The parallel execution of do loops. Commun.

ACM, 17(2):83–93, 1974.

[19] A. W. Lim and M. S. Lam. Maximizing parallelism and

minimizing synchronization with aﬃne transforms. In POPL
’97: Proceedings of the 24th ACM SIGPLAN-SIGACT
symposium on Principles of programming languages, pages
201–214, New York, NY, USA, 1997. ACM.

[20] G. Ottoni, R. Rangan, A. Stoler, and D. I. August. Automatic

thread extraction with decoupled software pipelining. In
MICRO-38: Proceedings of the 38th IEEE/ACM
International Symposium on Microarchitectureon, 0:105–118,
2005.

[21] D. A. Padua, R. Eigenmann, J. Hoeﬂinger, P. Petersen, P. Tu,

S. Weatherford, and K. Faigin. Polaris: A new-generation
parallelizing compiler for mpps. Technical report, In CSRD
Rept. No. 1306. Univ. of Illinois at Urbana-Champaign, 1993.

[22] H. Park, Y. Park, and S. Mahlke. Polymorphic pipeline array: a

ﬂexible multicore accelerator with virtualized execution for
mobile multimedia applications. In MICRO-42: Proceedings of
the 42nd Annual IEEE/ACM International Symposium on
Microarchitecture, pages 370–380, New York, NY, USA, 2009.
ACM.

[23] A. Raman, H. Kim, T. R. Mason, T. B. Jablin, and D. I.

August. Speculative parallelization using software
multi-threaded transactions. In ASPLOS XV: Proceedings of
the Fifteenth International Conference on Architectural
Support for Programming Languages and Operating Systems ,
March 2010.

[24] E. Raman, G. Ottoni, A. Raman, M. J. Bridges, and D. I.

August. Parallel-stage decoupled software pipelining. In CGO
’08: Proceedings of the 6th annual IEEE/ACM international
symposium on Code generation and optimization, pages
114–123, New York, NY, USA, 2008. ACM.

[25] L. Rauchwerger, F. Arzu, and K. Ouchi. Standard templates
adaptive parallel library. In 4th International Workshop on
Languages, Compilers and Run-Time Systems for Scalable
Computers (LCR), pages 402–409, 1998.

[26] S. Rul, H. Vandierendonck, and K. Bosschere. Extracting

coarse-grain parallelism in general-purpose programs. In
PPoPP ’08: Proceedings of the 13th ACM SIGPLAN
Symposium on Principles and practice of parallel
programming, Feb 2008.

[27] V. A. Saraswat, V. Sarkar, and C. von Praun. X10: concurrent

programming for modern architectures. In PPoPP ’07:
Proceedings of the 12th ACM SIGPLAN symposium on
Principles and practice of parallel programming, pages
271–271, New York, NY, USA, 2007. ACM.

[28] W. Thies, V. Chandrasekhar, and S. Amarasinghe. A practical
approach to exploiting coarse-grained pipeline parallelism in c
programs. In MICRO 40: Proceedings of the 40th Annual
IEEE/ACM International Symposium on Microarchitecture,
pages 356–369, Washington, DC, USA, 2007. IEEE Computer
Society.

[29] W. Thies, M. Karczmarek, and S. Amarasinghe. Streamit: A

language for streaming applications. Lecture Notes in
Computer Science, 2304:179–??, 2002.

[30] C. Tian, M. Feng, V. Nagarajan, and R. Gupta. Copy or
discard execution model for speculative parallelization on
multicores. In MICRO 41: Proceedings of the 41st annual
IEEE/ACM International Symposium on Microarchitecture,
pages 330–341, Washington, DC, USA, 2008. IEEE Computer
Society.

[31] G. Tournavitis, Z. Wang, B. Franke, and M. F. O’Boyle.

Towards a holistic approach to auto-parallelization: integrating
proﬁle-driven parallelism detection and machine-learning based
mapping. In PLDI ’09: Proceedings of the 2009 ACM
SIGPLAN conference on Programming language design and
implementation, pages 177–187, Dublin, Ireland, 2009. ACM.

[32] N. Vachharajani. Intelligent Speculation for Pipelined

Multithreading. PhD thesis, Princeton University, 2008.
[33] N. Vachharajani, R. Rangan, E. Raman, M. J. Bridges,

G. Ottoni, and D. I. August. Speculative decoupled software
pipelining. In PACT ’07: Proceedings of the 16th
International Conference on Parallel Architecture and
Compilation Techniques, pages 49–59, Washington, DC, USA,
2007. IEEE Computer Society.

388