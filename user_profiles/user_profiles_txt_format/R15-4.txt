Application Speciﬁc Non-Volatile Primary Memory for

Embedded Systems

Department of Computer Science and

Department of Computer Science and

Kwangyoon Lee

Engineering

University of California
San Diego, CA, USA

kwl002@cs.ucsd.edu

Alex Orailoglu

Engineering

University of California
San Diego, CA, USA
alex@cs.ucsd.edu

ABSTRACT
Memory subsystems have been considered as one of the
most critical components in embedded systems and further-
more, displaying increasing complexity as application re-
quirements diversify. Modern embedded systems are gener-
ally equipped with multiple heterogeneous memory devices
to satisfy diverse requirements and constraints. NAND ﬂash
memory has been widely adopted for data storage because of
its outstanding beneﬁts on cost, power, capacity and non-
volatility. However, in NAND ﬂash memory, the intrinsic
costs for the read and write accesses are highly dispropor-
tionate in performance and access granularity. The conse-
quent data management complexity and performance deteri-
oration have precluded the adoption of NAND ﬂash memory.
In this paper, we introduce a highly eﬀective non-volatile
primary memory architecture which incorporates applica-
tion speciﬁc information to develop a NAND ﬂash based pri-
mary memory. The proposed architecture provides a uniﬁed
non-volatile primary memory solution which relieves design
complications caused by the growing complexity in mem-
ory subsystems. Our architecture aggressively minimizes
the overhead and redundancy of the NAND based systems
by exploiting eﬃcient address space management and dy-
namic data migration based on accurate application behav-
ioral analysis. We also propose a highly parallelized memory
architecture through an active and dynamic data redistri-
bution over the multiple ﬂash memories based on run-time
workload analysis. The experimental results show that our
proposed architecture signiﬁcantly enhances average mem-
ory access cycle time which is comparable to the standard
DRAM access cycle time and also considerably prolongs the
device life-cycle by autonomous wear-leveling and minimiz-
ing the program/erase operations.

Categories and Subject Descriptors: C.3 [Special-
Purpose and Application-Based Systems]: Real-Time and
Embedded Systems
General Terms: Performance, Design, Experimentation

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CODES+ISSS’08, October 19–24, 2008, Atlanta, Georgia, USA.
Copyright 2008 ACM 978-1-60558-470-6/08/10 ...$5.00.

1.

INTRODUCTION

A disproportionate hardware cost in most embedded sys-
tems, especially in consumer electronic systems, is incurred
by the memory subsystem as a variety of diﬀerent memory
components are concurrently adopted. The memory require-
ments of embedded systems are far more diverse and con-
strained than those of generic computer systems. DRAM
has long been adopted as a single and universal standard
memory device in generic computer systems.
In contrast,
there exists no single or universal memory device for embed-
ded systems.
Instead, highly customized memory devices
inhabit the space of current oﬀerings. Embedded systems
are deﬁned by their own individual requirements and con-
straints and furthermore, these characteristics tend to grow
more complicated and diversiﬁed in future generations. The
memory subsystem plays a critical role in embedded systems
under the aforementioned technological trend and more em-
phasis should be placed on this in the near future.

Uniﬁcation of memory devices in embedded systems is
beneﬁcial in many ways. It generally guarantees low cost,
low power consumption and design ease for the systems.
Furthermore, attainment of the highest beneﬁts necessitates
that the memory possess the capability of direct interfacing
with the processor and that it fulﬁll speciﬁc non-volatility
requirements. Most embedded system designers anticipate
such non-volatility requirements as they can drastically ease
their ultimate memory management burden when power is-
sues are incurred.

NAND ﬂash memory has long been considered and adopt-
ed in practice as the most preferable data storage device
since it is suitable for embedded systems which generally
require special characteristics such as low cost, low power,
high capacity, non-volatility, and high reliability. There-
fore, NAND ﬂash memory is highly appealing as a secondary
or tertiary memory due to several beneﬁcial characteristics
such as large granularity of access and high sustained per-
formance. However, the random-bit inaccessibility, the large
and diverse granularity of accesses, and the long and imbal-
anced access latency make it diﬃcult for it to be adopted
as a primary memory. A signiﬁcant amount of research has
been undertaken to overcome these limitations in utilizing
NAND ﬂash memory as a uniﬁed primary memory and many
researchers still envision NAND ﬂash memory as the most
plausible candidate for this purpose.

In this paper, we introduce a novel uniﬁed non-volatile
primary architecture based on NAND ﬂash memory. Em-
bedded systems are generally composed of a ﬁxed set of func-

tionalities and exhibit a great degree of application speciﬁc
behaviors [7, 8]. This fact encourages many researchers to
exploit application information in a variety of ways. There
have been two most popular approaches in employing ap-
plication speciﬁc information. One consists of transforming
and relocating instructions/data to exhibit enhanced run-
time behavior. This information is gathered oﬀ-line from the
given set of applications with the help of compilers or linkers.
The other is extracting application speciﬁc information from
a given set of applications and tuning the hardware based
on the obtained information. We mainly utilize the latter
approach in this work as it oﬀers increased accuracy and ef-
ﬁciency at run-time. We introduce a novel primary memory
architecture that demonstrates non-volatility, highly opti-
mized and balanced average access time both in read and
write operations, and great enhancement in performance by
exploiting parallelism in multiple devices. Extensive simula-
tion using the SimpleScalar simulator with Spec2000 bench-
mark suites illustrates the signiﬁcant advantages of our ar-
chitecture over the generic DRAM or other baseline NAND-
based architectures.

The rest of the paper is organized as follows. Section
2 brieﬂy presents NAND ﬂash memory based memory sys-
tems. Section 3 delivers background and the basic techni-
cal motivation. Section 4 introduces our non-volatile pri-
mary memory architecture and the techniques of dynamic
data/address management, aggressive multi-level data mi-
gration and multi-device optimization. Section 5 presents
experimental results based on simulation and provides a
comparison with the baseline memory architecture. Section
6 provides an overall summary of the proposed architecture.

2. RELATED WORKS

Due to the increasing requirements and complexity in em-
bedded systems, memory subsystems have undergone diver-
siﬁcation and increased complexity. A signiﬁcant amount
of research has been performed in the ﬁeld to unify memo-
ries or reduce the design complexity of the memory systems
by employing non-volatile memory, especially NAND ﬂash
memory, with aggressive exploitation of application speciﬁc
information.

Over a decade, a considerable amount of research on the
NAND ﬂash based memory systems has been introduced.
Park et al.
in [6] employs NAND ﬂash memory as a pri-
mary memory. The authors add a simple direct mapped uni-
ﬁed cache between a processor and the NAND ﬂash mem-
ory. To minimize access latency for time critical applica-
tions, they prioritize applications based on their criticality.
However, this system requires a relatively large cache and
it suﬀers from overall performance degradation.
In [3],
OneNANDTM, a hybrid NAND ﬂash memory, has been in-
troduced as a high-performance non-volatile memory for em-
bedded systems. In [5, 2, 1], the authors develop NAND
ﬂash memory based code storage systems by incorporat-
ing the VM (Virtual Memory) subsystem of the processor
with no need for hardware modiﬁcations. However, these
approaches are limited to certain applications and require
considerable amount of VM page buﬀers.

3. BACKGROUND AND MOTIVATION

Memory is one of the most critical components in embed-
ded systems but the performance discrepancy between the

processor and memory widens as technology evolves. Tech-
nological advancements in memory mostly emphasize capac-
ity enlargement complemented by steady but nonetheless
meager performance enhancements. Most systems including
embedded systems try to relieve the performance problem
by introducing an eﬃcient hierarchical memory architecture
from cache memory down to secondary/tertiary memory.
However, in embedded systems, this standard memory hier-
archy complicates the system design from the vantage point
of cost, size and power due to the introduction of multiple
heterogeneous memory components. To unify memory com-
ponents in embedded systems, a superior primary memory
device whose characteristics satisfy every design requirement
should be present.

From the perspective of performance, access to memory is
one of the most critical and time consuming processes in the
execution of code in the processor. While SRAM/DRAM
demonstrates highly superior access behavior both in the
preload time and the transfer time, NAND ﬂash memory
demonstrates a signiﬁcantly long access time in terms of
both read and write for the predeﬁned access unit of a page
as can be seen in Table 1. Yet a sharp contrast can be drawn
between these two operations in the NAND ﬂash memory,
as reading a page is performed by sensing the voltage levels
of the given array, thus being relatively fast, while in con-
trast, writing a page is non-atomic and composed of mul-
tiple low-level operations, thus consuming a huge amount
of time. Based on the NAND ﬂash memory architecture,
each cell in a page can change its bit status by charging or
discharging their ﬂoating gate; the charging and discharging
operations cannot be performed though concurrently, which
induces unidirectionality in bit transitions. In NAND ﬂash
memory, there exist two distinct operations, erase and pro-
gram, based on the direction of bit transitions. The logical
write operations can be expressed by combining both erase
and program operations and furthermore those operations
are extremely time-consuming. The asymmetric nature of
the NAND ﬂash memory operations raises a lot of problems
and discourages the adoption of the NAND ﬂash memory.
In addition, the discrepancy between read and write opera-
tions in access time increases the design complexity of the
NAND ﬂash memory based systems.

From an architectural point of view, most embedded pro-
cessors incorporate a small size of instruction and data cache.
In general, the hit rate of these caches is substantially high
and the average cache miss time for the standard primary
memory, represented as the product of cache miss rate and
penalty, is maintained suﬃciently low compared to the cache
hit time. However, for the long latency memory device like
the NAND ﬂash memory, a relatively low cache hit rate
also results in unacceptably high average memory access
times due to the extremely high penalty on cache misses.
Furthermore, the performance asymmetry in NAND ﬂash
memory produces more critical issues. Consequently, a fair
amount of research to lessen or hide the latency of the NAND
ﬂash memory based embedded systems has been initiated.
Along with the performance issues, the NAND ﬂash mem-
ory is unable to update its page in place due to the intrinsic
“unidirectionality” in changing bits. Updating a page will
generally produce obsolete pages and the consequent intro-
duction of redundancy negatively impacts the performance
of the NAND ﬂash memory. Memory is traditionally con-
ceived as a passive component in computer systems. Most

Table 1: Timing Characteristics of Memory Devices

Device
Access Time (ns)
Read Bandwidth (MB/s)
Write Bandwidth (MB/s)

SRAM DRAM NAND FLASH OneNANDTM
25000
108
9.3

30000
17
6.8

5
2500
2500

50
1000
1000

research in memory is conducted to relieve the overhead to
the memory by manipulating memory accesses in the up-
per layer [4]. However, these techniques do not ﬁt well to
the NAND ﬂash memory whose underlying characteristics
diverge greatly from standard memory. Consequently, an
active memory can eﬃciently utilize their storage space by
dynamically processing the full range of information they
have acquired at run-time. Whenever data is transferred
to the memory, it provides a lot of meaningful information:
access behavior and frequency, and data types. The access
behavior and frequency can be fully utilized to reorganize
the physical data layout in the memory to reduce conﬂicts.
Data types may help in identifying access conﬂicts which in
general cause cache misses and result in long latency mem-
ory accesses.

Data accesses to memory chunks (pages) generally exhibit
diverse access frequency and patterns as can be seen in Fig 1.
This may cause a signiﬁcant degradation in the performance
of the NAND ﬂash memory based systems. If rarely accessed
pages are deﬁned as cold and frequently accessed pages as
hot in their access temperature, most applications demon-
strate a highly skewed statistical distribution on the access
temperature. Access patterns exhibit variability over time
enabling such change information to be dynamically cap-
tured to optimize memory behavior by adapting memory
organization according to their run-time information.

The highly skewed access behavior and high run-time vari-
ability in access patterns have long been an issue in the re-
search area of NAND ﬂash memory. While most previous
research on NAND ﬂash memory has focused on control-
ling local or global redundancy without knowledge of the
run-time behavior of the application, an active memory can
eﬀectively balance the access skew and dynamically adapt
itself to the run-time variability by introducing the inter-
nal hybridization of memory, which in turn enables memory
to systematically synthesize individually preprocessed infor-
mation from internal components of the memory and more
accurately control the ﬁne-grained redundancy.

4. PROPOSED ARCHITECTURE

Primary memory basically constitutes the ﬁrst level of oﬀ-
chip memory hierarchy and its performance and characteris-
tics greatly impact the performance of the complete system.
Traditionally, primary memory exhibits reasonable access
latency both in read and write operations without any per-
formance diﬀerentiation. To achieve the equivalent perfor-
mance in the NAND ﬂash memory based primary memory,
the characteristics and behaviors of the system especially
with regard to the memory subsystem should be accurately
identiﬁed, thus, enabling the aggressive utilization of the de-
rived information so as to optimize the subsequent behavior
of the memory.

Most standard memory systems ﬁnd a way to relieve per-
formance degradation by introducing multiple levels of mem-
ory hierarchy. This will help if a higher degree of locality can
be captured in the upper memory layers. As we observed in
Section 3, a frequently accessed group of pages can be ab-
sorbed by introducing a level of internal cache/buﬀer inside
memory. Along with the fast internal cache, the hardware
interface to the standard memory of the proposed primary
memory should be equivalently fast. Furthermore, the afore-
mentioned employment of the application speciﬁc informa-
tion can be processed in an application-speciﬁc control logic.
The novel ideas of the proposed memory architecture lie in
the fair and optimal resource allocation and usage within a
dynamically variable execution environment in conjunction
with an accurate understanding of the behavior of accesses
to memory through the internal hybridization of memory.
The detailed architectural ideas are introduced in the sub-
sequent subsections; the overall hardware architecture can
be seen in Fig 2.

4.1 Application Speciﬁc Dynamic Memory

Management

Physically, a number of pages constitute a block that is the
basic unit for erasure and fundamental unit for data manage-
ment in NAND ﬂash memory. In most NAND ﬂash memory
based systems, redundancy is unavoidable and incorporated

Figure 1: Example of Statistical Access Distribution
of Diﬀerent Applications

Figure 2: The Proposed Memory Architecture

either locally or globally to prevent excessive data overﬂow
resulting in block replacements. A simple ﬁxed local thresh-
old for each block can be proposed to relieve the data over-
ﬂow. However, it causes highly biased accesses that are ex-
tremely undesirable in the ﬂash memory. Global overﬂow
buﬀers can be alternatively employed but they would result
in excessive data migration and ineﬃciency in the level of
redundancy.

We propose a dynamic threshold on a per-block basis,
the valid page threshold, which controls the block replace-
ment cycles through incorporation of application speciﬁc ac-
cess frequency. Initially, the access temperature, degree of
access frequency, and average access frequency can be ac-
quired through the proﬁling of the given application. Then,
based on this information, a proper valid page threshold for
each block can be computed according to the following three
constraints, (1) 0 ≤ T hBi ≤ T hinit ≤ α (2) Pi T hBi =
β and i ≤ γ (3) minimize Pi(Fi − (Favg × i)), where α rep-
resents the total number of pages in a block in the NAND
ﬂash memory, β the number of pages consumed by the appli-
cation and γ the total number of blocks in the ﬂash memory.
Page clustering generally reduces the number of block re-
placements by properly allocating overﬂow buﬀers according
to their average temperature and also facilitates the wear-
leveling of the blocks by evenly controlling the rate of block
replacements for each block. Furthermore, the temperature
information is fed back to the write buﬀer to help the page
replacement scheme. As can be seen in Fig 2, the write buﬀer
is composed of two regions: active and dormant based on
their access temperature. Only pages in the dormant region
are chosen as a replacement page, which provides a more
accurate replacement policy in the buﬀer and consequently
guarantees fewer buﬀer misses.

i F = Pγ

i=T hi ti. If tB

Furthermore, the average time to ﬁll the overﬂow buﬀer is
already known and can be utilized to hide the times for
the block replacement. We assume that the rate of ac-
cesses to the block i is constant and deﬁned as ti. Then
the total amount of time spent to ﬁll the overﬂow buﬀer
can be expressed as tB
i F is compar-
atively long, the block replacement operation can be per-
formed while ﬁlling the overﬂow buﬀers. Thus, another
threshold, the reclaim initiation threshold, is introduced to
initiate in a timely manner block replacement to hide the
overhead of the redundant operations. A value for this
threshold can be obtained by applying the following formu-
lae; (1) T hBi ≤ T hREC
i F ≡ δ, where δ denotes
the time to program a page in the NAND ﬂash memory and
is provided as a constant value in the NAND ﬂash mem-
ory speciﬁcation. The timing diagram for the proposed al-
gorithm can be seen in Fig 3. Properly applied, most of
the time spent on block replacement can be eﬀectively over-
lapped with the time it takes to ﬁll the overﬂow buﬀer.

Bi ≤ γ (2) tB

4.2 Efﬁcient Address Generation and Data

Migration

Based on the thresholds that are described in the previ-
ous subsection 4.1, the physical memory space in the NAND
ﬂash memory is divided into non-uniformly sized contiguous
memory chunks delimited by the threshold of each block.
However, the temperatures of blocks may change over time
and the patterns of temperature changes of blocks are not
unique. These changes can be captured only at run-time
and dynamic threshold adjustment techniques need to be

Figure 3: Timing Diagram for Block Replacement
Based on Thresholds

employed. The dynamic threshold adjustment introduces
two additional challenges. Firstly, the address space may no
longer be contiguous as pages in the middle of a block may
be migrated. Secondly, it complicates the address genera-
tion mechanism. To begin with, we categorize the temper-
ature changes and resulting address changes as two distinct
types. The type 1 block, whose temperature changes from
cold to hot, contains fewer overﬂow buﬀers than it requires.
Instead of expanding the overﬂow buﬀer, we migrate away
pages based on the following criteria. Firstly, the coldest
page should be chosen to minimize the negative impact of
the temperature rise at the target block. Secondly, higher
priority is given to the pages at the edge to facilitate ad-
dress generation by keeping address contiguity. A type 2
block, whose temperature has undergone changes from hot
to cold, exhibits more space for the valid page to accommo-
date and mark that block as “borrowable”. By applying lo-
cal data migration based on information gathered, the levels
of overﬂow buﬀers of blocks are adjusted dynamically and
provide more eﬀective block replacements. However, from
the perspective of address generation, it raises two subse-
quent problems. Firstly, the logical address space needs to
be mapped to non-uniformly sized physical addresses. Sec-
ondly, random data migration between non-uniformly sized
blocks needs to be allowed. In representing a non-uniformly
sized physical address space, a B-tree can be a good can-
didate because it exhibits easy address space management,
fast address generation time, and dynamic address space ad-
justment. However, random data migration of arbitrary leaf
nodes in the traditional B-tree can readily deteriorate the
aforementioned intrinsic characteristics of the B-tree. Con-
sequently, we propose an extended B-tree that permits local
data migration in which source and destination blocks share
a common ancestor within a pre-deﬁned depth. A hashing
based extra addressing scheme hybridized with the origi-
nal B-tree is introduced to achieve a fast and eﬃcient data
migration. Further optimization can be eﬀectively accom-
plished by simplifying migration links by removing cascaded
links and circular links as in Fig 4. The proposed hybrid
scheme provides predictable and ﬂexible address generation
while bounding both the hashing hardware cost and indirect
traversal length.

In principle, the basic data migration is performed by
utilizing local block behaviors. Further global data migra-
tion can accelerate the eﬃcient utilization of the limited
page/block resources with more accurate global temperature
redistribution. This process can be performed in background
by continually searching and migrating proper blocks to ﬁt
the following criteria. A mergeable block set is composed of

Figure 4: Techniques for Simplifying Migration
Links

two or more adjacent blocks that experience steady tempera-
ture changes from hot to cold, while the total number of valid
pages of all the candidate blocks cannot exceed the number
of pages in a block. On the other hand, the separable block
is deﬁned as a block that experiences steady temperature
change from cold to hot, and no local data migration. As
can be seen in Fig 5, we can retrieve over-allocated pages and
redistribute those free pages/blocks to highly active blocks
by providing an eﬃcient coarse grained global data migra-
tion.

4.3 Exploiting Parallelism in Array of

Memories

As technology evolves, the cost of NAND ﬂash memory
continues to diminish but the performance of NAND ﬂash
memory remains near constant. Thus, there has been con-
siderable amount of research on high performance massive
storage device design incorporating an array of NAND ﬂash
memory to overcome this problem. By introducing and ex-
ploiting parallel access in multiple NAND ﬂash memories, we
logically can attain performance enhancement proportional
to the total number of ﬂash memories accessed in parallel.
However, in reality, without proper arbitration of the data
traﬃc to the devices, the full degree of parallelism cannot be
exploited. To avoid constant conﬂicts in accessing the same
physical device, we propose data reorganization and redis-
tribution over multiple devices based on their temperatures.

Figure 5: Techniques for Redistributing Pages by
Merging and Separation

Figure 6: Data Layout to Minimize Access Conﬂicts
by Redistributing Pages

A superpage and data layout to minimize single device ac-
cess conﬂicts by redistributing pages are depicted in Fig 6.
The basic ideas lie in the fact that the average temperature
in diﬀerent physical devices should be balanced and further-
more, the distance between the hottest pages in a block be
widened to prevent single device access conﬂicts with the
help of maximally parallel write-backs from the write buﬀer.

5. EXPERIMENTAL RESULTS

5.1 Experimental Framework

We extensively used the SimpleScalar toolset to perform
our experiments under various conditions. Our experimen-
tal architecture is equipped with two distinct 32KB direct
mapped L1 I/D caches with 2KB cache line size to eﬃciently
interact with the NAND ﬂash memory. The size of the read
cache and write buﬀer of the proposed architecture is 64KB
and both have 2KB cache lines. A 200MHz core with a
consequent 5ns clock cycle is assumed. We also assume 18
cycles for an L1 cache miss on the DRAM. To acquire the L1
miss penalty of the NAND based primary memory, we model
the timing of the proposed architecture based on the opera-
tional timing of the SAMSUNG K9K8G08U0A NAND ﬂash
memory for the NAND access and the advanced SRAM for
the interface with the processor. We performed simulation
runs of 100 million instructions per program with the initial
thresholds computed after running each program. We simu-
late a set of representative programs from Spec2000, which
approximate the behavior and complexity of real-life embed-
ded applications. Table 2 outlines the benchmarks used for
the experiments.

5.2 Average Memory Access Time for Data

As mentioned in Section 3, the disproportionate access
times in read and write operations in NAND ﬂash memory
result in serious performance degradations. Our proposed
memory architecture exhibits great performance enhance-
ment over generic ﬂash memory solutions utilizing ﬁxed and
global threshold algorithms by minimizing the write-back
operations and furthermore, eﬃciently hiding the time over-
head of block replacements. The average data access time

Table 2: Spec2000 Benchmark Applications Sum-
mary

Application Type
CRAFTY
Game
Interpreter
GAP
Compiler
GCC
GZIP
Compressor

Code Size Data Space
1150KB
1025KB
303KB
398KB

432KB
912KB
1956KB
345KB

)

B
K
2
/
s
µ
(
(
 
 
e
e
m
m

i
i
t
t
 
 
s
s
s
s
e
e
c
c
c
c
a
a
 
 
e
e
g
g
a
a
r
r
e
e
v
v
A
A

90

80

70

60

50

40

30

20

10

0

Average Data Access Time (10% initial overflow buffer)

Total Number of Block Replacements Per 10M Instructions

fixed threshold
global threshold
per-block threshold
per-block threshold (4 NAND)
DRAM

fixed threshold
global threshold
per-block threshold
per-block threshold (4 NAND)

s
s
t
t
n
n
e
e
m
m
e
e
c
c
a
a
l
l
p
p
e
e
r
r
 
 
f
f
o
o

 
 
r
r
e
e
b
b
m
m
u
u
N
N

180000

160000

140000

120000

100000

80000

60000

40000

20000

0

CRAFTY

GAP
Benchmark programs

GCC

GZIP

CRAFTY

GAP
GCC
Benchmark programs

GZIP

Figure 7: Average Access Time Comparisons

for a 4-NAND based memory architecture is as low as the
access time of DRAM as can be observed in Fig 7. All the
benchmark programs show similar patterns of results. The
average data access time with the proposed architecture is
less than 1/15 of the passive NAND ﬂash memory solution,
and slightly longer, generally up to 16%, than a DRAM
based system with the exception of the GAP program that
shows higher data access time due to a higher rate of write-
back operations as conﬂicts in the data buﬀer grow.

5.3 Expected Device Life-Cycle

The content validity of each NAND ﬂash memory block is
only guaranteed up to 100,000 program/erase cycles. Thus,
the total number of replacement blocks and subsequent rate
of block replacement directly impacts the life-cycle of the
NAND ﬂash memory device along with the distribution of
the block replacements over the blocks. The experimental
results observed in Fig 8 demonstrate that the proposed ar-
chitecture experiences less than 95% and 80% reduction in
block replacement compared to ﬁxed threshold and global
threshold approaches respectively. Furthermore, the pro-
posed architecture has autonomously distributed the block
replacement evenly due to the dynamic adjustment of the
size of overﬂow buﬀers. The overall expected device cycles
can be considerably prolonged accordingly.

5.4 System Overhead

The proposed architecture highly relies on the optimal re-
distribution of redundancy in the memory. As can be seen
in subsection 5.2, the system-wide redundancy should be de-
termined based on the requirements of the system. Along
with this space overhead, the proposed architecture exhibits
some degree of overhead in its logic and internal memory.
Under the current experimental conﬁguration, it consumes
128KB of SRAM for the read cache and write buﬀer.
It
further requires SRAM space for maintaining information
in the extended B-tree. The logic overhead is mainly dedi-
cated to cache/memory steering logic and B-tree based ad-
dress controlling logic. However, the degree of the memory
and logic overhead is already reasonably small with its im-
portance slated to diminish further in the near future as
semiconductor technology continues to evolve.

6. CONCLUSION

In this paper, we have proposed a highly eﬀective non-
volatile primary memory architecture incorporating NAND
ﬂash memory to provide a high-performance and low-cost
memory solution. The proposed architecture provides a uni-
ﬁed non-volatile primary memory solution which relieves the

Figure 8: Number of Replacement Blocks per 10
Million Instructions

growing design complexity in memory subsystems. We have
designed a hybrid memory architecture by intelligently in-
corporating application speciﬁc information to provide op-
timal physical data layout on the NAND ﬂash memory. We
further provide an eﬃcient local and global dynamic data
migration and ﬂexible address generation mechanism to ef-
fectively transform the data layout based on the dynamic
access behavior changes. All the redundant operations in-
cluding block replacement and free block preparation are
performed so as to minimize their negative impact on the
performance. The experimental results show that our pro-
posed architecture signiﬁcantly enhances average memory
access time and also extensively prolongs the device life-
cycle by minimizing the program/erase operations and by
autonomous wear-leveling.

7. REFERENCES
[1] J. In, I. Shin, and H. Kim. SWL: a search-while-load demand

paging scheme with NAND ﬂash memory. In LCTES ’07:
Proceedings of the 2007 ACM SIGPLAN/SIGBED
Conference on Languages, Compilers, and Tools, pages
217–226, 2007.

[2] Y. Joo, Y. Choi, C. Park, S. W. Chung, E. Chung, and

N. Chang. Demand paging for OneNAND
eXecute-In-Place. In CODES+ISSS ’06: Proceedings of the
4th International Conference on Hardware/Software
Codesign and System Synthesis, pages 229–234, 2006.

Flash

TM

TM

[3] B. Kim, S. Cho, and Y. Choi. OneNAND

: A high

performance and low power memory solution for code and data
storage. In Proceedings of 20th Non-Volatile Semiconductor
Workshop, 2004.

[4] P. R. Panda, F. Catthoor, N. D. Dutt, K. Danckaert,

E. Brockmeyer, C. Kulkarni, A. Vandercappelle, and P. G.
Kjeldsberg. Data and memory optimization techniques for
embedded systems. ACM Transactions on Design
Automation of Electronic Systems, 6(2):149–206, 2001.

[5] C. Park, J. Lim, K. Kwon, J. Lee, and S. L. Min.

Compiler-assisted demand paging for embedded systems with
ﬂash memory. In EMSOFT ’04: Proceedings of the 4th ACM
International Conference on Embedded Software, pages
114–124, 2004.

[6] C. Park, J. Seo, S. Bae, H. Kim, S. Kim, and B. Kim. A

low-cost memory architecture with NAND XIP for mobile
embedded systems. In CODES+ISSS ’03: Proceedings of the
1st IEEE/ACM/IFIP International Conference on
Hardware/Software Codesign and System Synthesis, pages
138–143, 2003.

[7] P. Petrov and A. Orailoglu. Energy frugal tags in

reprogrammable I-caches for application-speciﬁc embedded
processors. In CODES ’02: Proceedings of the Tenth
International Symposium on Hardware/Software Codesign,
pages 181–186, 2002.

[8] P. Petrov and A. Orailoglu. A reprogrammable customization

framework for eﬃcient branch resolution in embedded
processors. ACM Transactions on Embedded Computing
Systems, 4(2):452–468, 2005.

