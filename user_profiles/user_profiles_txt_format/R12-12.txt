Classifying High-Speed Text Streams

Gabriel Pui Cheong Fung1, Jeﬀrey Xu Yu1, and Hongjun Lu2

1 Dept. of Systems Engineering & Engineering Management

The Chinese University of Hong Kong

Hong Kong, China

{pcfung, yu}@se.cuhk.edu.hk
2 Dept. of Computer Science

The Hong Kong University of Science and Technology

Hong Kong, China

hjlu@cs.ust.edu.hk

Abstract. Recently, a new class of data-intensive application becomes
widely recognized where data is modeled best as transient open-end
streams rather than persistent tables on disk. It leads to a new surge
of research interest called data streams. However, most of the reported
works are concentrated on structural data, such as bit-sequences, and
seldom focus on unstructural data, such as textual documents. In this
paper, we propose an eﬃcient classiﬁcation approach for classifying high-
speed text streams. The proposed approach is based on sketches such that
it is able to classify the streams eﬃciently by scanning them only once,
meanwhile consuming a small bounded of memory in both model main-
tenance and operation. Extensive experiments using benchmarks and a
real-life news article collection are conducted. The encouraging results
indicated that our proposed approach is highly feasible.

1 Introduction

In the rapid growth of Web-technology, information becomes ever more perva-
sive and important. A new class of data-intensive application becomes widely
recognized where data is modeled best as transient open-end streams rather than
persistent tables on disk. As a result, it leads to a new surge of research interest
called data streams. As data streams are open-end in nature and are huge in
volume, it is impossible to: 1) hold the entire data streams in memory for anal-
ysis; and 2) store on disk as fast processing is desired. A feasible solution is to
reduce the number of data scans down to one with bounded memory space.

In this paper, we focus on text streams classiﬁcation. We propose a novel
text stream classiﬁcation, called MC (Match-and-Classify), by extending our
previous work on text classiﬁcation[5]. This approach is based on maintaining
two kinds of sketches: 1) class sketch; and 2) document sketch. The text streams
classiﬁcation is a multi-label classiﬁcation that assigns a document to multiple
classes, whereas our previous work deals with only single-label text classiﬁcation.
Extensive experiments are conducted using two benchmarks and a real-life
news article collection. The quality (accuracy) and eﬃciency (CPU/memory)

G. Dong et al. (Eds.): WAIM 2003, LNCS 2762, pp. 148–160, 2003.
c(cid:1) Springer-Verlag Berlin Heidelberg 2003

Classifying High-Speed Text Streams

149

Table 1. Symbols and their meanings

Symbol Meaning
Feature i
fi
Feature i in x, x ∈ {d, k}
fi,x
The number of features in x, x ∈ {d, k}
ld
The number of documents in k
Nk
N
The total number of classes
nfi,d
Frequency of fi in d
The number of documents containing fi in k
dfi,k
The set of classes containing feature fi
Ci
The relative importance of fi in x, x ∈ {d, k}
ωi,x
The weight of fi in y, y ∈ { ˜d, ˜k}
wi,y

of MC are compared with some of the existing text classiﬁcation approaches. It
shows that MC achieves high accuracy and exhibits low CPU cost and low mem-
ory consumption. The order sensitivity, model updating, as well as the signiﬁcant
of document sketches and class sketches are also studies.

2 Text Streams Classiﬁcation

Text streams classiﬁcation is a kind of text classiﬁcation but with some unique
characteristics added:

1. The documents queued in a stream is inﬁnite.
2. It requires continuously model maintenance. As the concepts of the doc-
uments in the streams may change from time to time, model updating is
necessary for maintaining a high quality classiﬁer.

3. Only limited memory space is provided to rebuild the classiﬁer and operate
the system. For space eﬃciency, the number of scan on the documents must
be few. For time eﬃciency, both updating and operation must be conducted
very fast.

Note that the ﬁrst issue causes the last two. However, the last two issues are
conﬂict. In traditional text classiﬁcation, rebuilding a classiﬁer requires feature
selection and/or feature weighting, which implies a large number of data scans
over the documents archived (high computational cost and large memory).

3 Similarity-Based Text Streams Classiﬁcation

Deﬁning two kinds of sketches: class sketch and document sketch. A class (docu-
ment) sketch is an approximator for the class (incoming document). Let d and k
be a document and a class, respectively; ˜d and ˜k be the corresponding document
sketch and class sketch, respectively. A high similarity between ˜kj and ˜di indi-
cates that di should belongs to kj. Hence, the quality and eﬃciency of MC relies
on how the sketches approximate the distinctive features. For fast performance,
the information in a sketch should be minimized. In MC, the information in a
sketch only includes the features appearing in the documents archived.

150

G.P.C. Fung, J.X. Yu, and H. Lu

3.1 Sketches and Text Streams Classiﬁcation

Table 1 shows a list of symbols and their meanings. The class sketch and docu-
ment sketch are:

˜d = (cid:1)(f1,d, w1,d), (f2,d, w2,d), . . . , (fld,d, wld,d)(cid:2)
˜k = (cid:1)(f1,k, W1,k), (f2,k, W2,k), . . . , (flk,k, Wlk,k)(cid:2)

Given ˜d and ˜k, their similarity are measured by Jaccard coeﬃcient:

S( ˜d, ˜k) =

(cid:1)

w2

i,d +

i∈ ˜d

(cid:1)
(cid:1)

i∈ ˜d(wi,d · Wi,k)
i∈ ˜d

W 2
i,k

(cid:1)

−

i∈ ˜d(wi,d · Wi,k)

Jaccard coeﬃcient is chosen because it expresses the degree of overlapping be-
tween ˜d and ˜k as the proportion of overlapping between them. It provides both
intuitive and practical ﬁtness to our model.

Given N predeﬁned classes, k1, k2, · · · , kN , MC ﬁrst computes ˜d on d. By
following Equation (1), MC sorts all class sketches such that S( ˜d, ˜ki) ≥ S( ˜d, ˜kj)
for i < j. Finally, MC determines the top class sketches, ˜k1, ˜k2, · · · , ˜kK, such
that E(˜ki) ≤ E(˜ki+1) for i < K and E(˜kK) > E(˜kK+1), where

E(˜ki) =

(S( ˜d, ˜ki)2 − S( ˜d, ˜ki+1)2)

1
2

and classiﬁes d into k1, k2, · · · , kK.

3.2 Class Sketch Generation

The weight Wi,k for fi in k is computed as follows:

(cid:2)

√

Wi,k = AIi,k ·

2 ·

(cid:3)

W C 2
i,k
W C 2

(cid:4)

· CC 2
i

i,k + CC 2

i

√

2 is used for normalization such that 0 ≤ Wi,k ≤ 1. W Ci,k, the Within-

Here,
Class Coeﬃcient, is used to measure the relative importance of fi within k:

W Ci,k =

log2(dfi,k + 1)
log2(Nk + 1)

W Ci,k reﬂects the fact that features appearing frequently within a class is critical
in term of classiﬁcation. In Equation (4), both numerator and denominator are
logarithmic, as the frequency of a feature appearing over many documents is
rare. Similar ﬁnding is also reported in[7].

CCi, the Cross-Class Coeﬃcient, is used to measure the relative importance

of fi among all classes:

CCi =

1

log N

· log

N
α

(1)

(2)

(3)

(4)

(5)

Classifying High-Speed Text Streams

151

(cid:1)

where α = (
tion such that 0 ≤ CCi ≤ 1.

N
k=1

(cid:1)

W Ci,k)/(maxk∈Ci{W Ci,k}). 1/ log(N ) is used for normaliza-

Note that α gives a ratio between
(cid:1)

W Ci,k for all classes k that contain fi
and the maximum of W Ci,k. Hence,
is used to gather the total importance of
a feature across all classes, and the maximum is used to average the summation
value. If a feature is regarded as important in many classes, then this feature is
obviously not important for classiﬁcation.

CCi gives a global view across classes. Suppose that there are two features,
fi and fj, where fi appears in m classes and fj appears in n classes. There are
two cases:
– Case-1 (m (cid:6) n): Obviously, fj provides far more precious information for
text classiﬁcation than fi. In other words, a feature is more valuable if its
occurrence is skewed.

– Case-2 (m = n): In this case, we argue that it is not realistic to weight the
importance of a feature, fi, by simply counting the number of classes that
fi appears, as in[17]. We assign a higher weight to the feature in which more
documents in a class contains it.
AIi, the Average-Importance Coeﬃcient, is used to measure the average im-

portance of fi,d over all individual documents in k:

(6)

(7)

(8)

(9)

(cid:4)βi,k

(cid:2)

ωi,k
dfi,k
ωi,d

(cid:5)

AIi,k =

ωi,k =

ωi,d =

d∈k
log2(nfi,d + 1)
log2(ld + 1)

βi,k =

1

1 + W Ci,k

Note that a feature appears n times does not imply that it is n times more
important[4,6,13], we therefore take a logarithmic relationship rather than a
linear relationship. The term within the bracket is to average the weights of fi
among all documents in k. The average estimator, βi,k, is used to determine the
suitability of this average:

For instance, given two features: fi and fj, such that fi appears in most docu-
ments but fj appears in a few documents. It is more conﬁdent to declare that
the average of fi is more likely to reﬂect the true status of it than that of fj.

Unlike W Ci,k and CCi which are designed at class level, AIi,k handles the
importance of a feature within an individual document. It is introduced to reduce
the discrepancy among weights, and increase the recall and precision of the
model.

As for memory consumption, MC only needs to keep dfi,k and ωi,k in memory.
Also, we can easily maintain the model to reﬂect the new features in the text
stream.

152

G.P.C. Fung, J.X. Yu, and H. Lu

3.3 Document Sketch Generation

When handling text streams, we need to ﬁgure out the feature importance. This
is usually done by considering the features’ distribution within the incoming
document. In practice, we may need to deal with a text stream in a document
basis (one document at a time). Thus, the sample size is insuﬃcient. Document
sketch is proposed to estimate the true signiﬁcance of features in an incoming
document:

AI i is the average weights of fi over all classes, which is similar to Equation (9):

(cid:2) (cid:1)

AI i =

(cid:1)

(cid:4)βi

N
k=1
N
k=1

ωi,k
dfi,k

β

i =

1

(cid:1)

1 + W C i
N
log2(
k=1
(cid:1)
N
log2(
k=1

dfi,k + 1)
Nk + 1)

W Ci =

However, only the above is insuﬃcient. We cannot assume that every incoming
document that contains fi shares the same distribution. In other words, there are
some risks in using Equation (10). Thus, a geometric distribution is formulated:

(cid:2)

Ri =

1

1 + nf

i

(cid:4)

(cid:2)

×

nf

i
1 + nf

i

(cid:4)nfi,d

nf

i = AI i × nfi,d

wi,d = ω1−Ri

i,d

× AI Ri
i

Finally, wi,d in ˜d is computed by combining Equation (10) to (14):

4 Related Work

In this section, we re-examine two well-known text classiﬁcation approaches:
Naive Bayes (NB) and Support Vectors Machine (SVM).

NB is a probabilistic-base algorithm that computes the posterior probability
of an unseen document given a particular class[9,10]. The merit of it lies on
little consumption of computational resources. Its building cost is linear with
the size of the training set[3,12], and its operation cost is also low. Based on
the independent assumption of words, it only stores the probabilities of the
features appearing in each class[14]. Consequently, it is possible to update the
model continuously and incrementally. However, NB requires signiﬁcant text
preprocessing, such as feature selection[11], or else its quality will be poor. Note
that feature selection needs to scan all of the features in each document from
scratch during model updating.

(10)

(11)

(12)

(13)

(14)

(15)

Classifying High-Speed Text Streams

153

SVM is based on statistical learning theory for solving two-class pattern
recognition problem[16]. It attempts to generate a decision hyperplane that max-
imizes the margin between the positive and the negative examples in a training
set using quadratic programming approach[2]. Based on our knowledge, SVM
is the best algorithm in terms of accuracy[1,8,19]. However, the building cost is
quadratic with the size of the training set[2]. Also, SVM requires feature weight-
ing or else it performance degrades[8]. These make SVM not much appropriate
for text stream classiﬁcation.

4.1 Memory Consumption

Let n be the number of documents; m be the average number of features in
a document; c be the total number of classes. Furthermore, all features are
digitalized.

For NB, each class needs to store the document frequency and the total term
frequency for each feature. The memory consumption for a feature in a class is
12 bytes (4 for feature, 4 for document frequency, and 4 for term frequency).
Recall that a feature may appear in multiple classes. The total memory for NB
is about double of 12 × m × c.

For SVM, each feature is associated with a weight. A corpus requires 8×m×n
byte to be stored. For building a classiﬁer, a matrix is needed that requires
additional 4 × n2 byte. Hence, it requires c × (8 × m × n + 4 × n2) bytes.

For MC, only a small bounded memory is needed. For each feature, it stores
a weight and the document frequency (total 8 bytes). Thus, about 12 × m × c
bytes is needed, which is about half of NB in the worst case scenario. Note that
MC is not a binary classiﬁer, such that the space would reduced signiﬁcantly.

5 Experimental Evaluations

All of the experiments were conducted on a Sun Blade-1000 workstation with
512MB physical memory and a 750MHz Ultra-SPARC-III CPU running Solaris
2.8. All features are stemmed, in which punctuation marks, stop-words, numbers,
web page addresses and email addresses are ignored.

For measuring the quality of classiﬁcation, recall, precision and F1-measure
are used[18]. In the following, we use M-X and m-X to denote macro-values and
micro-values respectively, where X belongs to recall, precision or F1.

5.1 Data Sets

Two benchmarks and a news collection received from Reuters (News-Collection)
are used for testing. Table 2 summarized them.

– Reuters-21578: We take the ModApte-split and select the classes that have
at least one document for training and testing. There are 9,128 documents
assigned to one class and 1,660 documents assigned to multiple classes. This
corpus is highly skewed. Figure 1 summarized its distribution.

154

G.P.C. Fung, J.X. Yu, and H. Lu

"reuters21578.dat"

"newsgroup20.dat"

"newsCollection.dat"

10

20

30

40

50

60

70

80

90

2

4

6

8

10

12

14

16

18

20

5

10

15

20

25

30

35

40

Class

Class

Class

(a) Reuters-21578

(b) Newsgroup-20

(c) News-Collection

s
t
n
e
m
u
c
o
D

 
f

o
 
r
e
b
m
u
N

2500

2000

1500

1000

500

0

0

s
t
n
e
m
u
c
o
D

 
f

o
 
r
e
b
m
u
N

900

800

700

600

500

400

300

200

100

0

1200

1000

800

600

400

200

s
t
n
e
m
u
c
o
D

 
f

o
 
r
e
b
m
u
N

0

0

s
t
n
e
m
u
c
o
D

 
f

o
 
r
e
b
m
u
N

3000

2500

2000

1500

1000

500

0

70000

60000

50000

40000

30000

20000

10000

s
t
n
e
m
u
c
o
D

 
f

o
 
r
e
b
m
u
N

0

0

s
t
n
e
m
u
c
o
D

 
f

o
 
r
e
b
m
u
N

80000

70000

60000

50000

40000

30000

20000

10000

0

2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Number of Classes

2

3

4

Number of Classes

2

3

4

5

6

7

8

9

10 11 12 13

Number of Classes

(d) Reuters-21578

(e) Newsgroup-20

(f) News-Collection

Fig. 1. Document distribution. (a) Highly skewed. (b) Evenly distributed. (c) Slightly
skewed. (d)-(f) Number of documents in multiple classes

Table 2. A summary of the corpora used.

Dataset
Reuters-21578
Newsgroup-20
News-Collection

Training Testing Classes Features
13,270
25,245
206,897

7,769
14,694
280,000

3,019
1,633
70,000

90
20
38

– Newsgroup-20: There are 13,126 documents assigned to one class and
3,201 documents assigned to multiple classes. The documents are evenly
distributed among diﬀerent classes. Figure 1 summarized its distribution.

– News-Collection: A very large set of news articles archived through
Reuters directly from October 2000 to October 2002. Our task is to assign
the news articles to the Morgan Stanley Capital International (MSCI) code.
Note that Reuters has already classiﬁed these articles into the corresponding
classes. Figure 1 summarized its distribution.

5.2 Implementations

For NB, the Multinomial Mixture model is used[11,19]. Features are selected
using information gain (IG) and are chosen based on m−F 1. Recall that classiﬁer
updating requires re-selecting the features by calculating IG again, which takes
most of the computational cost.

For SVM, we use the package SVMlight(http://svmlight.joachims.org).
The weight of each feature is calculated by tf · idf scheme and are normalized to
unit length[13]. The classiﬁer is updated base on the newly received documents

Classifying High-Speed Text Streams

155

1

2

3

4

5

6

7

8

9

1

2

3

4

5

6

7

8

9

1

2

3

4

5

6

Batch Number

Batch Number

Batch Number

(a) Reuters-21578

(b) Newsgroup-20

(c) News-Collection

SVM
MC
NB

SVM
MC
NB

SVM
MC
NB

)

%

(
 
s
s
e
n
e
v
i
t
c
e

f
f

E

0.86

0.84

0.82

0.8

0.78

0.76

0.74

0.72

0.7

)
d
n
o
c
e
S

i

(
 
e
m
T
U
P
C

 

1000

100

10

1

0.1

)

B
K

(
 
y
r
o
m
e
M

1000

100

10

1

SVM
MC
NB

SVM
MC
NB

SVM
MC
NB

)

%

(
 
s
s
e
n
e
v
i
t
c
e

f
f

E

0.75

0.7

0.65

0.6

0.55

0.5

0.45

)
d
n
o
c
e
S

i

(
 
e
m
T
U
P
C

 

1000

100

10

1

)

B
K

(
 
y
r
o
m
e
M

1000

100

10

1

)

%

(
 
s
s
e
n
e
v
i
t
c
e

f
f

E

)
d
n
o
c
e
S

i

(
 
e
m
T
U
P
C

 

SVM
MC
NB

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

100000

10000

SVM
MC
NB

1000

100

SVM
MC
NB

)

B
K

(
 
y
r
o
m
e
M

100000

10000

1000

100

10

1

1

2

3

7

8

9

1

2

3

7

8

9

1

2

3

4

5

6

5

4
6
Batch Number

5

4
6
Batch Number

Batch Number

(d) Reuters-21578

(e) Newsgroup-20

(f) News-Collection

1

2

3

7

8

9

1

2

3

7

8

9

1

2

3

4

5

6

5

4
6
Batch Number

5

4
6
Batch Number

Batch Number

(g) Reuters-21578

(h) Newsgroup-20

(i) News-Collection

Fig. 2. Classiﬁcation eﬃciency. (a), (b) and (c) show the accuracy in terms of m-F1;
(d), (e) and (f) show the CPU time; (g), (h) and (i) show the memory consumption.

and the previously learned support vectors[15]. The classiﬁer updating requires
re-calculating the feature weights, which leads to a signiﬁcant overhead.

MC is implemented using Java (300 lines implementation). Neither feature
selection nor weighing is necessary. Thus, no other extra operations are necessary
in classiﬁer updating.

5.3 Text Stream Classiﬁcation Quality
Documents are divided into n equal-sized batches. Classiﬁers are built using the
ﬁrst batch, and evaluate using the second batch. Then, we update the classiﬁer
using the second batches, and evaluate it using the third batch, and so on so
forth.

Table 3 shows the average accuracy for each of the three algorithms where
n = 10. The accuracy of MC is similar to that of SVM, and is superior to that of
NB. In particular, MC performs signiﬁcantly well in terms of recall (M-R and m-
R). Although MC does not outperform SVM in terms of accuracy, its low CPU

156

G.P.C. Fung, J.X. Yu, and H. Lu

Table 3. Results of text stream classiﬁcation

Newsgroup-28

Reuters-21578

Method m-P m-R m-F1 M-P M-R M-F1
SVM 0.840 0.789 0.824 0.543 0.609 0.569
0.800 0.831 0.815 0.557 0.721 0.628
MC
NB
0.741 0.799 0.767 0.336 0.524 0.406
SVM 0.722 0.625 0.685 0.694 0.619 0.667
MC
0.693 0.643 0.667 0.694 0.609 0.678
0.632 0.585 0.597 0.633 0.623 0.620
NB
News-Collection SVM 0.633 0.582 0.604 0.526 0.478 0.502
0.539 0.668 0.587 0.485 0.502 0.497
0.603 0.489 0.556 0.493 0.367 0.378

MC
NB

Table 4. Result of large batch testing

Newsgroup-20

Reuters-21578

Method m-P m-R m-F1 M-P M-R M-F1
SVM 0.912 0.802 0.857 0.533 0.479 0.505
0.807 0.851 0.828 0.498 0.650 0.564
MC
NB
0.776 0.751 0.763 0.401 0.378 0.381
SVM 0.882 0.617 0.726 0.873 0.618 0.724
0.721 0.669 0.694 0.722 0.686 0.703
MC
NB
0.671 0.608 0.638 0.670 0.639 0.655
News-Collection SVM 0.658 0.632 0.643 0.518 0.495 0.503
0.540 0.671 0.591 0.406 0.579 0.487
0.603 0.489 0.536 0.583 0.335 0.425

MC
NB

cost in both the classiﬁer updating and operation makes it highly recommended
for text stream classiﬁcation.

Figure 2 (a), (b) and (c) show the m-F1 of the three algorithms whenever
a new batch arrives. For all of the algorithms, m-F1 increases from the ﬁrst
batch and become saturated after 4-5 batches. This is because the classiﬁers
obtain a relatively suﬃcient samples from the corpus. In all cases, NB always
performs inferior to the other two. The accuracy of MC and SVM are similar.
M-F1 shows the similar behaviors, but are omitted due to the limit of space.
Figure 2 (d), (e) and (f) show the CPU cost (including both classiﬁer rebuilding
and preprocessing costs) using the three data sets. Note that MC outperforms
both NB and SVM, signiﬁcantly. Figure 2 (g), (h) and (i) show the estimated
memory consumption using the estimation given in previous section.

5.4 Large Batch Testing

In this section, we take the whole dataset as a single large batch. In other words,
we assume a large number of documents come together in a very small time
window. Table 4 and Figure 3 summarized the results. Note that CPU cost
includes preprocessing cost and maintenance cost.

5.5 MC Analysis

Four detailed analysis are conducted: a) the accuracy of diﬀerent components in
class sketches; b) the signiﬁcant of the average estimator in document sketch; c)
the classiﬁer rebuilding necessity; d) the sensitivity to documents arrival order.

Classifying High-Speed Text Streams

157

)
d
n
o
c
e
s
(
 
e
m
T
U
P
C

 

i

1000

100

10

1

1
2
3
4
5
6
7
8
9

SVM
MC
NB

SVM
MC
NB

SVM
MC
NB

)
d
n
o
c
e
s
(
 
e
m
T
U
P
C

 

i

1000

100

10

1

)
d
n
o
c
e
s
(
 
e
m
T
U
P
C

 

i

100000

10000

1000

100

Training (7769)

Classification (3019)

Training (14694)

Classification (1633)

Training (280,000)

Classification (70,000)

(a) Reuters-21578

(b) Newsgroup-20

(c) News-Collection

Fig. 3. Eﬃciency (text classiﬁcation)

Table 5. Experiment setup.

Table 6. Results of the testing.

Exp. No. use W Ci,k use CCi use AIi,k

Exp. No. Reuters-21578 Newsgroup-20

√

×
√
√

×
×
√

×
√

×
√
√
√

×
×
×
√
√

×
×
×
+
√

+
√
√
√

m-F1 M-F1 m-F1 M-F1
0.005 0.234
0.007 0.391
0.018 0.125
0.028 0.200
0.514 0.459
0.745 0.350
0.604 0.640
0.815 0.515
0.584 0.578
0.780 0.375
0.699 0.318
0.510 0.513
0.531 0.578
0.775 0.374
0.482 0.508
0.686 0.350
0.828 0.564
0.694 0.703

1
2
3
4
5
6
7
8
9

Class Sketch. Nine experiments are conducted (Table 5) to examine the sig-
niﬁcance of W Ci,k, CCi and AIi,k. + indicates that AIi,k is used without βi,k.
Table 6 shows the results using Reuters-21578 and Newsgroup-20. Exp. 9,
which includes all of the coeﬃcients, performs the best. Using either W Ci,k or
CCi (Exp. 1 and Exp. 2 ) gives an unsatisfactory results. A combination of
them yields a signiﬁcant improvement (Exp. 3). Ignoring the average estimator
(Exp.6) yields an inferior result.

Table 7. Document sketch testing

Exp. Reuters-21578 Newsgroup-20 News-Collection

m-F1 M-F1 m-F1 M-F1 m-F1 M-F1

with βi 0.828 0.564
no βi 0.803 0.552

0.694 0.703
0.673 0.682

0.591
0.562

0.487
0.470

Document Sketch. In our model, a document sketch is inﬂuenced by the
entire feature distribution. Table 7 shows the necessity of the inﬂuence. “with
β
i” denotes that β
i is set to 1. The
results show that β

i is used, whereas “no β
i plays an important role in text stream classiﬁcation.

i” denotes that β

Classiﬁer Rebuilding. Three diﬀerent cases are tested:

158

G.P.C. Fung, J.X. Yu, and H. Lu

Table 8. Order sensitivity testing

No. m-P m-R m-F1 M-P M-R M-F1
0.716 0.667 0.691 0.713 0.681 0.696
1
0.719 0.662 0.689 0.724 0.687 0.705
2
0.714 0.673 0.693 0.710 0.685 0.697
3
0.718 0.668 0.692 0.717 0.683 0.700
4
0.719 0.663 0.690 0.719 0.678 0.698
5
0.740 0.675 0.706 0.740 0.697 0.718
6
0.710 0.665 0.687 0.713 0.683 0.698
7
0.731 0.677 0.703 0.736 0.693 0.714
8
9
0.732 0.678 0.704 0.729 0.694 0.711
10 0.712 0.657 0.684 0.715 0.675 0.694
s.d. 0.0100 0.0069 0.0078 0.0104 0.0070 0.0083

Table 9. The accuracy of classiﬁer rebuilding

Exp. m-P m-R m-F1 M-P M-R M-F1
0.725 0.672 0.698 0.722 0.687 0.704
0.700 0.634 0.666 0.702 0.652 0.676
0.693 0.643 0.667 0.694 0.589 0.678

1
2
3

– Exp-1: Rebuild the classiﬁer whenever a document arrives. Note that neither
NB nor SVM can handle it, as they have to perform expensive document
preprocessing.

– Exp-2: MC is rebuilt in a random manner. The size of each batch varies.
This experiment simulates the real-life situation such that the classiﬁer needs
to be rebuilt in a certain time interval.

– Exp-3: MC is rebuilt such that the number of documents in a batch is ﬁxed.

Table 9 shows the results. The average accuracy of Exp-1 is the best. This
suggests that continuously classiﬁer rebuilding will increase the accuracy of a
text stream classiﬁer in a long term. The diﬀerence between Exp-2 and Exp-3,
in terms of the average accuracy, is not signiﬁcant.

Order Sensitivity. We test whether MC is sensitive to the order of document
arrival using Newsgroup-20. We place all documents in a queue randomly and
repeat the testing as described above. We repeat the whole process 10 times and
report the average. Table 8 shows the results. Since the standard deviation is
small, we concluded that MC may not be sensitive to the order of document
arrival.

6 Conclusion and Future Work

In this paper, we proposed a novel text stream classiﬁcation approach (MC).
MC does not need to generate any sophisticated models. The main advantages
of MC are: 1) it could achieve high accuracy; 2) model can be rebuilt eﬃciently
regardless the size of the batches; 3) only a small bounded memory is required;
4) no advance document preprocessing is necessary in any circumstances.

Classifying High-Speed Text Streams

159

As our future work, we will extend this research in several directions: 1) study
the timing at which the model should be rebuilt; 2) methods to automatically
remove unnecessary features; 3) the model resistance to noise; and 4) study the
possibility of combing text stream and data stream.

Acknowledgments. The work described in this paper was partially supported
by grants from the Research Grants Council of the Hong Kong Special Admin-
istrative Region, China (CUHK4229/01E, DAG01/02.EG14).

References

1. S. Chakrabarti, S. Roy, and M. V. Soundalgekar. Fast and accurate text classiﬁca-
tion via multiple linear discriminant projections. In Proceedings of the 28th Very
Large Database Conference, 2002.

2. N. Cristianini and J. Shaws-Taylor. An Intorduction to Support Vector Machines

and Other Kernel-Based learning Methods. Cambridge University Press, 2000.

3. R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. Wiley Interscience,

2nd edition, 2001.

4. W. B. Frakes and R. Baeza-Yates.

Information Retrieval: Data Structures and

Algorithms. Prentice Hall PTR, 1992.

5. G. P. C. Fung, J. X. Yu, and W. Lam. Automatic stock trend prediction by real
time news. In Proceedings of 2002 Workshop in Data Mining and Modeling, 2002.
6. W. R. Greiﬀ. A theory of term weighting based on exploratory data analysis.
In Proceedings of SIGIR-98 21th ACM International Conference on Research and
Development in Information Retrieval, pages 11–19, 1998.

7. J. D. Holt and S. M. Chung. Eﬃcient mining of association rules in text databases.
In Proceedings of 8th International Conference on Information and Knowledge
Management, pages 234–242, 1999.

8. T. Joachims. Text categorization with support vector machines: Learning with
many relevant features. In Proceedings of 13th European Conference on Machine
Learning, pages 137–142, 1998.

9. D. D. Lewis. An evaluation of phrasal and clustered representations on a text cat-
egorization task. In Proceedings of SIGIR-92 15th ACM International Conference
on Research and Development in Information Retrieval, pages 37–50, 1992.

10. D. D. Lewis. Naive (bayes) at forty: The independence assumption in information
retrieval. In Proceedings of 13th European Conference on Machine Learning, pages
4–15, 1998.

11. A. McCallum and K. Nigam. A comparison of event models for naive bayes text
classiﬁcation. In AAAI 1998 Workshop on Learning for Text Categorization, 1998.
12. D. Meretakis, D. Fragoudis, H. Lu, and S. Likothanassis. Scalable association-based
text classiﬁcation. In Proceedings of 10th International Conference on Information
and Knowledge Management, pages 5–11, 2001.

13. G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval.

Information Processing and Management, 24(5):513–523, 1988.

14. F. Sebastiani. Machine learning in automated text categorization. ACM Computing

Surveys, 34(1):1–47, 2002.

15. N. A. Syed, H. Liu, and K. K. Sung. Incremental learning with support vector ma-
chines. In Proceedings of SIGKDD-99, 5th International Conference on Knowledge
Discovery and Data Mining, pages 313–321, 1999.

160

G.P.C. Fung, J.X. Yu, and H. Lu

16. V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.
17. K. Yamamoto, S. Masuyama, and S. Naito. Automatic text classiﬁcation method
with simple class-weighting approach. In Natural Language Processing Paciﬁc Rim
Symposium, 1995.

18. Y. Yang. An evaluation of statistical approaches to text categorization. Informa-

tion Retrieval, 1-2(1):69–90, 1999.

19. Y. Yang and X. Liu. A re-examination of text categorization methods. In Proceed-
ings of SIGIR-99 22th ACM International Conference on Research and Develop-
ment in Information Retrieval, pages 42–49, 1999.

