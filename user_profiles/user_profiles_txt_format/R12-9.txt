Text Classiﬁcation without Labeled Negative Documents

Gabriel Pui Cheong Fung1, Jeffrey Xu Yu1, Hongjun Lu2, Philip S. Yu3

1 The Chinese University of Hong Kong, Hong Kong, China, {pcfung,yu}@se.cuhk.edu.hk
2 The Hong Kong University of Science and Technology, Hong Kong, China, luhj@cs.ust.hk

3 T. J. Watson Research Centre, IBM, USA, psyu@us.ibm.com

Abstract

beled under K .

This paper presents a new solution for the problem of
building a text classiﬁer with a small set of labeled posi-
tive documents (P) and a large set of unlabeled documents
(U). Here, the unlabeled documents are mixed with both
of the positive and negative documents. In other words, no
document is labeled as negative. This makes the task of
building a reliable text classiﬁer challenging. In general,
the existing approaches for solving this kind of problem use
a two-step approach: i) extract the negative documents (N)
from U; and ii) build a classiﬁer based on P and N. How-
ever, none of the reported studies tries to further extract any
positive documents (P(cid:1)) from U. Intuitively, extracting P(cid:1)
from U will increase the reliability of the classiﬁer. How-
ever, extracting P(cid:1) from U is difﬁcult. A document in U
that possesses some of the features exhibited in P does not
necessarily mean that it is a positive document, and vice
versa. It is very sensitive to extract positive documents, be-
cause those extracted positive samples may become noises.
The very large size of U and the very high diversity exhib-
ited there also contribute to the difﬁculty of extracting any
positive documents. In this paper, we propose a partition-
based heuristic which aims at extracting both of the posi-
tive and negative documents in U. Extensive experiments
based on three benchmarks are conducted. The favorable
results indicated that our proposed heuristic outperforms
all of the existing approaches signiﬁcantly, especially in the
case where the size of P is extremely small.

1 Introduction

Text classiﬁcation is the task of assigning a boolean value
to each pair (d j, ki) ∈ D × K , where D is a domain of docu-
ments and K is a set of predeﬁned classes. The task is to ap-
proximate the unknown (true) target function φ : D × K →
{1, 0} by means of a function (cid:1)φ : D × K → {1, 0}, such that
φ and (cid:1)φ coincide as much as possible [19]. The function (cid:1)φ is
called classiﬁer. The coincidence is measured as effective-
ness, which is also known as the quality of classiﬁer.

A classiﬁer can be built systematically using a set of
training documents D where all of the documents are la-

The major bottleneck of building a text classiﬁer is that
the cost of labeling the training documents, D, is very ex-
pensive. The precision of the labeling has great impacts on
the quality of classiﬁcation. If the labeling is not precise
enough, the classiﬁer built will be strongly biased, and poor
quality will be resulted.

Due to the rapid growth of information available, the task
of accurately labeling all of the training documents as posi-
tive and negative with respect to a particular class becomes
very difﬁcult, if not impossible. In order to overcome this
bottleneck, some researchers attempt to build classiﬁers by
using a small set of positive and negative documents with a
large set of unlabeled document [1, 15, 5, 2, 24].

Recently, a new direction of text classiﬁcation becomes
recognized where the classiﬁers are built using only a small
set of positive documents, P, and a large set of unlabeled
documents, U, for P ∩U = /0 [8, 11, 10, 9, 22, 23]. No doc-
uments are labeled as negative. Such kind of classiﬁcation
is sometimes known as partially supervised text classiﬁca-
tion [9].

The uniqueness of partially supervised text classiﬁcation
is that a large portion of training documents is unlabeled
and no labeled negative documents are given. This makes
the problem challenging. Some important observations are
summarized as follows. 1) The size of the labeled positive
documents, P, is very small; 2) Very often, |P| is so small
such that it cannot reﬂect the true feature distribution within
the positive class; 3) A large portion of the training docu-
ments is unlabeled; 4) The set of unlabeled training docu-
ments, U, is mixed with positive and negative documents;
and 5) No labeled negative document is given.

For all of the existing approaches on this problem, they
build classiﬁers in two stages. In the ﬁrst stage, negative
documents, N, are extracted from the unlabeled documents,
U. In the second stage, they build the classiﬁers by apply-
ing a classiﬁcation algorithm iteratively4 using the given la-
beled positive documents, P, and the extracted negative doc-
uments, N. Generally speaking, these exiting approaches do
not perform well when the size of P is very small, i.e., too

4Due to the space limited, the details of these iterative selecting algo-
rithms are not described in this paper. Readers are recommended to read
the original papers ([8, 10, 11, 23]) for details.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

small to reﬂect the true feature distribution within the posi-
tive class.

Note that none of the existing approaches attempts to en-
large |P| by further extracting the positive documents (P(cid:1))
from U automatically. The major difﬁculty of doing so is
that it is very difﬁcult to extract a proper set of positive doc-
uments, P(cid:1) ⊂ U. It cannot help to extract P(cid:1) by simply com-
paring the feature distributions in P and U. The reason is
that a document in U that possesses some of the features
exhibited in P does not necessarily mean that it is a positive
document. The very large size of U and very high diversity
in U also contribute to the difﬁculty of extracting positive
documents.

In this paper, we propose a partition-based heuristics to
solve this problem. 1) Based on the given labeled positive
documents, P, and the unlabeled documents, U, we extract
negative documents, N, according to the normalized doc-
ument features; 2) We further extract positive documents,
P(cid:1), and negative documents N(cid:1), from U (cid:1) = U − N, using a
partition-based heuristics; 3) We use P∪P(cid:1) as positive train-
ing documents and N ∪ N(cid:1) as negative training documents to
build a binary classiﬁer. The experimental results indicated
that our approach is highly feasible even with an extremely
small size of |P|. Our solution outperforms all of the exist-
ing approaches signiﬁcantly. It is worth noting that we do
not need to build a complex classiﬁcation model iteratively
as those reported in [8, 10, 11, 23].

The rest of the paper is organized as follows. Section
2 presents our heuristics. Section 3 reviews the major ap-
proaches that tackle this problem. Section 4 evaluates our
work. We summarize and conclude this paper in Section 5.

2 PN-SVM: A Positive-Negative Document

Enlarge Classiﬁer

Our Positive-Negative Document Enlarged Classiﬁer,
denoted PN-SVM, is outlined in Algorithm 1. The two in-
puts are: 1) A set of labeled positive documents, P; and 2)
A set of unlabeled documents, U, which is mixed with the
positive and negative documents. We discuss the two main
steps below, namely, extracting the reliable negative docu-
ments (line 1) and enlarging the sets of positive and negative
documents (line 3).

2.1 Extracting Reliable Negative Documents

The proposed approach in this paper is based on a set of
core vocabulary. The core vocabulary of class k, denoted
Vk, is a set of features that frequently appear in class k. For
the documents in P, its core vocabulary is VP. A document
that belongs to P must possess at least one feature listed in
VP. Therefore, a reliable negative document is a document
in U such that it does not contain any features listed in the

Algorithm 1 PN-SVM(P, U)
Input: a set of positive documents, P, and a set of unlabeled
documents, U;
Output: A SVM classiﬁer
1: extract N from U based on P; {Algorithm 2}
2: U (cid:1) ← U − N;
3: extract P(cid:1) and N(cid:1) from U (cid:1); {Algorithm 3}
4: build a SVM classiﬁer using P ∪ P(cid:1) as the positive train-
ing examples and N ∪ N(cid:1) as the negative training exam-
ples;

core vocabulary of the positive documents, VP. It is impor-
tant to know that a set of reliable negative documents is a
subset of the true negative documents. Based on the con-
cept of reliable negative documents, we emphasize that we
do not attempt to identify all possible negative documents
from the unlabeled documents by simply using the core vo-
cabulary, VP. In fact, it is impossible to do so by simply
comparing the feature distribution among different classes,
as the feature distribution in the text domain is sparse [18].
In the following, we focus on how to identify VP, because
the extraction of reliable negative documents is trivial if VP
is given. The identiﬁcation of VP plays a signiﬁcant role in
identifying the reliable negative documents.

We identify the core vocabulary, VP, by comparing the
feature strengths among the features that are exhibited in
the small set of positive documents, P. To be more speciﬁc,
let H( f j) be a function that computes the feature strength of
the feature f j. VP includes all those features f j if H( f j) > θ
for a θ > 0. We will discuss how to determine θ later in
this section. We say a feature fi is more effective than f j if
H( fi) > H( f j).

The most widely used techniques for approximating the
feature strength, H( f j), is properly came from the proba-
bilistic theory, such as information gain, χ2, odd ratio and
mutual information [19]. However, none of them is suitable
to solve our problem. The reason is that we do not have any
prior knowledge about the feature distribution between the
positive and negative documents over the entire document
domain. It is impossible to obtain or estimate these values.
As a result, obtaining the feature strengths is a very difﬁcult
and non-trivial task as we cannot use any of the traditional
methods to solve it.

In this paper, we derive H( f j) by proposing a heuris-
tics that can utilize the information presented in P and U.
Let nP( f j) and nU ( f j) denote the number of documents that
contain f j in P and U, respectively. H( f j) is computed by
measuring the differences of the normalized document fre-
quency between P and U:

H( f j) = nP( f j) − minP
maxP − minP

− nU ( f j) − minU
maxU − minU

(1)

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

where maxP (maxU ) and minP (minU ) are the maximum
value and minimum value of nP( f j) for f j ∈ P (nU ( f j) for
f j ∈ U). Note that H( f j) ≥ 0.

In Eq. (1), the ﬁrst and second components correspond
to the normalized numbers of documents that contain a par-
ticular feature, f j, in P and U, respectively. Although some
existing works [22, 23] also compute H( f j) based on some
heuristic functions, none of them considers normalization.
We believe that normalization is critical because the differ-
ence between |P| and |U| is extremely large (|P| (cid:9) |U|).
Normalization in Eq. (1) makes it possible to compare the
true relative frequencies of the features in both sets (P and
U).

Eq. (1) is robust since it captures the idea that a feature
is more important if its appearance is highly skewed toward
the positive set (P). To be more speciﬁc, if the frequency of
a feature in both P and U is similar, then its feature strength
will approach to zero (i.e. H( f j) = 0). If it is skewed to-
ward one set, its feature strength will be either be strictly
positive or strictly negative, which depends on which class
it is skewed.

Recall VP is identiﬁed by extracting f j such that H( f j) >
θ. We identify θ by adopting the idea of statistics quality
control [13]:

θ = 1
N

∑
f j∈P

H( f j) + δ · σ

(2)

where N is the number of f j ∈ P, σ is the standard deviation
of H( f j) for ∀ f j ∈ P.

j

The idea behind Eq. (2) is that if there exists a feature,
f ∗
j , such that its feature strength, H( f ∗
), is signiﬁcantly
higher than the average feature strength, then most of the
documents in P should contain it. Thus, a document in U
which contains f ∗
j may, but not necessary, belong to P. Re-
call that our aim in this stage is to extract a set of reliable
negative documents. Thus, any document that contains f ∗
j
should not be considered as a reliable negative document.

Eq. (2) indicates that the higher the value of δ is, the
more tight the control is. A tight control results in a smaller
size of VP. In this paper, we set δ = 6. Deﬁnitely, setting δ
as such a high value may result in a very small size of VP;
however, our experimental results indicated that this formu-
lation is effective and efﬁcient even under a very noisy and
ambiguous environment (i.e. an extremely small |P|).

Algorithm 2 outlines the procedure described in this sec-
tion to extract the reliable negative documents that do not
contain any feature listed in VP.

2.2 Enlarging the Sets of Positive and Negative

Documents

In the previous section, we discussed how to extract a
set of reliable negative documents, N, from the unlabeled
documents, U, based on the labeled positive documents, P.

Algorithm 2 ExtractNegative(P, U)
Input: a set of positive documents, P, and a set of unlabeled
documents, U;
Output: a set of negative documents N;
1: for all f j ∈ P do
2:
3: end for
4: compute θ using Eq. (2);
5: N ← /0;
6: for all di ∈ U do
7:
8:

if all f j ∈ di satisfy H( f j) < θ then

compute H( f j) using Eq. (1);

N ← N ∪ di;

end if
9:
10: end for

Recall that N contains documents that do not share any fea-
tures in the core vocabulary of P, VP. In this section, we
show how to extract a set of positive documents (P(cid:1)) and a
set of negative documents (N(cid:1)) from U (cid:1), where U (cid:1) = U − N.
By doing so, we can enlarge the given positive documents,
P, and the previously extracted reliable negative documents,
N. The enlargement is motivated by the following observa-
tions:

• If |P| is small, the number of positive documents for
training is inadequate. Enlarging the total number of
positive documents by adding P(cid:1) into P will increase
the recall of the classiﬁer.

• If |P| is large, the number of negative documents, N,
extracted in the previous stage will be smaller because
|VP| will be large. Enlarging the total number of neg-
ative documents by adding N(cid:1) into N will increase the
precision of the classiﬁer.

The main difﬁculty of extracting P(cid:1) from U (cid:1) is that sim-
ply comparing the feature distribution between P and U (cid:1) is
impossible to obtain a high quality of P(cid:1), as a document in
U (cid:1) possesses some features listed in VP may not necessar-
ily belong to P. Note also that the proportion of P(cid:1) in U (cid:1) is
usually very small. In the following, we ﬁrst discuss some
seemingly possible but actually infeasible approaches to ex-
tract P(cid:1). Then, we present our partition-based heuristics in
details.

2.2.1 Possible but Infeasible Solutions

We consider three possible but infeasible solutions here.

• Repeated Extraction Approach (RE) This approach
adopts the similar idea as proposed in the previous sec-
tion but implements in an reverse direction. We ﬁrst
construct a core vocabulary, VN, for the extracted neg-
ative documents, N, and then extract P(cid:1) from U (cid:1) (=
U − N) such that P(cid:1) do not possess any features in VN.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

kNN
k = 1
k = 2

Reuters-21578 Newsgroup-20 Web-KB

25%
18%

38%
30%

58%
52%

Table 1. Percentage of documents that their nearest
neighbors (NN) belong to different topics (classes).

inadequate for reﬂecting the true feature distribution
of the positive class P . To be more speciﬁc, let us take
SVM as an example. Figure 1 (a) and 1 (b) show how
SVM determines its decision boundary by maximizing
the boundary and minimizing the error. In both ﬁgures,
N (the crosses) are the same, but P (the black dots) are
different. Note that |P| in Figure 1 (a) is smaller than
that in Figure 1 (b). Let us refer to Figure 1 (a), when
the number of positive documents is inadequate, SVM
generates a wrong decision boundary, and cannot ad-
just it to the correct one as shown in Figure 1 (b). This
explains why a kernel based classiﬁer always extracts
P(cid:1) with high recall but very low precision, whereas it
extracts N(cid:1) with high precision but low recall.

• Instance Based Approach Although instance based
classiﬁers, like kNN [20, 21], do not relay on statis-
tical distribution of the training data, they are infea-
sible as well. Recall that a document contains some
features listed in the core vocabulary of positive class,
VP, does not imply that it belongs to P. According
to the probabilistic nature of feature distribution, any
two documents may share a high degree of similarity
even though they belong to different classes. In fact,
we observe that two documents can often be the near-
est neighbor but do not belong to the same class. Ta-
ble 1 shows the percentage of those documents that
their nearest neighbors belong to different topics using
three benchmarks.5 Although the percentage possibly
varies in different data sets, it does conﬁrm the afore-
mentioned nearest neighbor behavior. This is why
the threshold k in kNN will never be set to a small
value in text classiﬁcation. Typically, k is round 30-
50 [6, 20, 21]. Obviously, such a large k is impossible
to exist in our problem setting, because |P| is always
small. A small k will obtain a poor result obviously.

Proﬁle Based Approach (PB)

PB, such as Centroid based [19] or Rocchio [16], can pos-
sibly solve the problem of kNN. However, PB is still im-
practical. Let us consider two proﬁles, CP and CN, which
denote the documents in P and the documents in N, re-
spectively. Suppose we use cosine coefﬁcient for measuring

5We will discuss these benchmarks in Section 4.

(a) Small P (Inadequate)

(b) Large P (Adequate)

Figure 1. The SVM decision boundaries

• Text Classiﬁcation Approach (TC) This approach
ﬁrst builds a classiﬁer based on P and N, and then clas-
siﬁes U (cid:1). The documents that are classiﬁed as positive
are then regarded as P(cid:1). Such technique is reported in
[10, 9, 11, 23] for extracting N(cid:1). However, as we will
discuss later, we claim that it cannot extract P(cid:1).

• Proﬁle Based Approach (PB) This approach ﬁrst con-
structs a proﬁle for each class, P and N. Given a doc-
ument, d, in U (cid:1), in order to classify whether d belongs
to P(cid:1) or N(cid:1), we can compare the distance between d
and the proﬁle of P with the distance between d and
the proﬁle of N. Such technique is adopted in [9, 10]
for extracting N(cid:1), but not P(cid:1).

Repeated Extraction Approach (RE)

RE is infeasible. U always consists of many different topics
(classes) and does not focus on one single topic. The level
of diversity is usually very high. Since N ⊂ U, N inherits
this property. Accordingly, every topic in N contains its own
set of core vocabulary, a large number of different topics in
N cancels the signiﬁcance of each others’ core vocabulary.
Eventually, none of the features in N can be properly se-
lected or the features selected are of poor quality.

Text Classiﬁcation Approach (TC)
TC is impossible to extract a high quality of P(cid:1). Our ar-
gument is based on our observations on two major classi-
ﬁcation approaches: kernel based approaches, like SVM
and regression models, and instance based approaches, like
kNN.

• Kernel Based Approach. Kernel based approaches
may wrongly classify many negative documents into
P(cid:1). The reasons are as follows. Recall that N consists
of diverse topics and covers a very large region in the
feature space. On the other hand, P only focuses on
one single topic and covers a much smaller region in
the same feature space. When |P| is small, P may be

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

the similarity between a document, d, and a proﬁle, Ci for
i ∈ {P, N}:

we have

S(d,Ci) =

d ·Ci

(cid:12) d (cid:12) · (cid:12) Ci (cid:12)

for i ∈ {P, N}

(3)

Note: for most applications, d will be normalized to unit
length so as to account for the length of different documents
[19], i.e. (cid:12) d (cid:12) = 1.

For simplicity, let us use the centroids of the positive
class and negative class to denote their corresponding pro-
ﬁles, i.e.:

CP = 1

| P | ∑

di∈P

di

CN = 1

| N | ∑

di∈N

di

Consider S(d,CN) whose numerator (Eq. (3)) becomes

(4)

(5)

d ·CN = 1

| N | ∑

di∈N

S(d, di)

In short, the similarity between d and CN, S(d,CN), is noth-
ing more than the average similarity between d and all doc-
uments in N divided by (cid:12)CN(cid:12) ((cid:12) d (cid:12)= 1). As |N| is large and
N consists of many diverse topics, obviously d can never
be similar to all documents in N. Thus, S(d,CN) will be
extremely small. This explains why the precision of P(cid:1) ex-
tracted from U (cid:1) is very low. The same observation is applied
to other coefﬁcients for computing similarity, such as Jac-
card coefﬁcient or Dice coefﬁcient.

Similarly, for Rocchio, it also suffers from the above
difﬁculties. Other classiﬁcation techniques, such as Naive
Bayes, decision tree or Neural Network, share some simi-
lar difﬁculties that are described in this section as well. In
short, it is impossible to extract P(cid:1) from U (cid:1) by simply build-
ing any kind of classiﬁer using P and N directly.

2.2.2 A Feasible Partition-Based Heuristics

In this section, we present our partition-based heuristics for
extracting P(cid:1) and N(cid:1) from U (cid:1). Recall that the extracted neg-
ative documents consist of diverse topics. It is this diversity
that makes all of the approaches that we discussed so far
impractical to extract P(cid:1) and/or N(cid:1) from U (cid:1). The key issue
is thus to ﬁnd a way to deal with the problem of diversity.

In brief, the partition-based approach further extends the
idea of proﬁle based approach by partitioning N into k clus-
ters, N1, N2, · · · , Nk, such that the documents in each parti-
tion share a high degree of similarity. In order to extract the
positive and negative documents from U (cid:1), we compare the
similarity between a document in U (cid:1) and the centroid of a
cluster Ni, CNi. We do not compare d with CN, directly.

The advantage of partitioning is to increase S(, ). We
) > S(d,CN) in general. Since ||d|| = 1,

observe that S(d,CNi

S(d,CNi

) =

S(d,CN) =

||

1
||CNi
1

||CN||

di∈Ni

· 1
|Ni| ∑
· 1
|N| ∑

di∈N

S(d, di)

S(d, di)

On one hand, since |Ni| (cid:9) |N| and ||CNi
|| < ||CN||, we ob-
tain 1/|Ni| (cid:13) 1/|N| and 1/||CNi
|| > 1/||CN||. On the other
hand, it is uncertain whether ∑di∈Ni S(d, di) is greater than
∑di∈N S(d, di) or not. However, in practice, we observe that
|Ni| and ||CNi
|| are dominate factors, as the features are
sparse in nature and the summation always have a small
value. We conﬁrmed our observation using Reuters-21578.
Therefore, we take it as maxi S(d,CNi
) > S(d,CN). We can
extract more negative documents from U (cid:1) in comparison
with the PB approach. The probability of extracting wrong
negative documents from U (cid:1) decreases, and the precision of
P(cid:1) increases. In other words, if d is a negative document
and is extracted to N(cid:1) using the PB approach, it will be ex-
tracted to N(cid:1) using the partition-based approach. The docu-
ments that cannot be classiﬁed to N(cid:1) due to the problem of
diversity are now able to be extracted to N(cid:1). Two properties
are given below.

Property 1 Suppose d is a negative document and belong
to N(cid:1) by some classical classiﬁcation techniques, d will be
extracted to N(cid:1) using our partition-based solution.

Property 2 Suppose d is negative and does not belong to
N(cid:1) by some classical classiﬁcation techniques, d is still pos-
sible to be extracted to N (cid:1) using our partition-based solu-
tion.

In this paper, we adopt the classical K-Means cluster-
ing algorithm [3, 4, 7] for partitioning the set of reliable
negative documents, N. It should be noted that any other
clustering algorithms can be adopted for partitioning.

A common question regarding K-Mean clustering algo-
rithm is the best number of clusters. If the number of parti-
tions is large, the problem that appears in the instance based
approach appears. If the number of partitions is small, the
problem that appears in the proﬁle based approach appears.
An optimal number of partitions thus exist. A detailed anal-
ysis would be given in Section 4.

Without loss of generality, assume that we have parti-
tioned N into k clusters, N1, N2, · · · , Nk, by some clustering
algorithms. In order to determine whether a document, d,
in U (cid:1) belongs to P(cid:1) or N(cid:1), we cannot rely our decision solely
on Eq. (3). The reason is that, in terms of similarity mea-
surement, d may be similar to both P(cid:1) and some Ni. What
we need is to extract the document d which is signiﬁcantly
similar to either P or some Ni. Documents which are simi-
lar to both P(cid:1) and N(cid:1) are regarded as ambiguous and will be

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

Algorithm 3 Partition(P, N, U (cid:1))
Input: a set of positive documents, P, a set of negative doc-
uments, N, and and a set of unlabeled documents, U (cid:1);
Output: a set of positive documents, P(cid:1), and a set of nega-
tive documents, N(cid:1);
1: P(cid:1) ← /0; N(cid:1) ← /0;
2: partition N into k clusters;
3: for all d ∈ U (cid:1) do
4:

f1 ← f alse;
if d satisﬁes the conditions in Eq. (9) then

f2 ← f alse;

f1 ← true;

end if
if d satisﬁes the conditions in Eq. (10) then

Algorithm 4 I-Partition(P, N, U (cid:1))
Input: a set of positive documents, P, a set of negative doc-
uments, N, and and a set of unlabeled documents, U (cid:1);
Output: a set of positive documents, P, and a set of nega-
tive documents, N;
1: get P(cid:1) and N(cid:1) from Partition(P, N, U (cid:1));
2: while P(cid:1) (cid:14)= /0 and N(cid:1) (cid:14)= /0 do
3:

P ← P ∪ P(cid:1);
N ← N ∪ N(cid:1);
U (cid:1) ← U (cid:1) − P − N;
get P(cid:1) and N(cid:1) from Partition(P, N, U (cid:1));

4:

5:
6:
7: end while
8: return P and N;

5:
6:

7:

8:
9:

10:
11:

12:

13:
14:

15:

f2 ← true;

end if
if f1 (cid:14)= f2 then

if f1 = true then

P(cid:1) ← d ∪ P(cid:1);

else

N(cid:1) ← d ∪ N(cid:1);

end if

16:
end if
17:
18: end for
19: return P(cid:1) and N(cid:1);

ignored. We give our methods below in detail. A document
d in U (cid:1) belongs to P(cid:1) can be determined as follows.

S(d,CN) > µN
MN(d) = max S(d,CNi

) − S(d,CP) < γN

S(d,CP) > µP
MP(d) = S(d,CP) − maxS(d,CNi

) > γP

where

(6)

deviation of S(d,CP) and MP(d)). t-distribution is sensitive
to the size of the sample where the sample size is |P| in
our problem setting. In this paper, a document d belongs
to P(cid:1) iff the following two hypothesis (H0 and H(cid:1)
0) are both
rejected:

H0 : S(d,CP) = µP, H1 : S(d,CP) > µP
H(cid:1)
H(cid:1)
1 : MP(d) > γP
0 : MP(d) = γP,

(9)

In a similar fashion, a document d belongs to N(cid:1) iff the fol-
lowings hold:

(10)

(11)

(12)

µN = 1
k

γN = 1
k

k
∑
i=0

k
∑
i=0

1

|Ni| ∑

d j ∈Ni

1

|Ni| ∑

d j ∈Ni

S(d j,CNi

)

(cid:2)
S(d j,CNi

(cid:3)
) − S(d j,CP)

Note that we do not need to apply t-test for extracting d to
N(cid:1), because N is always large in size.

Algorithm 3 summarizes the whole procedure for ex-
tracting P(cid:1) and N(cid:1) from U (cid:1). Following Algorithm 3, we can
obtain a set of positive training documents by merging the
given P and the extracted P(cid:1), i.e. P ∪ P(cid:1), and a set of nega-
tive training documents by merging the two sets of extracted
negative documents, i.e. N ∪ N(cid:1). Finally, SVM will be used
for building the classiﬁcation model.6

2.2.3 Iterative Partition-Based Heuristics

A general question regarding to the partition-based heuris-
tics is whether there would be any improvement in terms of

6SVM is chosen because it is the best reported text classiﬁcation al-
gorithm nowadays [6]. Other classiﬁcation techniques could also be used
however.

Here, µP is the average similarity between the documents
in P and the centroid of P, and γP is the average difference
between the documents in P and the closest Ni (⊆ N).

S(di,CP)
(cid:2)

µP = 1

γP = 1

di∈P

|P| ∑
|P| ∑

di∈P

S(di,CP) − max
j=1,···,k

S(di,CN j

(7)

(8)

(cid:3)
)

Thus, from Eq. (6), S(d,CP) > µP guarantees that d is sim-
ilar to P, while MP(d) > γP ensures that d is not similar to
both P and N. However, in practice, the above formulation
may result in high precision but low recall when extracting
P(cid:1) from U (cid:1) if |P| is very small. When |P| is too small, there
are very few different features available, which cannot rep-
resent the true core vocabulary for the positive class. Thus,
most of documents in U (cid:1) will be regarded as ambiguous and
ignored.

In order to solve this problem, we use a one-tail t-test
[14] to test whether S(d,CP) and MP(d) are greater than
µP and γP, respectively, with evidence (mean and standard

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

the quality of classiﬁcation if the classiﬁer is trained itera-
tively, such as implementing those iterative training proce-
dures as described in [8, 10, 11, 23]. Algorithm 4 outlines
the idea of iterative training. Its general idea is to run the
partition-based heuristics recursively until no more docu-
ments, either positive or negative, can be extracted. Based
on Algorithm 4,

We propose a new heuristics, called PN-SVM-I, to en-
hance PN-SVM (Algorithm 1) by replacing Algorithm 3 at
line 3 of PN-SVM with Algorithm 4. More discussions will
be given in the Section 4.

Some existing works [8, 10, 11, 9, 22, 23] are discussed

3 Related Work

below.

3.1 Roc-SVM

3.3 PEBL

PEBL is proposed in [23]. It ﬁrst builds a feature set, F,
which contains features, f j, that occur in P more frequently
than in U based on the following equation.

f j ∈ F if

nP( f j)

|P|

> nU ( f j)
|U|

(14)

where nP( f j) and nU ( f j) are the number of documents that
contain the feature f j in P and U, respectively. Any doc-
ument in U that does not contain the features in F is clas-
siﬁed as N. Similar to the idea of Roc-SVM, SVM is then
trained iteratively so as to discover N(cid:1) from U (cid:1). Heteroge-
neous Learner (HL) [22] uses a similar but more sophisti-
cated approach for web page classiﬁcation. However, we do
not include it in the experimental studies, as we cannot get
any satisfactory result if we use HL.

3.4 Weighted Logistic Regression (Log)

Roc-SVM is proposed in [9, 10]. It ﬁrst builds a tradi-

tional Rocchio classiﬁer [16]:

WP =

WU =

α

d∈P

|P| ∑
|U| ∑

α

d∈U

d −

d −

β

d∈U

|U| ∑
|P| ∑

β

d∈P

d

d

This heuristics is proposed in [8]. It uses a logistic re-
gression on the weighted examples together with a perfor-
mance measure that is estimated from both the labeled posi-
tive documents and the unlabeled documents, so as to select
a regularization parameter for a validation set. The authors
claim that this heuristics can be applied in any data set, not
just text data.

(13)

where α and β are user-given parameters. By comparing
the documents in P with WP and WU using cosine coefﬁ-
cient, documents that are more similar to WU are classiﬁed
as N. SVM is then trained iteratively using P, N and U (cid:1).
The iterative training aims at extracting N(cid:1) from U (cid:1) so as to
enlarge the negative training set. They do not enlarge the
positive training set.

3.2 S-EM and Spy-SVM-IS

S-EM is proposed in [11]. It ﬁrst randomly selects a set
of documents, S ⊂ P, and puts them into U. The default size
of S is 15% of P. Here, S serves as spy documents from P to
U. It then runs an I-EM (iterative-EM) algorithm using the
set of S(cid:1) (S(cid:1) = P − S). After I-EM completes, the resulting
classiﬁer uses a probabilistic approach based on S to decide
a threshold to identify N. An EM algorithm with a selection
heuristics is then used for generating the ﬁnal classiﬁer.

Spy-SVM-IS [10] is the same as S-EM except that the ﬁ-
nal classiﬁer is generated by training SVM iteratively rather
than using EM algorithm. [10] shows that Spy-SVM-IS per-
forms better than S-EM when the number of positive docu-
ments given increases.

4 Experimental Studies

We have implemented our PN-SVM as well as
SVM, S-EM, Spy-SVM-IS, Roc-SVM, PN-SVM, PEBL
and Log using JavaTM.
S-EM and Spy-SVM-IS are
downloaded from http://www.cs.uic.edu/˜liub/LPU/
LPU-download.html. Multinomial version of Naive Bayes
(NB) [12] is also implemented to serve as a base line ap-
proach.

For the document pre-processing, features are stemmed,
in which punctuation marks, numbers, web page addresses
and email addresses are removed. Features that only appear
in one document are ignored. All features are weighted us-
ing the traditional t f · id f schema [17], and normalized to
unit length [19]. Different versions of t f · id f schema are
implemented and their inﬂuences are insigniﬁcant or none.
For the training and testing of the SVM model, we use linear
kernel with C = 1.0 [6, 21], whereas for the case of NB, we
use Multinomial version with Laplacean prior for smooth-
ing.

Due to the space limited, for evaluating the quality of
classiﬁcation, we only report the micro-F1 [19], which takes
a balance between precision and recall.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

4.1 Experimental Setup

Three benchmarks are used in our experimental study.

Their descriptions are given below.

• Reuters-215787: Following the recent studies, we use
the ModApte split to separate the data into training set
and evaluation set. We select the 10 largest classes for
training and evaluation. The dataset consists of 7,770
training articles and 3,019 testing articles. Reuters-
21578 is highly skewed. The largest class contains
2,877 positive articles while the smallest contains 181
positive articles. The standard deviation is around 700.

• Newsgroup-208: There are totally 20 different classes,
where each class contains about 1,000 messages. The
number of messages is nearly evenly distributed in
all classes. There are 19,126 messages assigned to
a single class and 320 messages assigned to multiple
classes. For each class, we randomly select 80% of the
messages as training data, and the remaining 20% as
testing data.

• Web-KB9: There are 8,282 web pages. Each web page
is classiﬁed into one of the 7 pre-deﬁned classes. Web-
KB is skewed. The largest class contains 3,764 web
pages, while the smallest contains 137 web pages. The
standard deviation is around 1,200. For each class, we
randomly select 80% of the messages as training and
the remaining 20% as data.

We pick up a class k in a benchmark as the positive class,
and randomly select x% documents from the class k to form
a set of labeled positive documents, P. The reminding train-
ing documents are regarded as U. Here, x is ranged from
1, 2, . . . , 9 (extremely small cases) and from 10, 20, . . . , 100
(large cases). Totally, there are 19 cases (9 small cases, 9
large cases and 1 benchmark case).

4.2 The Signiﬁcant of Partitioning

This experiment attempts to verify our analysis and
claims in Section 2.2. Several approaches are tested in com-
parison with PN-SVM.

• Negative-Only (NO) We implement PN-SVM but
does not extract any positive documents, i.e. we do
not extract P(cid:1).

7http://www.daviddlewis.com/resources/testcollections/

reuters21578/

naive-bayes.html

www/wwwkb/

8http://www.cs.cmu.edu/afs/cs/project/theo-11/www/

9http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/

%
1
2
3
4
5
6
7
8
9
10
20
30
40
50
60
70
80
90
100

PN-SVM

0.422
0.473
0.566
0.582
0.605
0.604
0.643
0.632
0.639
0.638
0.676
0.685
0.714
0.748
0.809
0.824
0.834
0.843
0.848

NO
0.106
0.171
0.200
0.225
0.274
0.321
0.389
0.462
0.509
0.553
0.675
0.767
0.792
0.816
0.832
0.853
0.851
0.855
0.862

PB
0.410
0.512
0.553
0.530
0.551
0.562
0.509
0.520
0.522
0.505
0.604
0.663
0.701
0.761
0.782
0.817
0.834
0.839
0.836

BA
0.135
0.151
0.167
0.193
0.225
0.263
0.294
0.321
0.359
0.396
0.592
0.682
0.736
0.765
0.790
0.813
0.827
0.836
0.846

RE
0.293
0.465
0.530
0.503
0.508
0.523
0.491
0.500
0.502
0.482
0.415
0.296
0.291
0.287
0.267
0.265
0.244
0.241
0.240

Table 2. The signiﬁcant of partitioning and the im-
portance of extracting positive documents.

• Repeated Extraction (RE) We implement the ﬁrst
of the three approaches which is described in Section
2.2.1.

• Proﬁle Based Approach (PB) We implement the third
of the three approaches which is described in Section
2.2.1. We use Rocchio as the proﬁle based classiﬁer
[16].

• Benchmark Approach (BA) We build a SVM clas-
siﬁer using the given percentage of P as the positive
training documents, and all of the N as negative train-
ing documents. Note that N is not mixed with P. Thus,
this approach is served as an benchmark approach.

Table 2 shows the results of using Reuters-21578. Due
to space limited, we cannot report the results of all bench-
marks. However, they behave in the similar way. Let us
discuss some of the interesting ﬁndings below.

From Table 2, it is interesting to see that NO is better
than BA in the 100% case, i.e.
in the perfect information
situation. This is because some labeled negative documents
are actually noise in the negative training set.
Including
such kind of documents in the negative training set will af-
fect the consistence of the SVM model.
In other words,
such kind of data is not only unhelpful, but also degrades
the overall accuracy. Another interesting ﬁnding is that, in
general, NO is better than BA for all cases. The reason
is that when the positive documents are few, the negative
documents extracted by NO will also be fewer. However,
for BA, |N| is constant. If |P| is too small but |N| is too
large, this will certainly introduce bias towards the negative
set, thus degrade the accuracy of the classiﬁer. Certainly,
by setting an appropriate biasing value for SVM, such kind

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

0.65

0.6

0.55

0.5

0.45

0.4

0.35

1
F
-
o
r
c
a
m

2%
5%
8%

20%
50%
80%

0.85

0.8

1
F
-
o
r
c
a
m

0.75

0.7

0.65

0.3

0

10

30

40
20
Number of Clusters

50

60

0.6

0

10

30

40
20
Number of Clusters

50

60

(a) Small P

(b) Large P

Figure 2. The effect of different cluster numbers

of bias could be minimized. However, this requires further
complex and time consuming tuning strategy.

When the size of the labeled positive documents are few,
i.e. < 20%, PN-SVM proposed in this paper performs the
best. In particular, when |P| is small, BA performs poorly
even though it does not contain any positive documents in
the negative training set. This supports our claim and mo-
tivation of this paper: the quality of a classify can be en-
hanced by including more positive documents in the train-
ing set. When the percentage of positive documents in-
creases, the accuracy of NO becomes better, and eventually
outperforms all approaches. Since the negative documents
extracted by both PN-SVM and NO (Negative-Only) are the
same, the main reason that NO performs better is certainly
because some negative documents are incorrectly regarded
as positive documents, and this harms the quality of PN-
SVM.

4.3 The Effect of the Number of Partitions

Figure 2 shows the effect of varying the number of parti-
tions in the stage of enlarging positive-negative documents
using the benchmark Reuters-21578. We show two cases:
small |P| and large |P|. For the small |P|, we use 2%, 5%
and 8% of the true positive documents, and for the large |P|,
we use 20%, 50% and 80%.

Here, the number of partitions varies from 1, 15, 30, 45
to 60. When the size of the positive documents is large
(80%), the effectiveness of varied numbers of partitions is
small, and is about the same. When the size of the positive
documents is small (2%), a large number of clusters will
signiﬁcantly decrease the accuracy. The optimum number
of partition is around 20-30 regardless of the size of the la-
beled positive documents given. This suggests that the opti-
mum number of partitions is most likely to be in this range.
We plan to investigate the optimum number of clusters as
our future work.

PN-SVM PN-SVM-I (# of Iterations)

%
1%
3%
5%
7%
10%
30%
50%
70%
90%

0.422
0.566
0.605
0.639
0.638
0.685
0.748
0.824
0.843

1
F
-
o
r
c
a
m

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.424 (3)
0.569 (4)
0.602 (3)
0.643 (4)
0.642 (3)
0.678 (3)
0.748 (2)
0.823 (2)
0.843 (1)

1%
3%
5%
7%
10%

Table 3. Results of iterative training.

1

2

3

4

5

6

Stage

Figure 3. The results of training iteratively.

4.4 The Results of the Iterative Partition-Based

Heuristics

Table 3 shows the results of the iterative partition-based
heuristics using Reuters-21578. The bracket is the aver-
age number of iteration performed by Algorithm 4. Table
3 shows that the number of iteration decreases as the size of
the labeled positive documents increases. This is expected
because if the number of labeled documents increases the
number of unlabeled documents would therefore decreases,
which implies that it would be even more difﬁcult to further
extract any of the positive and/or negative documents. As a
result, the number of iterations should be minimized.

Consider the classiﬁcation accuracy (Table 3). For the
1%, 3%, 7% and 10% cases, the accuracy of the iterative
partition-based heuristics is slightly better, but it becomes
slightly worse in other cases. It seems that getting a few
more positive or negative samples may not necessarily im-
prove the accuracy. It is difﬁcult to tell whether they can im-
prove the quality of classiﬁcation or further introduce some
errors. This suggests that our PN-SVM without iterative
partition-based heuristics can achieve reasonable accuracy.
Therefore, there is no need to make any decisions on the
best time to terminate the iterative training, and computa-
tional cost can be minimized.

Figure 3 shows a more detailed analysis with regard to
the iterative training for the 1%, 3%, 5%, 7% and 10%
cases. In the ﬁgure, the x-axis is the Stage, which has the
following means: Stage 1 – the result of after NO (Negative-
Only); Stage 2 – the result of PN-SVM; Stage 3, 4, 5, and
6, are the results of PN-SVM-I after 2, 3, 4 and 5 itera-
tions.10 From Figure 3, it is easily to see that the accuracy

10Note that only the 3% and 7% cases have the 4th iteration.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

Iteration

0
1
2
3
4
5

1%
0.510
0.513
0.514
–
–
–

Earn
10%
0.790
0.788
0.788
–
–
–

90%
0.978
–
–
–
–
–

1%
0.341
0.348
0.351
0.347
0.348
0.346

Wheat
10%
0.588
0.586
0.590
0.591
0.590
–

90%
0.710
0.710
0.704
–
–
–

Table 4. Partition-Based Heuristic Result.

is nearly constant from stage 2 to 6. No ﬂuctuation is ob-
served. It suggests that, when accuracy decreases by adding
some positive documents, extracting more positive docu-
ments cannot improve the accuracy. It conﬁrms our pre-
vious argument that simple iterative training may not lead
to noticeable improvement.

Table 4 shows a more detailed result of the 1% and 10%
cases for two categories in Reuters-21578: wheat and earn.
They are chosen because wheat usually has the largest num-
ber of iterations whereas earn has the smallest number of
iterations. In Table 4, the “-” indicates that no more posi-
tive documents can be extracted. Since the size of the cat-
egory of wheat is much smaller than that of earn, we be-
lieve that it is the major reason that it will have more iter-
ations than earn. Furthermore, notice that the accuracy of
earn and wheat performs quite stable in different iterations.
Therefore, we conclude once again that our partition-based
heuristics is suboptimal in the sense that it cannot be im-
proved noticeably throughout a simple iterative extraction
method.

4.5 Different Approaches Comparison

Table 5, Table 6 and Table 7 show the results of dif-
ferent approaches for Reuters-21578, Newsgroup-20, and
Web-KB benchmark datasets.

PEBL is acceptable only when more than 70% of the true
positive documents is given with Reuters-21578. Weighted
Logistic Regression (Log) outperforms PEBL, but is infe-
rior to the other heuristics.

SVM is the best reported text classiﬁcation up-to-date
[6, 21]. However, as shown in the three tables, it is ex-
tremely sensitive to noise11. It even performs worse than
NB [12] in most of the cases with the three benchmarks.
This replicates some previous ﬁndings in [10]. Certainly,
the accuracy of SVM may be improved by changing some
of its default parameters, such as setting different trade-off

11In general, there are two kinds of noise: 1) Noise in the feature; and
2) Noise in the training data. The former one is usually solved by apply-
ing some feature selection techniques, such as Information Gain and χ2,
whereas the latest one is related to the precision of labeling the positive
training examples and the negative training examples. In this paper, unless
otherwise speciﬁed, when we state the term noise, we are referring to the
noise in the training data

between the training error and the hyperplane, by using a
validation set. However, this may requires complex and
time consuming parameter tuning strategy. Furthermore, n-
cross validation or leave one out estimation may become
unreliable if |P| is very small.

Roc-SVM performs better when the percentage of the
true positive documents is larger ≥ 20% in Reuters-21578,
and is the best when the percentage is between 20% and
60% in Reuters-21578. Roc-SVM does not perform well
in the other two benchmarks. S-EM outperforms Roc-SVM
when the percentage of the true positive documents is less
than 20% in all the three benchmarks. However, the accu-
racy of S-EM shows an inverse U-shape in Newsgroup-20
and Web-KB benchmarks. S-EM performs the best around
30% to 40%. Towards both ends, it deteriorates. It is not de-
sirable because it does not show monotonicity. Spy-SVM-
IS outperforms S-EM only when the percentage intends to
be large, as also reported in [10]. The main difference be-
tween S-EM and Spy-SVM-IS is the ﬁnal classiﬁer gen-
erated. Note: S-EM uses NB whereas Spy-SVM-IS uses
SVM. Because NB is more tolerant to noise, S-EM per-
forms better in noisy situations, whereas Spy-SVM-IS is
better in noiseless situations.

The proposed PN-SVM signiﬁcantly outperforms the
others especially when the percentage of the true positive
documents is extremely small (≤ 20%). It does not vary
much regardless of the size of P. In all the three bench-
marks, PN-SVM performs the best on almost all cases in
Web-KB. One of the characteristics of Web-KB is that all
web pages belong to one and only one class. On the other
hand, Reuters-21578 has many articles assigned to more
than one classes. Newsgroup-20 is properly the most dif-
ﬁcult dataset in which no approach has dominate when the
size of P becomes large. Consider Reuters-21578, note that
Roc-SVM has a sudden jump from 10% to 20%, while PN-
SVM growths steadily. This observation is very interesting.
More investigation will be done on this issue as the future
work.

5 Conclusion

This paper presents a partition-based solution on the text
classiﬁcation problem where only a small set of labeled pos-
itive documents (P) and a large set of unlabeled documents
which are mixed with both positive and negative documents
(U) are given.

Existing techniques focus on how to extract the negative
documents (N) from the unlabeled documents U. They are
unable to extract the positive documents from the unlabeled
documents, and therefore cannot fully utilize the informa-
tion presented in the unlabeled documents. Extracting posi-
tive documents from the unlabeled documents is important,
but is difﬁcult. We observe the difﬁculty of extracting posi-

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

SVM S-EM Spy-SVM-IS

Roc-SVM PN-SVM PEBL

Table 5. The results of Reuters21578.

SVM S-EM Spy-SVM-IS

Roc-SVM PN-SVM PEBL

Log

%

1
2
3
4
5
6
7
8
9
10
20
30
40
50
60
70
80
90
100

%
1
2
3
4
5
6
7
8
9
10
20
30
40
50
60
70
80
90
100

%
1
2
3
4
5
6
7
8
9
10
20
30
40
50
60
70
80
90
100

NB

0.100
0.189
0.239
0.294
0.351
0.413
0.446
0.481
0.509
0.528
0.686
0.718
0.735
0.742
0.749
0.748
0.746
0.740
0.728

NB
0.158
0.164
0.189
0.197
0.234
0.253
0.289
0.323
0.448
0.530
0.579
0.617
0.616
0.614
0.615
0.613
0.626
0.632
0.631

NB
0.04
0.076
0.123
0.179
0.225
0.262
0.309
0.344
0.374
0.499
0.596
0.634
0.655
0.665
0.674
0.675
0.682
0.687
0.688

0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.002
0.054
0.169
0.399
0.546
0.674
0.748
0.789
0.846

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0

0.003
0.02
0.051
0.196
0.249
0.421
0.596
0.672
0.729

0.021
0.031
0.044
0.168
0.232
0.341
0.439
0.537
0.645
0.712

0.131
0.248
0.284
0.387
0.426
0.493
0.538
0.571
0.594
0.615
0.702
0.724
0.734
0.737
0.732
0.727
0.726
0.718
0.706

0.151
0.076
0.16
0.14
0.186
0.255
0.277
0.282
0.31
0.340
0.417
0.434
0.506
0.595
0.599
0.601
0.626
0.719
0.729

0.077
0.122
0.212
0.276
0.355
0.372
0.433
0.438
0.56
0.587
0.632
0.649
0.654
0.657
0.654
0.649
0.657
0.659
0.669

0.000
0.098
0.144
0.163
0.149
0.193
0.297
0.345
0.290
0.402
0.622
0.683
0.738
0.753
0.777
0.795
0.829
0.837
0.850

0.002
0.007
0.006
0.007
0.009
0.011
0.053
0.078
0.119
0.154
0.271
0.335
0.493
0.547
0.566
0.581
0.697
0.698
0.710

0.001

0

0.01
0.02
0.047
0.042
0.07
0.095
0.160
0.252
0.328
0.491
0.551
0.599
0.642
0.653
0.699
0.731
0.746

0.011
0.075
0.162
0.261
0.332
0.402
0.500
0.538
0.568
0.596
0.716
0.754
0.774
0.798
0.809
0.822
0.830
0.835
0.834

0.001
0.007
0.007
0.006
0.007
0.010
0.020
0.160
0.181
0.207
0.280
0.394
0.516
0.622
0.671
0.652
0.690
0.760
0.785

0.01
0.012
0.019
0.03
0.039
0.07
0.091
0.114
0.147
0.268
0.432
0.525
0.581
0.611
0.642
0.655
0.688
0.726
0.747

0.422
0.473
0.566
0.582
0.605
0.604
0.643
0.632
0.639
0.638
0.676
0.685
0.714
0.748
0.809
0.824
0.834
0.843
0.848

0.274
0.302
0.318
0.315
0.311
0.398
0.402
0.426
0.430
0.479
0.523
0.565
0.628
0.662
0.691
0.707
0.709
0.718
0.747

0.128
0.497
0.394
0.498
0.558
0.566
0.574
0.585
0.590
0.595
0.605
0.61
0.614
0.666
0.671
0.674
0.689
0.690
0.710

Log

0.000
0.000
0.000
0.000
0.090
0.108
0.135
0.146
0.144
0.165
0.325
0.444
0.535
0.590
0.647
0.713
0.758
0.792
0.855

0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.002
0.032
0.044
0.114
0.289
0.485
0.651
0.706
0.782
0.821

0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0

0.013
0.058
0.196
0.238
0.321
0.492
0.578
0.616
0.67
0.654
0.652

0.168
0.237
0.389
0.42
0.561
0.594
0.628
0.664

0.005
0.013
0.081
0.157
0.202
0.284
0.391
0.479
0.514
0.55
0.668
0.696

0.047
0.054
0.064
0.171
0.243
0.304
0.362
0.423
0.473
0.522
0.579
0.644
0.696

Table 6. The results of Web-KB.

SVM S-EM Spy-SVM-IS

Roc-SVM PN-SVM PEBL

Log

Table 7. The results of Newsgroup20.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

tive documents from the unlabeled documents is due to the
diversity exhibited in the unlabeled documents. In order to
solve the diversity problem, we ﬁrst extract negative doc-
uments N from U based on core vocabulary presented in
P with feature normalization. Second, we partition N into
clusters, which assists us to extract more positive/negative
documents from U (cid:1) = U − N. Our approach is to extract
more positive and negative documents, and can be imple-
mented using any existing text classiﬁcation approaches.

We conducted extensive testing using three benchmarks,
Reuters-21578, Newsgroup-20, and Web-KB benchmark
datasets. The testing results show that our approach is
highly feasible, and signiﬁcantly outperforms the others in
particular when the number of positive documents is ex-
tremely small.

Acknowledgement

The work described in this paper was supported by
grants from the Research Grants Council of the Hong
Kong Special Administrative Region (CUHK4229/01E,
HKUST6175/03E).

References

[1] D. Bennett and Demiritz. A semi-supervised support vec-
tor machines. Advances in Neural Information Processing
Systems, 11, 1998.

[2] J. Bockhorst and M. Craven. Exploiting relations among
In Pro-
concepts to acquire weakly labeled training data.
ceedings of the 19th International Conference on Machine
Learning, 2002.

[3] P. Bradley and U. Fayyad. Reﬁning initial points for k-
means clustering. In Proceedings of the 15th International
Conference on Machine Learning, 1998.

[4] D. R. Cutting, D. R. Karger, J. O. Pederson, and J. W. Tukey.
Scatter/gather a cluster-based approach to browsing large
document collections. In Proceedings of the 15th Interna-
tional Conference on Research and Development in Infor-
mation Retrieval, 1992.

[5] R. Ghani. Combining labeled and unlabeled data for multi-
class text categorization. In Proceedings of the 19th Inter-
national Conference on Machine Learning, 2002.

[6] T. Joachims. Text categorization with support vector ma-
chines: Learning with many relevant features. In Proceed-
ings of 10th European Conference on Machine Learning,
1998.

[7] B. Larsen and C. Aone. Fast and effective text mining us-
ing linear-time document clustering. In Proceedings of the
5th International Conference on Knowledge Discovery and
Data Mining, 1999.

[8] W. S. Lee and B. Liu. Learning with positive and unlabeled
In Proceed-
example using weighted logistics regression.
ings of 20th International Conference on Machine Learning,
2003.

[9] X. Li and B. Liu. Learning to classify texts using positive
In Proceedings of 2003 International

and unlabeled data.
Joint Conference on Artiﬁcial Intelligence, 2003.

[10] B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. Building text
In Pro-
classiﬁers using positive and unlabeled examples.
ceedings of 3rd International Conference on Data Mining,
2003.

[11] B. Liu, P. S. Yu, and X. Li. Partially supervised classiﬁca-
tion of text documents. In Proceedings of 19th International
Conference on Machine Learning, 2002.

[12] A. McCallum and K. Nigam. A comparison of event mod-
els for naive bayes text classiﬁcation. In The 15th National
Conference on Artiﬁcial Intelligence Workshop on Learning
for Text Categorization, 1998.

[13] D. C. Montgomery. Introduction to Statistical Quality Con-

trol. Wiley Text Books, forth edition, 2000.

[14] D. C. Montogomery and G. C. Runger. Applied Statistics
John Wiley & Sons, Inc.,

and Probability for Engineers.
second edition, 1999.

[15] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text
classiﬁcation from labeled and unlabeled documents using
em. Machine Learning, 39, 2000.

[16] J. Rocchio. Relevance feedback information retrieval.

In
G. Salton, editor, The Smart retrieval System – experiments
in automatic document processing, pages 313–323. Prentice
Hall, 1971.

[17] G. Salton and C. Buckley. Term-weighting approaches in
automatic text retrieval. Information Processing and Man-
agement, 24(5):513–523, 1988.

[18] H. Schutze, D. A. Hull, and J. O. Pedersen. A compari-
son of classiﬁers and document representations for the rout-
ing problem. In Proceedings of the 18th International Con-
ference on Research and Development in Information Re-
trieval, 1995.

[19] F. Seabastiani. Machine learning in automated text catego-

rization. ACM Computing Surveys, 34(1):1–47, 2002.

[20] Y. Yang. A study on thresholding strategies for text cate-
gorization. In Proceedings of the 24nd International Con-
ference on Research and Development in Information Re-
trieval, 2001.

[21] Y. Yang and X. Liu. A re-examination of text categorization
In Proceedings of the 22nd International Con-
methods.
ference on Research and Development in Information Re-
trieval, 1999.

[22] H. Yu, K. C.-C. Chang, and J. Han. Heterogeneous learner
for web page classiﬁcation. In Proceedings of 2nd Interna-
tional Conference on Data Mining, 2003.

[23] H. Yu, J. Han, and K. C.-C. Chang. Pebl: Positive exam-
ple based learning for web page classiﬁcation using svm. In
Proceedings of the 9th International Conference on Knowl-
edge Discovery and Data Mining, 2003.

[24] T. Zhang. The value of unlabeled data for classiﬁcation
In Proceedings of the 17th International Con-

problems.
ference on Machine Learning, 2000.

Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE

