Investigations of Continual Computation

Dafna Shahaf∗

Carnegie Mellon University

dshahaf@cs.cmu.edu

Eric Horvitz

Microsoft Research

horvitz@microsoft.com

Abstract

Autonomous agents that sense, reason, and act in
real-world environments for extended periods often
need to solve streams of incoming problems. Tradi-
tionally, effort is applied only to problems that have
already arrived and have been noted. We examine
continual computation methods that allow agents
to ideally allocate time to solving current as well
as potential future problems under uncertainty. We
ﬁrst review prior work on continual computation.
Then, we present new directions and results, in-
cluding the consideration of shared subtasks and
multiple tasks. We present results on the compu-
tational complexity of the continual-computation
problem and provide approximations for arbitrary
models of computational performance. Finally, we
review special formulations for addressing uncer-
tainty about the best algorithm to apply, learning
about performance, and considering costs associ-
ated with delayed use of results.

Introduction

1
Agents immersed in real-world environments over extended
periods of time typically face a continuing stream of problems
over their lifetimes. In competitive, time-critical situations,
they cannot afford the luxury of ceasing their problem solv-
ing during periods that might typically be referred to as idle
time between challenges. Continual computation ideally par-
titions all available computation to current as well as to future
problems under uncertainty in the nature and arrival time of
the future problems [Horvitz, 2001]. The methods consider
how best to reason, reﬂect, and recall solutions to forthcom-
ing challenges. The methods also describe how to shift com-
putational resources from real-time challenges to problems
that have not yet arrived.

We shall review prior work on continual computation
(CC ). Then, we focus on several extensions. We ﬁrst con-
sider how agents can handle multiple tasks and subtasks,
extending the prior work on handling streams of separate,
single tasks. Then, we explore continual computation for
problems with shared subtasks. The earlier work introduced

∗Dafna Shahaf performed this research during an internship at

Microsoft Research.

formulations and families of utility models describing the
value achieved with computational reﬁnement that enabled
the tractable composition of optimal continual computation
policies. The tractability of the approach hinges on proofs
that demonstrate that greedy selections are optimal for the
utility models. We show how the extensions can ﬁt into the
same paradigm for computing tractable policies. Then we re-
lax the constraints on utility models and discuss the complex-
ity of computing continual computation policies for general
utility functions. We introduce a polynomial approximation.
We move on to introduce special formulations of continual
computation and their solutions. We consider situations in-
cluding the case where the freshness of precomputed results
is important and where the value of stored results degrades
with time. We also consider methods for learning and reason-
ing about the performance of algorithms. For several models,
we share simulations of studies of continual computation on
synthetic datasets.

2 Background
Prior results on continual computation provide ideal policies
for allocating time to solving current and future problems,
whether the solution procedures are all-or-nothing algorithms
(where results have no value unless a problem is solved com-
pletely), or ﬂexible, anytime procedures, where partial results
of increasing quality are produced with computation. The
work work also describes policies that include caching and
reuse of results. We shall brieﬂy review several core ideas of
continual computation but refer readers to [Horvitz, 2001] for
details.

Assume that an agent can infer probabilities, p(Ii|E), of
seeing different problem instances Ii ∈ I in the next time
period within an environment E, based on its experiences.
Given probabilities of future challenges, how should an agent
spend the time available between its real-time challenges?

The earlier work provides methods for both minimizing the
time required for a system to solve problems and for max-
imizing the expected utility of computational problem solv-
ing. If we take T to be the period of time available between
real-time challenges, and tp
i as the idle time allocated to initi-
ating or completing the solution of problem instance Ii ahead
of time, then the maximum quantity of time that can be allo-
cated solving a future problem instance is just the time needed
to solve the problem completely, t(Ii). Without precompu-
tation, the delay expected for solving future potential prob-

285

(cid:2)

lem instances under uncertainty is
If the
agent begins solving or completely solves one or more future
problems in advance of its arrival, the expected delay in the
i ≤ t(Ii) and
next period is
(cid:2)

i p(Ii|E)(t(Ii) − tp

i ), where tp

i p(Ii|E)t(Ii).

(cid:2)

i tp

i ≤ T .

Theorem 2.1 (Policy for Minimizing Computational Delay
[Horvitz, 2001]). Given an ordering over problem instances
by probability p(I1|E) ≥ p(I2|E) ≥ ... ≥ p(In|E), of being
passed to a problem solver in the next period, the idle-time
resource policy that minimizes the expected computation time
in the next period is to apply all resources to the most likely
problem until it is solved, then the next most likely, and so
on, until the cessation of idle time or solution of all problems
under consideration in the next period.

The greatest reduction in the expected real-time delay is
obtained by the greedy algorithm of applying all resources
to the most likely instance until it is solved, removing that
instance from consideration and then making the same argu-
ment iteratively with the remaining n − 1 problems and re-
maining idle time.

The prior work on continual computation also provides re-
sults for minimizing the expected cost of delay. We are given
a time-dependent cost function for each future problem in-
stance, Cost(Ii, t), that takes as arguments that instance and
the period of time required to compute each instance follow-
ing a challenge (the delayed response). If we apply compu-
tation to begin solving (or solve completely) one or more po-
tential problems, the expected delay after the arrival of a chal-
i )). Tractable
lenge is
strategies for speciﬁc classes of cost functions were intro-
duced. For example, consider the case where costs increase
linearly with increases in delay until a solution is available,
Cost(Ii, t) = Ci · t.

i p(Ii|E)(Cost(Ii, t(Ii)) − Cost(Ii, tp

(cid:2)

The expected value following the arrival of a challenge is
maximized by allocating idle time available in advance of the
challenge to commence solving the instance associated with
the greatest expected rate of cost of diminishment, p(Ii|E)Ci,
and to continue until it is solved, and then to solve the in-
stance with the next highest expected rate, and so on, until all
of the instances have been solved.

Studies of continual computation also examined policies
that trade off the resources applied to continue to solve prob-
lems that have already arrived with proactive computation of
problems that might arrive in the future. The idea is that an
agent might wish to slow down or cease problem solving on
a current problem given the expected value that might come
from precomputing an answer to a problem that is expected
to arrive later with some probability [Horvitz, 1999].

The prior work on continual computation includes policies
for allocating time to ﬂexible algorithms . We brieﬂy review
these policies. Assume a algorithm S that reﬁnes an initial
problem instance I or previously computed partial result,
π(I), into a new result, terminating on a ﬁnal, or complete
result, φ(I). As reasoning systems may often be uncertain
about the value associated with future computation, agents
must consider a probability distribution over results achieved
in return for some investment of time, , conditioned on the
current partial result, and the procedure selected, resources,

S(π(I), t) → p(π(cid:2)(I)|π(I), S, t)

(1)

Studies of continual computation with ﬂexible procedures
make use of expected value of computation (EVC) [Horvitz et
al., 1989]. The expected value of computation (EVC) is the
change in the net expected value of a system’s behavior with
the reﬁnement of one or more results with the allocation of
computational resources, t. The EVC considers the probabil-
ity distribution over outcomes of computation and the costs
of computation. The EVC for reﬁning a single result is,

(cid:3)

EVC(π(I), Si, t) =

p(π(cid:2)

j(I)|π(I), Si, t)uo(π(cid:2)

j(I))

j
−uo(π(I)) − Cost(t)

(2)

where uo(π(I)) is the value of a previously computed re-

sult π(I).

A key concept in deriving continual computation policies
for ﬂexible procedures is the expected value of precompu-
tation (EVP) and its derivative, the EVP ﬂux. EVP is the
EVC expected with precomputing potential future problem
instances Ii with procedures Si with time tp
i ,

EVP =

p(Ii|E)EVC(Si, Ii, tp
i )

(3)

(cid:4)

i

(cid:2)

where tp

i (Ii) ≤ t(Ii) and

i tp

i ≤ T

The EVP ﬂux, ψ(S, I, tp), is the instantaneous rate at
which the expected value of the future result changes at the
point of allocating tp seconds of precomputation time to solv-
ing a problem

ψ(Si, Ii, tp

i ) =

dEV P (Si, Ii, tp
i )

dtp
i

(4)

Continual computation policies for sequencing precomputa-
tion effort among potential future problem instances can be
composed by considering ψ(I, t). The EVP of policies is
computed by integrating over the EVP ﬂux for resources allo-
cated to each instance and summing together the EVP derived
from precomputing each problem instance,

EV P =

ψ(Ii, t)dt

(5)

(cid:4)

(cid:3) tp

i

0

i

Theorems are presented in the prior work for several classes
of algorithmic that specify how time should be allocated to
problems based on a following of the maximal EVP ﬂux.

We note that, although probability distributions over future
problems are called out in CC methods, key policies for guid-
ing the allocation of computation time can take as input qual-
itative orderings over likelihood, EVP, and EVP ﬂux. Thus,
coarser probabilistic knowledge can be used to ideally triage
idle time.

3 Handling Subtasks and Multiple Tasks
The prior work considered a stream of individual, indepen-
dent problems. We ﬁrst consider dependencies among tasks,
where portions of solutions can be cached and later shared
among tasks. Then, we present policies to handle the cases
where multiple tasks can arrive.

286

3.1 Subtasks
A key consideration is that work performed to solve one in-
stance might be reused to solve another instance. Such a sit-
uation occurs when instances share a result computed by a
common procedure.

Let S = {S1, ..., Sk} a set of subtasks, such that all tasks
are composed of them (Ii ⊆ S). Suppose the subsets are inde-
pendent – that we can solve each of the instances separately.
Proposition 3.1 (Policy for minimal expected time to com-
pletion with subtasks). Let p(Sj|E) =
p(Ii|E) ,
the probability that the next problem instance needs subtask
Sj. Given an ordering over subproblem instances by proba-
bility p(S1|E) ≥ p(S2|E) ≥ ... ≥ p(Sk|E), the policy that
minimizes the expected computation time in the next period is
to apply all resources to the most likely subproblem until it is
solved, then the next most likely, and so on, until the cessation
of idle time or solution of all subproblems under considera-
tion in the next period.

i.Sj ∈Ii

(cid:2)

When we relax the assumption of independent subtasks,
the problem becomes harder. Suppose, for example, that
S = {S1, ..., Sk} has a corresponding partial precedence or-
der, imposed by a directed-acyclic graph (DAG). In other
words, a subtask cannot be executed until all the preceding
subtasks have been completed.

We now consider the corresponding decision problem:
given S, and total precomputation time T , is there a (perhaps
fractional) subset of S we can perform to gain expected time
of at least K?
Theorem 3.2. If the set of subtasks S = {S1, ..., Sk} can
have an arbitrary precedence order, the decision problem is
N P-hard.

Proof sketch: We show a reduction from k-clique. Given
a graph G = (V, E), construct the following instance. Let
S = V ∪ E – i.e., there is a task for every edge and a task
for every vertex. Deﬁne the precedence ordering such that
an edge task can be picked only after the completion of its
two corresponding vertex tasks. Let the task probabilities be
p(v) = |E|2 + 1, p(e) = 1 + (cid:5), and the total time needed
(cid:6)
be t(v) = |E|2 + 1,t(E) = 1. Let T = k(|E|2 + 1) +
,
(cid:5)
and K = k(|E|2 + 1) + (1 + (cid:5))
. Since p’s represent
probabilities, we need to normalize all of these at the end.

If there is a k-clique in G = (V, E), picking the corre-
sponding vertices and then edges is a valid solution. If there
is a valid solution, it is easy to see that it has to include at
least k vertices. It cannot include k + 1 vertices, since the
capacity of T is not large enough. Since the ratio of p to t is
lower than (1 + (cid:5)) for every v, fractional vertices cannot lead
to a valid solution as well.

k
2

k
2

(cid:5)

(cid:6)

(cid:5)

(cid:6)

A valid solution, therefore, must include

edges. The
precedence constraints imply that those edges form a k clique
with the chosen vertices. Therefore, the graph contains a k-
clique.

k
2

3.2 Multiple tasks
Prior work on continual computation focused on the case of
an idle or problem solving period disrupted by the arrival of
single task arriving. We can generalize this formulation to
the case where multiple tasks can arrive simultaneously. Let

p(S|E) be the probability that subset S ⊆ I appears. We
want to maximize the EVP.

Corollary 3.3. We can consider sets of tasks as single
tasks and the elemental components of the larger reformu-
lated task as subtasks. Therefore, the greedy approxima-
tion as described in Proposition 3.1 applies, using marginals
p(Ii|E) =

Ii∈S p(S|E) is optimal.

(cid:2)

Note that the greedy algorithm is not optimal for maximiz-
ing many other natural objective functions (e.g., it cannot be
used to minimize the total response time, the time since the
instance was posed until it was solved). Other useful prop-
erties that hold in the single-task model do not hold in this
model as well, e.g. we cannot assume that future tasks arriv-
ing with the same probability are interchangeable.

We conducted experiments to evaluate the utility of con-
tinual computation with multiple tasks. We generated a syn-
thetic dataset of tasks, where each task is assigned a probabil-
ity of appearing, and a function describing how the utility of
partial results increases with computation. The probabilities
were modeled based on analyses done on a project centering
on the application of CC policies for web page prefetching.
The probabilities and utilities for the study were obtained
by crawling a subset of the web every few hours (details about
the crawl can be found here [Adar et al., 2009]). We take the
task of refreshing a cache of webpages i as a task Ii. If a
webpage has changed signiﬁcantly between two consecutive
snapshots (determined by Jaccard similarity), then we assert
that Ii is active; intuitively, the task is to refresh a local copy
of the web page. We used this data to calculate a probability
of activation for every task. We assumed that each task be-
comes active independently of previous time steps, or other
tasks. Rather than assuming that documents provide value to
users only when they are downloaded completely, we allow
value to be drawn from portions of documents that are imme-
diately available, similar to the approach taken in [Horvitz,
1999].

In other words, we can prefetch partial content from a web
page. The utility function we used was a nonlinear curve,
capturing the notion of decreasing marginal returns with ad-
ditional fetched content. We employed the class of utility
function for all pages, but we scaled the functions for dif-
ferent pages to reﬂect the page’s popularity. In other words,
refreshing portions or all of more popular pages have higher
utility than doing the same for less popular pages. Due to the
diminishing returns property, we may prefer to start fetching
less popular pages before completing the popular ones.

We simulated different test sequences of tasks being passed
to the solver. We compared the value generated by a tradi-
tional reactive algorithm, that initiates problem solving only
when a problem appears, versus a CC algorithm, while keep-
ing the size of the web cache constant for all conditions.

Figure 1 shows the results of these experiments. The ﬁgure
shows two lines, each representing the ratio of the utility gen-
erated across the cache for CC and for the default Reactive
procedure over time. The dashed line corresponds to a study
with tasks arriving with a mean arrival time that is 1.5 times
faster than the case represented by the solid line, in order to
see how the rate of instance arrival affects the utility ratio.
CC shows improvements of 10-30% over the reactive com-

287

Proof. CC is in N P (given a solution, checking it requires
polynomial time). CC is also N P-hard. We show a reduc-
tion from the knapsack problem:

In the knapsack problem, we are given n items. Each item
i has a value ri and a weight wi. The maximum weight that
we can carry in our bag is W . The problem is to ﬁnd out if
we can put items in the bag with a total value ≥ R.

Given an instance of a knapsack problem, we construct the
following CC instance: Let T = R + W . Each item i cor-
responds to a piecewise-linear function fi, s.t. fi(t) = 0 if
t ≤ wi, fi(t) = t − wi for t ∈ [wi, wi + ri], and fi(t) = ri
for t ≥ wi + ri.

A solution to this problem corresponds exactly to a solution
to the knapsack problem. Let S = {i | tp
i ≥ wi}, the items
corresponding to instances whose corresponding precompu-
tation time was at least their weight. Without loss of gener-
alization, we can assume that for no instance tp
i > wi + ri
(since the utility is constant after that point). This is feasible:

X

wi =

(tp

i − fi(tp

i )) ≤ T − R = W

X

i∈S

X

X

ri ≥

fi(tp

i ) ≥ R

since tp

i ∈ [wi, wi + ri], fi(tp

i ) = tp

i − wi, fi(tp

i ) ≤ ri.

Therefore, CC is N P-hard.

4.2 Approximations for CC
As CC is hard, we cannot hope for an optimal polynomial al-
gorithm. Instead, we investigate an approximation algorithm
for general utility functions, and show a constant-factor al-
gorithm. We formulate the problem in terms of set cover.
Let f1(t), ..., fn(t) be monotonic piecewise-linear functions
as before. For every instance Ii, we create items xi,k, for
k = 1... min(T, t(Ii)). xi,k represents the kth computa-
tion unit spent on Ii. We deﬁne the reward of an item as
r(xi,k) = fi(k) − fi(k − 1), the marginal utility from spend-
ing an extra unit of Ii after having spent k−1 already. We then
create sets Si,k = ∪j=1...kxi,j. Si,k corresponds to spending
k units on Ii. It is associated with cost k, the time it takes to
compute all its members. Let S denote the set of all those sets.
The total number of sets is polynomial in the representation
size of CC .

Our problem is to ﬁnd A ⊆ S that maximizes the total

reward of covered elements

(cid:2)

F (A) =
s.t. Cost(A) =

x∈Au r(x)

(cid:2)

S∈A Cost(S) ≤ T .

(where Au = ∪S∈AS)

Theorem 4.4. Let ACB be the cost-beneﬁt greedy solution,
and AUC be the unit-cost greedy solution (ignoring costs).
Then max F (ACB), F (AUC) is a 1
e ) approximation al-
gorithm that runs in O(T n) time.

2 (1 − 1

The proof is based on [Leskovec et al., 2007]; note that
F (A) is submodular, and Cost(A) is modular. If we are will-
ing to pay O(T n4), we can also get an (1 − 1
e ) approximation
[Sviridenko, 2004]. Lazy evaluations can also speed up the
algorithm.

Figure 1: Ratio of the utility of CC and of traditional, reactive
computation over time. The mean arrival rate for instances with
utility ratio captured by the dashed line is 1.5 times faster than for
the utility ratio captured by the solid line.

putation most of the time. However, CC performed worse
at some points. At these times, continual computation was
solving future instances rather than current ones. The invest-
ment in the precomputation proves to be useful over time. Not
surprisingly, CC performs better when there are longer idle
periods (solid line).

4 Hardness and Approximation for General

Utility Models

The prior work on continual computation presented ideal
policies for several classes of ﬂexible procedures, as deﬁned
by the functional form of the reﬁnement of utility of partial re-
sults with computation. These classes include algorithms that
reﬁne results with constant ﬂux, piecewise linear ﬂux with
decreasing returns, and smoothly increasing utility with de-
creasing returns [Horvitz, 2001].

A natural question arises: Can we generalize those opti-
mal policies to other classes? We now show that the most
general form of the continual-computation problem (CC ) is
N P-hard, and we give a constant-factor approximation algo-
rithm for it.
4.1 N P-hardness of CC
We start by formally deﬁning CC . We are given I =
{I1, ..., In} problem instances with corresponding utility
functions.
Deﬁnition 4.1 (Utility Functions). Let f1(t), ..., fn(t) be a
set of monotonic functions. We say that fi is the utility func-
tion of instance Ii if fi(t) is the utility for partial results if we
spend t computation units on instance Ii.

As we are working with computation units, we can assume
they are piecewise-linear functions on N → R+ (the problem
is essentially discrete; we cannot have a temporal grain size
that is arbitrarily ﬁne). Also, wlg fi(0) = 0.

The CC problem is ﬁnding the max

tp
i ≤
T . We now consider the decision variant of that problem:
Deﬁnition 4.2 (CC Decision Problem). Given utility func-
tp
tion fi and integers R and T , can we ﬁnd tp
i ≤ T
and

i fi(tp

i ) ≥ R?

i fi(tp

i ) s.t.

i s.t.

(cid:2)

(cid:2)

(cid:2)

(cid:2)

Standard techniques (i.e., binary search) allow us to switch

between the decision and the search problems.
Theorem 4.3. CC is N P-hard.

288

5 Special Formulations
We now move to special formulations of continual compu-
tation that introduce elements that have not been explicitly
modeled in prior studies. These formulations include con-
siderations of hard time constraints on the generation of so-
lutions, the use of explicit models of learning and reasoning
about the performance of algorithms, and models focused on
the value of updating or refreshing aging of results, where we
will to such tasks as web caching and crawling and explore
special analyses for such tasks.

5.1 Hard Constraints on Solution Time
Previous work considered the problem of reasoning with a
procedure that provides smoothly increasing utility with de-
creasing returns in the context of cost that increases con-
stantly with time (decreasing returns, constant cost scenar-
ios). In such settings the continuous processes of reﬁnement
and cost are weighed and computation or precomputation
ceases when the cost of computation outweighs the beneﬁt of
additional reﬁnement of a result. In contrast to such smooth
processes, we shall now consider the case of continual com-
putation with hard deadlines.

Let us suppose that each task Ii has an associated deadline
d(Ii). If the task is not completed d(Ii) time units after it ap-
pears, we cannot complete it, and we gain nothing from the
computation. Our task is to maximize the probability of com-
pleting a task or maximizing the expected utility associated
with a task being completed.

First, we note that, if t(Ii) ≤ d(Ii), we do not need to
spend any time on Ii; if it appears, we have enough time to
solve it on the spot. Without a loss of generalization, we as-
sume t(Ii) > d(Ii) for all Ii. If this is not true, we do not
spend any precomuptation on the other tasks. If the inequal-
ity relationship is true, it only makes sense to precompute Ii if
we spend exactly d(Ii) − t(Ii) units of precomputation time.
This is exactly the knapsack problem. We have a knapsack
with capacity T , and each task Ii corresponds to an item of
size d(Ii) − t(Ii) and value p(Ii|E).

We can also optimize the total expected value via approxi-
mate greedy or semi-greedy algorithms in an attempt to max-
imize the overall expected value. For example, with a value-
density approximation, we prioritize partial results in an or-
der dictated by the ratio of their value and cost. The value-
density approach is used in an approximation procedure to
come within a factor of two of the maximal value storage
policy [Sahni, 1975]. Other very efﬁcient FPTAS exist for the
knapsack problem; [Lawler, 1977] achieves a running time of
O(n log 1

(cid:2)4 ) for a 1 + (cid:5) approximation.

(cid:2) + 1

To explore the performance of a knapsack approxima-
tion for a real-time CC procedure, we generated synthetic
datasets of tasks with hard deadlines and tested the quality
of solutions when the algorithm runs for limited amounts of
time. Like the experiment described earlier, we base the ar-
rival rates for instances on webpage change rates. The dead-
lines and the times needed to solve each instance were chosen
uniformly at random for each dataset, while making sure that
t(Ii) > d(Ii).

We used the algorithm of [Botev and Kroese, 2008] and
manipulated the number of iterations. Figure 2 shows the
results. Each point represents the ratio of the value of the

Figure 2: Approximation ratio of knapsack approximation for hard
deadline model. Each point represents the ratio for a different trial,
with a new random choice of deadlines and completion times of
tasks. The triangles represent the approximate solution ratio after
10 iterations, and the solid line after 20. The approximation recov-
ered a near-optimal solution for all datasets after twenty iterations.

approximated and the optimal solution—as computed via dy-
namic programming. Following 10 iterations, the approxima-
tion algorithm was achieving very good results. After as little
as 20 iterations, the results were near-optimal. The approach
took only a fraction of a second to execute, running orders of
magnitude faster than the dynamic-programming solution.

5.2 Result Freshness and Time of Use
Most of the formulations of continual computation to date al-
low for an arbitrary delay before a precomputed solution is
used to solve a real-time challenge. Continual computation
policies can also be developed that provide special constraints
and costs on the timing of usage of a precomputed result. For
example, a precomputed result might have to be immediately
harnessed by an agent so as to draw any value from the pre-
computation, lest the result is rendered worthless. If Ii ar-
rives at time t, the reward obtained is fi(tp
i (t)). That is, the
reward is only available for an immediate use and reﬂects the
amount of precomputation that has been performed when the
task arrives. In a related setting, the utility of a precomputed
solution can diminish with the amount of time that passes be-
fore it is used. This situation can be viewed as being forced
to pay a rental fee for storing precomputed results in memory.
Thus, it is best to produce results so that they are ready near
the time when they are needed, rather than producing them
far in advance and paying for their storage.

As an example of a real-world scenario of continual com-
putation with these characteristics, consider the case where
computer users access web pages via a proxy server. The
server has cached copies of web pages. However, page i can
change with probability ci at every step. Although we do not
know when a page changes, we can fetch a new copy of it,
guaranteeing that it will be up to date in the next step.
In
addition, some websites are more popular than others; the ex-
pected number of requests arriving for a page every step is ri.
If a reward is received for responding to user requests with an
updated page, what is the ideal fetching policy? We wish to
maximize (cid:4)

(cid:4)

ri · 1I(page i is fresh at time t)

t

i

The optimal CC policy is periodic, since the (discrete)
number of states is ﬁnite. A natural extension is to seek an op-

289

timal stochastic-periodic policy. We refer the reader to [Sha-
haf et al., 2009] for the derivation and studies of the operation
of an algorithm for ﬁnding an optimal stochastic periodic pol-
icy that maximizes the expected number of requests fulﬁlled
in polynomial time. The algorithm refomulates the problem
as a convex optimization problem.

5.3 Probabilistic Performance Model
Many real-world problems involve uncertainty about the out-
come of actions. Such uncertainty can apply to problem solv-
ing. Let us assume that the result of spending one unit of com-
putation on Ii is not completely under a system’s control. In
addition, when the system is targeted at running indeﬁnitely
in an environment, we may want to minimize the discounted
reward.

We assume known action models, full observability (i.e.,
we know the result of our past actions), and the Markov prop-
erty: given the state of the system, transition probabilities to
the next state are independent of all previous states or actions.
Markov decision processes (MDPs) provide a useful mathe-
matical framework for modeling this type of situation. We
brieﬂy review the MDP framework, and then show how to
model a CC problem as an MDP.

An MDP is deﬁned as a 4-tuple (X, A, R, P ) where X is a
ﬁnite set of N states, A is a ﬁnite set of actions, R : X ×A →
R is a reward function , such that R(x, a) represents the re-
ward obtained by the agent in state x after taking action a. P
is a Markovian transition model where P (x(cid:2)|x, a) represents
the probability of going from state x to state x(cid:2) with action a.
Let us assume that problem instances can appear at any
time along a timeline. At every time step, we can either spend
computation on an instance or return a solution to an active
instance. The reward we receive is based on the amount of
reﬁnement of that instance. In addition, we have a storage
limit on the amount of reﬁnement that can be stored, M .

This can easily be modeled as an MDP: X is the state of
the system (how many computation units have been applied
to each instance Ii, which instances are currently active). We
have three types of actions: Addi (spend the next time-step on
computation for Ii), Freei (free from memory the last time-
step on computation you did for Ii), and Solvei (submit the
best current reﬁnement of Ii). The rewards are −∞ for any
illegal action (trying to add something when the memory is
full, freeing unit i when there is none is memory, or solving a
problem that is not active). The reward for solving an active
task Ii is based on its utility function fi and the number of
computation units have been applied to reﬁne Ii in X.

There exist efﬁcient algorithms for MDPs that enable us to
compute an optimal update policy. However, the MDP above
has an exponential number of states, and thus is untractable
for even small ns. Fortunately, this MDP has signiﬁcant inter-
nal structure, and can be modeled compactly if this structure
is exploited in the representation.

Factored MDPs [Boutilier et al., 2000] is one approach
to representing large, structured MDPs compactly.
In this
framework, a state is implicitly described by an assignment
to some set of state variables. A dynamic Bayesian network
(DBN) can then allow a compact representation of the tran-
sition model, by exploiting the fact that the transition of a
variable often depends only on a small number of other vari-

We

ables. Furthermore, the momentary rewards can often also be
decomposed as a sum of rewards related to individual vari-
ables or small clusters of variables.

states

represent CC as

factored MDP. Our

is deﬁned via a set of

set
of
random variables
{X1, ..., Xn, Y1, ..., Yn, K} where Xi denotes the num-
ber of computation units we have cached for task Ii, Yi
binary variables denoting whether there is an active instance
of Ii passed to the solver, and K is
i Xi, the number of
cache units used.

(cid:2)

a

Actions and rewards are the same as in the MDP version.
The transitions for the deterministic case are straightforward:
Addi causes K and Xi to increase (if possible), Freei causes
them to decrease. Solvei resets Yi. Any action causes Yj to
switch from 0 to 1 with probability p(Ij|E), and never the
other way around.
In the case of non-deterministic transi-
tions, the formulation is slightly different:Xi denotes the cur-
rent point of Ii’s utility function, and the possible outcomes
are modeled in the transition model of Addi.

In general, the factored MDP framework is very ﬂexible.
Many other plausible scenarios can be modeled – e.g., the
case of immediate response (with or without cost of delay),
or the case of non-reusable computation (where Solvei resets
Yi and Xi). We used the techniques of [Guestrin et al., 2003]
to efﬁciently solve MDPs for a large set of artiﬁcial tasks.

5.4 Learning about Performance

So far, we have assumed that agents have some knowledge
about the performance of algorithms. We now focus on meth-
ods that can support continual computation in agents by en-
dowing agents with the ability to learn about performance
over time. Several classes of learning procedures can be used
to update distributions over future streams of instances and
the performance of algorithms on each instance. On the lat-
ter, predictive models of performance can be conditioned on
attributes of the instances as well as observations provided by
the solution procedure as prior and/or the current instance is
solved [Horvitz et al., 2001]. We focus here on CC meth-
ods that manage the exploration-exploitation challenge. We
would like our agents to balance exploration in pursuit of new
data on performance with exploitation that applies the learned
policy to reﬁne results.

Assume we have a set of tasks I which might be passed
to the solver in the next instance. In addition, we have A, a
set of solver procedures we can apply. For example, in the
case of prefetching webpages, a procedure may be the name
of the server to use. Each task Ii is associated with a subset
of A that can be applied to it. Each algorithm has its utility
function fA.

Theorem 5.1 (Optimal Policy for Multiple Algorithms
with Known Utility Functions). If the reward proﬁles of
applying every algorithm A ∈ A are known in advance, the
greedy policy is optimal.

What happens if the algorithms in A are probabilistic pro-
cedures with a linear utility function of unknown slope? As
before, we assume full observability: every time we apply
an algorithm to an instance, we know the outcome. Choos-
ing an algorithm provides some information about its utility
function, but it is partial.

290

allocate their time to current and future problems. The meth-
ods extend deliberation to the handling of multiple and in-
terrelated tasks. We provide complexity results on continual
computation for general utility models and then offer polyno-
mial approximation procedures. Finally, we described meth-
ods for handling special case situations, including models of
the uncertainty in the performance of algorithms and hard
time constraints on computing problem solutions. We believe
that continual computational procedures can endow agents
immersed in the open world with principles of intelligence for
allocating their computational resources. We hope these ex-
tensions and analyses will be harnessed to increase the value
of computational efforts in a variety of real-world environ-
ments.

References
[Adar et al., 2009] E. Adar, J. Teevan, and S. T. Dumais. Reso-
nance on the web: web dynamics and revisitation patterns.
In
Dan R. Olsen Jr., Richard B. Arthur, Ken Hinckley, Mered-
ith Ringel Morris, Scott E. Hudson, and Saul Greenberg, editors,
CHI, pages 1381–1390. ACM, 2009.

[Auer et al., 1995] P. Auer, N. Cesa-bianchi, Y. Freund, and R. E.
Schapire. Gambling in a rigged casino: The adversarial multi-
armed bandit problem. In FoCS 95, 1995.

[Botev and Kroese, 2008] Z. I. Botev and D. P. Kroese. An efﬁ-
cient algorithm for rare-event probability estimation, combinato-
rial optimization, and counting. Methodology and Computing in
Applied Probability, 2008.

[Boutilier et al., 2000] C. Boutilier, R. Dearden, and M. Gold-
szmidt. Stochastic dynamic programming with factored repre-
sentations. Artiﬁcial Intelligence, 2000.

[Guestrin et al., 2003] Carlos Guestrin, Ronald Parr, and Shobha
Venkataraman. Efﬁcient solution algorithms for factored MDPs.
JAIR, 19, 2003.

[Horvitz et al., 1989] E.J. Horvitz, G.F. Cooper, and D.E. Hecker-
man. Reﬂection and action under scarce resources: Theoretical
principles and empirical study. In IJCAI 89, 1989.

[Horvitz et al., 2001] E. Horvitz, Y. Ruan, C. Gomes, H. Kautz,
B. Selman, and M. Chickering. A Bayesian approach to tackling
hard computational problems. In UAI, pages 235–244, 2001.

[Horvitz, 1999] E.J. Horvitz. Thinking ahead: Continual compu-
tation policies for allocating ofﬂine and real-time resources. In
IJCAI 99, 1999.

[Horvitz, 2001] E. Horvitz. Principles and applications of continual

computation. Artiﬁcial Intelligence, 126:159–196, 2001.

[Lawler, 1977] E. L. Lawler. Fast approximation algorithms for

knapsack problems. FoCS 77, 0, 1977.

[Leskovec et al., 2007] J. Leskovec, A. Krause, C. Guestrin,
C. Faloutsos, J. Vanbriesen, and N. Glance. Cost-effective out-
break detection in networks. In KDD ’07, 2007.

[Sahni, 1975] S. Sahni. Approximate algorithms for the 0/1 knap-

sack problme. J. ACM, 1975.

[Shahaf et al., 2009] D. Shahaf, Y. Azar, E. Lubetzky,

and
E. Horvitz. Utility-directed policies for continual caching and
crawling. Technical report, 2009.

[Sviridenko, 2004] M. Sviridenko. A note on maximizing a sub-
modular set function subject to knapsack constraint. Operations
Research Letters, 2004.

Figure 3: Regret over time with learning. Each line represents a
different choice of algorithms (modeled as a noisy process with a
random mean), and their association to tasks. The regret decreases
as the gambler learns more.

This problem can be solved using techniques from the
multi-armed bandit problem. This problem is based on an
analogy with a traditional slot machine (one-armed bandit)
but with more than one lever. When pulled, each lever pro-
vides a reward drawn from a distribution associated with that
speciﬁc lever. The gambler wants to maximize the collected
reward sum through iterative pulls. Again, the gambler faces
a tradeoff at each trial between exploitation of the lever that
has the highest expected payoff and exploration to get more
information about the expected payoffs of the other levers.

In the case at hand, the algorithms correspond to levers. At
every turn we choose Ii with highest probability that is not
solved yet, and then concentrate on the subset of algorithms
that can solve it, and apply bandits techniques until the in-
stance is solved. We can use knowledge gained from previous
iterations when solving the next instance.

(cid:7)

A typical goal in bandit settings is to minimize the regret.
The regret after T rounds is deﬁned as the difference be-
tween the average reward associated with an optimal strategy
and the average of the collected rewards. Applying a simple
weighted-majority algorithm, we can achieve regret of order
O(

n log n/T ).

In order to evaluate the bandit framework for CC we gen-
erated a synthetic dataset. We considered a set of 500 tasks
and 20 algorithms. The tasks are again modeled after the
web-page fetching scenario. Each task was associated with
a randomly selected subset of algorithms. Each algorithm
was modeled as a noisy process with a different mean, cho-
sen uniformly at random. We used the EXP3 algorithm [Auer
et al., 1995] in the evaluation. Figure 3 shows the regret over
time for three different sets of randomly selected algorithms.
As expected, the regret decreases as the gambler learns more.
Examining the results revealed that the reduction of regret
comes with the diminishing of the empirical variance of the
algorithms’ performance.

6 Summary
We reviewed key aspects of prior work on continual compu-
tation and then provided new analyses that extend the ear-
lier work. The analyses provide agents which face streams
of challenges with new capabilities for optimizing how they

291

