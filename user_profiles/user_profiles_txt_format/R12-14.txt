News Sensitive Stock Trend Prediction

Gabriel Pui Cheong Fung, Jeﬀrey Xu Yu, and Wai Lam

Department of Systems Engineering & Engineering Management

The Chinese University of Hong Kong

Shatin, N.T., Hong Kong

{pcfung,yu,wlam}@se.cuhk.edu.hk

Abstract. Stock market prediction with data mining techniques is one
of the most important issues to be investigated. In this paper, we present
a system that predicts the changes of stock trend by analyzing the in-
ﬂuence of non-quantiﬁable information (news articles). In particular, we
investigate the immediate impact of news articles on the time series based
on the Eﬃcient Markets Hypothesis. Several data mining and text min-
ing techniques are used in a novel way. A new statistical based piecewise
segmentation algorithm is proposed to identify trends on the time se-
ries. The segmented trends are clustered into two categories, Rise and
Drop, according to the slope of trends and the coeﬃcient of determi-
nation. We propose an algorithm, which is called guided clustering, to
ﬁlter news articles with the help of the clusters that we have obtained
from trends. We also propose a new diﬀerentiated weighting scheme that
assigns higher weights to the features if they occur in the Rise (Drop)
news-article cluster but do not occur in its opposite Drop (Rise).

1 Introduction

Autoregressive and moving average are some of the famous stock trend pre-
diction techniques which have dominated the time series prediction for several
decays [1,6,17]. With the help of data mining, several approaches using inductive
learning for prediction have also been developed, such as k-nearest neighbor and
neural network. However, their major weakness is that they rely heavily on struc-
tural data, in which they neglect the inﬂuence of non-quantiﬁable information.
Fawcett and Provost [5] formulated the stock forecasting problem as an activity-
monitoring problem by monitoring the relationship between news articles and
stock prices. However, a detailed procedure is absent in their paper. Lavrenko
et al. [13] proposed a language modeling approach for the stock trend forecast-
ing problem. In their approach, all news articles that are announced ﬁve hours
before the happening of any given trend are aligned to that trend, which is then
used as the basis for generating the prediction model. However, this approach
will not only lead to many conﬂicts 1, but also lead to contradiction against the
Eﬃcient Markets Hypothesis (EMH).

1 For example, the same article may align to more than one type of trend.

M.-S. Chen, P.S. Yu, and B. Liu (Eds.): PAKDD 2002, LNAI 2336, pp. 481–493, 2002.
c(cid:1) Springer-Verlag Berlin Heidelberg 2002

482

Gabriel Pui Cheong Fung, Jeﬀrey Xu Yu, and Wai Lam

According to EMH, it states that the current market is an eﬃcient informa-
tion processor which immediately reﬂects the assimilation of all of the informa-
tion available [2,3,17]. Thus, a long time lag is normally impossible.

In this paper, we present a system, which is based on EMH, to predict the
future behavior of the stock market using non-quantiﬁable information (news
articles). No ﬁxed periods are needed in our system. Predictions are made ac-
cording to the contents of the news articles. Thus, our system is event-driven.
The unique features of our system are summarized as follows: ﬁrst, a new t -test
based piecewise segmentation algorithm for trend discovery is proposed. Second,
interesting trends are clustered using an agglomerative hierarchical clustering
algorithm based on slopes and coeﬃcient of determination. Third, news arti-
cles are clustered and aligned to the interesting trends according to a newly
proposed algorithm based on incremental K-mean. It is used to ﬁlter the news
articles which are announced under the trend but do not support it. Fourth, a
new diﬀerentiated weighting scheme is proposed for assigning higher weights to
those features that are present in the set of articles which are aligned to only
one trend type.

The reminding of this paper is organized as follows. Section 2 presents the
detailed system architecture. Section 3 evaluates the performance of our system.
A conclusion of this paper is given in Section 4.

Fig. 1. The System Overview

News Sensitive Stock Trend Prediction

483

2 A News Sensitive Stock Prediction System

The overview of our system is shown in Figure 1(a) and Figure 1(b) using the
Uniﬁed Model Language. It consists of two phases: system training phase and
operational phase. The system training phase includes six main procedures: 1)
trend discovery, which identiﬁes trends within diﬀerent periods; 2) trend labeling,
which clusters similar trends together; 3) feature extraction, which extracts the
main features in the news articles; 4) articles-trend alignment, which associates
related news articles with trends; 5) feature weighting, which assigns diﬀerent
weights to diﬀerent features according to their importance; and 6) model genera-
tion, which generates the desire prediction model. The operational phase is used
to predict the future trends according to the contents of the newly broadcasted
news articles. The unique features and details of these procedures are discussed
in the following subsections in details.

2.1 Stock Trend Discovery

The general trend on the time series is always more interesting to study than
the exact ﬂuctuation, as stock traders are always adhere to the trends only.
Some well-known time series segmentation techniques include Fourier coeﬃcients
[4] and parametric spectral models [19]. However, if a time series consists of
transient behavior or unstationary with respect to time, such as stock prices, it
will possess very weak spectral signatures even locally [10]. Thus, instead of using
a spectral representation, we propose a new piecewise segmentation algorithm.
This algorithm adopts the similar ideas given in [16] for ﬁnding the minimum
k segments with the error norm below a threshold. Our piecewise segmentation
algorithm consists of two phases: splitting phase and merging phase.

The splitting phase is aimed at discovering all of the interesting trends on
the time series regardless of the duration of the trend, while the merging phase is
aimed at avoiding over-segmentation. The details of the algorithm are as follows.
Initially, the time series is regarded as a single segment, and a regression line
is formulated to represent it. For every point (xi, yi) on the time segment, its
error norm with respect to the regression line is:

Ei = |sin θ · xi + cos θ · yi − d|

(1)
where θ is the angle between the regression line and the x-axis; d is the perpen-
dicular distance from the origin to the regression line. The error norm calculated
is the Euclidean distance between (xi, yi) and the regression line. Note that Ei
has a normally independent distribution. In order to determine whether the time
series should be split, an one tailed t -test is formulated:

where µ is the mean error norm of interest. The required t -statistics is:

H0 : µ = 0
H1 : µ > 0

t =

E − µ

s√
n

(2)

(3)

484

Gabriel Pui Cheong Fung, Jeﬀrey Xu Yu, and Wai Lam

where, n is the number of points within the segment, s is the standard deviation
of the error norm, and E is the mean error norm. The t -statistics is compared
with t -distribution with n − 1 degree of freedom using α = 0.05. If the null
hypothesis is rejected, then the time series will be split at the point where the
error norm is maximum, i.e. max {Ei}, and the whole process will be executed
recursively on each segment.

Since the splitting phase is carried out on each segment independently, the
resulting segments may not be globally optimal. The merging phase is aimed
at avoiding over-segmentation by merging adjacent segments, provided that the
error norm after merging will not exceed the threshold which is determined by t -
test. The whole process of the merging phase is exactly the same as the splitting
phase, except that the adjacent segments are merged. An example is given in
Figure 2. If the hypothesis is rejected, it implies that the error norm will exceed
the threshold after merging, therefore merging would not be performed.

Fig. 2. t -test based piecewise segmentation

2.2 Stock Trend Labeling

In reality, a stock trader will never be interested in the trends that are relatively
steady, as they provide neither opportunities nor threats. In order to cluster
the trends into diﬀerent interesting categories, a two dimensional agglomerative
hierarchical clustering algorithm is formulated, which is based on: 1)the slope of
the segment (m) and 2) the coeﬃcient of determination (R2). Slope is chosen
because it is one of the most concerning issues for the stock traders. Coeﬃcient
of determination is chosen because it is a measurement for the goodness of ﬁt of
the regression model. R2 is deﬁned as:
(cid:1)
(cid:1)

n

R2 =

(4)

i=1( (cid:2)yi − y)2
i=1(yi − y)2

n

where yi is the original data; (cid:2)yi is the segmented data corresponding to yi; and
y is the mean of the original data in that segment. Thus, the molecule is the

News Sensitive Stock Trend Prediction

485

regression sum of squares and the denominator is the corrected sum of squares.
All slopes are normalized within 1 and -1 in order to have consistent clustering
results across diﬀerent stocks. Note that R2 is always between 0 and 1. Each
segment is thus represented by (m, R2), and is regarded as an individual clus-
ter object. The segments are merged according to the minimum group average
distance (GAD ):

GAD(Ci, Cj) =

(cid:1)

(cid:1)

i∈Ci

j∈Cj
|Ci||Cj |

dij (i, j)

where |Ci| and |Cj| are the magnitudes of the cluster Ci and Cj respectively;
d(i, j) is the Euclidean distance between the objects inside Ci and Cj:

(cid:3)

dij(i, j) =

(mi − mj)2 + (R2
i

− R2

j )2

The clustering procedure terminates when the number of clusters are equal to
three. This is because we are interested in clustering the trends into three classes
(Rise/Drop/Steady). Based on the average slope in the three clusters, those
segments in the cluster having the maximum average slope are labeled as Rise.
Similarly, those segments in the cluster having the minimum average slope are
labeled as Drop. Segments in the remained cluster are labeled as Steady. Note
that simple threshold could also be used for classifying the trends. However, we
claim that it is not scientiﬁc enough and the results may vary from stock to
stock.

(5)

(6)

2.3 Article and Trend Alignment

After trends are grouped into clusters, relevant news articles are then aligned
to them. By alignment, we mean that the contents of the news articles would
support and account for the happening of the trends. Obviously, not every piece
of news article announced under the time series would support the happening of
the trends. In this section, we propose a new algorithm, named guided clustering,
which can ﬁlter out news articles that do not support the trends. Our algorithm
is an extension of the incremental K-Means [11], together with the help of the
Rise/Drop trends, in which they are regarded as “guides”. In other words, we
use trends to govern the clustering of the news articles. Incremental K-Means is
chosen because recent research ﬁndings showed that K-Means outperforms the
hierarchical approach for textual documents clustering [20], and is more eﬃcient
and robust [12].

In the following discussions, let T -clusters be the clusters of trends and let
N -clusters be the clusters of news articles. T -cluster Rise (Drop) is regarded as
guide-Rise (guide-Drop). A guide-Rise (guide-Drop) includes all of the trends
in the T -cluster Rise (Drop). The guided clustering algorithm works as follows.
First, all of the news articles that are broadcasted within T -cluster Rise (Drop)
are grouped together. Each article is represented by a normalized vector-space
model:

di = (w1, w2, ..., wn)

(7)

(8)

(9)

(10)

(11)

486

Gabriel Pui Cheong Fung, Jeﬀrey Xu Yu, and Wai Lam

where the element wt corresponds to the score of the term t in the article di,
and it is calculated by the standard tf · idf scheme:

where tfd,t is the frequency of the term t in the article d; dft is the number
of article(s) containing the term t; N is the total number of articles contained
in the particular T -cluster (Rise/Drop). Incremental K-Means is then used for
splitting the weighted articles into two clusters [20]. The centroid of the cluster
Ci is deﬁned as:

where Si is the set of articles within the cluster Ci and |Si| is the number of
articles in this set. The similarity between the article di and the centroid Cj is
determined by the cosine measure which is recommended by most researchers in
document categorization:

wt = tfd,t × log

N
dft

Ci =

1
|Si|

(cid:4)

d

d∈Si

cos(di, Cj) =

di · Cj
|di||Cj |

where |di| and |Cj| is the magnitude of the article di and the cluster Cj respec-
tively.

The above procedure is taken out by both guide-Rise and guide-Drop. Thus,
on completion, four N -clusters exist: two N -clusters for guide-Rise, Cr1 and
Cr2 , and two N -clusters for guide-Drop, Cd1 and Cd2. In order to align the
correct N -cluster to the corresponding T -cluster, the following formula is used
for comparison:

cos(Ci, Cj) =

Ci · Cj
|Ci||Cj |

The Rise N -cluster, Cri (for i = 1, 2), which has the highest average similarity
with the two Drop N -clusters, Cdk (for k = 1, 2), is regarded as insuﬃcient
to diﬀerentiate the trend Rise. Thus, it will be removed. Suppose we removed
Cr1 , then all of the news articles within Cr2 will be aligned to T -cluster Rise.
Similarity, the Drop N -cluster, Cdk (for k = 1, 2), which has the highest average
similarity with the two Rise N -clusters, will also be removed. Filtering is thus
achieved.

2.4 Diﬀerentiated News Article Weighting

After the alignment process, a set of news articles are aligned to the trends.
In order to distinguish the importance of each feature in each N -cluster, the
weight of each feature has to be re-calculated. For most of the existing weighting
algorithms, the basic idea is that features which rarely occur over a collection
of articles are valuable. However, we are interested in ﬁnding out the features

News Sensitive Stock Trend Prediction

487

which frequently occur in one of the N -cluster but rarely occur in the other one.
Thus, none of the existing weighting algorithms fully ﬁt into our system.

A new weighting scheme is proposed here. Note that word independence is
assumed under this scheme, which is a common practice in text classiﬁcation
research. Furthermore, preserving word dependence may not necessarily help to
improve the accuracy of the model, or to some extents may even make it worse
[18].

In order to diﬀerentiate the features appearing in one of the cluster but not
the other, two coeﬃcients are introduced: inter-cluster discrimination coeﬃcient
(CDC ) and intra-cluster similarity coeﬃcient (CSC ):

CDC = (

CSC =

(cid:5)

)2

ni,t
Nt
ni,t
ni

(12)

(13)

where ni,t is the number of articles in the N -cluster i containing the term t; Nt
is the total number of articles containing the term t; ni is the total number of
diﬀerent terms appearing within the N -cluster i.

The intuition of CDC and CSC is, in fact, according to the special distri-
bution of words across news articles [7]. As noted in [7], the occurrence of any
keyword across news articles is extremely rare. Thus, instead of assuming a lin-
ear relationship, a quadratic relationship should be expected. Note that both
CDC and CSC are always between 0 and 1. For CDC, the higher the value of it,
the more powerful for it to discriminate the feature, t, across clusters. For CSC,
the higher the value of it, the more of the articles in the cluster contains the
speciﬁc feature. The weight of each feature in each document is ﬁnally calculated
as follows:

w(t, d) = tft,d × CDC × CSC

(14)
where tft,d is the frequency of the term t in the article d. Term frequency is used
along with the weighting algorithm in order to improve the recall of the model
[21]. Finally, each news article is represented by a vector-space model in which
it is normalized to unit length, so as to account for documents with diﬀerent
lengths.

2.5 Learning and Prediction

The associations between diﬀerent features and diﬀerent trend types are gen-
erated based on Support Vector Machine (SVM)[8]. SVM is a new learning
algorithm proposed by Vapnik to solve the two-class pattern recognition prob-
lem using the structural risk minimization principle [22]. It obtains very accurate
result in text classiﬁcation, and outperforms many other techniques such as neu-
ral network and Naive Bayes [9,23]. Since SVM is a binary classiﬁer, therefore
we need to have a pair of classiﬁers. One classiﬁer is responsible for classifying
whether a news article will trigger the rise event, the other classiﬁer is responsible
for the event of drop.

488

Gabriel Pui Cheong Fung, Jeﬀrey Xu Yu, and Wai Lam

For the prediction, we simply pass the newly collected news article to the pair
of classiﬁers and decide to which class the article should belong. For example,
an article is believed to signal a rise (drop) event if the output value of the rise
(drop) classiﬁer is positive. If the output values of both classiﬁers are negative,
we will classify that article as no recommendation, as it belongs to neither trend
types. If the output values of the two classiﬁers are positive, then it certainly
leds to ambiguous, and therefore it is classiﬁed as no recommendation as well.

3 Evaluation

We have developed a prototype system to evaluate our system. Stock data and
news articles are archived through Reuters Market 3000 Extra2, and are stored
into IBM DB2. Features of the articles are extracted using IBM Intelligent
MinerT M for Text. The training of the classiﬁers, as well as the prediction task,
is carried out using the package of SVMlight. The whole system is implemented
using Java under Unix platform.

The data and news articles that we used for testing are 614 stocks in Hong
Kong exchange market during seven consecutive months. The total number of
news articles archived is about 350,000. The number of stock data varies from
stock to stock, and is around 2,000 ticks for each stock. The total number of
stock data archived is about 1,228,000. The data/news from the ﬁrst six months
are used for training, while those from the last month are used for testing.

3.1 Trends Discovery and Labeling

A typical result after the time series segmentation is shown in Figure 3. It is easy
to see that the shape of the time series after segmentation is preserved, while
the number of data points would be reduced up to one-tenth of the original one.

Fig. 3. A time series before and after segmentation. On the left: original time
series. On the right: t -test based piecewise segmented time series

One of the biggest concerns about trend discovery process is its quality and
reliability. It is a great danger of using a model that is a poor approximation

2 Reuters has maintained a list of stock symbols that are relevant to each news article.

News Sensitive Stock Trend Prediction

489

of the true functional relationship. In our trend discovery algorithm, each trend
is the regression line of the original data points in that segment. A common
practice in statistics for measuring the adequacy of a regression line is the use of
coeﬃcient of determination (R2). Having a higher value of R2 implies that the
regression line formulated is more ﬁt to the original data points. Figure 4 shows
a typical result of the plots of m versus R2.

Each symbol in the graph corresponds to a distinct segment. Note the special
“T” shape of the graph, in which those segments with steep slopes will have high
values of R2. The “T” shape shows that the quality of our stock trend labeling
is high. Misclassiﬁcation is unlikely to happen, since we are interested in those
steep slopes only. It is very important because it could maintain suﬃcient high
quality training examples.

Fig. 4. A plot of slope (m) Vs coeﬃcient of determination (R2) for the same
time series in Figure 3

3.2 Trends Alignment and Weighting Scheme

To evaluate the robustness of the guided clustering algorithm, receiver operating
characteristic (ROC) curve is chosen. ROC is a common evaluation method for
the classiﬁcation problem. In order to have an unbiased estimator of the mean
performance, a ten-fold cross-validation experiment is conducted. For all of the
news aligned to a stock, 90% of them are randomly grouped into a training set
and the remaining 10% of them are grouped into a testing set. The prediction
model is built according to the training set, which is then used to evaluate the
testing set. This procedure is repeated ten times with diﬀerent training sets and
testing sets. The average result obtained is plotted against the ROC curve.

Another alignment approach without using guided clustering is also con-
ducted to examine the necessity of our guided clustering. Here, all of the news

490

Gabriel Pui Cheong Fung, Jeﬀrey Xu Yu, and Wai Lam

articles announced within a given trend are aligned to that trend. A typical re-
sult is shown in Figure 5, in which the x-axis denotes the false positive rate and
y-axis denotes the true positive rate. The true positive rate is the percentage
of news articles that were classiﬁed correctly, while the false positive rate is the
percentage of news articles that were misclassiﬁed. The performance of guided
clustering outperforms either random or without a guide.

1.0

0.8

0.6

0.4

0.2

e

t

a
r
 

e
v
i
t
i
s
o
p

 

e
u
r
t

0.0

0.0

0.2

0.4
false positive rate

0.6

random

without guided 
clustering
our system

0.8

1.0

Fig. 5. A ROC curve for the evaluation of: 1) guided clustering; 2) without
guided clustering; 3) random alignment

3.3 Overall System Performance

The best way to evaluate the reliability of a prediction model is certainly to
conduct a market simulation which could mimic the behaviors of a stock trader
using real life data. A Buy-and-Hold test3 is conducted like [6]. The rate of return
r of the test is:

r =

t(cid:4)

yi+1 − yi

yi

i=0

(15)

In this test, proﬁt (loss) is made when shares are sold (short are bought). The
assumption of zero transaction cost is taken out. Two strategies that are used
to govern the decisions of buy, sell and hold are:

– if the prediction of the upcoming trend is positive, then shares of that stock
are purchased. If a proﬁt of 1% or more could be made within an hour, then
all shares are sold immediately; otherwise they are sold at the end of that
hour.

– if the prediction of the upcoming trend is negative, then shares of that stock
are sold for short. If the trading price is dropped 1% or more than the shorted

3 Some relaxation about the stock exchange restriction is assumed under the buy-and-

hold test, such as the acceptance of shorting the stock.

News Sensitive Stock Trend Prediction

491

price within an hour, then shares of that stock are purchased immediately;
otherwise they are purchased at the end of that hour.

Since we are concerned with the rate of return, how much shares are bought in
each transaction can be ignored.

We compare our system with the ﬁxed period alignment approach[13]. As
noted before, the underlying assumption between the ﬁxed period approach and
ours is diﬀerent. Our model is based on the EMH which states that the market
reacts immediately according to the new information arises. However, the ﬁxed
period approach assumes that every related piece of information has an impact
on the market after a ﬁxed time interval.

Since all of the predictions made are based on the news articles, the frequency
of the news articles broadcast must be a critical factor of aﬀecting the prediction
performance. Figure 6 shows a comparison between our system and the ﬁxed
period approach with the frequency of news announced versus resulting proﬁt.
All of the stocks are ranked based on the total number of news articles that are
associated with in the training period. They are further divided into diﬀerent
categories in which every category contains the same number of stocks.

In the ﬁgure, the smaller the number of the x-axis is, the fewer number of
news articles are aligned to that stock. In general, our approach is highly superior
to the ﬁxed period approach. The reason is that we use all news articles, but ﬁxed
period approach only uses the news articles within a ﬁxed interval preceding the
happening of a trend. However, when a stock received too many news articles,
ﬁxed period approach outperforms us due to the fact that the probability of
having noise would be higher.

Cumulative

Profit

current system
fixed period approach

Category

Frequency Range

1        2         3        4         5        6         7        8         9       10       11      12       13      14      

Frequency
Category

1
2
3
4
5
6
7
8
9
10
11
12
13
14

26-110
116-193
195-219
221-274
282-337
339-425
427-494
547-618
628-696
749-891
903-2030
2127-2733
2773-3565
3688-8091

Fig. 6. The relationship between the frequency of news announced and the re-
sulting proﬁt

492

Gabriel Pui Cheong Fung, Jeﬀrey Xu Yu, and Wai Lam

4 Conclusions

In this paper, we demonstrated a sophisticated system which monitors the stock
market and predicts its future behaviors. The major diﬀerence between our sys-
tem and the existing forecasting techniques is that we take immeasurable factors
or non-quantiﬁable information into account during prediction. News articles are
served as the major source of the non-quantiﬁable information that we rely on.
Our approach does not need any assumptions that require a ﬁxed period for
aligning news articles to a trend. Several data mining and text mining tech-
niques are incorporated in the system architecture. A t -test based piecewise
segmentation algorithm and an agglomerative hierarchical clustering algorithm
are used for discovering and labeling trends respectively. Two new algorithms are
introduced: guided clustering and new weighting scheme. The guided clustering
is developed based on incremental K-means. It is used for article ﬁltering and
article-trend alignment. The new weighting scheme is formulated to identify the
important features within the collection of articles. Finally, a market simulation
using a very simple trading strategy based on the Buy-and-Hold test is carried
out, and the results indicated that our approach is proﬁtable.

References

1. S. B. Achelis. Technical Analysis from A to Z. Irwin Professional Publishing,

2. P. A. Adler and P. Adler. The Social Dynamics of Financial Markets. Jai Press

Chicago, 2nd edition, 1995.

Inc., 1984.

3. W. J. Eiteman, C. A. Dice and D. K. Eiteman. The Stock Market. McDGraw-Hill

Book Company, 4th edition, 1966.

4. C. Faloutsos, M. Rangantathan and Y. Manalopoulos. Fast Subsequence Match-
ing in Time-Series Database. In Proceedings of the ACM SIGMOD International
Conference on Management of Data, 419-429, Minneapolis, May 1994.

5. T. Fawcett and F. Provost. Activity Monitoring: Noticing Interesting Changes in
Behavior. In Proceedings of the 5th International Conference on KDD, San Diego,
California, 1999.

6. T. Hellstrom and K. Holmstrom. Predicting the Stock Market. Technical Report

Series IMa-TOM-1997-07, 1998.

7. J. D. Holt and S. M. Chung. Eﬃcient Mining of Association Rules in Text
Databases. In Proceedings of the 8th International Conference on Information
Knowledge Management, 234-242, ACM Press, 1999.

8. T. Joachims. Making large-Scale SVM Learning Practical. Advances in Kernel
Methods - Support Vector Learning. B. Sholkopf and C. Burges and A. Smola,
MIT-Press, 1999.

9. T. Joachims. Text Categorization with Support Vector Machines: Learning with
many relevant features. In Proceedings of the European Conference on Machine
Learning, Springer, 1998.

10. E. Keogh and P. Smyth. A Probabilistic Approach to Fast Pattern Matching
in Time Series Databases. In Proceedings of the 3rd International Conference of
KDD, 24-40, AAAl Press, 1997.

News Sensitive Stock Trend Prediction

493

11. L. Kaufman and P. J. Rousseeuw. Finding Groups in Data - An Introduction to

Cluster Analysis. John Wiley & Sons, Inc., 1990.

12. B. Larsen and C. Aone. Fast and Eﬀective Text Mining Using Linear-time Doc-
ument Clustering. In Proceedings of the 5th International Conference on KDD,
San Diego, California, 1999.

13. V. Lavrenko, M. Schmill, D. Lawire, P. Ogilvie, D. Jensen and J. Allan. Min-
ing of Concurrent Text and Time Series, In Proceedings of the 6th International
Conference on KDD, Boston, MA, 2000.

14. W. Mendenhall and T. Sincich. A Second Course in Business Statistics: Regres-

sion Analysis. Dellen Publishing Company, 1989.

15. D. C. Montgomery and G. C. Runger. Applied Statistics and Probability for En-

gineers. John Wiley & Sons, Inc., 2nd edition, 1999.

16. T. Pavlidis and S. L. Horowitz. Segmentation of Plan Curves. IEEE Transactions

on Computers, Vol. c-23, No. 8, August 1974.

17. C. Pratten. The Stock Market. Cambridge University Press, 1993.
18. C. J. van Rijsbergen. A Theoretical Basis for the use of Co-occurance Data in

Information Retrieval. Journal of Documentation, 33:106-119, 1977.

19. P. Smyth. Hidden Markov Models for Fault Detection in Dynamic Systems. Pat-

20. M. Steinbach, G. Karypis and V. Kumar. A Comparison of Document Clustering

tern Recognition, 27(1), 149-164, 1994.

Techniques. Technical Report, 2000.

21. T. Takenobu and I. Makoto. Text Categorization Based on Weighted Inverse

Document Frequency. Technical Report, ISSN 0918-2802, 1994.

22. V. N. Vapnik, The Nature of Statistical Learning Theory. Springer, 1995.
23. Y. Yang and X. Liu. A Re-examination of Text Categorization Methods. In Pro-
ceedings of the 22nd Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, 42-49, 1999.

