4
0
0
2

 

n
u
J
 

2
1

 
 
]
I

N
.
s
c
[
 
 

1
v
9
1
0
6
0
4
0
/
s
c
:
v
i
X
r
a

Providing Service Guarantees in High-Speed

Switching Systems with Feedback Output Queuing∗

Victor Firoiu Xiaohui Zhang Emre G¨und¨uzhan
{vﬁroiu,xiaohui,egunduzh}@nortelnetworks.com christin@sims.berkeley.edu

Nicolas Christin†

Advanced Technology

Nortel Networks

600 Technology Park

S.I.M.S.

UC Berkeley
102 South Hall

Billerica, MA 01821 USA

Berkeley, CA 94720 USA

February 7, 2008

Abstract

We consider the problem of providing service guarantees in a high-speed packet switch.
As basic requirements, the switch should be scalable to high speeds per port, a large number of
ports and a large number of trafﬁc ﬂows with independent guarantees. Existing scalable solu-
tions are based on Virtual Output Queuing, which is computationally complex when required
to provide service guarantees for a large number of ﬂows.

We present a novel architecture for packet switching that provides support for such service
guarantees. A cost-effective fabric with small external speedup is combined with a feedback
mechanism that enables the fabric to be virtually lossless, thus avoiding packet drops indis-
criminate of ﬂows. Through analysis and simulation, we show that this architecture provides
accurate support for service guarantees, has low computational complexity and is scalable to
very high port speeds.

Keywords: Computer networks, Packet switching, Quality of service, Feedback control, Congestion control.

∗This paper is a revised and extended version of [8].
†Work done while visiting Nortel Networks.

1 Introduction

High speed communication between businesses has been a large share of telecommunications mar-

ket in recent years. This communication needs to be of high quality, secure and reliable. Tradi-

tionally, these services were provided using ATM and Frame Relay technologies, but at a premium

cost. Recent advances in trafﬁc engineering and the advent of Voice over IP technologies provide

an opportunity to carry all enterprise trafﬁc (voice, streaming and non-real-time data) at a lower

cost. Virtual Private Networks (VPNs) [11] and Virtual Private LAN Services (VPLS) [2] are two

examples of such network services. A main requirement for such services is to provide quality

of service (QoS) guarantees. Interactive media such as VoIP needs low delay and low loss, other

trafﬁc needs minimum throughput guarantees.

In this paper we consider the problem of providing such guarantees in a high-speed, cost-

effective switch at the interface (edge) between enterprise and service provider networks. At a

minimum, the switch is required to provide three types of service: Premium, Assured and Best

Effort [6],[14]. Premium service provides low loss and small delay for a ﬂow sending within a pre-

determined rate limit (anything above the limit is discarded). Assured service guarantees delivery

for trafﬁc within a limit, but allows and forwards extra trafﬁc within a higher limit if transmit

opportunities are available.

A provider edge switch is required to differentiate between trafﬁc from different customers

(here called ﬂows) and provide separate guarantees to each ﬂow. A requirement is to support a

large number (in the order of hundreds or even thousands) of such ﬂow guarantees per port, where

each port must support speeds in the order of several Gbps. Trafﬁc from one customer (ﬂow)

can enter through one or multiple ingress ports and exit through one or multiple ports. On the

other hand, to come up with practical solutions, we assume that the provided service guarantees

only need to be enforced over timescales in the order of a few milliseconds, which is enough for

most applications, thereby alleviating the traditional requirement that service guarantees have to be

enforced over timescales as small as a single packet transmission time. We consider the problem

of providing 1-to-1 and N-to-1 services (or “Pipe” and “Funnel scope” as deﬁned in [12]), as 1-

to-N and N-to-N can be provided as combinations of services of the ﬁrst two kinds. In the case

2

of Assured N-to-1 service, it is also desirable to provide a fair distribution of service among the N

components of the ﬂow.

Current state-of-the-art switch architectures are based on Virtual Output Queuing (VOQ), which

requires a fabric speedup s ≥ 2 and a matching algorithm to ﬁnd which packets are sent into the

fabric at each fabric cycle. However, realizing a speed-up of s ≥ 2 may be impractical at very

high line speeds (> 10 Gbps) given the limitations on memory access speeds. Furthermore, even

though some of the VOQ architectures can support service guarantees, a major problem is that

the matching algorithms have high complexity, are run at each fabric cycle, and all virtual output

queues at all input lines in the system need to participate in a centralized algorithm [20].

To provide a low-complexity switch architecture that fulﬁlls the above requirements, we ob-

serve that the main cause for high complexity in current architecture resides in the necessity of ad-

dressing congestion at an output line. Short term congestion can be absorbed by buffers, whereas

long term congestion results in packet loss. We also observe that many measurement studies (for

example [17]) have shown that trafﬁc in the Internet is dominated by the TCP protocol, which

accounts for about 90% of all trafﬁc. A salient feature of TCP is that packet transmission is con-

trolled by a congestion avoidance algorithm [15], [24]. As an effect, the average sending rate of a

TCP ﬂow is a decreasing function of drop probability and of round trip time (see [22] for a quan-

titative evaluation of this function). In practice, TCP ﬂows have a stable (long-term) operation at

when the drop probability is between 0 and 0.1, corresponding to loss rates less than 10%, and very

rarely operate above 0.2 [22]. Heavy long-term congestion that results in a drop probability above

0.2 can be produced by non-TCP (and more generally, non-congestion-controlled) trafﬁc such as

multimedia trafﬁc over UDP.

Our proposed architecture, named “Feedback Output Queuing” (FOQ), exploits these observa-

tions by efﬁciently supporting fast fabrics with relatively slow output memory interfaces and hence

a small effective speedup. For example, a speedup of 1.25 at the fabric-to-line interface is sufﬁcient

to maintain an output drop probability up to 0.2 for trafﬁc ﬂows fully utilizing this interface. For

higher levels of long-term congestion (e.g., drop probability above 0.2), the FOQ architecture uses

a feedback mechanism to reducing the trafﬁc volume before it enters the switch fabric. This FOQ

3

mechanism provides support for the Assured service, 1-to-1 and N-to-1 scope.

As far as Premium trafﬁc is concerned, given that rate guarantees are ensured to be within

switch capacity by some admission control procedure, policing Premium trafﬁc at its guaranteed

rate at the ingress guarantees that Premium trafﬁc cannot create congestion in the absence of other

types of trafﬁc. Thus, Premium service can be provided through a simple priority scheduling in

OUT ports and fabric, bypassing the FOQ mechanism.

In the following we show through analysis and simulation studies that the proposed FOQ ar-

chitecture can alleviate congestion at the output lines of an output queued switch with slow output

memory interface, and can thus provide deterministic QoS guarantees. FOQ requires only a mod-

est speedup (e.g., 1.3) at the output interface of the switch. The congestion control algorithm in the

FOQ architecture is fully parallelized at the input and output lines, requiring O(1) complexity at

each input and output line. This low complexity enables implementation of the FOQ architecture

at very high line rates (> 10 Gbps).

The rest of the paper is organized as follows. In the next section we discuss the related work in

more details. Then, we give a detailed description of the FOQ architecture in Section 3 In Section 4

we develop an analytical model for FOQ, based on a PI controller, and analyze its performance

under step-shaped trafﬁc bursts, before introducing a quantized version of a PI controller. We

present our simulation results in Section 5, and conclude the paper with a comparison between

FOQ and VOQ in Section 6.

2 Related Work

Several switch architectures with QoS capabilities have been proposed in the literature, with par-

ticular advantages and shortcomings.

An early architecture is Output Queuing (OQ). An OQ switch having N inputs and N outputs

with each line of speed c bits/second requires a switching fabric of speed Nc, i.e., a speedup s = N.

In this case, no congestion occurs at the inputs or at the fabric, only at the output lines. To manage

congestion and provide QoS support, a set of queues and a scheduling mechanism is implemented

at each output. The main advantage of this architecture is that it can provide QoS support with

4

simple mechanisms of queuing and scheduling, but the main problem is that the fabric speedup of

N can be impractical. In fact current technology enables fast interconnection networks operating

at current high speed line rates and with typical number of lines (for example c = 10 Gbps and

N = 16), but writing the packets coming out of the interconnection network into output buffers at

high speeds remains a problem. In other words, although the fabric may have an internal speedup

of N, the effective speedup seen at an output buffer is limited by the memory write speed which is

usually much less.

An alternative to OQ is Virtual Output Queuing (VOQ) [1], [18], which requires a smaller

fabric speedup, such as s in the range between 2 and 4. Unlike OQ, VOQ requires a matching

algorithm to ﬁnd which packets will be sent into the fabric at each fabric cycle. There are quite

a few such algorithms proposed in the literature, which are based on Parallel Iterative Matching,

Time Slot Assignement, Maximal Matching, or Stable Matching (see [20] and references therein).

Some of these algorithms can also support service guarantees. The advantage of VOQ is its ability

to switch high speed lines with low fabric speedup. However its main problem is that the matching

algorithms are complex (O(M 2N 2) where M is the number of independent service guarantees per

port, N is the number of ports), have to be run at each fabric cycle, and all VOQs at all input

lines in the system need to participate in a centralized algorithm. We note that Output Queued

switches can also be perfectly emulated by Combined Input-Output Queued (CIOQ) switches with

a speed-up s ≥ 2 [5]. Unfortunately, the arbitration algorithm has a computational complexity of

O(N 2), which can be reduced to O(N), but in that case, the space complexity becomes linear in

the number of cells in the switch. Therefore, emulating an OQ switch by a CIOQ switch or a VOQ

switch appears to have limited scalability.

In recent years, these potential scalability concerns have been addressed by implementing a

very small number of independent service guarantees. Under the Differentiated Services frame-

work [3], ﬂows are aggregated in M = 6 classes, and service guarantees are offered for classes.

The downside is that the realized QoS per ﬂow has a lower level of assurance (higher probability

of violating the desired service level) than the QoS per aggregate [13], [25]. Moreover, recently

proposed VPN and VLAN services [23], [4] require per-VPN or VLAN QoS guarantees. All the

5

above are arguments in favor of implemeting a number of independent service guarantees per port

much larger than six.

More recent proposals [16] decrease the time interval between two runs of the matching al-

gorithm, but with a tradeoff in increased burstiness and additional scheduling algorithms for miti-

gating unbounded delays. Moreover, the service presented in [16] is of type Premium 1-to-1, but

cannot provide Assured N-to-1 service.

Last, similar to the FOQ architecture proposed in this paper, the IBM Prizma switch archi-

tecture [19] uses a shared memory, and no centralized arbitration algorithm. However, Prizma

relies on on-off ﬂow control while the feedback scheme proposed in the present paper dynamically

controls the amount of trafﬁc admitted into the fabric, and FOQ feedback is based on the state of

the output queues, while Prizma relies on the state of internal switch queues. Both the origin of

the information and the dynamic control of the drop level lead us to believe that FOQ can use the

capacity available in the switch more efﬁciently.

3 Feedback Output Queuing Architecture

We consider a switch as in Figure 1 with a fabric having internal speedup of N and an internal

buffer capability.1 We also assume that the fabric has one or a very small number of queues per

port. In the following we present an architecture for providing per-ﬂow service guarantees where

the number of ﬂows per port M is large, that is, M ≫ 1.

Packets enter through a set of N input ports of speed c. As a packet is received at port i, a

destination port j is determined by a routing module, its QoS ﬂow k is determined by a classiﬁer

and an IN dropper determines if the packet is discarded. If not discarded, the packet is transmitted

to the fabric through a line of speed sc. We assume a fabric with internal speed of Nsc, i.e., at each

fabric cycle one packet from each IN line can be moved to an OUT line while sustaining speeds of

sc from all IN lines. Multiple (up to N) packets can be received at an OUT line in one cycle, and

in that case the packets are placed in a fabric queue F Qj corresponding to the destination line j.

1This fabric has a cost-effective implementation using shared memory technology. The case of zero/small memory

fabric with no/small internal speedup is a separate problem, and we report our study elsewhere.

6

Figure 1: Detailed FOQ switch architecture

Packets are forwarded by the OUT line j at speed sc, separated into OUT queues {OQj,k}k

based on their QoS ﬂow, and scheduled for transmission to OUT port j of speed c. The OUT

scheduling implements various service guarantees such as priority, minimum rate guarantee, max-

imum rate limit, maximum delay guarantee. This OUT scheduling results in a certain service rate

(in general variable in time) for each OUT queue.

If trafﬁc to OQj,k has a rate higher than the current service rate of ﬂow k, packets accumulate

in this queue and some of them may be dropped by a queue management mechanism such as drop-

tail or RED (see [9] for details). If the trafﬁc to all queues at OUT line j amounts to an aggregate

rate above sc, then packets accummulate at the fabric queue F Qj. If this situation persists, F Qj

ﬁlls and packets get dropped in the fabric. In this case, QoS guarantees for some ﬂow k may be

violated since fabric drops do not discriminate between different ﬂows.

We deﬁne the relative congestion at a queue

where rI and rO are trafﬁc rates input to and output from the queue respectively. It is easy to see

that, as long as the trafﬁc coming out of OUT line j is such that the relative congestion Cj,k at each

(1)

C = 1 −

rO
rI

7

queue {OQj,k}k is below a threshold dmax < 1 − 1/s, and the OUT port j is utilized at its full

capacity c, then the trafﬁc throughput at the interface of fabric to OUT line j is below sc, and thus

there is no congestion at that interface and no fabric drop.

In the FOQ architecture, a feedback mechanism is introduced to control the relative congestion

at each OUT queue below a threshold. When the relative congestion at an OUT queue increases,

the feedback mechanism instructs the input modules to drop a part of the trafﬁc destined to this

queue. By keeping the trafﬁc below a congestion threshold, the fabric drop is avoided. Thus, packet

are dropped only from those ﬂows that create congestion, and the QoS guarantees are provided to

all ﬂows as conﬁgured.

It is worth noting that the ﬂows having packets dropped at ingress by FOQ would have packets

dropped in the same amount at egress in the case of an ideal Output Queuing with speedup of

N. Thus, FOQ reduces the demand of fabric throughput by eliminating the need for forwarding

packets that are later discarded.

Realizations of FOQ We next consider options for a practical realization of the FOQ architec-

ture. More precisely, we consider implementations of FOQ as a discrete feedback control system.

A certain measure of congestion is sampled at intervals of duration T at each OUT queue. A

control algorithm computes a drop indication based on the last sample and an internal state, and

transmits it to all IN modules. There, packets of the indicated class are randomly dropped with a

probability that is a function of the drop indication.

We have several ways to measure the congestion at a queue. A simple method is to compute

the average drop probability at the queue during the sampling interval:

DropP rob(T ) = DroppedP kts(T )/InP kts(T ) .

Another measure is the relative congestion during the interval T , similar to (1):

RelCong(T ) = 1 − OutP kts(T )/InP kts(T ) .

Observe that, unlike the drop probability, the relative congestion takes into account the variation of

the queue size during T . Since the FOQ objective is to keep the trafﬁc rate at the fabric interface

8

below a critical level, it is apparent that the relative congestion is more effective in controlling that

trafﬁc rate. This is conﬁrmed by the model in Section 4 and the simulation in Section 5.

We consider a discrete Proportional-Integrator (PI) [10] for the feedback control algorithm. In

Section 4 we derive its conﬁguration from stability conditons. The PI algorithm outputs a value of

drop probability between 0 and 1 transmitted to the IN droppers every interval.

An implementation issue is the data rate of feedback transmission. Considering K classes at

each of the N OUT ports and that the drop information is coded in F bits, the total feedback data

rate is KNF/T . For example, for K = 1000, N = 32, F = 8, T = 1 ms, the feedback data

rate is 256 Mb/s. It is possible to reduce this rate by reducing the precision of the feedback data,

and thus its encoding. In an extreme case, the feedback has three values: increase, decrease or

keep same drop level. All IN modules use this indication in conjunction with a pre-deﬁned table

of drop levels. We call this the “Gear-Box algorithm” (GB), model it in Section 4 and show its

performance in Section 5.

4 A Control Theoretical Model for the GB Algorithm

In this section we develop an analytical model for the FOQ architecture by a control theoretical

approach. In our analysis, we use a classical discrete PI controller to adjust the drop rate of each

ﬂow. We simplify our analysis by assuming only a single ﬂow at ﬁrst, and later discuss how and

under what conditions our results may apply to the general multi-ﬂow case. We also assume in

our analysis that there is no limitation to the capacity of the feedback channel in the system. We

then show that an efﬁcient algorithm for limited-capacity feedback channels can be obtained by

quantizing the control decisions of the PI controller, which we call the Gear Box algorithm.

The basic control structure at a particular OUT port j and for a particular ﬂow k is shown

in Figure 2. If there are a total of K ﬂows in each OUT port, then each OUT port has K such

controllers. All variables we use in this section are for the aggregate trafﬁc in ﬂow k originating

from all IN ports and destined to OUT port j, unless we note otherwise (i.e., we don’t use the

subscript (j, k) for notational convenience). λ is the total arrival rate for trafﬁc destined for the

OUT queue OQj,k. A total portion, ρ, of the arriving trafﬁc is dropped at the IN droppers, and the

9

Figure 2: FOQ architecture.

surviving portion goes into the fabric queue F Qj at a rate u = λ − ρ. This trafﬁc shares the fabric

queue with other trafﬁc destined to OUT line j, and then it is delivered to OUT dropper (j, k) at a

rate r. In the analysis we assume the fabric queue is sufﬁciently large, so that there are no drops

due to queue overﬂow.

The total drop rate, ρ, is adjusted by a controller (how ρ is distributed among the N IN droppers

is not relevant for this analysis; we explain how we implement the actual drop mechanism in the

next section). The purpose of the controller is to keep the fabric output rate for packets destined to

OQj,k at a desired level, ropt. The desired rate can be chosen according to the current rate out of

OQj,k

ropt = αsrO(j,k),

where α is a constant smaller than but close to 1. In this way the desired rate will be close to the

capacity, sc, of fabric output line when the OUT queue OQj,k is the only busy queue and utilizing

the entire speed of port j. Furthermore it will be reduced in proportion to the service rate of OQj,k

when multiple OUT queues are contending for the OUT port. The two nonlinearities in the ﬁgure

simply state that the drop rate can not be negative or greater than the arrival rate λ. In our analysis

we assume that the controller is operating in the linear region, and ignore the nonlinearities.

The delay T between the output of the controller and the arrival rate models a zero-order hold

at the controller output. The controller operates on time-average of the error signal taken over an

interval T , rather than the signal itself, and modiﬁes its output only at intervals of T . In the rest of

10

this section we denote the time-average of a signal x(t) over the period T by the discrete notation

x[n]. For example the time-average of the fabric output rate is given by

When the system is in steady state, the amount of trafﬁc, q, in the fabric queue destined to OQj,k

does not change signiﬁcantly during the interval T . Therefore, we can approximate the average

fabric output rate by

r[n] =

r(t)dt.

(n+1)T

1
T

nT

Z

r[n] ≈

u(t)dt

(n+1)T

1
T

nT

Z

= λ[n] − ρ[n − 1].

For a discrete PI controller the drop rate for the next interval is calculated using the error between

the average fabric output rate, r[n], and the desired fabric output rate, ropt[n],

n

ρ[n] = Ke[n] + KI

e[m]

m=0
X
= K(r[n] − ropt[n])

n

n

+KI

r[m] −

ropt[m]

.

 

m=0
X

m=0
X

!

We can now investigate the step response of the system, setting λ[n] = λ0 and ropt[n] = ropt

for n ≥ 0, for the case of a single ﬂow. The magnitude of the arrival rate can in general be larger

than the maximum fabric output rate, i.e., λ0 > sc. In this case the fabric output will be constant

at r[n] = sc for an initial period 0 ≤ n < N0. During this period the fabric queue will always be

non-empty and the controller can not sense the actual magnitude of the arrival rate. Therefore the

controller output will increase linearly,

ρ[n] = K(sc − ropt) + (n + 1)KI(sc − ropt).

The fabric queue size, measured at the end of each period, will increase until the drop rate reaches

λ0 − sc and then decrease back to zero
n

(2)

(3)

qn = T

(λ0 − sc − ρ[m − 1])

m=0
X

= T [(n + 1)(λ0 − sc) − nK(sc − ropt)

n(n + 1)

−

2

KI(sc − ropt)].

11

The duration of this initial period, N0, and the maximum queue size can easily be calculated from

this quadratic equation setting qN0−1 = 0. To ﬁnd the behavior of the system for n ≥ N0 we use a
new time axis, n′ = n − N0, with an initial condition for the accumulator memory

ρ[n′] = K(r[n′] − ropt[n′])

+KI

r[m] −

ropt[m]

+ SN0

!

n′

 

m=0
X

n′

m=0
X

(4)

where

SN0 = KIN0(sc − ropt).

Equations (2) and (4) describe a closed-loop control system. We show in the appendix that the

two poles of this system are at

z1 = −

z2 = −

K + KI − 1

K + KI − 1

2

2

+

−

1
2
1
2

p

p

(K + KI − 1)2 + 4K

(K + KI − 1)2 + 4K.

It follows that we have the stability condition given by the proposition below.

Proposition 1. The closed-loop system described by (2) and (4) is stable iff

0 < KI < 2(1 − K).

(5)

Proof. If K + KI > 1 then |z2| > |z1|, and both poles are inside the unit circle iff

K + KI − 1 +

(K + KI − 1)2 + 4K < 2,

which yields

which yields

On the other hand if K + KI < 1 then |z2| < |z1|, and both poles are inside the unit circle iff

−(K + KI − 1) +

(K + KI − 1)2 + 4K < 2,

Combining the two cases gives the condition for stability.

p

K +

< 1.

KI
2

p

KI > 0.

12

In the appendix we solve the system with the stability condition (5, and show that the controller

output is given by

ρ[n] =

[K + (n + 1)KI](sc − ropt),
+ A2zn−N0
D(1 − A1zn−N0

2

1

n < N0

), n ≥ N0

(6)






where

and

A1 =

A2 =

SN
D z1
0

z2
1 −
z1 − z2
SN
D z2
0

z2
2 −
z1 − z2

,

,

D = λ0 − ropt

is the difference between the arrival and the desired rates. We observe that after the initial linear in-

crease, the drop rate approaches exponentially to the difference between the arrival and the desired

rates. Furthermore, since the absolute value of the negative pole is relatively larger for KI > 1−K,

the system will show more oscillatory behavior in this case compared to the KI < 1 − K case.

Multiple ﬂows When there are multiple ﬂows, the analysis for the initial period (n < N0) needs

to be updated. Let v be the total rate of the trafﬁc that does not belong to ﬂow k but destined to

port j. If the step size for ﬂow k is such that λ + v > sc then for an initial period the average fabric

output rate for ﬂow k is approximately

r[n] = sc

u[n]

v[n] + u[n]

.

Since r is not constant anymore, the previous results for the initial period do not apply in general.

However, once the transient is over and u and v are adjusted so that u[n] + v[n] ≤ sc, the approxi-

mation (2) holds, and the results for the single-ﬂow case can be used replacing SN0 by a new initial

condition. We defer a detailed analysis of the initial transient period for the multi-ﬂow case to a

future study. However, in two cases, when u or v is negligible compared to the other, the results

for the single-ﬂow case can be used with some changes. If u ≫ v, then r[n] ≈ sc and we can

13

approximate the multiple-ﬂow case by the single-ﬂow case. On the other hand, if u ≪ v then we

can assume that v is constant since the effect of the new trafﬁc, u will be negligible. Therefore

with σ = sc/v during the initial period n < N0. In this case N0 is deﬁned by

For n < N0 the drop rate can be calculated by replacing (2) with

r[n] ≈ sc

= σu[n]

u[n]

v

λ0 − ρ[N0 − 1] + v = sc.

r[n] ≈ σ(λ[n] − ρ[n − 1]).

The response for n ≥ N0 is still given by (6) but with a new initial condition replacing SN0.

Quantized PI - the Gear Box algorithm A practical implementation of the discrete-time PI

control described above requires a few modiﬁcations to the control loop. The ﬁrst modiﬁcation

is related to how the bytes will actually be dropped at the desired drop rate calculated by the

controller. The drop rate has to be divided fairly among the N IN droppers. Furthermore it is well-

known that dropping consecutive packets may result in poor performance in the affected ﬂows.

Therefore it is desirable to spread the drop rate to an interval and to introduce some randomness

into the drop process. For these reasons we introduce a packet drop probability, p[n], which is

updated at intervals of T according to the desired drop rate and the estimated average arrival rate,

ρ[n]

(1 − p[n − 1])ρ[n]

p[n] =

=

ˆλ[n + 1]

r[n]

.

(7)

Note that here we used the fabric output rate divided by the admit probability (i.e., 1 − p[n − 1])

as an estimate of the next average arrival rate. This is justiﬁed for the cases where the average

arrival rate is a slowly varying function relative to interval T and the delay

The second modiﬁcation to the feedback structure is related to the constraint on the size of the

feedback channel, which becomes a limiting factor on the precision of the feedback signal at high

speeds. Our goal is to use only a ﬁnite number of drop probability values, and to derive a controller

14

that will have a similar performance with the PI controller. For this purpose we expand (7) as

p[n] =

Ke[n] + KI

e[m]

ˆλ[n + 1]  

1

1

=

ˆλ[n + 1]

n

!

m=1
X

n−1

m=1
X

(Ke[n − 1] + KI

e[m] + Ke[n] + KIe[n] − Ke[n − 1]) .

Using again the assumption ˆλ[n + 1] ≈ ˆλ[n], we can rewrite the above equation as

p[n] ≈ p[n − 1] +

(Ke[n] + KIe[n] − Ke[n − 1])

= p[n − 1] +

(Ke[n] + KIe[n] − Ke[n − 1])

=

1 −

(cid:18)
Now, if we deﬁne

(K + KI)e[n] − Ke[n − 1]

(K + KI)e[n] − Ke[n − 1]

p[n − 1] +

(cid:19)

r[n]

.

1

ˆλ[n + 1]
(1 − p[n − 1])

r[n]

r[n]

then the update for the drop probability simply becomes

(K + KI)e[n] − Ke[n − 1]

δ[n] =

r[n]

p[n] = (1 − δ[n])p[n − 1] + δ[n].

In order to use ﬁnite values of p[n] we quantize δ[n] to three levels

Then the update for discrete probability values becomes

β

0

β

β−1

δ[n] > ∆max

−∆min ≤ δ[n] ≤ ∆max

δ[n] < −∆min

δq[n] = 



pq[n] = (1 − δq[n])pq[n − 1] + δq[n],

which can also be written as an update of admit probabilities as

1 − pq[n] = (1 − δq[n])(1 − pq[n − 1]).

15

(8)

If we set K = 0, then (8) can also be expressed in terms of the relative congestion C[n] =

1 − rO[n]/r[n] as

where

and

β

C[n] > dmax

β
β−1 C[n] < dmin
0

otherwise

,

δq[n] = 



dmax = 1 −

1
αs

+

∆max
αsKI

,

dmin = 1 −

1
αs

−

∆min
αsKI

.

We call the quantized mechanism with K = 0 the Gear Box (GB) controller, since there are

only three possible actions: increase the drop probability, decrease the drop probability, and no

change. With the GB controller it is sufﬁcient to have a 2-bit feedback signal every T seconds.

Furthermore the different levels of the admit probabilities are the different powers of (1 − β).

Therefore the calculation at the IN droppers can be implemented by storing

Pk = 1 − (1 − β)k

as a table in the memory and just updating a pointer to this table based on the feedback signal.

To increase the stability of the control loop, in our implementation of the GB algorithm, we

choose the value for β such that the relative congestion after a step increase or decrease in IN drop

probability be equal. To ﬁnd the value for β that has this property, when note that when the relative

congestion C reaches dmax, the drop step is increased, and the relative congestion immediately

changes to a different value Cnew,1. More precisely, if we have:

then rI changes to rI,new = rI(1 − β), so

which can be rewritten as

C = 1 −

= dmax ,

rO
rI

Cnew,1 = 1 −

rO

rI(1 − β)

Cnew,1 = 1 −

1 − dmax

1 − β

16

,

.

Likewise, when C reaches dmin, the drop step is decreased and the relative congestion immediately

changes to a different value Cnew,2. That is,

has the effect of changing rI to rI,new = rI

(1−β) , yielding

C = 1 −

= dmin ,

rO
rI

Cnew,2 = 1 −

rO(1 − β)

,

rI

that is

Cnew,2 = 1 − (1 − dmin)(1 − β) ,

and we want to have Cnew,1 = Cnew,2. Hence,

1 −

1 − dmax

1 − β

= 1 − (1 − dmin)(1 − β) ,

which reduces to

giving ﬁnally

probability be equal.

1 − dmax
1 − dmin

= (1 − β)2 ,

β = 1 −

1 − dmax
1 − dmin

r

as the value for β such that the relative congestion after a step increase or decrease in IN drop

(9)

We illustrate the behavior of the system when subject to the conﬁguration of (9) in Figure 3,

where dmid = 1 −

(1 − dmin)(1 − dmax). When the input rate increases such that the output

relative congestion goes from dmin to dmax, the input drop probability remains at the same level,

p

and jumps to P1 when the output relative congestion reaches dmax. This jump in the input drop

probability has the immediate effect of causing the output relative congestion to decrease to a value

dmid. Then, if the output relative congestion increases again to dmax, the input drop probability

remains at P1 before jumping to P2 when the output relative congestion reaches dmax. Now, if the

input drop probability is at P2, and the relative congestion decreases from dmid to dmin, the input

drop probability remains at P2, and jumps down to P1 as soon as the relative congestion reaches

17

Figure 3: FOQ dynamics and stability

dmin. The decrease in the input drop probability from P2 to P1 immediately increases the output

relative congestion to dmid.

As shown in Figure 3, this conﬁguration has the key advantage of providing hysteresis to the

GB control, by always trying to have the relative congestion come back to dmid, thereby providing

stability against small perturbations. We will use this conﬁguration in our simulations presented in

the following.

5 Simulation Experiments

The objective of this section is to present a set of experimental results that illustrate the salient

properties of FOQ. First, we describe a relatively simple experiment with three classes of trafﬁc

and constant-bit-rate (CBR) trafﬁc, before presenting experimental results gathered for a more

realistic situation where trafﬁc consists of a large number of non-synchronized TCP sources.

18

(a) without FOQ

(b) with FOQ

Figure 4: Throughput plots

5.1 FOQ and Service Guarantees

We simulate a 16x10 Gbps-port switch with a 5 MB shared memory fabric having external speedup

s = 1.28, 2 MB drop-tail OUT queues per ﬂow, and no ingress queues. The FOQ-GB mechanism

has a sampling rate T = 1 ms and feedback thresholds dmax = 0.17, dmin = 0.02. We run each

simulation for 200 ms.

The offered load is composed of three ﬂows sending at constant rates starting at t = 0: ﬂow 0:

0.952 Gbps, ﬂow 1 and 2: 9.52 Gbps each, all ingressing on separate ports and exiting the same

port. Given that the total offered load is 20 Gbps, the OUT port has a potential 200% overload.

The required guarantee for ﬂow 0 is Premium service (0.952 Gbps rate guarantee), and minimum

rate guarantees of 7.75 Gbps and 1.3 Gbps are required for ﬂows 1 and 2 respectively. Flow

0 is assigned to Fabric queue 0 at high priority, and ﬂows 2 and 3 to Fabric queue 1 at lower

priority. At the OUT scheduler, each ﬂow is assigned a separate queue. Queue 0 is scheduled at

high priority, whereas queues 2 and 3 are scheduled at lower priority in a Weigted Fair Queuing

discipline between them with 6 : 1 weights, corresponding to the required rate guarantees.

In Figure 4 we plot the evolution in time of the service rate for the three ﬂows, without and

with FOQ respectively. In Figure 5 we show the dynamics of drop rate for the same scenarios. In

all plots, each datapoint corresponds to an average over a sliding window of size 1 ms. Flow 0 is

serviced at its arrival rate in both cases, due to its high priority assignment in the fabric and OUT

scheduler. But the rate received by ﬂow 1 in the non-FOQ case, 5.93 Gbps (Figure 4(a)), is below

19

(a) Input drop rate without FOQ

(b) Input drop rate with FOQ

(c) Fabric drop rate without FOQ

(d) Fabric drop rate with FOQ

(e) Output drop rate without FOQ

(f) Output drop rate with FOQ

Figure 5: Drop rate plots

20

(a) without FOQ

(b) with FOQ

Figure 6: Delay plots

its requirement. This is due to the drop in the fabric queue 1 (Figure 5(c)) without discrimination

between ﬂows 1 and 2. When using FOQ (Figure 4(b)), ﬂow 1 receives 7.62 Gbps and ﬂow 2

1.37 Gbps, thus both achieving their minimum rate guarantees. This is explained by the FOQ

action reﬂected in Figure 5(b) where we see an increase of input drop for ﬂows 1 and 2 as a

reaction to output congestion. As a consequence, the fabric drop is zero almost all the time in the

FOQ case, in contrast with the high drop rate in the base case. The spike in fabric drop is due

to the transient state where ingress drop is increasing but not yet sufﬁcient for eliminating fabric

congestion. With FOQ, fabric drop occurs only at bursts with high rate and long duration. It can

be mitigated by larger fabric memory or higher frequency of feedback. Also note that ﬂow 0 is not

affected even during the FOQ transient due to its assignment to the high priority fabric queue.

In Figure 6 we show the dynamics of packet transit delay through the whole switch. While

ﬂow 0 receives minimum delay in both cases due to its high priority assignment, ﬂows 1 and

2 experience delays that are proportional to their respective service rates (their OUT queues are

close to full in the steady state due to the drop-tail queue management).

5.2 FOQ Dynamics with TCP Trafﬁc

Next, we examine the interaction of FOQ-GB with TCP trafﬁc. To that effect, we run a simulation

where 4,500 TCP sources send trafﬁc through a switch.

In this experiment, we only consider

one class of trafﬁc. Four subnets containing 1,000 TCP sources each and one subnet containing

21

(a) Input drops

(b) Fabric queue length

Figure 7: Ingress drops and fabric queue. FOQ manages to maintain a low fabric queue by dropping

packets at the input links. When FOQ is not present, there are no input drops.

500 TCP sources are connected to the switch by ﬁve independent 1 Gbps links. All sources send

trafﬁc to the same destination subnet, which is also connected to the switch by a 1 Gbps link, with

a one-way propagation delay of 20 ms. We have the number of active TCP ﬂows increase over

time as follows. Each source in the ﬁrst subnet starts sending trafﬁc between t = 0 s and t = 1 s,

according to a uniform random variable. Then, each source in the second subnet starts sending

trafﬁc between t = 2 s and t = 3 s. Subsequently, every two seconds, sources in an additional

subnet start transmitting. Hence, we have no overload between t = 0 s and t = 2 s, a potential 2:1

overload in the fabric between t = 2 s and t = 4 s, a 3:1 overload between t = 4 s and t = 6 s,

a 4:1 overload between t = 6 s and t = 8 s, and a 5:1 overload then on. There is a potential s : 1

bottleneck at the output port of the switch governing the 1 Gbps link to the destination subnet after

t = 2 s. All TCP sources send 1,040-byte packets.

The FOQ parameters, are chosen as in the previous experiment, i.e., s = 1.28, dmax = 0.17

and dmin = 0.02. The fabric queue has now a size of 500 KB and the output queue has a size of

400 KB. The output queue runs RED, with maxP = 0.5, maxT H = 300 KB, minT H = 100 KB,

a sampling time of 1 ms, and a weight wq = 0.1. We compare the performance of the switch with

and without FOQ.

We ﬁrst observe in Figure 7(b), where each datapoint represents a moving average over a

sliding window of size 50 ms, that, regardless of the potential overload, FOQ consistently manages

22

(a) Fabric losses

(b) Output losses

Figure 8: Fabric and output losses. FOQ manages to completely avoid fabric losses, and also signiﬁcantly

reduces the amount of trafﬁc dropped at the output link.

to maintain the fabric backlog extremely close to zero, by dropping packets at the input links. As

illustrated in Figure 7(a), input drops increase with the overload. Conversely, without FOQ, and

therefore in the absence of input drops, the fabric buffer is ﬁlling up with the number of active TCP

sources, and is eventually completely full once all sources have started transmitting. Ultimately, as

illustrated in Figure 8(a), trafﬁc is dropped in the fabric. There are no fabric drops when FOQ is

used.

Last, we observe in Figure 8(b) that the output loss rate is limited by 1 − 1/s ≈ 21.8% when

FOQ is disabled. On the other hand, FOQ maintains the egress relative congestion close to dmid =

0.098, as shown in Figure 9(a), and consequently, the output loss rate remains close to 9.8%. When

the loss rates become roughly constant, the output queue length, represented in Figure 9(b), also

becomes constant, by virtue of a stable RED control [7].

As a conclusion to this second experiment, we have shown that FOQ’s objectives of preventing

fabric drops and regulating the trafﬁc that arrives at the output link were met in the case of an

experiment with a large number of TCP sources. The results were even more positive than those

obtained with constant-rate sources, as FOQ does not exhibit transient behaviors in this scenario.

This can be justiﬁed by the fact that FOQ feedback is run at a much higher frequency (every

T = 1 ms) than the TCP congestion control algorithms, which are run with an approximately

40-ms delay here.

23

(a) Egress relative congestion

(b) Output queue

Figure 9: Relative congestion and output queue. FOQ maintains the relative congestion between dmin
and dmax.

6 Discussion and Conclusions

In this paper we presented the Feedback Output Queuing architecture for packet switching that

provides support for service guarantees when the switching speed is limited by the memory read

and write speeds. Using a fast switching fabric in this case leads to a build-up in fabric buffers

and eventually either to buffer overﬂow and packet discarding or to unbounded delays at the fabric

inputs due to backpressure. The FOQ architecture solves this problem by triggering packet discard

only from ﬂows that exceed their allocated bandwidth, and therefore limiting the build-up and

delay at the fabric buffers. In the worst case the arrival rate will be λmax, the total input capacity

of the fabric. For the PI controller the maximum fabric queue size and the maximum delay in the

fabric can be calculated from (3) by inserting λ0 = λmax. Any delay value above this number

can be deterministically guaranteed to a ﬂow by using a proper scheduler (e.g. WFQ-based) at the

output queues after the fabric.

An alternative approach to solve the same problem is to use VOQ at fabric inputs. Recent

studies show that VOQ can also provide deterministic delay bounds [21]. This is however at the

expense of computational complexity. VOQ algorithms require O(N 2) computations per packet

slot to determine which packets will be sent to their destinations. This high computational com-

plexity makes the VOQ approach less feasible for high bit-rate switches. In contrast, the FOQ

requires a total of O(N) computations per packet slot and O(KN) computations per feedback

24

interval, where K is the number of supported classes. Since the feedback interval is much larger

than a packet slot, computations for the feedback are actually negligible. Furthermore, the com-

putations are distributed to the inputs and outputs, so that each input and output performs O(1)

computations. In other words, FOQ’s computational complexity is much lower than VOQ, the

current state of the art.

We applied discrete feedback control theory to derive a stable conﬁguration for FOQ. Through

analysis and simulations we showed that a quantized version of a PI controller named “Gear-Box

control” is stable, responds quickly to trafﬁc bursts and provides highly accurate QoS guarantees.

We believe that this work has sparked many venues for future research. There is a range

of control algorithms to be investigated besides those presented here. The interaction between the

TCP congestion control algorithm and FOQ (and RED queue management) is an interesing control

problem. The FOQ architecture can be extended with a set of input queues in order to provide zero

loss for a wider range of bursty trafﬁc, given a limited fabric memory size.

The authors would like to thank Eric Haversat, Tom Holtey and Franco Travostino of Nortel Net-

Acknowledgments

works for many useful discussions.

References

[1] T. Anderson, S. Owicki, J. Saxe, and C. Thacker. High speed switch scheduling for local area

networks. ACM Transactions on Computer Systems, 11(4):319–352, November 1993.

[2] W. Augustyn, G. Heron, V. Kompella, M. Lassere, P. Menezes, H. Ould-Brahim, and

T. Senevirathne. Requirements for Virtual Private LAN Services (VPLS). IETF draft, draft-

ietf-l2vpn-vpls-requirements-00.txt, October 2002.

[3] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, and W. Weiss. An architecture for

differentiated services. IETF RFC 2475, December 1998.

25

[4] M. Carugi, D. McDysan, L. Fang, F. Johansson, A. Nagarajan, J. Sumimoto, and R. Wilder.

Service requirements for layer 3 provider provisioned virtual private networks. IETF draft,

draft-ietf-ppvpn-requirements-04.txt, March 2002.

[5] S.-T. Chuang, A. Goel, N. McKeown, and B. Prabhakar. Matching output queueing with a

combined input-output queued switch. In Proceedings of IEEE INFOCOM ’99, volume 3,

pages 1169–1178, New York, NY, March 1999.

[6] B. Davie, A. Charny, J. Bennett, K. Benson, J.-Y. Le Boudec, W. Courtney, S. Davari,

V. Firoiu, and D. Stiliadis. An expedited forwarding PHB. IETF RFC 3246, March 2002.

[7] V. Firoiu and M. Borden. A study of active queue management for congestion control. In

Proceedings of IEEE INFOCOM’00, volume 3, pages 1435–1444, Tel-Aviv, Israel, April

[8] V. Firoiu, X. Zhang, and E. G¨und¨uzhan. Feedback output queueing: a novel architecture for

efﬁcient switching systems. In Proceedings of Hot Interconnects X, Stanford, CA, August

2000.

2002.

[9] S. Floyd and V. Jacobson. Random early detection for congestion avoidance. IEEE/ACM

Transactions on Networking, 1(4):397–413, July 1993.

[10] G. Franklin, J. Powell, and M. Workman. Digital control of dynamic systems. Addison-

Wesley, Menlo Park, CA, 3rd edition, 1998.

[11] B. Gleeson, A. Lin, J. Heinanen, G. Armitage, and A. Malis. A framework for IP based

virtual private networks. IETF RFC 2764, February 2000.

[12] D. Goderis, S. Van Den Bosch, Y. T’joens, O. Poupel, C. Jacquenet, G. Memenios, G. Pavlou,

R. Egan, D. Grifﬁn, P. Georgatsos L. Georgiadis, and P. Van Heuven. Service level speciﬁca-

tion semantics and parameters. IETF draft, draft-tequila-sls-02.txt, February 2002.

[13] R. Gu´erin and V. Pla. Aggregation and conformance in differentiated service networks: A

case study. ACM Computer Communication Review, 31(1):21–32, January 2001.

26

[14] J. Heinanen, F. Baker, W. Weiss, and J. Wroclawski. Assured forwarding PHB group. IETF

RFC 2597, June 1999.

[15] V. Jacobson. Congestion avoidance and control.

In Proceedings of ACM SIGCOMM’88,

pages 314–329, Stanford, CA, August 1988.

[16] K. Kar, T.V. Lakshman, D. Stiliadis, and L. Tassiulas. Reduced complexity input buffered

switches. In Proceedings of Hot Interconnects VIII, Stanford, CA, August 2000.

[17] S. McCreary and K. Claffy. Trends in Wide Area IP Trafﬁc Patterns. CAIDA, May 2000.

[18] N. McKeown and T. Anderson. A quantitative comparison of iterative scheduling algorithms

for input-queued switches. Computer Networks and ISDN Systems, 30(24):2309–2326, De-

cember 1998.

[19] C. Minkenberg and T. Engbersen. A combined input and output queued packet-switched

system based on Prizma switch-on-a-chip technology.

IEEE Communications Magazine,

38(12):70–77, December 2000.

[20] G. Nong and M. Hamdi. On the provisioning of Quality of Service guarantees for input

queued switches. IEEE Communications Magazine, 38(12):62–69, December 2000.

[21] G. Nong and M. Hamdi. Providing QoS guarantees for unicast/multicast trafﬁc with ﬁxed and

variable-length packets in multiple input-queued switches. In Proceedings of IEEE ISCC’01,

pages 166–171, 2001.

April 2000.

[22] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling TCP Reno Performance: A Simple

Model and Its Empirical Validation. IEEE/ACM Transactions on Networking, 8(2):133–145,

[23] E. Rosen, C. Filsﬁls, G. Heron, A. Malis, L. Martini, and S. Vogelsang. An architecture for

L2VPNs. IETF draft, draft-ietf-ppvpn-l2vpn-00.txt, July 2001.

[24] W. Stevens. TCP slow start, congestion avoidance, fast retransmit, and fast recovery algo-

rithms. IETF RFC 2001, January 1997.

27

[25] Y. Xu and R. Guerin. Individual QoS versus aggregate QoS: A loss performance study. In

Proceedings of IEEE INFOCOM ’02, volume 3, pages 1170 – 1179, New York, NY, June

2002.

28

Appendix

In this appendix we give a detailed derivation of some of the equations.

Taking the z-transforms of (2) and (4), we get

Transfer functions of this system between the output rate, R, and the two inputs and initial state,

λ, Ropt, and SN0, are given by

(10)

(11)

and

and

ρ(z) = K (R(z) − Ropt(z))

+KI

(R(z) − Ropt(z))

z

z − 1

+SN0(z)

R(z) = λ(z) − z−1ρ(z).

R(z)
λ(z)

=

R(z)
Ropt(z)

=

R(z)
SN0(z)

=

z(z − 1)

z2 + (K + KI − 1)z − K

(K + KI)z − K

z2 + (K + KI − 1)z − K

1 − z

z2 + (K + KI − 1)z − K

,

,

.

z2
1,2 + (K + KI − 1)z1,2 − K = 0.

Let z1 and z2 be two roots of the system characteristic equation, i.e.

Then without loss of generality

K + KI − 1

z1 = −

+

(K + KI − 1)2 + 4K

K + KI − 1

z2 = −

−

(K + KI − 1)2 + 4K.

2

2

We showed in Proposition 1 that the system is stable if

0 < KI < 2(1 − K).

1
2

1
2

p

p

29

We next ﬁnd the solution for the drop rate ρ assuming this stability condition is satisﬁed. For step

inputs and initial condition, λ(z) = zλ/(z − 1), Ropt(z) = zropt/(z − 1), SN0(z) = zSN0/(z − 1),

and deﬁning D = λ − ropt as the difference between the arrival and the desired rates, we have from

(10) and (11):

This can be written as a partial fraction expansion as

where

and

ρ(z) =

KD z

z−1 +KI D z2
(z−1)2 +SN
0
KI
K
z +
z−1
0 ]z−KD−SN
= z2 [(K+KI )D+SN

(z−1)(z2+(K+KI −1)z−K) .

z−1

1+

0

z

ρ(z) = D

z

z − 1

(cid:18)

−

A1z
z − z1

+

A2z
z − z2 (cid:19)

A1 =

SN
D z1
0

z2
1 −
z1 − z2

A2 =

SN
D z2
0

z2
2 −
z1 − z2

,

which can be solved easily. Finally recall that this system was obtained initially by deﬁning a new

time axis for n ≥ N0. Therefore after taking the inverse z-transform we combine the result with

n < N0 case to get

ρ[n] =

[K + (n + 1)KI](sc − ropt),
+ A2zn−N0
D(1 − A1zn−N0

2

1

n < N0

.

), n ≥ N0






30

