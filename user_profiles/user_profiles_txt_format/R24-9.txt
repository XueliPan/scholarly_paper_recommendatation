Paciﬁc Graphics 2009
S. Lee, D. Lischinski, and Y. Yu
(Guest Editors)

Volume 28 (2009), Number 7

Yiming Liu1,2†

Jiaping Wang2

Su Xue2,3† Xin Tong2

Sing Bing Kang4 Baining Guo2

Texture Splicing

1Nanyang Technological University

2Microsoft Research Asia

3Yale University

4Microsoft Research Redmond

Abstract
We propose a new texture editing operation called texture splicing. For this operation, we regard a texture as
having repetitive elements (textons) seamlessly distributed in a particular pattern. Taking two textures as input,
texture splicing generates a new texture by selecting the texton appearance from one texture and distribution from
the other. Texture splicing involves self-similarity search to extract the distribution, distribution warping, context-
dependent warping, and ﬁnally, texture reﬁnement to preserve overall appearance. We show a variety of results to
illustrate this operation.

Categories and Subject Descriptors (according to ACM CCS):
I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Texture I.4.7 [Image Processing and Computer Vision]: Feature Measurement—Texture

1. Introduction

Textures are fundamental in computer graphics, and the
topic of texture synthesis has been well-explored. The typ-
ical goal of texture synthesis is to reproduce existing tex-
tures in different sizes, forms, and contexts (e.g., [HB95,
Deb97, EL99, EF01, KSE∗03, ZZV∗03, KEBK05]). Interest-
ingly, relatively little has been done on texture editing, where
new textures are created by modifying existing ones.

Textures are recognizable not only by the appearance of
its basic elements (textons), but also their placement dis-
tribution. The placement distribution may include transla-
tion, rotation, and scaling of textons. Example-based texture
synthesis generates textures by mimicking the texton and
its placement distribution simultaneously (without separat-
ing the two properties). As a result, it is difﬁcult to generate
new textures with different attributes using current example-
based texture synthesis algorithms.

In this paper, we propose a new texture editing opera-
tion which we call texture splicing. Unlike texture synthesis,
texture splicing decomposes textures into two parts: texton
appearance and placement distribution. Given two textures,

† Yiming Liu and Su Xue were visiting students at Microsoft Re-
search Asia.

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

texture splicing generates a new texture by using the texton
appearance from one texture and distribution from the other.
This operation is especially useful when a database of tex-
tures is available, in which case the user only need to specify
which texton appearance and distribution are desired—these
attributes can be inherited from different textures. An inter-
esting special case is self-splicing. In this case, the two input
textures are the same and the user has to modify the distri-
bution manually to generate new textures. Figure 1 shows a
new texture created using texture splicing.

2. Related Work

Most existing approaches for texture synthesis are pixel-
based [HB95, Deb97, EL99] or patch-based [EF01, KSE∗03,
ZZV∗03, KEBK05]. While they are capable of producing
compelling-looking results, they do not explicitly separate
the texton and its distribution. As a result, it is not apparent
how they can generate new textures with separate attributes.

Techniques for

for example, Texture Transfer

texture editing have also been pro-
posed,
Image
Analogies [HJO∗01], and Texture Flow [KEBK05, LH06,
OiAIS09]. Fundamentally, these approaches generate a new
output by transferring a given texture to an image or video;

[EF01],

Y. Liu & J. Wang & S. Xue & X. Tong & S. B. Kang & B. Guo / Texture Splicing

Texture A

Texton 

Distribution 

A

Texture B

Texton 

Distribution 

B

Figure 1: A new texture is generated by combining textons from texture A with the placement distribution from texture B.

Texture A

Distribution DA

Displacements fAB

Warp Field WAB

Backward
Warp WBA

Initial New
Texture TAB0

Refined New
Texture TAB

Texture B

Distribution DB

Foreground Mask MA

Figure 2: Overview of texture splicing.

however, the new output basically retains the properties of
the original texture.

There are techniques that do model the texton [BA89,
TTG01,LF06] and its spatial distribution [LLH04,HLEL06].
Explicit modeling of such texture properties provides more
ﬂexibility for texture editing; it allows one property to
be modiﬁed while preserving the other. The technique of
[LDR09] uses manifold diffusion distance to separate the
different texture contents in an image. It then generates new
textures by manipulating the decomposed parts and recom-
posing. However, as we shall see, there are issues with cur-
rent approaches that limit their use.

In [BD02], a self-similarity based method is proposed to
propagate the editing operation on a texton to the neighbor-
hood regions that are self-similar. Examples of such edit-
ing operations include expanding, color editing, and warp-
ing. Matusik et al. [MZD05], on the other hand, use a more
global model. They represent the space of textures from a
database by a simplicial complex, where each vertex rep-
resents a texture. Oriented edge features in each texture are
used to compute warping ﬁelds and establish similarity; sim-
ilar textures are interconnected in the complex. New textures
are generated by nonlinear interpolation. Basically, the new
texture is a morphed version of two closest textures in the
complex. Other than specifying the path in the complex, the
user has little other control over the texture to be generated.

The technique of [LH05] allows the user to use cir-
cular regions to manually specify position of texture tex-
tons. However, the synthesized textons are possible to ad-
here partial regions of their neighboring ones when they
lie close in the source texture. Furthermore, since the fore-
ground/background area is not distinguished explicitly, the
user needs to manually delete textons on the synthesized tex-
ture.

There are many approaches for texton extraction [VP88,
LM96, SZ99, TTG01, LF06, AT07], but it is still an open
problem. These approaches do not address the issue of tex-
ture synthesis. Liu et al. [LLH04] propose a texture edit-
ing and analysis scheme of near-regular textures by having
the user specify the distribution. In a followup work, Hays
et al. [HLEL06] propose a technique to automatically ex-
tract the texton distribution of near-regular textures through
regularized thin-plate spline warping. However, the methods
of [LLH04, HLEL06] are limited to topology-regular distri-
butions. Dichler et al. [DMLC02] propose a method to au-
tomatically extract the distribution by simple RGB quanti-
zation. However, this method cannot work well for textures
with connected elements or textures with closely adjacent
elements (e.g., brick wall textures).

Gal et al. [GSCO06] propose a feature-sensitive inhomo-
geneous texture mapping method. This method maps tex-
tures according to a warping function W while preserving

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.

Y. Liu & J. Wang & S. Xue & X. Tong & S. B. Kang & B. Guo / Texture Splicing

Figure 4: Tags based on property of foreground or back-
ground, with representative examples.

3.2. Self-Similarity Search

=

=

=

+

+

+

Texture

Foreground

Background

Figure 3: Examples of textures and their foregrounds and
backgrounds.

Foreground:

Rigid 
Disjoint

Foreground:

Rigid

Connected

Background:

Rigid

Connected

features. The user is required to provide a feature mask and
manually specify the input warping function W .

Our texture splicing technique relies on a database of tex-
tures from which the user chooses to produce a new texture.
Prior to its use, however, the texture database is preprocessed
ﬁrst.

3. Texture Database

We preprocess each texture in our database to generate
the following information: foreground/background types (ei-
ther connected or disjoint) TF , TB, texton foreground (bi-
nary) mask M(x, y), and the spatial distribution D = {pi, i =
1, ..., ND}. D lists the 2D coordinates pi associated with the
placement of textons; ND is the number of textons in the
sample texture. Note that the database preprocessing phase
is not completely automatic.

3.1. Foreground and Background Type

For a given texture, the extent and appearance of a tex-
ton may be spatially varying. Given a texton, we deﬁne its
foreground and background areas (their segmentation is de-
scribed in Section 3.3). The deﬁnitions are somewhat ar-
bitrary, but generally the foreground is used to designate a
structure within the texton that either is visually prominent

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.

or occupies a large central area. The background is typi-
cally used to designate areas that are almost uniform. Fig-
ure 3 shows three different textures with their predeﬁned
foregrounds and backgrounds.

Once the decision is made as to which is the foreground
and which is background, they are tagged as either rigid con-
nected (c) or rigid disjoint (d); the tagging process is done
manually. As the tag names imply, the foreground is con-
nected (TF = c) if it is connected across neighboring textons,
and disjoint otherwise (TF = d). For the background, it can
only be tagged as connected (TB = c). The term “rigid” is
used to indicate that during texture synthesis, the foreground
or background is not to be distorted. Figure 4 shows exam-
ples of textures tagged in this manner. Note that it is sufﬁ-
cient to tag a texture as TF = c, TF = d, or TB = c.

The next two steps are self-similarity search (to estimate
the texton spatial distribution) and foreground-background
separation. Both rely on a manual initialization step. We
manually select one foreground area (from which we can
compute its center pe and size se) and scribble on the back-
ground. This lets the system know the approximate shape of
the foreground part of the texton as well as the color distri-
butions of the background and foreground.

The step of self-similarity search is required to estimate the
spatial distribution of texton placement. We use the appear-
ance vector map [LH06] to accomplish this. This map en-
codes neighborhood information of a texture. The channels
in the appearance vector map reveal spatial structure that
arises from factors such as color, intensity, edge orientation,
and patterns at different scales. From experiments, we found
it is adequate to use the ﬁrst 8 channels of the appearance
vector map for this search step.

Our algorithm searches for self-similar locations as fol-
lows: For the pixel location pe (manually initialized) and
its neighborhood Na(pe) in channel a of the appear-
ance vector map, we deﬁne a similarity distribution map
Dpe,a with respect to pixel pe and its neighborhood as
Dpe,a(x, y) = G(Na(pe), Na(x, y)), where G(., .) is a similar-
ity measure between two neighborhoods (we use normalized
cross-correlation [Lew95]). The similarity distribution map
Dpe,a(x, y) indicates the likelihood of neighborhood Na(pe)
appearing at location (x, y) in channel a. The similarity dis-
tribution map is computed for a = 1, ..., 8; the actual map
Dpe used is the one with minimum entropy.

The local maxima in Dpe are potential candidates for tex-
ton locations. They are then sorted according to the similar-
ity value, from highest to lowest. To determine the actual lo-
cations, our algorithm applies non-maximal suppression on
the candidates one by one, in the order they were sorted.
Non-maximal suppression is done by removing neighboring
pixels of the current candidate. The size of the neighborhood

Y. Liu & J. Wang & S. Xue & X. Tong & S. B. Kang & B. Guo / Texture Splicing

is determined by se, which was produced in the manual ini-
tialization step. Our algorithm stops when 95% pixels on the
similarity distribution map are either picked or removed.

polation, with non-rigid regions reﬁned using conventional
texture synthesis.

3.3. Foreground-background Separation of Textons

At this point, we have the following information: fore-
ground mask of one texton and scribbles on the background
(from manual initialization), and spatial distribution Dpe
(described in the previous section). Thus, we have sam-
ples of foreground colors C(F) and background colors C(B).
Note that F and B are the sets of foreground and background
colors, respectively.

We use a graph-cut segmentation algorithm [GPS89] to
extract the texton foreground (binary) mask. To segment the
foreground and background in texture A, we minimize

E(LA) = (cid:229)

Dp(L(p)) + (cid:229)

S(L(p), L(q)),

(1)

p∈A

p∈A

q∈N4(p)

where LA = {L(p) ∈ {B, F}|p ∈ A} is a labeling of A, L(p)
is the label (either B, for background, or F, for foreground)
at pixel p, and N4(p) is the 4-neighborhood of p. Dp(·) is the
data term associated with labeling costs, while S(L(p), L(q))
is the regularization term that prefers spatial smoothness of
labels. They are deﬁned as

Dp(L(p)) = − min

kA(p) − ck2 and

(2)

c∈C(L(p))

S(L(p), L(q))) = d (L(p) − L(q))e−kA(p)−A(q)k2/2s 2

,

(3)

where d (L(p) − L(q)) is 1 if L(p) 6= L(q), 0 if L(p) = L(q).
Note that in (2), C(L(p)) = C(F) or C(B), both of which
are known from the manual initialization and self-similarity
steps. In our experiments, s = 1. The energy minimiza-
tion problem is modeled as a min-cut/max-ﬂow problem
on a ﬂow network G, and solved using a standard tech-
nique [FF62, EK72, BK04].

Given the database with the preprocessed information, we
can now proceed to generate new textures. From this point
on, all the steps associated with texture splicing are auto-
matic.

4. Texture Splicing

Our technique is summarized in Figure 2. Given source tex-
tures A and B from the database, a new texture is generated
by setting B’s distribution DB as the target distribution for
source texture A. A set of discrete displacements f is es-
tablished to warp A’s distribution DA to the target distribu-
tion DB. A dense forward warping ﬁeld W is computed using
f . Depending on the foreground/background tag, W is com-
puted under the constraint of selective rigidity [GSCO06].
For example, if the tag is set to TF = d, the shapes of the fore-
ground elements are to preserved as much as possible. Fi-
nally, the new texture is generated through barycentric inter-

4.1. Correspondence of Placement Distributions

Given source distribution DA and target distribution DB, we
seek the mapping fAB : DA → DB. First, we normalize the
scales of DA and DB such that the average distances to k-
nearest positions of both distributions are unity (k is between
4 and 6, depending on texture density). We then align them
through their centroids. We also align the orientations using
the Radon transform [JK05].

A cost matrix CAB is deﬁned as 2D distances between
points in DA and DB. fAB is computed by minimizing the
total cost of corresponding (unique) pairs:

fAB = arg min
f :DA→DB

i∈DA

CAB(i, f (i)).

(4)

We use the classical Hungarian method [Mun57] to ﬁnd the
global minimum.

4.2. Context Dependent Deformation

Given input texture A, foreground binary mask MA, and dis-
placements fAB, we would now like to compute the dense
warping ﬁeld WAB. WAB maps a point p = (px, py) in A to
another point q = (qx, qy) in B, i.e., q = WAB(p).

Let us deﬁne the shift D (p) = (D x(p), D y(p)) = WAB(p) −
p; we formulate the problem as a minimization of deforma-
tion energy. Since deformation on a 2D plane is separable,
without loss of generality, we describe how we extract the
x-component of the deformation Ux (Uy is computed in the
similar way). We minimize
Ex(D x) =l G (cid:229)

(D x(p) − ( fx(p) − px))2 +

p∈CA

l R

D x(p) − (cid:229)

˜upqD x(q)

,

p|MA(p)=1  

q∈N4(p)

!

2

(5)

with N4(p) being the 4-neighborhood of p and ˜upq is a nor-
malized quantity described shortly. Each term with x as its
subscript refers to its x-component. The ﬁrst term tends to
preserve displacements fAB while the second term encour-
ages spatial smoothness of displacements. Since we prefer
to preserve fAB, we set l G ≫ l R (more speciﬁcally, to 104
and 102, respectively).

Note that the second term is computed over pixels that
ought to be rigid only (i.e., where MA(p) = 1). The deﬁni-
tion of which pixel should be rigid depends on the type, as
indicated in Table 1. This enables the shape of speciﬁc parts
of the texture to be preserved while the other parts can be
distorted. ˜upq is the normalized weight of each neighboring
pixel: ˜upq = upq/ (cid:229) q∈N4(p) upq, with upq computed as fol-
lows:

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.

(cid:229)
(cid:229)
(cid:229)
Y. Liu & J. Wang & S. Xue & X. Tong & S. B. Kang & B. Guo / Texture Splicing

p ∈ F

p ∈ B

Type
TF = c MA(p) = 1 MA(p) = 0
TF = d MA(p) = 1 MA(p) = 0
TB = c MA(p) = 0 MA(p) = 1

l P
1
1
0

10−3
10−3

1

Table 1: Mask values based on tag. F and B are the sets of
foreground and background pixels, respectively. Recall that
TF and TB are the type of foreground and background, re-
spectively; c refers to rigid connected while d refers to rigid
disjoint. Note that the case TB = d is not used.

p

q

q

p

p

q

q

p

Figure 5: Four special conditions on the foreground mask
MA, any of which yields upq = MA(q) + e . Each block rep-
resents a pixel. Black: MA(.) = 0, white: MA(.) = 1, grey:
MA(.) = 1 or 0 (“don’t care”).

• upq = MA(q) + e

if any of these conditions are met: px 6=
qx, or MA(p) = MA(q) = 0, or any one of the four condi-
tions illustrated in Figure 5.

• Otherwise, upq = l P(MA(q) + e ).

The parameter e controls the consistency of foreground and
background deformations, while l P controls the constraint
between horizontal and vertical borders of rigid areas. Both
depend on the texture tag (see Table 1). For textures with
a rigidly connected background (TB = c), we set l P = 0 to
decouple constraints on the horizontal and vertical borders.
The seams for the red brick texture in Figure 7 are preserved
because of this constraint decoupling. Although the bricks
are moved and scaled, their basic shapes are preserved. For
other types of textures, l P = 1.

D can be obtained using a sparse equation solver. In prac-
tice, similar as [GSCO06], we use double back substitu-
tion to solve it. The warping ﬁeld can then be computed:
WAB = I + D
, where I is the identity function. WAB typically
maps to fractional pixel locations; we use barycentric inter-
polation to extract color or mask value.

More speciﬁcally, we ﬁrst triangulate the source texture,
using the center of pixels as vertices. We warp each triangle
(see Figure 6) and rasterize it on the output texture. For each
node q inside a warped triangle, we use barycentric coordi-
nates to determine the inverse mapping WBA(q). It is pos-
sible for a point in the output to be inside multiple warped
source triangles. In this case, we select the triangle whose
vertices have a maximum sum of texton mask values. Once
the backward mapping WBA has been established, we use it
to produce the initial output texture TAB0.

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.

4.3. Final Reﬁnement

There are two basic problems with TAB0: First, WBA may not
be deﬁned for all pixels in the output texture—this results in
holes. Second, rigid parts of the source texture may still be
slightly deformed, with attendant loss of visual ﬁdelity.

We adopt Lefebvre and Hoppe’s texture synthesis
method [LH06] to handle these two problems. Suppose
WBA(q) does not exist, which creates a hole at q. We remove
the hole by randomly selecting a non-rigid pixel p in the
source texture, i.e., WBA(q) = p. This produces a hole-free
texture T ′
AB0. We then apply the two-level multi-resolution
synthesis on T ′
AB0, and jitter the reference positions of non-
rigid areas. This produces the ﬁnal new texture TAB.

5. Results

Figures 7 and 8 show a variety of results using our texture
splicing technique. Figures 7 shows results with different
texture types providing the texton appearance. (It is irrele-
vant what the texture type is for the texture providing the
distribution information.) Notice that the resulting textures
preserve either the foreground or background (depending on
the texture tag) of one texture while assuming the distribu-
tion of the other texture.

Figures 8, on the other hand, shows all possible textures
that can be obtained from textures in the leftmost column
providing the texton appearance and textures in the top row
providing the distribution. This matrix is very telling—while
most results look reasonable, others show that not all texture
pairs are compatible. For example, the apple texture (8th row
inside the matrix) and water droplet texture (10th column in-
side the matrix) produced a bizarre and unattractive texture.
While this can be regarded as a “failure” case, it is not clear
what the right answer is given these two textures as input.

Self-splicing results are shown in Figures 9. Here, only
one source texture is used to create a result; the new texture
is generated by directly manipulating its distribution. The
manipulation can be in form of scaling (1), randomization
(2), arrangement into a regular pattern (3), or speciﬁc rear-
rangement (4).

WBA(q)

source

q

output

Figure 6: Illustration for backward interpolation. The blue
points in the output texture are the warped vertices of a tri-
angle. The backward warping of q is done using barycentric
coordinates.

e
Y. Liu & J. Wang & S. Xue & X. Tong & S. B. Kang & B. Guo / Texture Splicing

1

2

3

(a)

(b)

(c)

(a)

(b)

(c)

Figure 7: Synthesized textures. The groups 1 (rigid connected foreground, or TF = c), 2 (rigid disjoint foreground, or TF = d),
and 3 (rigid connected background, or TB = c), are in reference to the source textures providing the texton appearance (a).
Combined with source textures providing the distribution information (b), we get target textures (c).

The pre-computation step of k-nearest-neighbor search to
build candidate sets for k-coherence search of texture syn-
thesis took on average 21.1 seconds for textures of size
128 × 128 and 33.6 seconds for 192 × 192. This timing is
not critical to our texture splicing operation, since this pre-
computation step constitute a once-only cost and can be done
ofﬂine.

can be pre-computed once and added to the texture database
(currently not done). At present, our texture synthesis oper-
ation runs on the CPU. It is amenable to GPU implementa-
tion, which is likely to allow our tool to run at interactive
speeds.

6. Concluding Remarks

Texture splicing and self-splicing took on average 1.4 sec-
onds for 128 × 128 textures and 2.9 seconds for 192 × 192
textures on a workstation with an Intel Core 2 Duo 2.66GHz
CPU and 2GB RAM. The most expensive operation (taking
> 95% of the time) is the Cholesky factorization of the left-
hand matrix for deformation and texture synthesis. Since the
factorization does not depend on the texton displacement, it

We have proposed texture splicing as a novel means for tex-
ture editing. The idea is simple: decompose each texture into
texton appearance and its spatial distribution, and combine
the texton appearance from one texture with the distribution
from another. This makes texture editing extremenly simple
for the user, as only two textures need to be speciﬁed. The
user has the option of directly editing the texton placements,

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.

Y. Liu & J. Wang & S. Xue & X. Tong & S. B. Kang & B. Guo / Texture Splicing

Figure 8: Edited textures over combinations of source textures. First column: source textures providing texton appearance. First
row: source textures providing distribution information. The texture within the matrix is the synthesized result using a source
texture from the ﬁrst column and another from the ﬁrst row.

which require more interaction, but provides more control
on texture design. We showed a variety of results to demon-
strate the effectiveness of texture splicing.

There are limitations in our current work. The self-
similarity search does not take account texton rotation and
scale. In addition, our method relies on the user-speciﬁed

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.

pe to initialize the self-similarity search. Future extensions
include fully automatic texton distribution extraction algo-
rithm and handling of textures with a variety of texton scale
and rotation.

Y. Liu & J. Wang & S. Xue & X. Tong & S. B. Kang & B. Guo / Texture Splicing

1

2

3

4

(a)

(b)

(c)

(a)

(b)

(c)

Figure 9: Texture self-splicing results: (a) source texture, (b) manipulation of distribution, and (c) edited texture.

References

[AT07] AHUJA N., TODOROVIC S.: Extracting texels in 2.1D
natural textures. In International Conference on Computer Vision
(ICCV) (2007). 2

[BA89] BLOSTEIN D., AHUJA N.: Shape from texture: Inte-
grating texture-element extraction and surface estimation. IEEE
Transactions on Pattern Analysis and Machine Intelligence 11,
12 (1989), 1233–1251. 2

[BD02] BROOKS S., DODGSON N.: Self-similarity based tex-
ture editing. Computer Graphics (SIGGRAPH ’02 Proceedings)
(2002), 653–656. 2

[BK04] BOYKOV Y., KOLMOGOROV V.: An experimental com-
parison of min-cut/max-ﬂow algorithms for energy minimization
in vision. IEEE Transactions on Pattern Analysis and Machine
Intelligence 26, 9 (2004), 1124–1137. 4

[Deb97] DEBONET J. S.: Multiresolution sampling procedure for
analysis and synthesis of texture images. Proceedings of ACM
SIGGRAPH 97 (1997), 361–368. 1

[DMLC02] DICHLER J.-M., MARITAUD K., LEVY B., CHAZ-
ANFARPOUR D.: Texture particles. In Eurographics conference
proceedings (Sep 2002). 2

[EF01] EFROS A. A., FREEMAN W. T.: Image quilting for tex-
ture synthesis and transfer. Proceedings of SIGGRAPH 2001
(August 2001), 341–346. 1

[EK72] EDMONDS J., KARP R. M.: Theoretical improvements

in algorithmic efﬁciency for network ﬂow problems. Journal of
ACM 19, 2 (1972), 248–264. 4

[EL99] EFROS A. A., LEUNG T.: Texture synthesis by non-
parametric sampling. International Conference on Computer Vi-
sion (1999), 1033–1038. 1

[FF62] FORD L., FULKERSON D.: Flows in Networks. Princeton

University Press, Princeton, NJ, 1962. 4

[GPS89] GREIG D., PORTEOUS B., SEHEULT A.: Exact max-
imum a posteriori estimation for binary images. Journal of the
Royal Statistical Society. Series B (Methodological) 51, 2 (1989),
271–279. 4

[GSCO06] GAL R., SORKINE O., COHEN-OR D.: Feature-
aware texturing. In Proceedings of Eurographics Symposium on
Rendering (2006), pp. 297–303. 2, 4, 5

[HB95] HEEGER D. J., BERGEN J. R.: Pyramid-based texture
analysis synthesis. In Proceedings of ACM SIGGRAPH 95 (Au-
gust 1995), pp. 229–238. 1

[HJO∗01] HERTZMANN A., JACOBS C., OLIVER N., CURLESS
B., SALESIN. D.: Image analogies. SIGGRAPH ’01: Proceed-
ings of the 28th annual conference on Computer graphics and
interactive techniques (2001), 327–340. 1

[HLEL06] HAYS J. H., LEORDEANU M., EFROS A. A., LIU Y.:
Discovering texture regularity as a higher-order correspondence
problem. ECCV 2 (2006), 522–535. 2

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.

Y. Liu & J. Wang & S. Xue & X. Tong & S. B. Kang & B. Guo / Texture Splicing

[JK05]

JAFARI-KHOUZANI K.: Radon transform orientation es-
timation for rotation invariant texture analysis. IEEE Trans. Pat-
tern Anal. Mach. Intell. 27, 6 (2005), 1004–1008. Sr. Member-
Soltanian-Zadeh„ Hamid. 4

[KEBK05] KWATRA V., ESSA I., BOBICK A., KWATRA N.:
Texture optimization for example-based synthesis. ACM Trans-
actions on Graphics, SIGGRAPH 2005 (2005), 795–802. 1

[KSE∗03] KWATRA V., SCHÖDL A., ESSA I., TURK G., BO-
BICK A.: Graphcut textures: image and video synthesis using
graph cuts. ACM Transactions on Graphics, SIGGRAPH 2003
(2003), 277–286. 1

[LDR09] LU J., DORSEY J., RUSHMEIER H. E.: Dominant tex-
ture and diffusion distance manifolds. Comput. Graph. Forum
28, 2 (2009), 667–676. 2

[Lew95] LEWIS J.: Fast normalized cross-correlation. Vision In-

terface (1995), 120–123. 3

[LF06] LOBAY A., FORSYTH D.: Shape from texture without
International Journal of Computer Vision 67, 1

boundaries.
(2006), 71–91. 2

[LH05] LEFEBVRE S., HOPPE H.: Parallel controllable texture
synthesis. ACM Transactions on Graphics, SIGGRAPH 2005 24,
3 (2005), 777–786. 2

[LH06] LEFEBVRE S., HOPPE H.: Appearance-space texture
synthesis. ACM Transactions on Graphics, SIGGRAPH 2006
(2006), 541–548. 1, 3, 5

[LLH04] LIU Y., LIN W.-C., HAYS J.: Near-regular texture anal-
ysis and manipulation. ACM Transactions on Graphics, SIG-
GRAPH 2004 23, 3 (2004), 368–376. 2

[LM96] LEUNG T. K., MALIK J.: Detecting, localizing and
grouping repeated scene elements from an image.
In ECCV
’96: Proceedings of the 4th European Conference on Computer
Vision-Volume I (London, UK, 1996), Springer-Verlag, pp. 546–
555. 2

[Mun57] MUNKRES J.: Algorithms for the Assignment and
Transportation Problems. Journal of the Society for Industrial
and Applied Mathematics 5, 1 (1957), 32–38. 4

[MZD05] MATUSIK W., ZWICKER M., DURAND F.: Texture
design using a simplicial complex of morphable textures. ACM
Transactions on Graphics, SIGGRAPH 2005 (2005), 787–794. 2

[OiAIS09] OKABE M., ICHI ANJYO K., IGARASHI T., SEIDEL
H.-P.: Animating pictures of ﬂuid using video examples. Com-
put. Graph. Forum 28, 2 (2009), 677–686. 1

[SZ99] SCHAFFALITZKY F., ZISSERMAN A.: Geometric group-
ing of repeated elements within images. In Shape, Contour and
Grouping in Computer Vision LNCS 1681, 1-2 (1999), 165–181.
2

[TTG01] TURINA A., TUYTELAARS T., GOOL L. V.: Efﬁcient
grouping under perspective skew. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2001), 247–254. 2

[VP88] VOORHEES H., POGGIO T.: Computing texture bound-

aries from images. Nature 333 (1988), 364–367. 2

[ZZV∗03] ZHANG J., ZHOU K., VELHO L., GUO B., SHUM H.-
Y.: Synthesis of progressively-variant textures on arbitrary sur-
faces. ACM Transactions on Graphics, SIGGRAPH 2003 (2003),
295–302. 1

c(cid:13) 2009 The Author(s)
Journal compilation c(cid:13) 2009 The Eurographics Association and Blackwell Publishing Ltd.

