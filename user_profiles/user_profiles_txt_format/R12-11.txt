Feature Article: The Predicting Power of Textual Information on Financial Markets  
  

  1

The Predicting Power of Textual Information on

Financial Markets

Gabriel Pui Cheong Fungy, Jeffrey Xu Yuy, Hongjun Luz

Abstract— Mining textual documents and time series concur-
rently, such as predicting the movements of stock prices based
on the contents of the news stories, is an emerging topic in data
mining community. Previous researches have shown that there
is a strong relationship between the time when the news stories
are released and the time when the stock prices ﬂuctuate. In this
paper, we propose a systematic framework for predicting the
tertiary movements of stock prices by analyzing the impacts of
the news stories on the stocks. To be more speciﬁc, we investigate
the immediate impacts of news stories on the stocks based on
the Efﬁcient Markets Hypothesis. Several data mining and text
mining techniques are used in a novel way. Extensive experiments
using real-life data are conducted, and encouraging results are
obtained.

I. INTRODUCTION

I N the ﬁnancial markets, the movements of the prices are

the consequences of the actions taken by the investors
on how they perceive the events surrounding them as well
as the ﬁnancial markets. Investors’ decisions on bidding,
asking or holding the securities are greatly inﬂuenced by what
others said and did within the ﬁnancial markets [1], [2]. The
emotions of fear, greed, coupled with subjective perceptions
and evaluations of the economic conditions and their own
psychological predispositions and personalities, are the major
elements that affect the ﬁnancial markets’ behaviors [3], [4].
Yet, human behaviors are not random. People’s actions
in the ﬁnancial markets, although occasionally irrational, are
predominantly understandable and rational with respect to the
social structure, social organization, perceptions and collective
beliefs of this complex arena [1], [2], [5], [6]. At times,
collective movements are launched which are in turn based on
a group beliefs about how the markets will act or react [3], [4].
In these instances, trends develop which can be recognized,
identiﬁed and anticipated to continue for some periods.

Nowadays, an increasing amount of crucial and valuable
information highly related to the ﬁnancial markets is widely
available on the Internet.1 However, most of these information
are in textual format, such as news stories, companies’ reports
and experts’ recommendations. Hence, extracting valuable
information and ﬁguring out
the relationship between the
extracted information and the ﬁnancial markets are neither
trivial nor simple.

yDepartment of Systems Engineering and Engineering Management, The

Chinese University of Hong Kong, fpcfung, yug@se.cuhk.edu.hk

zDepartment of Computer Science, The Hong Kong University of Science

and Technology, luhj@cs.ust.hk

1E.g. Wall Street Journal: www.wsj.com, Financial Times: www.ft.
com, Reuters: www.reuters.com, Bloomberg: www.bloomberg.com,
CNN: www.cnnfn.com, etc

In this paper, we investigate how to utilize the rich textual
information in predicting the ﬁnancial markets. In contrast to
the traditional time series analysis, where predictions are made
based solely on the technical data (historical movements of
the time series) and/or the fundamental data (the companies’
proﬁles), this paper focuses on the problem of predicting the
impacts of the textual information on the ﬁnancial markets.
To be more speciﬁc, real-time news stories and intra-day stock
prices are used to denote respectively the information obtained
from textual documents and the the movements of the ﬁnancial
markets. In other words, predictions are made according to the
contents of news stories, rather than using the numerical data.
This kind of problem is sometimes known as mining time
series and textual documents concurrently [7], [8], [9], which
is an emerging topic in the data mining community nowadays
[10], [11], [12], [13]. The main contributions of this paper are
summarized below:

1) Figuring out the tertiary movements of the stock
prices A tertiary movement lasts for less than three
weeks, which denotes the short-term stock market be-
havior [14], [15], [16]. In other words, tertiary move-
ment is highly affected by the events surrounding the
ﬁnancial market. A new piecewise linear approximation
algorithm, called t-test based split and merge segmenta-
tion algorithm, is proposed for ﬁguring out the tertiary
movements automatically.

2) Detecting the relationship between the events men-
tioned in the news stories and the tertiary movements
of stock prices We have to correctly align and select
the news stories to the tertiary movements of the stock
price such that the aligned news stories are most likely
to trigger or support
the movements of the trends.
The alignment process is based on the Efﬁcient Market
Hypothesis [1], [2] and the selection of the useful news
stories is based on a (cid:31)2 estimation on the keywords
distribution over the entire document collection.

3) Predicting the impact of a newly released news story
on the stock price Three kinds of impact are deﬁned:
positive, negative and natural. A piece of
news story is said to have positive (or negative)
impact if the stock price rises (or drops) signiﬁcantly for
a period after the news story is released; otherwise, if
the the stock price does not ﬂuctuate even after the news
story is released, we said that the impact of the news
story is natural. The major learning and prediction
process is based on the text classiﬁcation algorithm,
Support Vector Machines [17].

IEEE Intelligent Informatics Bulletin 

       

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  June 2005  Vol.5 No.1 

2                                                                                     Feature Article: Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun Lu
 

The rest of this paper is organized as follows. Section II
reviews the major preliminaries related to the discussion of
this paper. Section III presents our proposed system. Section
IV evaluates various aspects related to our proposed approach.
A summary and conclusion is given in Section VI.

II. PRELIMINARIES

The ﬁrst systematic examination against

the impacts of
textual information on the ﬁnancial markets is conducted by
Klein and Prestbo [18]. Their survey consists primarily of
a comparison of the movements of Dow Jones Industrial
Average2 with general news during the period from 1966 to
1972. The news stories that they have taken into consideration
are the stories appearing in the “What’s New” section in the
Wall Street Journal, as well as three featured stories3 carried
on the Journal’s front page. The major criticism of their study
is that too few news stories are taken into consideration in
each day. It is rather simple to assume that stories carried
on the front page of the Wall Street Journal are enough
for summarizing and reﬂecting the information appear in
the whole newspaper. Interestingly, even with such a simple
setting, Klein and Prestbo found that the pattern of directional
correspondence, whether upwards or downwards, between the
ﬂow of the news stories and stock price movements manifested
itself 80% of the time. Their ﬁndings strongly suggest that
news stories and ﬁnancial markets tend to move together.

Fawcett and Provost [10] formulate an activity monitoring
task for predicting the stock price movements based on the
content of the news stories. Activity monitor task is deﬁned
as the problem that involves monitoring the behaviors of a
large population of entities for interesting events which require
actions. The objective of the activity monitoring task is to issue
alarms accurately and quickly. In the stock price movements
detection, news stories and stock prices for approximately
6,000 companies over three months period are archived. An
interesting event is deﬁned to be a 10% change in stock price
which can be triggered by the content of the news stories.
The goal is to minimize the number of false alarms and to
maximum the number of correctly predicted price spikes. It
is worth noting that, the authors only provide a framework
for formulating this predicting problem. The implementation
details and an in-depth analysis are both missing. Perhaps this
is because their main focus is not on examining the possibility
of detecting stock price movements based on news stories,
but is on outlining a general framework for formulating and
evaluating the problems which require continuous monitoring
their performance.

Thomas and Sycara [12] predict the stock prices by inte-
grating the textual information that are downloaded from the
web bulletin boards4 into trading rules. The trading rules are
derived by genetic algorithms based on numerical data. For the
textual data, a maximum entropy text classiﬁcation approach

2A ﬁnancial

index which composed of 30 blue-chip stocks listed on the

New York Stock Exchange.

3Klein and Prestbo did not describe in details how they selected these three

stories among all stories carried on the Journal’s front page.

4Thomas and Sycara chose Forty discussion boards

from www.

ragingbull.com

[19] is used for classifying the impacts5 of the posted messages
on the stock prices. For the trading rules, they are constructed
by genetic algorithms based on the trading volumes of the
stocks concerned, as well as the number of messages and
words posted on the web bulletin boards per day. A simple
market simulation is conducted. The authors reported that
the proﬁts obtained increased up to 30% by integrating the
two approaches rather than using either of them. However, no
analysis on their results is given.

Wuthrich et al. [13] develop an online system for predicting
the opening prices of ﬁve stock indices6 by analyzing the
contents of the electronic stories downloaded from the Wall
Street Journal. The analysis is done as follows: for each story,
keywords are extracted and weights are assigned to them
according to their signiﬁcance in the corresponding piece of
news story and on the corresponding day. By combining the
weights of the keywords and the historical closing prices of a
particular index, some probabilistic rules are generated using
the approach proposed by Wuthrich [20], [21]. Based on these
probabilistic rules, predictions on at least 0.5% price changes
are made. The weaknesses of their system is that only the
opening prices of ﬁnancial markets could be predicted. Some
others more challenging and interesting issues, such as intra-
day stock price predictions, could not be achieved.

Following the techniques proposed by Wuthrick et al., Per-
munetilleke and Wong [11] repeat the work but with different
a domain. News headlines (instead of news contents) are used
to forecast the intra-day currency exchange rate (instead of the
opening prices of stock indices). These news headlines belong
to world ﬁnancial markets, political or general economic news.
They show that on a publicly available commercial data set, the
system produces results are signiﬁcantly better than random
prediction.

Lavrenko et al. [9] propose a system for predicting the
intra-day stock price movements by analyzing the contents
of the real-time news stories. Analyst is developed based on
a language modeling approach proposed by Ponte and Croft
[22]. While a detailed architecture and a fruitful discussion
are both presented in their paper, the following questions
are unanswered: The authors claim that there should be a
period, t, to denote the time for the market to absorb any
new information (news stories) release, where t is deﬁned
as ﬁve hours. We have to admit that the market may spent
time to digest information. However, such a long period may
contradicts with most economic theories [1], [2]. In addition,
news stories may frequently classify to trigger both the rise
and drop movements of the stock prices in the training stage,
which is a dilemma. Finally, in their evaluations, the impact of
the news stories are “immediate” (without 5 hours time lag).
This contradicts to the training phase of the system.

III. THE PROPOSED SYSTEM

In this paper, we are interested in determining whether a
news story would have any impacts on the stock prices, and

5Two impacts are deﬁned in their paper: up and down.
6These ﬁ ve stock indices are: Dow Jones Industrial Average, Nikkei 225,

Financial Times 100 Index, Hang Seng Index and Singapore Straits Index

June 2005  Vol.5 No.1 

 

IEEE Intelligent Informatics Bulletin 

 

Fig. 1. The operation of the proposed system. After we received a news story from a news agent, we determine which of the three impact it has: positive,
negative or neutral. A news story is said to have positive impact (or negative impact) if the stock price rise (or drop) signiﬁcantly for a
period after the news story is released. If the stock price does not change after the news story is released, then the news story is regarded as neutral.

Fig. 2. The architecture of the proposed system. Four major processes are deﬁned: 1) News Stories Alignment; 2) Time Series Segmentation; 3) Time Series
Segmentation and 4) System Learning.

if so, what kinds of impact is this news story. Three impacts
are deﬁned: positive, negative and neutral. A news
story is said to have positive impact (or negative
impact) if the stock price rise (or drop) signiﬁcantly for
a period, T , after the news story has been broadcasted. If
the stock price does not change after the news story is
broadcasted, then the news story is regarded as neutral.
Figure 1 illustrates the motivation described here.

Figure 2 shows the architecture of the proposed system. For
any prediction system to operate successfully, we ﬁrst archive
and label some sets of data and present them to the system for
learning their relationships. These data are known as training
data. The training data that we have taken are real-time news
stories and intra-day stock prices. Since there are too many
news stories and stocks in the market, such that it is impossible
for us to read through the news stories one by one and classify
their impact manually, we therefore must have a heuristics for
selecting them automatically. We explain the details of the
system in the following sections.

A. News Stories Alignment

In order to obtain a set of reliable training data, we have
to correctly align the news stories to the stock trend such that
the aligned news stories is believed to trigger or support the
movements of the trends. For aligning news stories to the stock
time series, there could be three different formulation under
different assumptions. They are further explained below:

1) Formulation 1 – Observable Time Lag In this for-
mulation, there is a time lag between the news story is
broadcasted and the stock price moves. It assumes that

the stock market needs a long time for absorbing the
new information. Let us take Figure 3 (a) to illustrate this
idea. In this formulation, the Group X (news stories), is
responsible for triggering Trend B, while Group Y does
nothing with the two trends. Some reported works used
this representation [9], [11], [12].

2) Formulation 2 – Efﬁcient Market In this formulation,
the stock price moves as soon as after the new story
is released. No time lag is observed. This formulation
assumes that the market is efﬁcient and no arbitrary
opportunity normally exists. To illustrate this idea, let
us refer to Figure 3 (a). Under this formulation, Group
X is responsible for triggering Trend A, whild Group Y
is responsible for triggering Trend B.

3) Formulation 3 – Reporting In this formulation, new
stories are released only after the stock price has moved.
This formulation assumes that the stock price move-
ments are neither affected nor determined by any new
information. The information (e.g. news stories) are only
useful for reporting the situation but not predicting the
future. Again, let us use Figure 3 (a) to illustrate this
idea. Under this formulation, Group Y is responsible
for accounting why Trend A would happened. Group X
does nothing with the two trends.

if not

Different scholars may in favor of one of the formulation.
It
is difﬁcult,
impossible, for ﬁnding a completely
consensus. In this paper, we take the second formulation
(Formulation 2 – Efﬁcient Market), which is based on the
Efﬁcient Market Hypothesis. Thanks to Efﬁcient Market Hy-
pothesis, which states that the current market is an efﬁcient
information processor, such that it reﬂects the assimilation of

4                                                                                    Feature Article: Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun Lu
 

 

 

(a) An ideal situation.

General idea of the t-test based split and merge segmentation
Fig. 4.
algorithm. The splitting phase aims at discovering all of the possible trends on
the time series, while the merging phase aims at avoiding over-segmentation.

   

(b) The truth situation.

1: Ttmp = split(T [t0; tn]);
2: Tf inal = merge(Ttmp);
3: return Tf inal

Algorithm 1 segment(T ) – segment a time series T

Fig. 3. The alignment process. On the left: for the ideal situation, stories
broadcast under the time series should be responsible for the ﬂuctuation of
the time series in that period. On the right: in reality, stock time series exhibit
a high level of noise such that the impact of most stories are determined
incorrectly.

all of the information available immediately [23], [24], we
therefore align the news stories to the time series using the
second formulation.

More formally, let di be a news story; D denote all of
the news stories archived; DSk denote the documents that are
aligned to segment Sk; trel(di) denote the timestamp when the
document di is released; tbegin(Sk) and tend(Sk) denote the
timestamp of segment Sk begin and the timestamp of segment
Sk end, respectively. According to the second formulation that
that documents which are broadcasted within a segment are
aligned back to that segment:

di 2 fDSk j trel(di) (cid:21) tbegin(Sk) and trel(di) < tend(Sk)g
(1)
However, no matter which formulation we take, note that
all stock time series contain a high level of noise. Since every
stock time series contains a high level of noise, such that even
though the general trend is rising (or dropping), some dropping
(or rising) segments can be observed. If we simply align the
news stories based on the type of the time series segments
(rise or drop), wrong alignment must be resulted. Figure 3 (b)
illustrates this idea. In Figure 3 (b), even though the general
trends from ti
to tj is rising, the segment A2 is slightly
dropping. The news story releases under segment A2 should
be regarded as having positive impact (general trend)
rather than having negative impact (exact observation).
Similar situation is observed from tj to tk.

In order to remedy the above phenomenon, a higher level
re-describing the time series into trends is necessary, e.g.
re-describing Figure 3 (b) to Figure 3 (a) is necessary. This
process is also known as time series segmentation. We provide
our segmentation algorithm in the next section.

B. Time Series Segmentation

As with most data mining problems, data representation is
one of the major elements to reach an efﬁcient and effective
solution. Since all stock time series contains a high level
of noise, a high level time series segmentation is necessary
for recognizing the trends on the times series. A sound time
series representation involves issues such as recognizing the
signiﬁcant movements or detecting any abnormal behaviors,
so as to study and understand its underlying structure.

Piecewise linear segmentation, or sometimes called piece-
wise linear approximation, is one of the most widely used
technique for time series segmentation, especially for the
ﬁnancial time series [9], [25], [26]. It refers to the idea of
representing a time series of length n using K straight lines,
where K (cid:28) n [27], [28]. Most studies in this area are
pioneered by Pavlidis et al. [28] as well as Dedua and Harts
[29].

In this paper, we propose a t-test based split-and-merge
piecewise linear approximation algorithm. The splitting phase
aims at discovering trends on the time series, while the
merging phase aims at avoiding over-segmentation. Figure 4
and Algorithm 1 illustrate the general idea of the proposed
segmentation algorithm.4

June 2005  Vol.5 No.1 

 

IEEE Intelligent Informatics Bulletin 

Feature Article: The Predicting Power of Textual Information on Financial Markets  
  

   5

t-test Based Split and Merge Segmentation Algorithm – Split-
ting phase

Algorithm 2 split(T [ta; tb]) – split a time series T of length
n from time ta to time tb where 0 (cid:20) a < b (cid:20) n

(4)

Algorithm 3 merge(T ) – attempt
segments on the time series T

to merge two adjacent

1: Ttemp = ;
2: "min = 1;
3: "total = 0;
4: for i = a to b do
"i = (pi (cid:0) ^pi)2;
5:
if "min > "i then
6:

"min = "i;
tk = ti;

7:

8:
9:

end if
"total = "total + "i;

10:
11: end for
12: " = "total=(tb (cid:0) ta);
13: if t-test.reject(") then
14:

15:
16: end if
17: return Ttemp;

(2)

(3)

Ttemp = Ttemp[ split(T [ta; tk]);
Ttemp = Ttemp[ split(T [tk; tb]);

3:

1: while true do
"min = 1;
2:
repeat
i = 0
"i =
if "min > "i then

t0
i+2
j=t0
i

P

4:

5:

(pj (cid:0) ^pj)2;

"min = "i;
k = i + 1;

end if

until end of the time series
if t-test.accept("min) then

drop (tk; pk);

6:
7:

8:

9:
10:

11:
12:

13:

else

break;

end if

14:
15:
16: end while
17: return T

Let T = f(t0; p0); (t1; p1); : : : ; (tn; pn)g be a ﬁnancial time
series of length n, where pi is the price at time ti for i 2 [0; n].
Initially, the whole time series is regarded as a single large
segment, and is represented by a straight line joining the ﬁrst
and the last data points of the time series (Figure 4). In order
to decide whether this straight line (segment) can represent the
general trend of the time series, a one tail t-test is formulated:

where " is the expected mean square error of the straight line
with respect to the actual ﬂuctuation on the time series:

H0 : " = 0
H1 : " > 0

" =

1
k

(cid:1)

k

X
i=0

(pi (cid:0) ^pi)2

t =

"
^(cid:27)2=n

p

where k is the total number of data points within the segment,
^pi is the projected price of pi at time ti. The required t-
statistics is:

where ^(cid:27) is the standard deviation of the mean square error, ".
The t-statistics is therefore compared with the t-distribution
with n (cid:0) 1 degree of freedom using (cid:11) = 0:05. In other words,
there is a probability of 0.05 that the null hypothesis (H0 : " =
0 in Equation (2), would be accepted given that it is incorrect.
The motivation of this formulation is that if the null hypoth-
esis (H0 : " = 0) in Equation (2) is accepted, then the mean
square error between the actual data points and the projected
data points should be very small. Thus, the straight line, which
is formulated by joining the ﬁrst and the last data points of
the segment, should be well enough to represent the trends of
the data points in that segment. In contrast, if the alternative
hypothesis is accepted (H1 : " > 0), then a single straight line
is not well enough to represent the trend of the data points in
the corresponding segment.

Let us consider for the case where the null hypothesis is
rejected. If the null hypothesis is rejected, then the straight
line is split at the point where the error norm is maximum,
, and the whole process will be executed
i.e. maxi
recursively on each segment (Figure 4 (b) – (c)). Algorithm 2
outlines the procedure of the splitting phase.

(pi (cid:0) ^pi)2

(cid:9)

(cid:8)

t-test Based Split and Merge Segmentation Algorithm – Merg-
ing phase

After the splitting phase, over-segmentation will frequently
occur. Over-segmentation refers to the situation where there
exist two adjacent segments such that their slopes are similar,
and they should be merged to from a single large segment.
Let us refer to Figure 4 again. If we only perform the splitting
phase, four segments would be resulted. However, note that the
slopes of segment A2 and segment B1 are very similar. Hence,
merging them is possible. After merging A2 and B1, three
segments are remained. All of the segments now have different
slopes. In other words, merging phase aims at combining all

of the adjacent segments, provided that the mean square error,
", would still be accepted by the t-test after merging. The
hypothesis for the t-test is the same as Equation (2).

(cid:8)

(t0

time

i; p0

1; p0

0; p0

m; p0

0); (t0

i); (t0

i+1; p0

1); : : : ; (t0

More formally, consider the time series T which has
series Ttemp =
m)g of length m after the splitting

been transformed into another
f(t0
phase, such that m (cid:28) n. Deﬁne Si =
i+1)
(cid:9)
as a segment in Ttemp. If the null hypothesis over two adjacent
segments, Si and Si+1, is accepted, then these two segments
are regarded as a candidate merging pair. Let Lmerge be a
list containing all of these candidate merging pairs. One of the
candidate merging pair resides in Lmerge would be selected
to merge if merging of it would be resulted in the minimum
increase in the total error norm. The whole process is executed
continuously until the t-test over all of the segments on the
time series is rejected, i.e. Lmerge = (cid:30). Algorithm 3 illustrates

IEEE Intelligent Informatics Bulletin 

 

  June 2005  Vol.5 No.1 

 6                                                                                    Feature Article: Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun Lu 
 

TABLE I

A 2 (cid:2) 2 CONTINGENCY TABLE SUMMARIZED THE DISTRIBUTION OF
FEATURE fj IN THE DOCUMENT COLLECTION. THIS TABLE COULD BE
MODELED BY A (cid:31)2
DISTRIBUTION WITH ONE DEGREE OF FREEDOM.

#documents have fj

#documents do not have fj

Segment = Sk
Segment 6= Sk

case 1
case 2

case 3
case 4

Algorithm 4 select(D, T 0
m) – select positive training examples
from a collection of documents D given a segmented time
series T 0
m.
1: for each Sk in T 0
Lf eature;k = (cid:30);
2:
3: DSk = (cid:30);
4:

if trel(di) (cid:21) tbegin(Sk) and trel(di) < tend(Sk) then

m do

the whole procedure of merging phase.

C. Useful News Stories Selection

In reality, many news stories are valueless in prediction, i.e.
they do not contribute to the prediction of the stock prices.
In this section, we present how to select the valuable news
stories.

a

be

are

feature

story that

that news

in the news

Deﬁne f eatures to be any words in the news story
story
collection. Let fj
released
collection. Recall
within a segment are aligned back to that segment,
i.e.
di 2 fDSk j trel(di) (cid:21) tbegin(Sk) and trel(di) < tend(Sk)g.
By counting the presence or absence of fj appearing during
a given segment, a statistic model for discrete events could
be formulated. In such model, the frequency of any feature
appearing within the news story collection would be random
with unknown distribution. In a model that features are emitted
at a random process, two assumptions could be made: 1) The
process of generating the features is stationary; and 2) The
occurrence of every feature is independent of each other, i.e.
P (fa) = P (fajfb).

For the ﬁrst assumption, if a feature is stationary, then in
any arbitrary period, the probability of getting it is the same
as at any other periods. In other words, if the probability of
a feature appearing in some periods change dramatically, we
can conclude that this feature exhibit an abnormal behavior in
those periods, and it would be regarded as an important feature
in there. Speciﬁcally, by counting the number of documents
that: 1) contains feature fj and is in Segment Sk; 2) contains
feature fj but is not in Segment Sk; 3) does not contain feature
fj but is in segment Sk; and 4) does not contain feature fj
and is not in segment Sk, a 2 (cid:2) 2 contingency table could be
formulated (Table I). Note that this table could be modeled by
a (cid:31)2 distribution with one degree of freedom.

For the second assumption, it is known as the independent
assumption of feature distribution, which is a common as-
sumption in text information management, especially for infor-
mation retrieval, clustering and classiﬁcation. Researches show
that this assumption will not harm the system performance
[30], [31], [32], [33]. Indeed, maintaining the dependency of
features is not only extremely difﬁcult, but also may easily
degrade the system performance [34], [32], [33], [35].

For each feature fj under each segment Sk, we calculate
its (cid:31)2 value, i.e. (cid:31)2(fj; Sk). If it is above a threshold, (cid:11), i.e.
(cid:31)2(fj; Sk) (cid:21) (cid:11), we conclude that the occurrence of feature fj
in segment Sk is signiﬁcant, and this feature is appended into
a feature list, Lf eature;Sk . Lf eature;Sk stores all the features

Assign di to DSk ;

5:
end if
6:
7: end for
8: for each Sk in T 0
9:

m do

for each fi 2 DSk do
if (cid:31)2(fi) (cid:21) (cid:11) then

10:
11:

12:

20:

21:
22:

23:

append fi to Lf eature;k;

end if
end for

13:
14: end for
15: DR = (cid:30);
16: DD = (cid:30);
17: for each fj 2 Lf eature;k do
18:
19:

if slope(Sk) > 0 then

if fj 2 fdi j di 2 DSk g then

Assign di to DR;

else

end if

Assign di to DD;

end if
24:
25: end for

in which their occurrence in segment Sk are signiﬁcant:

fj 2 Lf eature;Sk if (cid:31)2(fj; Sk) (cid:21) (cid:11)

(5)

Deﬁne DR and DD be two sets containing the documents
that support the rise movement and drop movement, respec-
tively. Hence, these two sets are served as the positive training
examples for the rise and drop trends. A document, di, which
belongs to segment Sk (di 2 DSk ), would be assigned
to DR if and only if the slope of Sk is positive and di
contains a feature listed in Lf eature;Sk (i.e. di 2 DR iff fj 2
fLf eature;Sk and di 2 DSk g). Similar strategy applies to DD.
Note that for (cid:31)2 = 7:879, there is only a probability
of 0.005 that a wrong decision would be made such that a
feature from a stationary process would be identiﬁed as not
stationary, i.e. a random feature is wrongly identiﬁed as a
signiﬁcant feature. Hence, (cid:11) is set to 7.879. Besides, only the
features that appear in more than one-tenth of the documents
in the corresponding period would calculate their (cid:31)2 value.
This is because rare features are difﬁcult to estimate correctly
and this can reduce signiﬁcant computational cost. Algorithm
4 outlines the procedure of selecting positive training news
stories.

D. System Learning

that

Recall

in our training data,

two types of data are
available: DR and DD. DR and DD represents the training
documents correspond to the rise trend and the drop trend,6

June 2005  Vol.5 No.1 

 

IEEE Intelligent Informatics Bulletin 

Feature Article: The Predicting Power of Textual Information on Financial Markets  
  

  7

respectively. Let NR and ND be the number of documents in
DR and DD, respectively. For the sake of simplicity, let us
deﬁne X 2 fR; Dg.

Following the common practise of document preprocessing,
for each document in X, dj;X ;a vector space model is con-
structed to represent it [33]:

dT (cid:12) + (cid:12)0 = 0

(cid:24)(cid:3)
1

(cid:24)(cid:3)
4

(cid:24)(cid:3)
2

(cid:3)
3

(cid:24)

dj;X = hf0 : w0;X ; f1 : w1;X ; : : : ; fn : wn;X i

(6)

C = 1
k(cid:12)k

C = 1
k(cid:12)k

where fi is the ith feature in D and wi;X is the weight of fi
in X. wi;X indicates the importance of fi in dj;X . Follow the
existing works, we use a tf (cid:1) idf schema for calculating the
weights [36], [33]:

tfi;j (cid:1) logNX

NX
dfi;X

wi;X = 8
<
0

:

if dfi;X 6= 0;

if dfi;X = 0:

(7)

where tfi;j is the term frequency (i.e. the number of times
fi appears in dj) and dfi;X is the document frequency (i.e.
the number of documents contains fi in X). Finally, wi;X is
normalized to unit length so as to account for the differences
in the length of each document.

In this formulation, each feature is regarded as a single
dimension and the weight of the feature is regarded as the
coordinate for that dimension. In other words, each docu-
ment has n-dimension (<n), where n is the total number of
features in D. Thus, our training data consists NX pairs of
(d1;X ; y1;X ); (d2;X ; y2;X); : : : ; (dNX ;X ; yNX ;X ), with di;X 2
<n and yi;X 2 f(cid:0)1; 1g. This problem then reduced to a two
class pattern recognition problem in which we are trying to
ﬁnd a hyperplane:

f : DT

X (cid:12)X + (cid:12)0;X = 0 and

k (cid:12)X k= 1

(8)

which maximize the margin, CX , between the positive training
examples in X and negative training examples in X. Thus, this
problem reduced to the following optimization problem:

max

: CX

(cid:12)X ;(cid:12)0;X ;k(cid:12)X k=1

subject to :

yi;X (dT

i;X (cid:12)X + (cid:12)0;X ) (cid:21) CX ; 8i 2 X

By dropping the norm constraint on (cid:12)X , solving this problem
is equivalent to solve the following optimization problem:

min

(cid:12)X ;(cid:12)0;X

:

k (cid:12)X k

subject to :

yi;X (dT

i;X (cid:12)X + (cid:12)0;X ) (cid:21) 1; 8i 2 X

Since document vectors are very sparse, the two classes will
share a high overlapping region in the feature space. To deal
with it, slack variables, (cid:24)X = ((cid:24)1;X ; (cid:24)2;X ; : : : ; (cid:24)NX ;X),
is
introduced (Figure 5):

min

(cid:12)X ;(cid:12)0;X

:

subject to :

1
2
yi;X (dT

k (cid:12) k2 +C

(cid:24)i;X

NX

X
i=1

i;X (cid:12)X + (cid:12)0;X ) (cid:21) 1 (cid:0) (cid:24)i;X ; 8i 2 X

(9)

(10)

(11)

(12)

(13)

(14)
(15)

(cid:24)i;X = 0; 8i 2 X

Fig. 5. The basic concept of the classiﬁer in a two-dimensional situation.
The decision boundary is the solid line, while the broken lines bound the
maximal margin of width 2C. The points labeled (cid:24)(cid:3)
j are on the wrong side
of their margin by an amount (cid:24)(cid:3)
= C(cid:24)j . Points on the correct side have
j
(cid:24)(cid:3)
j = 0.

Constraint (14) requires that all training examples are classi-
ﬁed correctly up to some slack (cid:24)i;X . If a training example lies
on the wrong side of the hyperplane, the corresponding (cid:24)i;X
NX
is (cid:21) 1. Therefore
i=1 (cid:24)i;X is an upper bound on the number
of training errors.

P

Consequently, solving this optimization problem is equiva-
lent to solve a Support Vectors Machine (SVM) problem [37].
For computational reason, it is far more efﬁcient to convert the
above primal optimization problem to the Lagrangian (Wolfe)
dual optimization problem [17], [37]:

min : LX ((cid:11)) = (cid:0)

(cid:11)i +

Qij

(16)

NX

X
i=1

1
2

NX

NX

X
i=1

X
j=1

subject to :

(cid:11)i;X yi;X = 0

NX

X
i=1

Qij = (cid:11)i;X (cid:11)j;X yi;X yj;X xT

i;X xj;X

0 (cid:20) (cid:11)i;X (cid:20) C; 8i 2 X

The size of the optimization problem depends on the number
of training examples N . Deﬁning a matrix Q = yiyjxT
i xj.
Note that the size of Q is around N 2. For learning task with
thousand of features and thousand of training documents, it
becomes impossible to keep Q in memory. Standard imple-
mentations require either explicit storage of Q or re-compute
Q every time when it
is needed. However, this becomes
prohibitively expensive. In this paper, we applied the technique
describe by Joachims [38], which decompose the learning task
into several sub-tasks. The solution of (cid:12)X , (cid:12)0;X and (cid:24)i;X can
be computed as:

NX

X
i=1

(cid:12)X =

(cid:11)i;X yi;X di;X

(cid:12)0;X = ysv;X (cid:0) (cid:12)X dsv;X

(cid:24)X = maxf1 (cid:0) yi;X ((cid:12)X di;X + (cid:12)0;X ); 0g

where the pair (dsv;X ; ysv;X ) must be a support vector with
(cid:11)sv < C. Support vectors are those observations with the
coefﬁcient (cid:11)i;X 6= 0.

(17)

(18)

(19)

(20)

(21)

(22)

IEEE Intelligent Informatics Bulletin 

 

  June 2005  Vol.5 No.1 

 8                                                                                    Feature Article: Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun Lu 
 

(23)

V. PREDICTION EVALUATIONS

TABLE II

THE CATEGORIES POWERED BY REUTERS.

Category

Hotel

Financial
Properties

Utility

Category

Category

Industry (A-G)
Industry (H-O)
Industry (P-Z)

Germany

Consultant (A-G)
Consultant (H-O)
Consultant (P-Z)

Miscellaneous

E. System Operation

After the system training process, two classiﬁcation models
are generated in which they are responsible for determining
whether an unseen document would trigger the rise event and
drop event, respectively. Given the solutions of (cid:12)X and (cid:12)0;X,
the decision function for any unseen document, ^d, can be
written as:

GX ( ^d) = sign[f ( ^d)]

= sign[ ^dT (cid:12)X + (cid:12)0;X ]

If GR > 0 (GD < 0), then the unseen document, ^d is
classiﬁed as triggering the rise event (drop event). In other
words, ^d is believed to affect the time series such that it goes
upward (downward). If both GR and GD are < 0, then ^d
is classiﬁed as noise which means that ^d does nothing with
the time series. If both GR and GD are > 0, then the actual
impacts of ^d is ambiguous since it triggers both the rise and
drop events, which is impossible. In such a case, we would
ignore ^d also, and classiﬁed it as noise as well.

IV. EVALUATION

A prototype system using JavaTM is developed to evaluate
the proposed system. All of the experiments are conducted on
a Sun Blade-1000 workstation running Solaris 2.8 with 512MB
physical memory and with a 750MHz Ultra-SPARC-III CPU.
Intra-day stock prices and real-time news stories are archived
through Reuters Market 3000 Extra7 from 20th January 2003
to 20th June 2003. All data are stored into IBM DB2 Version
7.18.

For the real-time news stories, there are more than 350,000
documents archived. Note that Reuters has assigned to which
sectors, countries, etc, the news stories should belong. There-
fore, we do not need to worry about how these news stories
should be organized. All features from the news stories are
stemmed and converted to lower cases, in which punctuation
and stop-words are removed, numbers, web page addresses
and email addresses are ignored.

For the stock data, intra-day stock prices of all the Hong
Kong stocks are recorded9. The stocks belong to one of the
categories listed in Table II. According to the observations
given by the technical analysis that price movements asso-
ciated with light volumes denotes only temporal movements,
but not trends, thus, for each stock, the transactions that are
associated with light volumes (e.g. few hundred shares) are

7http://www.reuters.com
8http://www.ibm.com
9The stocks which have too few transaction records are ignored. This is

ignored. In order to account for the different price range of
different stocks, stock prices of all stocks are normalized.

A. Time Series Evaluations

Figure IV-A shows the typical results after applying the t-
test based split and merge segmentation algorithm on three
stocks. Due to the space limited, we report only three cases.
Other stocks behave in the similar way. The reported stocks
are: 1) Cheung Kong (0001.HK); 2) Cathay Paciﬁc (0293.HK)
and 3) TVB (0511.HK). The unmodiﬁed stock data are shown
on the top while the segmented data are shown on the bottom.
We can see that the trends generated are quite reasonable and
suitable. Note that the longest trend lasts for 2 weeks while
the shortest one lasts for 3 days. This means that all of the
trends generated are tertiary movements.

One of the best way to evaluate the reliability of a prediction
system is to conduct a market simulation which mimics the
behaviors of investors using real-life data. As a result, two
market simulations are conducted:10

(cid:15) Simulation 1: Proposed System: Shares are bought or
sold based solely on the content of the news stories. Two
strategies are adopted:

– For each stock, if the prediction of its upcoming
trend is positive, then shares of it are bought im-
mediately. The shares would be sold after holding
for m day(s).

– For each stock, if the prediction of its upcoming
trend is negative, then shares of that stock are sold
for short. The shares would be bought back after m
day(s).

An analysis of how m affects the evaluation results is
given in Section V-B. In this section, m is set to 3
working days for simplicity. If the market is closed when
the decision is made, then shares will be bought or sold
in the beginning of the next active trading day.

(cid:15) Simulation 2: Buy-and-Hold Test: For each stock,
shares of that stock is bought at the beginning of the
evaluation period. At the end of the evaluation period,
all of the shares remain on hand are sold. This simula-
tion serves as a base-line comparison which is used to
demonstrate the do-nothing strategy.

In the above market simulations, rate of return, r, is calculated.
As a result, how much shares are bought in each transaction
could be ignored.

A. Simulation Results

Table III shows the results of the market simulations.
From the table, Simulation 1 far outperforms Simulation 2.
In order to see whether the earnings from the proposed
system are statistically signiﬁcant, another 1,000 simulations
are conducted. In these simulations, the decisions of buying
and selling were made at the same time as the proposed

simply because there are not enough data for training and/or evaluation.

10The assumption of zero transaction cost is carried out

June 2005  Vol.5 No.1 

 

IEEE Intelligent Informatics Bulletin 

Feature Article: The Predicting Power of Textual Information on Financial Markets  
  

  9

Original data

Original data

Original data

e
c
i
r

P

e
c
i
r

P

e
c
i
r

P

e
c
i
r

P

Time

Time

Time

(a) 0001.HK Original

(b) 0293.HK Original

(c) 0511.HK Original

Segmented data

Segmented data

Segmented data

e
c
i
r

P

e
c
i
r

P

Time

Time

Time

(d) 0001.HK Segmented

(e) 0293.HK Segmented

(f) 0511.HK Segmented

Fig. 6. Before and after applying the t-test based split and merge segmentation algorithm. On the top: The original time series. On the bottom: The segmented
time series. Three stocks are selected to report here: (1) Cheung Kong (0001.HK); (2) Cathay Paciﬁc (0293.HK); and (3) TVB (0511.HK).

TABLE III

THE OVERALL EVALUATION RESULTS OF THE TWO MARKET SIMULATION.

HERE, r IS THE RATE OF RETURN.

Simulation 1

Simulation 2

Accumulative r
Stand. Dev. of r
Maximum r
Minimum r
Top ten average r
Least ten average r

18.06
3.40
12.42
-9.83
8.18
-3.69

-20.56
2.15
2.21
-18.10
1.11
-18.56

system, but without referencing to the contents of the news
stories, i.e. the decisions are random. We then compare the
cumulative earnings that are produced from the randomized
trials and Simulation 1. For the randomized system, there are
only 78 out of 1,000 trials that have rate of return exceed our
proposed work. Thus, the proposed system is signiﬁcant at the
0.5% level.

B. Hit Rate Analysis

Hit rate is another important measurement for the pre-
dictability of a forecasting system, especially for those kind of
systems that are similar to our proposed one. Hit rate analysis
can indicate how often the sign of return is correctly predicted.
Figure 7 illustrates this idea. Assume that at t0 a prediction
which states that the stock price will go upward is made. Since
from t0 to t1 (T1), the stock prices are above p0, we conclude
that the prediction is correct in this period, i.e. hit. However,
from t1 to t2 (T2), the stock prices are below p0, we therefore
conclude that the prediction is wrong in T2, i.e. missed. Thus,
if the prediction period is varied, different conclusion could be

Fig. 7. A simple diagram illustrates the meaning of hit rate.

TABLE IV

THE HIT RATE OF THE PROPOSED SYSTEM BY VARYING HOLDING PERIOD.

HERE, THE RETURN IS CALCULATED IN RATE OF RETURN.

Hit Rate

Acc. Return

S.D. of Return

1 day (m = 1)
3 day (m = 3)
5 day (m = 5)
7 day (m = 4)

51.0%
61.6%
65.4%
55.7%

6.58
18.06
21.49
7.22

1.147
3.400
4.135
3.791

drawn. In other words, the value of m in the market simulation
presented in Section V-A is a critical factor.

Table IV shows the hit rate and the rate of return of the
proposed system by varying the value of m. The accumulative
return and hit rate both increase as m increase. It suggests
that the system is most stable and suitable for applying the
prediction within 3-5 days. It also suggests that such kinds of
movements should be tertiary movements.

A careful examination of the prediction results would realize

IEEE Intelligent Informatics Bulletin 

 

  June 2005  Vol.5 No.1 

10                                                                                   Feature Article: Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun Lu 
 

that one of the major reasons for making error is that two news
stories may be very similar in their contents, but have totally
different implications.

VI. CONCLUSION

Scholars and professionals from different areas have shown
that there is a high relationship between the news stories and
the behaviors of the ﬁnancial markets. In this paper, we revisit
the problem and use real-time news stories and intra-day stock
prices for our study. These data are chosen because they are
readily available and the evaluation results obtained can easily
be veriﬁed.

Several data mining and text mining techniques are in-
corporate in the system architecture. The tertiary movements
on the stock price movements are identiﬁed by a novel
piecewise linear approximation approach: a t-test based split
and merge segmentation algorithm. News stories are aligned
to the stock trends basic on the idea of Efﬁcient Market
Hypothesis. A document selection heuristics that is based on
the (cid:31)2 estimation is used for selecting the positive training
documents. Finally, the relationship between the contents of
the news stories and trends on the stock prices are learned
through support vectors machine. Different experiments are
conducted to evaluate various aspect of the proposed system.
In particular, a market simulation using real-life data is con-
ducted. Encouraging results are obtained in all of the experi-
ments. Our study show that there is a high relationship between
news stories and the movements of stock prices. Furthermore,
by monitoring this relationship, actionable decisions could be
made also.

REFERENCES

[1] P. A. Adler and P. Adler, “The market as collective behavior,” in The
Social Dynamics of Financial Markets, P. A. Adler and P. Adler, Eds.
Jai Press Inc., 1984, pp. 85–105.

[2] H. Blumer, “Outline of collective behavior,” in Readings in Collective
Behavior, 2nd ed., R. R. Evans, Ed. Chicago: Rand McNally College
Pub. Co, 1975, pp. 22–45.

[3] J. M. Clark, “Economics and modern psychology,” Journal of Political

Economy, vol. 26, pp. 136–166, 1918.

Sons, Inc., 2002.

[5] L. Festinger, A theory of cognitive dissonance. Stanford, Calif.: Stanford

Univesity Press, Reprinted in 1968.

[6] M. Klausner, “Sociological theory and the bechavio of ﬁnancial mar-
kets,” in The Social Dynamics of Financial Markets, P. A. Adler and
P. Adler, Eds.

Jai Press Inc., 1984, pp. 57–81.

[7] G. P. C. Fung, J. X. Yu, and W. Lam, “News sensitive stock trend predic-
tion,” in Proceedings of the 6th Paciﬁc-Asia Conference on Knowledge
Discovery and Data Mining, Taipei, Taiwan, 2002, pp. 289–296.

[8] ——, “Stock prediction: Integrating text mining approcah using real-
time news,” in Proceedings of the 7th IEEE International Conference
on Computational Intelligence for Financial Engineering, Hong Kong,
China, 2003, pp. 395–402.

[9] V. Lavrenko, M. D. Schmill, D. Lawire, P. Ogivie, D. Jensen, and
J. Allan, “Mining of concurrent text and time series,” in Proceedings
of the 6th International Conference on Knowledge Discovery and Data
Mining Workshop on Text Mining, Boston, MA, USA, 2000, pp. 37–44.
[10] T. Fawcett and F. J. Provost, “Activity monitoring: Noticing interesting
changes in behavior,” in Proceedings of the 5th International Conference
on Knowledge Discovery and Data Mining, San Diego, California, USA,
1999, pp. 53–62.

[11] D. Permunetilleke and R. K. Wong, “Currency exchange rate forecasting
from news headlines,” in Proceedings of the 13h Australian Database
Conference, Melbourne, Australia, 2002, pp. 131–139.

Wiley and Sons, Inc., 1999.

5th ed. Springﬁeld, 1966.

Publishing, 1903.

[12] J. D. Thomas and K. Sycara, “Integrating genetic algorithms and text
learning for ﬁnancial prediction,” in Proceedings of the Genetic and
Evolutionary Computing 2000 Conference Workshop on Data Mining
with Evolutionary Algorithms, Las Vegas, Nevada, USA, 2000, pp. 72–
75.

[13] B. Wuthrich, D. Permunetilleke, S. Leung, V. Cho, J. Zhang, and
W. Lam, “Daily prediction of major stock indices from textual www
data,” in Proceedings of the 4th International Conference on Knowledge
Discovery and Data Mining, New Youk, USA, 1998, pp. 364–368.

[14] R. J. Bauerm Jr and J. R. Dahlquist, Technical Market Indicators.

John

[15] R. D. Edwards and J. Magee Jr, Technical Analysis of Stock Trends,

[16] S. A. Nelson, The ABC of Stock Market Speculation, 3rd ed.

Fraser

[17] N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector
Cambridge

Machines and Other Kernel-based Learning Methods.
University Press, 2002.

[18] F. Klein and J. A. Prestbo, News and the Market.

Chicago: Henry

Regenry, 1974.

[19] K. Nigam, J. Lafferty, and A. McCallum, “Using maximunm entropy
for text classiﬁcation,” in Proceeding of the 16th International Joint
Conference Workshop on Machine Learning for Information Filtering,
Stockholm, Sweden, 1999, pp. 61–67.

[20] B. Wuthrich, “Probabilistic knowledge bases,” IEEE Transactions of

Knowledge and Data Engineering, vol. 7(5), pp. 691–698, 1995.

[21] ——, “Probabilistic knowledge bases,” International Journal of Intelli-
gent Systems in Accounting Finance and Management, vol. 6, pp. 269–
277, 1997.

[22] J. M. Ponte and W. B. Croft, “A language modeling approach to infor-
mation retrieval,” in Proceedings of the 21th International Conference
on Research and Development in Information Retrieval, Melbourne,
Australia, 1998.

[23] P. A. Adler and P. Adler, The Social Dynamics of Financial Markets.

Jai Press Inc, 1984.

[24] W. J. Eiteman, C. A. Dice, and D. K. Eiteman, The Stock Market,

forth ed. McDGraw-Hill Book Company, 1966.

[25] Y. Qu, C. Wang, and X. S. Wang, “Supporting fast search in time series
for movement patterns in multiples scales,” in Proceedings of the 7th
International Conference on Information and Knowledge Management,
Bethesda, Maryland, USA, 1998, pp. 251–258.

[26] C. Wang and X. S. Wang, “Supporting content-based searches on time
series via approximation,” in Proceedings of
the 12th International
Conference on Scientiﬁc and Statistical Database Management, Berlin,
Germany, 2000, pp. 69–81.

[27] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani, “An online algorithm
for segmenting time series,” in Proceedings of the 1st IEEE International
Conference on Data Mining, San Jose, California, USA, 2001, pp. 289–
296.

[28] T. Pavlidis and S. L. Horowitz, “Segmentation of plane curves,” IEEE

Transactions on Computers, vol. c23(8), pp. 860–870, 1974.

[29] R. O. Duda and P. E. Harts, Pattern Classiﬁcation and Scene Analysis.

[30] P. Domingos and M. Pazzani, “On the optimality of the simple bayesian
classiﬁer under zero-one loss,” Machine Learning, vol. 29(2-3), pp. 103–
130, 1997.

[31] D. D. Lewis, “A sequential algorithm for training text classiﬁers,” in
Proceedings of
the 17th International Conference on Research and
Development in Information Retrieval, Dublin, Ireland, 1994, pp. 3–12.
[32] ——, “The independence assumnption in information retrieval,” in
Proceedings of the 10th European Conference on Machine Learning,
Chemnitz, Germany, 1998, pp. 4–15.

[33] F. Seabastiani, “Machine learning in automated text categorization,”

ACM Computing Surverys, vol. 34(1), pp. 1–47, 2002.

[34] W. B. Croft, “Boolean queries and term dependencies in probabilistic
the American Society for Information

retrieval models,” Journal of
Science, vol. 37(2), pp. 71–77, 1983.

[35] C. J. van Rijsbergen, “A theoretical basis for the use of co-occurrence
data in information retrieval,” Journal of Documentation, vol. 33(2), pp.
106–119, 1977.

[36] G. Salton and C. Buckley, “Term-weighting approaches in automatic
text retrieval,” Information Process Management, vol. 24(5), pp. 513–
523, 1998.

[37] V. Vapnik, The Nature of Statistical Learning Theory. Springer, 1995.
[38] T. Joachims, “Making large-scale svm learning practical,” Computer
Science Department, University of Dortmund, Tech. Rep. LS-8 (24),
1998.

[4] L. Tvede, The Psychology of Finance, revised ed.

John Wiley and

New York: Wiley, 1973.

June 2005  Vol.5 No.1 

 

IEEE Intelligent Informatics Bulletin 

