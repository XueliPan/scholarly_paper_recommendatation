A Balanced Ensemble Approach to Weighting Classi(cid:2)ers for

Text Classi(cid:2)cation

Gabriel Pui Cheong Fung1, Jeffrey Xu Yu1, Haixun Wang2, David W. Cheung3, Huan Liu4

1 The Chinese University of Hong Kong, Hong Kong, China, fpcfung; yug@se:cuhk:edu:hk

2 IBM T. J. Watson Research Center, New York, USA, haixun@us:ibm:com
3 The University of Hong Kong, Hong Kong, China, dcheung@cs:hku:hk

4 Arizona State University, Arizona, USA, hliu@asu:edu

Abstract

This paper studies the problem of constructing an effec-
tive heterogeneous ensemble classi(cid:2)er for text classi(cid:2)ca-
tion. One major challenge of this problem is to formu-
late a good combination function, which combines the de-
cisions of the individual classi(cid:2)ers in the ensemble. We
show that the classi(cid:2)cation performance is affected by three
weight components and they should be included in deriv-
ing an effective combination function. They are: (1) Global
effectiveness, which measures the effectiveness of a mem-
ber classi(cid:2)er in classifying a set of unseen documents; (2)
Local effectiveness, which measures the effectiveness of a
member classi(cid:2)er in classifying the particular domain of
an unseen document; and (3) Decision con(cid:2)dence, which
describes how con(cid:2)dent a classi(cid:2)er is when making a deci-
sion when classifying a speci(cid:2)c unseen document. We pro-
pose a new balanced combination function, called Dynamic
Classi(cid:2)er Weighting (DCW), that incorporates the afore-
mentioned three components. The empirical study demon-
strates that the new combination function is highly effective
for text classi(cid:2)cation.

1 Introduction

Let U be a set of unseen documents and C be a set of
prede(cid:2)ned categories. Automated text classi(cid:2)cation is the
process of labeling U with C , such that every d 2 U will
be assigned to some of the categories in C . Note that d can
be assigned to none of the categories in C . If the number
of categories in C is more than two (jC j > 2), it is a multi-
label text classi(cid:2)cation problem. Since every multi-label
text classi(cid:2)cation problem can be transformed to a binary-
label text classi(cid:2)cation problem, we focus on the binary
problem in this paper (jC j = 2). Let c 2 C . Binary-label
text classi(cid:2)cation is to construct a binary classi(cid:2)er, denoted
by F((cid:1)), for c such that:

F(d) =

1
(cid:0)1

(

if f (d) > 0;
otherwise;

where F(d) = 1 indicates that d belongs to c and F(d) =
(cid:0)1 indicates that d does not belong to it.
f ((cid:1)) 2 ´ is a
decision function. Every classi(cid:2)er, Fi, has its own decision
function, fi((cid:1)). If there are m different classi(cid:2)ers, there will
be m different decision functions. The goal of constructing
a binary classi(cid:2)er, F((cid:1)), is to approximate the unknown true
target function (cid:11)F((cid:1)), so that F((cid:1)) and (cid:11)F((cid:1)) are coincident as
much as possible [17].

In order to improve the effectiveness, ensemble classi-
(cid:2)ers (a.k.a classi(cid:2)er committee) were proposed [1, 3, 5, 6,
7, 8, 9, 15, 16, 17, 18, 19]. An ensemble classi(cid:2)er is con-
structed by grouping a number of member classi(cid:2)ers. If the
decisions of the member classi(cid:2)ers are combined properly,
the ensemble is robust and effective. There are two kinds of
ensemble classi(cid:2)ers: homogeneous and heterogeneous.

A homogeneous ensemble classi(cid:2)er contains m binary
classi(cid:2)ers in which all classi(cid:2)ers are constructed by the
same learning algorithm. Bagging and boosting [19] are
two common techniques [1, 15, 16, 18].

A heterogeneous ensemble classi(cid:2)er contains m binary
classi(cid:2)ers in which all classi(cid:2)ers are constructed by differ-
ent learning algorithms (e.g., one SVM classi(cid:2)er and one
kNN classi(cid:2)er are grouped together) [19]. The individual
decisions of the classi(cid:2)ers in the ensemble are combined
(e.g., through stacking [19]):

Q(d) =

if g

1
(cid:0)1 otherwise;

(cid:0)

(

F1(d); F2(d); : : : ; Fm(d)

> 0;

(2)

(cid:1)

where Q((cid:1)) is an ensemble classi(cid:2)er; g((cid:1)) is a combination
function that combines the outputs of all Fi((cid:1)). The effec-
tiveness of the ensemble classi(cid:2)er, Q((cid:1)), depends on the ef-
fectiveness of g((cid:1)). In this paper, we concentrate on ana-
lyzing heterogeneous ensemble classi(cid:2)ers. Our problem is
thus to examine how to formulate a good g((cid:1)).

Four widely used g((cid:1)) are: (1) Majority voting (MV)
[8, 9]; (2) Weighted linear combination (WLC) [7]; (3) Dy-
namic classi(cid:2)ers selection (DCS) [3, 8, 6, 5]; and (4) Adap-
tive classi(cid:2)ers combination (ACC) [8, 9]. Except for MV,
the other three functions assign different weights to the clas-
si(cid:2)ers in the ensemble. The bigger the weight, the more ef-

(1)

1

Proceedings of the Sixth International Conference on Data Mining (ICDM'06)0-7695-2701-9/06 $20.00  © 2006accurate (effective) than Naive Bayes (NB) [20]). Although
it does not imply all of the decisions made by SVM must
be superior than NB, it does imply that we should value the
judgment of SVM higher than that of NB in general. In this
paper, we term this kind of effectiveness as global effective-
ness of a classi(cid:2)er, denoted by ai (E.g. aSVM > aNB). ai
gives us good insight about how to weight the classi(cid:2)ers in
an ensemble. Intuitively, if we construct an ensemble clas-
si(cid:2)er by grouping Fa((cid:1)) and Fb((cid:1)) together, where aa > ab,
then we should value Fa((cid:1)) higher than Fb((cid:1)).

Yet, a globally effective classi(cid:2)er may sometimes per-
form poorly on some speci(cid:2)c dataset (domain). As an ex-
ample, consider two classi(cid:2)ers, SVM and NB. According to
the benchmark Reuters21578, the micro-F1 scores for SVM
and NB are respectively 0.860 and 0.788. Unfortunately,
the F1 score for SVM when classifying Retail (Retail (cid:26)
Reuters21578) is 0.0, but it is 0.667 for NB. As a result,
an effective classi(cid:2)er may not always perform well in all
domains (e.g., SVM performs poorly in Retail). This can
be further illustrated in Figure 1. The two ovals, A and B,
represent two different domains. Oval A covers over the
decision boundary, whereas Oval B resides in the lower tri-
angle. All of the documents within the domain of Oval A
are aligned near the decision boundary. An unseen docu-
ment that belongs to this domain may easily be classi(cid:2)ed
wrongly. On the other hand, the documents within the do-
main of Oval B are well separated by the decision boundary.
An unseen document that belongs to this domain will most
likely be classi(cid:2)ed correctly. So, the effectiveness of the
classi(cid:2)er also relies on the domain of the unseen data. We
term this kind of effectiveness as local effectiveness of the
classi(cid:2)er, denoted by bi. bi helps us to adjust the weights of
the classi(cid:2)ers in the ensemble. If the ai of Fi is very high
but it is not effective in classifying the domain of the unseen
document, we should re-consider its effectiveness.

For every decision a classi(cid:2)er makes, one may ask how
con(cid:2)dent the classi(cid:2)er is about the decision? Consider the
two unseen documents, document 1 and document 2, in the
same domain (Oval B) in Figure 1. While both document
1 and document 2 reside near the boarder of their domain,
document 2 locates closer to the decision boundary (the
dashed line) whereas document 1 locates far away from it.
Since both document 1 and document 2 belong to the same
domain, the local effectiveness of the classi(cid:2)er upon them
are the same. Yet, the con(cid:2)dence in making a correct deci-
sion for document 1 should be higher than that of document
2, as document 1 is further away from the decision boundary
(d1 > d2). In this paper, we term it as decision con(cid:2)dence.
It is estimated according to the distance between the unseen
document and the decision boundary.

We summarize the needs for the above components as
follows: if we ignore ai, over-(cid:2)tting may result as we ne-
glect the combined in(cid:3)uence of all domains. If we ignore

Figure 1. Illustration of local effectiveness and deci-
sion con(cid:2)dence.

fective is that classi(cid:2)er. In MV, all classi(cid:2)ers in the ensem-
ble are equally weighted. It can end up with a wrong deci-
sion if the minority votes are signi(cid:2)cant. WLC assigns static
weights to the classi(cid:2)ers based on their performance on a
validation data. However, a generally well-performed clas-
si(cid:2)er can perform poorly in some speci(cid:2)c domains. For in-
stance, the micro-F1 scores of SVM and Naive Bayes (NB)
for the benchmark Reuters21578are respectively 0:860 and
0:788. In this sense, SVM excels NB. Yet, for the categories
Potatoand Retail in Reuters21578, the F1 scores for NB are
both 0.667, but are both 0.0 for SVM. DCS and ACC weight
the classi(cid:2)ers by partitioning the validation data (domain
speci(cid:2)c), they do not combine the classi(cid:2)ers’ decisions, but
select one of the classi(cid:2)ers from the ensemble and rely on
it solely. We will show in the experiments that this will lead
to inferior results.

In this paper, we propose a new combination function
called Dynamic Classi(cid:2)ers Weighting (DCW). We consider
three components when combining classi(cid:2)ers: (1) Global
Effectiveness, which is the effectiveness of a classi(cid:2)er in an
ensemble when it classi(cid:2)es a set of unseen documents; (2)
Local effectiveness, which is the effectiveness of a classi(cid:2)er
in an ensemble when it classi(cid:2)es the particular domain of
the unseen document; and (3) Decision con(cid:2)dence, which
is the con(cid:2)dence of a classi(cid:2)er in making a decision of the
ensemble for a speci(cid:2)c unseen document.

2 Motivations

Let F1((cid:1)); F2((cid:1)); : : : ; Fm((cid:1)) be m different binary classi-
(cid:2)ers and f1((cid:1)), f2((cid:1)); : : : ; fm((cid:1)) be their corresponding deci-
sion functions. Conceptually, Fi((cid:1)) divides the entire do-
main into two parts according to fi((cid:1)). Figure 1 illustrates
this idea. The dashed lines are the decision boundaries. If
the unseen document, d, falls into the upper (lower) trian-
gle, it would be labeled as positive (negative). Usually, if d
is further away from the decision boundary, the decision of
d by Fi(d) is more con(cid:2)dent.

Every classi(cid:2)er has different effectiveness. For instance,
Support Vectors Machine (SVM) is being regarded as more

2

Proceedings of the Sixth International Conference on Data Mining (ICDM'06)0-7695-2701-9/06 $20.00  © 2006bi, over-generalization may ensue as it relies on the domain
where the unseen document appears. ai and bi do not mea-
sure the classi(cid:2)er’s decision con(cid:2)dence, gi is proposed as
it indicates how much con(cid:2)dence a classi(cid:2)er has when it
classi(cid:2)es the unseen documents.

3 Dynamic Classi(cid:2)ers Weighting (DCW)

In the previous section, we have explained why the three
weight components (ai, bi and gi) are helpful in construct-
ing an effective combination function, g((cid:1)). We now de-
scribe how they are estimated and how they are combined
in an ensemble classi(cid:2)er.

ai is the effectiveness of the classi(cid:2)er when we use it
to classify a set of unseen documents. During the training
phase, although we do not have a set of labeled unseen doc-
uments, we can estimate ai from the training data, D: we
estimate ai by 10-folded cross validation. While our expe-
rience suggested that estimating the effectiveness of a clas-
si(cid:2)er based on cross validation would always yield an op-
timistic result than evaluating it from the unseen data, this
would not be a problem in our situation, as we are not target-
ing for evaluating the real global effectiveness of the clas-
si(cid:2)ers, but aiming at obtaining the relative global effective-
ness. We normalize ai such that 0 < ai < 1 and (cid:229)m
i=1 ai = 1.
bi is the effectiveness of the classi(cid:2)er when we use it to
classify the domain of the unseen document, d. For an un-
seen document, we would never know what the true domain
of d is. As above, we can only estimate its domain accord-
ing to the training data, D. Let D be a subset of documents
in the training data, i.e., D (cid:18) D. We can (cid:2)nd the domain
of the unseen document, d, by using D, to extract the docu-
ments in D that are similar to d. Accordingly, the extraction
of D is based on a nearest neighbor strategy. We extract
the top n documents that are most similar to d from D. The
value n can be readily obtained through a validation dataset.
The similarities among these n documents are measured by
the cosine coef(cid:2)cient [13]. Since D is a subset of the train-
ing data (D (cid:18) D), we will know precisely the labels of those
documents that appear in D. We estimate bi by evaluating
D using the F1 score. bi is normalized such that 0 < bi < 1
and (cid:229)m

i=1 bi = 1.

gi is a measure about how con(cid:2)dent the classi(cid:2)er is when
it makes a decision upon d. From Eq. (1), the classi(cid:2)ca-
tion decision of the classi(cid:2)er, Fi((cid:1)), is based on the decision
function, fi((cid:1)). For most cases, if not all, the higher the mag-
nitude of fi((cid:1)), the more con(cid:2)dent are their decisions. Con-
sequently, we can compute gi by using the decision function,
fi((cid:1)). Unfortunately, the range of fi((cid:1)) varies among differ-
ent algorithms. For example, Fi((cid:1)) may have fi((cid:1)) in the
range of [(cid:0)1; 1], whereas F j((cid:1)) may have another f j((cid:1)) in
the range of ((cid:0)¥; +¥). Since different decision functions
have different ranges, a direct comparison among them is

inappropriate. We solve the problem as follows: Let D be
the domain of the unseen document. D is obtained by the
technique described previously. We compute gi as follows:

gi =

(cid:181)i =

;

fi(d)
(cid:181)i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
jDj (cid:229)
(cid:12)
(cid:12)
d02D

fi(d0);

(3)

(4)

m
(cid:229)
i

m
(cid:229)
i

where (cid:181)i is the average con(cid:2)dence of the decisions made
by fi((cid:1)) among the documents in D. Since D (cid:18) D, we can
presume that (cid:181)i is non-zero. When gi > 1, fi(d), has more
than average con(cid:2)dence to make a correct classi(cid:2)cation on
d, where d will be far away from the decision boundary
(e.g., document 1 in Figure 1). When gi < 1, the decision
function, fi(d), has less than average con(cid:2)dence to make
a correct classi(cid:2)cation on d, where d will be closer to the
decision boundary (e.g., document 2 in Figure 1). We nor-
malize gi such that 0 < gi < 1 and (cid:229)m

i=1 gi = 1.

We now present how ai, bi and gi are combined. Assume
In the most

that there are m classi(cid:2)ers in the ensemble.
simplest form, the combination function, g((cid:1)) is:

g((cid:1)) =

decisioni;

(5)

where decisioni = Fi(d) 2 f1;-1g (Eq. (eq:c)). Here, all
classi(cid:2)ers in the ensemble are equally weighted (i.e. MV).
In DCW, since a con(cid:2)dence (gi) is associated with each
decisioni, therefore:

g((cid:1)) =

decisioni (cid:2) gi:

(6)

Yet, even for a con(cid:2)dent decision, we need to review
whether the classi(cid:2)er, which makes this decision, is effec-
tive in the ensemble. Consequently:

g((cid:1)) =

decisioni (cid:2) gi (cid:2) effectivenessi:

(7)

Since there are two kinds of effectiveness for each of the
classi(cid:2)er (ai and bi), we have:

g((cid:1)) =

Fi(d) (cid:2) ai (cid:2) bi (cid:2) gi

;

(8)

(cid:17)

4 Experimental Study

The purpose of the experiments is twofold. (1) We want
to examine how effective the Dynamic Classi(cid:2)ers Weight-
ing (DCW) is, when it is compared with the other kinds
of heterogeneous ensemble classi(cid:2)ers. As such, we imple-
mented four existing ensemble classi(cid:2)ers for comparison:

m
(cid:229)
i

m
(cid:229)
i (cid:16)

3

Proceedings of the Sixth International Conference on Data Mining (ICDM'06)0-7695-2701-9/06 $20.00  © 2006No.

Combination

Reuters21578

Newsgroup20

1
2
3
4
5
6
7
8
9
10
11

S+N
S+R
S+K
R+N
K+N
K+R

S+K+R
S+K+N
S+R+N
K+R+N

S+K+R+N

(cid:150)
(cid:150)
(cid:150)
(cid:150)
(cid:150)
(cid:150)

MV WLC
0.874
0.883
0.862
0.833
0.824
0.825
0.879
0.874
0.872
0.825
0.851

0.872
0.855
0.852
0.857

(cid:150)

DCS
0.859
0.862
0.862
0.831
0.827
0.832
0.862
0.859
0.861
0.837
0.861

ACC
0.852
0.874
0.843
0.821
0.820
0.821
0.876
0.865
0.856
0.823
0.859

DCW
0.876
0.885
0.863
0.834
0.829
0.831
0.882
0.873
0.874
0.830
0.853

(cid:150)
(cid:150)
(cid:150)
(cid:150)
(cid:150)
(cid:150)

MV WLC
0.817
0.800
0.813
0.762
0.780
0.762
0.815
0.819
0.815
0.782
0.720

0.776
0.783
0.777
0.775

(cid:150)

DCS
0.761
0.762
0.763
0.738
0.746
0.764
0.763
0.762
0.761
0.750
0.762

ACC
0.794
0.793
0.780
0.759
0.769
0.760
0.812
0.809
0.801
0.775
0.761

DCW
0.817
0.800
0.815
0.765
0.780
0.765
0.816
0.821
0.815
0.784
0.763

Table 1. The results of the micro-F1 for different ensemble classi(cid:2)ers.

Majority voting (MV) [8, 9], Weighted linear combination
(WLC) [7], Dynamic classi(cid:2)ers selection (DCS) [3, 8, 6, 5],
and Adaptive classi(cid:2)ers combination (ACC) [8, 9]. We re-
port the results in Section 4.1. (2) We want to understand
how signi(cid:2)cant the results are whenever one of the ensem-
ble classi(cid:2)ers outperforms the others. As such, we per-
formed a pairwise signi(cid:2)cant test in Section 4.2.

In the

experiments,

two benchmarks are used:
Reuters21578 and Newsgroup20.
For Reuters21578,
we separate the dataset into training data and testing data
using the ModApte split [2]. For Newsgroup20, for each of
the categories, we randomly select 80% of the postings as
training data, and the remaining as testing data.

For the data preprocessing, punctuation, numbers, web
page addresses, and email addresses are removed. All fea-
tures are stemmed and converted to lower cases, and are
weighted using the standard tf(cid:1) idf schema [14]. Features
that appear in only one document are ignored. All features
are ranked based on the NGL Coef(cid:2)cient[12], and the top X
features are selected. This X is tuned for different classi(cid:2)ers
and for different benchmarks.

For creating the ensemble classi(cid:2)ers, different combi-
nations of four kinds of classi(cid:2)ers are used: (1) Support
Vectors Machine (SVM); (2) k-Nearest Neighbor (kNN);
(3) Rocchio (ROC); (4) Naive Bayes (NB). Their default
settings are as follows: For SVM, we use linear kernel
with C = 1:0. No feature selection is required [4]. For
kNN, we set k = 50 and select 2,750 and 4,900 features for
Reuters21578 and Newsgroup20. For ROC, we implement
the version in [11] and selects 2,750 and 7,500 features for
Reuters21578 and Newsgroup20. For NB, we implement
the multinomial version [10] and selects 2,750 and 9,500
features for Reuters21578and Newsgroup20.

4.1 Effectiveness Analysis

Table 1 shows the results of the micro-F1 score for all en-
semble classi(cid:2)ers (MV, WLC, DCS, ACC and DCW) when
they are created using different combinations of the binary

classi(cid:2)ers for both benchmarks. The left most column de-
notes which of the binary classi(cid:2)ers are used for creating
the corresponding ensemble classi(cid:2)er. We use S, K, R and
N to denote SVM, kNN, Rocchio and Naive Bayes, respec-
tively. For example, S+K+R represents an ensemble classi-
(cid:2)er which is comprised of SVM, kNN and Rocchio. Note
that MV cannot be created if the number of binary clas-
si(cid:2)ers in the ensemble is an even number, hence the (cid:147)(cid:150)(cid:148)
entries in Table 1.

At the (cid:2)rst glance, the results are promising. DCW, the
proposed approach, dominates over all other approaches
when they are being created using the same set of binary
classi(cid:2)ers. Similar results are obtained when we use the
macro-F1 score. The only case where DCW performs in-
ferior is case 6 when DCW is created by kNN and Roc-
chio (K+R), meanwhile it is evaluated using Reuters21578.
Its micro-F1 is 0.831, which is 0.001 lower than DCS (Dy-
namic Classi(cid:2)ers Weighting). Nevertheless, such a differ-
ence can be negligibled.

Concerning DCW, the best combination of binary clas-
si(cid:2)ers in the ensemble is SVM and Rocchio (case 2) for
Reuters21578. The micro-F1 score is 0:885. It is also the
best results obtained among all of the ensemble classi(cid:2)ers
that we have evaluated. For Newsgroup20the best result is
obtained by comprising SVM, kNN and Rocchio together
(case 8). The micro-F1 score is 0:821. It is also the best
result obtained among all approaches.

For MV, its philosophy is to take the majority agreement
among the binary classi(cid:2)ers in the ensemble. Hence, the
number of binary classi(cid:2)ers must be an odd number. So we
can only create MV using three different binary classi(cid:2)ers.
Interestingly, all combinations perform similarly.

best

combination

Concerning WLC,

the
for
Reuters21578 (case 2),
its micro-F1 score is 0.883,
which is higher than all ensemble classi(cid:2)ers (except DCW).
For Newsgroup20, similar observations are made, where
its best combination is case 8. Although the idea of WLC
is very simple (cid:150) assigns static weights to the classi(cid:2)ers in
the ensemble according to their global effectiveness and

4

Proceedings of the Sixth International Conference on Data Mining (ICDM'06)0-7695-2701-9/06 $20.00  © 2006combines them linearly (cid:150) it performs surprisingly well.
Another interesting (cid:2)nding is that when SVM is included in
the ensemble, the effectiveness of WLC would be increased
dramatically. This suggests that the choice of the classi(cid:2)ers
in WLC is particularly important.

its

score

best micro-F1

Concerning DCS,

for
Reuters21578 (case 2) is 0:862 only.
It is far lag be-
hind all the other approaches. For Newsgroup20, none of
the F1 score is higher than 0:77. We believe that the reasons
of why DCS performs poorly are because: (1) It does not
combine the classi(cid:2)ers’ decisions. Rather, it selects one of
the classi(cid:2)er in the ensemble and relies on it completely.
(2) It neither pays attention to the global effectiveness of
the classi(cid:2)ers nor the decision con(cid:2)dence.

ACC performs slightly better than DCS. This may be be-
cause the decision strategy for ACC is more sophisticated
that DCS. The best ensembles for Reuters21578and News-
group20 are both case 7. However, these results are all in-
ferior than both WLC and our DCW.

4.2 Signi(cid:2)cant Test

In this section, we conduct a pairwise comparison among
them using the signi(cid:2)cant test [20]. Given two classi(cid:2)ers,
FA((cid:1)) and FB((cid:1)), the signi(cid:2)cant test determines whether
FA((cid:1)) performs better than FB((cid:1)) based on the errors that
FA((cid:1)) and FB((cid:1)) made. Let N be the total number of the
unseen documents, and ai = f0; 1g (bi = f0; 1g) indicate
whether FA((cid:1)) (FB((cid:1))) makes a correct classi(cid:2)cation upon
the ith unseen document. ai = 0 means FA((cid:1)) makes an in-
correct classi(cid:2)cation whereas ai = 1 means FA((cid:1)) makes a
correct one. Similar de(cid:2)nition is also applied to bi. Let
da be the number of times that FA((cid:1)) performs better than
FB((cid:1)), and db be the number of times that FB((cid:1)) performs
better than FA((cid:1)). In this test, the null hypothesis is that both
classi(cid:2)ers perform the same (H0 : da = db). The alternative
is that FA((cid:1)) and FB((cid:1)) performs differently (H1 : da 6= db).
Table 2 shows the results of comparing the performance
of DCW with the other ensemble classi(cid:2)ers. A (cid:29) B means
A performs signi(cid:2)cantly better than B (P-Value (cid:20) 0:01).
A > B means A performs slightly better than B. A (cid:24) B means
no evidence indicates A and B has any differences in terms
of the errors they made. A summary is given below:

Reuters21578: fDCW, WLCg > fMV, ACCg (cid:29) DCS
Newsgroup20: DCW > WLC > ACC (cid:29) MV (cid:29) DCS

5 Conclusions

In order to formulate an effective combination function
for heterogeneous ensemble classi(cid:2)er, three weight compo-
nents are necessary: Global Effectiveness, Local Effective-
ness, and Decision Con(cid:2)dence. We compare DCW with

5

Reuters21578

Newsgroup20

B

A
MV WLC
DCS
MV
ACC
MV
DCW
MV
WLC
DCS
ACC
WLC
DCW
WLC
ACC
DCS
DCW
DCS
ACC
DCW

<
(cid:29)
(cid:24)
(cid:28)
(cid:29)
>
(cid:24)
(cid:28)
(cid:28)
<

(cid:28)
(cid:29)
(cid:28)
(cid:28)
(cid:29)
>
<

(cid:28)
(cid:28)
(cid:28)

Table 2. Results of the signi(cid:2)cant test.

four other kinds of heterogeneous ensemble classi(cid:2)ers us-
ing two benchmarks. The results indicated that DCW can
effectively balance the contributions of the three compo-
nents and outperforms the existing approaches.

References

[1] W. W. Cohen and Y. Singer. Context-sensitive learning methods for text categorization. ACM Transactions on

Information Systems (TOIS), 17(2):141(cid:150)173, 1999.

[2] F. Debole and F. Sebastiani. An analysis of the relative hardness of Reuters-21578 subsets. Journal of the

American Society for Information Science and Technology, 56(6):584(cid:150)596, 2004.

[3] G. Giacinto and F. Roli. Adaptive selection of image classi(cid:2)ers. In Proceedings of the 9th International Confer-

ence on Image Analysis and Processing (ICIAP’97), pages 38(cid:150)45, Florence, Italy, 1997.

[4] T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In Pro-
ceedings of 10th European Conference on Machine Learning (ECML’98), pages 137(cid:150)142, Chemnitz, Germany,
1998.

[5] K. B. Kevin Woods, W. Philip Kegelmeyer. Combination of multiple classi(cid:2)ers using local accuracy estimates.

IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 19(4):405(cid:150)410, 1997.

[6] W. Lam and K.-Y. Lai. A meta-learning approach for text categorization. In Proceedings of the 24th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01), pages
303(cid:150)309, New Orleans, Louisiana, USA, 2001.

[7] L. S. Larkey and W. B. Croft. Combining classi(cid:2)ers in text categorization. In Proceedings of the 19th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’96), pages
289(cid:150)297, Zurich, Switzerland, 1996.

[8] Y. H. Li and A. K. Jain. Classi(cid:2)cation of text documents. The Computer Journal, 41(8):537(cid:150)546, 1998.

[9] R. Liere and P. Tadepalli. Active learning with committees for text categorization.

In Proceedings of 14th

National Conference on Arti(cid:2)cial Intelligence (AAAI’97), pages 591(cid:150)596, Providence, Rhode Island, 1997.

[10] A. McCallum and K. Nigam. A Comparison of Event Models for Naive Bayes Text Classi(cid:2)cation. In The 15th
National Conference on Arti(cid:2)cial Intelligence (AAAI’98) Workshop on Learning for Text Categorization, 1998.

[11] A. Moschitti. A study on optimal parameter tuning for rocchio text classi(cid:2)er. In Proceedings of the 25th European

Conference on Information Retrieval Research (ECIR’03), pages 420(cid:150)435, Pisa, Italy, 2003.

[12] H. T. Ng, W. B. Goh, and K. L. Low. Feature selection, perception learning, and a usability case study for
text categorization. In Proceedings of the 20th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR’97), pages 67(cid:150)73, Philadelphia, PA, USA, 1997.

[13] E. Rasmussen. Clustering algorithm. In W. B. Freakes and R. Baeza-Yates, editors, Information Retrieval Data

Structures & Algorithms, pages 419(cid:150)442. Prentice Hall PTR, 1992.

[14] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information Processing and

Management (IPM), 24(5):513(cid:150)523, 1988.

[15] R. E. Schapire and Y. Singer. BoosTexter: a boosting-based system for text categorization. Machine Learning,

39(2(cid:150)3):135(cid:150)168, 2000.

[16] R. E. Schapire, Y. Singer, and A. Singhal. Boosting and Rocchio applied to text (cid:2)ltering. In Proceedings of
the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR’98), pages 215(cid:150)223, Melbourne, Australia, 1998.

[17] F. Seabastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1(cid:150)47, 2002.

[18] S. M. Weiss, C. Apte, F. J. Damerau, D. E. Johnson, F. J. Oles, T. Goetz, and T. Hampp. Maximizing text-mining

performance. IEEE Intelligent Systems, 14(4):63(cid:150)69, 1999.

[19]

I. H. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann,
second edition, 2005.

[20] Y. Yang and X. Liu. A re-examination of text categorization methods.

In Proceedings of the 22nd Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’99), pages
42(cid:150)49, Berkeley, California, USA, 1999.

Proceedings of the Sixth International Conference on Data Mining (ICDM'06)0-7695-2701-9/06 $20.00  © 2006