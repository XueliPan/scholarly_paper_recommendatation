Learning the Bayesian Network Structure: Dirichlet Prior versus Data

Harald Steck

Computer-Aided Diagnosis (IKM CKS)

Siemens Medical Solutions, 51 Valley Stream Parkway E51

Malvern, PA 19355, USA
harald.steck@siemens.com

Abstract

In the Bayesian approach to structure learning
of graphical models, the equivalent sample size
(ESS) in the Dirichlet prior over the model pa-
rameters was recently shown to have an impor-
tant effect on the maximum-a-posteriori estimate
of the Bayesian network structure.
In our ﬁrst
contribution, we theoretically analyze the case
of large ESS-values, which complements previ-
ous work: among other results, we ﬁnd that the
presence of an edge in a Bayesian network is
favoured over its absence even if both the Dirich-
let prior and the data imply independence, as
long as the conditional empirical distribution is
notably different from uniform.
In our second
contribution, we focus on realistic ESS-values,
and provide an analytical approximation to the
‘optimal’ ESS-value in a predictive sense (its
accuracy is also validated experimentally): this
approximation provides an understanding as to
which properties of the data have the main effect
determining the ‘optimal’ ESS-value.

1

INTRODUCTION

The posterior probability and the marginal likelihood of the
graph are one of the most popular scoring functions for
learning Bayesian network structures, e.g. [Buntine, 1991,
Heckerman et al., 1995]. Both of them require the value
of a free parameter to be speciﬁed by the researcher: the
so-called equivalent sample size (ESS), which originates
from the Dirichlet prior over the model parameters, cf.
[Buntine, 1991, Heckerman et al., 1995]. In the elaborate
experiments on 20 UCI data sets in [Silander et al., 2007],
it was shown that
the chosen ESS-value has a deci-
sive effect on the resulting maximum-a-posteriori (MAP)
graph estimate:
in the experiment shown in Figure 1
in [Silander et al., 2007], the number of edges increases
monotonically from zero to the maximum as the ESS-

value grows.
These two extremes (empty and com-
plete graph) are indeed reached for several data sets, and
they are almost reached for the remaining data sets in
[Silander et al., 2007].

The effect of the ESS-value on the MAP graph estimate
was earlier noticed in [Steck & Jaakkola, 2002], and one
of its main contributions was the theoretical analysis of the
case of small ESS-values. That paper also provided a result
for the case ESS→ ∞: the Bayes factor converges to zero
in the limit, which means that it favours neither the pres-
ence nor the absence of an edge. Given this inconclusive
behavior in the limit, our ﬁrst contribution in this paper is
concerned with the case of large but ﬁnite ESS-values: in
our theoretical analysis in Section 3, we derive the proper-
ties of the given data under which the presence of an edge
in the graph is favoured. This contribution concerning the
case of large but ﬁnite ESS-values, combined with the re-
sults for small ESS-values [Steck & Jaakkola, 2002], im-
plies immediately that, under these conditions, the number
of edges increases when the ESS-value grows, although not
necessarily monotonically. Our second contribution is con-
cerned with the case of realistic / intermediate ESS-values
(Section 4): our goal is to understand as to which proper-
ties of a given data set determine the ‘optimal’ ESS-value
(in the predictive sense). We achieve this by deriving an ex-
plicit analytical approximation (its validity is also assessed
experimentally).

2 BRIEF REVIEW OF BAYESIAN

NETWORK LEARNING

A Bayesian network model comprises a directed acyclic
graph G, and the model parameters θ are conditional prob-
abilities, e.g., see [Cowell et al., 1999] for an overview. It
describes a joint probability distribution p(X|G) over the
vector of random variables X = (X1, ..., Xn), where n is the
number of variables. We consider discrete random vari-
ables Xi with a multinomial distribution in this paper. Ac-
cording to the graph G, the joint distribution factors recur-
sively, p(X|G) = ∏n
i=1 θXi|Πi; Πi denotes the set of parents

of variable/node Xi in the graph G; the (joint) states of Πi
will be denoted by πi, and the states of Xi by xi.

Various approaches to learning the graph G from given
data D have been developed in the literature, e.g., see
[Cowell et al., 1999] for an overview. The Bayesian ap-
proach appears to be most popular. As a scoring function,
it employs the posterior probability of the graph G given the
data, p(G|D) ∝ p(D|G)p(G), or the marginal likelihood of
the graph, p(D|G). Both are equivalent if the prior dis-
tribution over the graphs, p(G), is chosen to be uniform.
The marginal likelihood of the graph can be expressed in
closed form when using the Dirichlet prior over the model
parameters [Buntine, 1991, Heckerman et al., 1995]. More
importantly, the Dirichlet prior is the only distribution that
ensures likelihood equivalence [Heckerman et al., 1995], a
desirable property in structure learning. In particular, like-
lihood equivalence holds only if the hyper-parameters αxi,πi
of the Dirichlet prior can be expressed as αxi,πi = α · qxi,πi
for all xi, πi and for all i = 1, ..., n; q is a prior distribution
over X, and α is a positive constant independent of i, the
so-called scale-parameter or equivalent sample size (ESS).
When q is chosen to be a uniform distribution, one arrives
at the BDeu score [Buntine, 1991]: qxi,πi = 1/(|Xi| · |Πi|),
where | · | denotes the number of (joint) states of the vari-
able(s). Hence,

αxi,πi =

α

,

|Xi| · |Πi|

so that the ESS α is the only remaining free parameter in
the marginal likelihood of the graph G: p(D|α, G) equals
[Buntine, 1991, Heckerman et al., 1995]

n
∏
i=1

Γ(απi)

∏
πi

Γ(Nπi + απi) ∏

xi

Γ(Nxi,πi + αxi,πi)

Γ(αxi,πi)

,

(2)

where Γ is the Gamma function; the ﬁrst product extends
over all the variables, while the second one is over all the
joint states πi of the parents Πi of variable Xi, and the third
product is over all the states xi of Xi. The cell counts Nxi,πi
in the contingency tables serve as sufﬁcient statistics; the
sample size is N = ∑xi,πi Nxi,πi. In this Bayesian approach
with the Dirichlet prior, the regularized parameter estimates
are [Buntine, 1991]

¯θxi|πi ≡ Ep(θxi|πi

|D,G)[θxi|πi] =

Nxi,πi + αxi,πi

Nπi + απi

,

(3)

which is the expectation with respect to the parameter’s
posterior distribution.

For ﬁxed data D and ESS α, ﬁnding the maximum-a-
posteriori (MAP) estimate of the graph G, or the maximum
with respect to the marginal likelihood in Eq. 2, is an NP-
complete problem [Chickering, 1996]. One thus has to re-
sort to heuristic search strategies, like local search (e.g.,
[Heckerman et al., 1995]), as to ﬁnd a close-to-optimal
graph. If the number of variables is not prohibitively large,

the exact solution, i.e., the globally optimal graph G, can
be found in reasonable computation-time due to recent ad-
vances in structure learning [Silander & Myllym¨aki, 2006,
Koivisto & Sood, 2004].

Besides this popular Bayesian score, various other scor-
ing functions have been devised for structure learn-
ing,
including the Bayesian information criteria (BIC)
[Schwarz, 1978, Haughton, 1988], the Akaike Information
Criteria (AIC) [Akaike, 1973], and the Minimum Descrip-
tion Length (MDL) [Rissanen, 1978]. We will use the AIC
in our second contribution in Section 4.2.

3 LARGE ESS-VALUE: ITS EFFECT ON

THE LEARNED GRAPH

In this section we present our ﬁrst contribution, namely
as to how a large but ﬁnite value of
the ESS af-
fects the learned optimal Bayesian network structure.
This complements the theoretical results for small ESS-
values derived in [Steck & Jaakkola, 2002] and the exper-
imental results for ‘intermediate’ ESS-values obtained in
[Silander et al., 2007].

3.1 UNIFORMITY-MEASURE

(1)

In this section, we deﬁne a new ‘uniformity’ measure and
discuss its properties. This measure (and its properties) de-
termines the result of structure learning for large ESS val-
ues, as we will see in the next section.

Deﬁnition: We deﬁne the uniformity-measure U of a condi-
tional multivariate distribution p(A, B|Π) over two random
variables A and B given a set of variables Π, as follows:

U(p(A, B|Π))
= ∑
a,b,π

p(a, b, π)

(cid:16)

|A, B, Π| · p(a, b, π) − |A, Π| · p(a, π)

−|B, Π| · p(b, π) + |Π| · p(π)

(4)

(cid:17)

,

where |A, B, Π| denotes the number of joint states.

Obviously, this deﬁnition is equivalent to:

U(p(A, B|Π)) = |Π| ·∑
π

p(π)2 ∑

a,b

p(a, b|π)

(cid:16)
·

|A, B| · p(a, b|π) − |A| · p(a|π) − |B| · p(b|π) + 1

= |A, B, Π| · ∑
a,b,π
−|B, Π| · ∑
b,π

p(a, b, π)2 − |A, Π| · ∑
a,π
p(π)2,

p(b, π)2 + |Π| ·∑

π

p(a, π)2

(cid:17)

(5)

where U is rewritten in terms of conditional probabilities
in the ﬁrst line, and in terms of squared probabilities in the
second line.

The interesting property of U (in each representation) is
that it is a weighted sum of four terms, where the weights
are the number of (joint) states of the variables.

Proposition 1: The measure U has the following three
basic properties:

• symmetry: U(p(A, B|Π)) = U(p(B, A|Π))

• non-negativity: U(p(A, B|Π)) ≥ 0

• minimality: U(p(A, B|Π)) = 0 if and only if

– (conditional) independence:
p(A, B|Π) = p(A|Π)p(B|Π)

– and, for each state π with p(π) > 0, at least
one of the marginal distributions is uniform:
p(A|π) = 1/|A| or p(B|π) = 1/|B|.

Concerning the minimality, note that U > 0 if a state π
with p(π) > 0 exists such that neither one of p(A|π) or
p(B|π) is uniform: this includes distributions where A and
B are conditionally independent, but both distributions are
skewed. In other words, U is not necessarily equal to zero
for independent variables.

Proof: The symmetry is obvious. As to proof the other
two properties, we focus on the representation in terms of
conditional probabilities in Eq. 5; obviously, if these prop-
erties hold for each state π, then they hold also for U, the
sum. As the remainder of the proof is understood to be con-
ditional on a state π with p(π) > 0, we now omit π from
the notation. A necessary condition for minimization of
U under the normalization-constraint ∑a,b p(a, b) = 1 (ac-
counted for by introducing the Lagrange multiplier λ ), is
that all the ﬁrst partial derivatives vanish, i.e., for all states
a, b:

2|A, B|p(a, b) − 2|A|p(a) − 2|B|p(b) + λ = 0

(6)

With the normalization constraint it follows immediately
that λ = 2. It follows for the particular choice of consid-
ering the difference between each pair of these equations
pertaining to the same state b, but different states a and a0:

p(a, b) − p(a0, b) = [p(a) − p(a0)]/|B|

Hence, if p(a) = p(a0) for all a, a0, then p(A) is uniform
and hence p(A, B) = p(A)p(B) with arbitrary p(B). Oth-
erwise, i.e., if p(a) 6= p(a0) for some a, a0, then p(A, B) =
p(A)p(B), where p(B) has to be uniform, and p(A) can be
arbitrary. Conversely, it can be veriﬁed easily that such a
distribution indeed fulﬁlls the original condition in Eq. 6.
Moreover, U = 0 for such a distribution. Finally, it can be
shown that the second derivative is positive deﬁnite, which
(cid:3)
completes the proof.

also like to mention that each of the four terms themselves
is related to well-known quantities: the squared probabili-
ties in Eq. 5 suggest a relationship to the well-known Gini
index, HG(p(X)) = 1 − ∑x p2
x, which is used as an impu-
rity measure when learning decision trees. It can also be
viewed as a special case of the Tsallis entropy HT
β (p(X)) =
(1 − ∑x pβ
x )/(β − 1) with parameter β = 2 [Tsallis, 2000].
The Tsallis entropy is used in statistical physics, and can
be understood as the leading-order Taylor expansion of the
Renyi entropy, HR
x )/(1 − β ), which is
a generalization of the Shannon entropy. In the limit β → 1,
the Tsallis entropy and the Renyi entropy both coincide
with the Shannon entropy.

β (p(X)) = (log ∑x pβ

3.2 LEADING-ORDER APPROXIMATION OF

BAYES FACTOR

In this section, we theoretically analyze the behavior of
the Bayesian approach to structure learning, as reviewed
in Section 2, for the case of large but ﬁnite ESS-values.

As to determine the most likely graph structure, note that
the marginal likelihood p(D|α, G+) of a graph G+ is im-
portant only relative to the marginal likelihood p(D|α, G−)
of a competing graph G−. In particular, we consider two
graphs in the following that are identical except for the
edge A ← B, which is present in G+ and absent in G−.
Let Π denote the parents of A in graph G−; this implies
that, in G+, the parents of A are Π and B. The presence
of the edge A ← B given the parents Π is favoured over
its absence (i.e., graph G+ over G−) if the log Bayes fac-
tor log p(D|α, G+)/p(D|α, G−) is positive; and if negative,
the absence of A ← B is favoured. Given complete data
(i.e., no missing data), the marginal likelihood factorizes
(cf. Eq. 2), so that most terms in the Bayes factor cancel
out, and it depends only on the variables A and B and the
parents Π:

log

p(D|α, G+)
p(D|α, G−)

= ∑
a,b,π

log

Γ(Na,b,π + αa,b,π )

Γ(αa,b,π )

Γ(Na,π + αa,π )

Γ(Nb,π + αb,π )

log

− ∑
b,π

Γ(αb,π )

log

−∑
a,π

log

+∑
π

Γ(αa,π )

Γ(Nπ + απ )

Γ(απ )

(7)

Note that this Bayes factor is symmetric in A and B, as
expected for independence tests; the asymmetry of the edge
A ← B is caused by Π being the parents of A (rather than
of B) in the graph. Now we can present our main result for
large but ﬁnite ESS-values:

Aside: Besides the interesting fact that U is a weighted
sum where the number of states function as the weights, we

Proposition 2: Given the BDeu score, the leading-order
approximation of the Bayes factor of the two graphs G+

and G− deﬁned above reads for large ESS-values (α (cid:29) N):

log

p(D|α, G+)
p(D|α, G−)

(

=

N
2α

N ·U( ˆp(A, B|Π)) − dF

+ O(N2/α 2)

(8)

)

where U is the uniformity measure as deﬁned above,
ˆp(A, B|Π) is the empirical distribution implied by the data,
i.e., ˆp(a, b|π) = Na,b,π /Nπ , and dF = |Π|(|A| − 1)(|B| − 1)
are the (well known) degrees of freedom.1

Before we give the proof at the end of this section, let us
ﬁrst discuss interesting insights that follow from this ap-
proximation:

First, note that if U were replaced by the (conditional)
mutual information I,
then the term N · I − dF would
be identical
to the Akaike Information Criteria (AIC)
[Akaike, 1973]. This analogy suggests that, in Eq. 8, dF
serves as a penalty for model complexity. In other words,
N · U( ˆp) − dF > 0 means that U( ˆp) is notably (or signiﬁ-
cantly) larger than zero, while N · U( ˆp) − dF < 0 refers to
U( ˆp) being not notably larger than zero. Given the proper-
ties of the new measure U (as discussed in Section 3.1), it
hence follows directly:

Corollary 1: Given the BDeu score, there exists a value
α + ∈ R such that for all ﬁnite α > α + the presence of
the edge A ← B given the parents Π is favoured over its
absence if N · U( ˆp(A, B|Π)) − dF > 0, i.e., if the empirical
distribution ˆp implies

• a notable dependence between A and B given Π,

• or a notable skewness (i.e., non-uniformity) of both
distributions ˆp(A|Π) and ˆp(B|Π). Note that A and B
can be conditionally independent here.

ESS-value is chosen, assuming that, for each variable con-
ditioned on any set of parents, the data implies notable de-
pendence or a sufﬁciently skewed distribution. Note, how-
ever, that the latter condition may not necessarily hold for
large sets of parents when the given data set is small; this is
because many zero-cell counts occur if the joint number of
states of the variables is larger than the number of samples
in the data.

Proof of Proposition 2: Each of the four terms in the
7 takes the form ∑y log(Γ(Ny +
Bayes factor in Eq.
αy)/Γ(αy)), where y denotes a (joint) state y of a set Y of
random variables. If αy (cid:29) Ny, we obtain for each y (using
Γ(z) = (z − 1)! in the ﬁrst line):

log

Γ(Ny + αy)

Γ(αy)

=

log(k − 1 + αy)

=

log αy +

Ny
∑
k=1

k − 1 + αy

αy

= Ny log αy +

+ O(N2

y /α 2
y )

log

Ny
∑
k=1
Ny
∑
k=1
Ny
∑
k=1
Ny(Ny − 1)

k − 1
αy

2αy

= Ny log αy +

+ O(N2

y /α 2

y ).

(9)

Inserting this approximation into the log Bayes factor of
Eq. 7, we obtain (note that αa,b,π = α/|A, B, Π| for the
BDeu score, where | · | denotes the number of joint states of
the random variables):

log

p(D|G+)
p(D|G−)

Na,b,π log

= ∑
a,b,π
1
2α ∑

+

a,b,π

αa,b,π απ
αb,π αa,π

Na,b,π

(cid:0)|A, B, Π| · Na,b,π − |A, Π| · Na,π

−|B, Π| · Nb,π + |Π| · Nπ

(cid:1)

(cid:0)|A, B, Π| − |A, Π| − |B, Π| + |Π|(cid:1)

(10)

Conversely, the absence of the edge A ← B given the par-
ents Π is favoured for all large values α (cid:29) N if N ·
U( ˆp(A, B|Π)) − dF < 0, i.e., if the empirical distribution
implies that

Na,b,π

−

1
2α ∑
+O(N2/α 2)

a,b,π

• A and B are not notably dependent given Π,

• and for each state π with p(π) > 0 there is one
marginal distribution ˆp(A|π) or ˆp(B|π) that is not no-
tably different from a uniform distribution.

where the ﬁrst
term vanishes because αA,B,Π is uni-
form for the BDeu score, the second term equals N2 ·
U( ˆp(A, B|Π))/(2α),2 and the third term equals dF ·
(cid:3)
N/(2α).

3.3

ILLUSTRATION

Second, given that this applies to any edge in the graph, it
follows immediately that the complete graph achieves the
largest marginal likelihood if a sufﬁciently large (but ﬁnite)

1Here, dF must not be corrected for zero-cell counts, as it orig-
inates from the prior in the BDeu score, cf. Eq. 10. Note that this
is different from deff

G in Section 4.2.

This section provides a simple example that shows how the
combination of a uniform (prior) distribution and a skewed
empirical distribution can create dependence while each
individual distribution implies independence. The key is

2Because of Na,b,π = N · ˆp(a, b, π).

4 UNDERSTANDING THE OPTIMAL

ESS-VALUE

Having theoretically analyzed the cases of large ESS-
values in the ﬁrst part of this paper, and the case of small
ESS-values in [Steck & Jaakkola, 2002], the remainder of
this paper focuses on the ‘optimal’ ESS-value (in a pre-
dictive sense): we tackle the question as to why the ‘op-
timal’ ESS-value is about 50 for some of the 20 UCI data
sets in the elaborate experiments in [Silander et al., 2007]
(their results are also reproduced in columns α M and α I in
our Table 1 for comparison), while it ranges between 2 and
13 for the remaining data sets. This section aims to provide
an answer to this question, i.e., the goal is to understand
as to which properties of the data determine the ‘optimal’
ESS-value.

4.1 OPTIMIZATION OF GRAPH AND ESS

Given that
the ESS-value can have a crucial effect
on the learned network structure [Silander et al., 2007,
Steck & Jaakkola, 2002], we now depart from the ortho-
dox Bayesian approach of choosing the prior—including
the ESS-value— without having seen the data. In the re-
mainder of this paper we treat the ESS α as a latent random
variable to be learned in light of the data D. Various ob-
jective functions for determining the ‘optimal’ ESS value
α were discussed in [Silander et al., 2007], and found to
yield essentially the same result. In this paper, we choose
to jointly optimize the graph structure and the ESS-value:

(α ∗, G∗) = arg max
(α,G)

p(D|α, G)

(11)

While joint optimization of Eq. 11 is computationally
very expensive, a simple coordinate-wise ascent is more
tractable. This approach comprises two alternate update-
steps in each round k = 0, 1, ... (until convergence):

1. optimize the graph for a ﬁxed ESS-value:

G∗

k = arg maxG p(D|α ∗

k−1, G),

2. optimize the ESS-value for a ﬁxed graph:

α ∗
k = arg maxα p(D|α, G∗
k).

The ﬁrst step can be solved by any standard algorithm for
structure learning (cf. the review in Section 2). Concern-
ing the initialization of the graph G0 in this iterative algo-
rithm, there are several options. A convenient choice is to
learn the graph G0 that optimizes the Bayesian Information
Criteria (BIC) [Schwarz, 1978, Haughton, 1988]. This cri-
teria not only is a large-sample approximation to the log
marginal likelihood in Eq. 2, but it also is independent of
ESS, i.e., it does not contain a free parameter. It can hence
be used for initialization of G0 when no initial ESS-value
is known, and promises to yield an initial graph G0 that is

Figure 1: This example illustrates how the dependence in-
creases with the skewness implied by the data for several
ESS-values: 100, 10 , 1 (solid curves, from top to bottom);
1,000 and 10,000 (dashed curves, from top to bottom).

that, in this Bayesian approach with the Dirichlet prior, the
log Bayes factor is essentially based on a weighted sum
of two distributions (see Section 2 and Eq. 3):
the em-
pirical distribution ˆp (with weight N) and the uniform dis-
tribution q of the ’virtual’ data points due to the Dirich-
let prior (with weight α). Obviously, the sum of two
distributions (where each implies independence) does not
necessarily result in a distribution that implies indepen-
dence: N ˆp(A) ˆp(B) + αq(A)q(B) 6= c · p(A)p(B) for any
distribution p and any constant c in general. The devi-
ation from this equality is typically not statistically sig-
niﬁcant for uniform q and skewed ˆp as long as the ESS-
value α is sufﬁciently small; but as the ESS-value α in-
creases in this weighted sum, the deviation can become
statistically signiﬁcant, as illustrated in the following ex-
ample: let us consider an artiﬁcial data set D with N = 100
samples implying independence between two binary vari-
ables A and B, i.e., the empirical distribution factorizes
like ˆp(A, B) = ˆp(A) ˆp(B); for simplicity, we assume that
ˆp(A) = ˆp(B). We control the skewness of the marginal
distributions with a parameter z ∈ [0, 0.5]:
ˆp(A = 1) = z
and ˆp(A = 0) = 1 − z; z = 0.5 implies uniform distribution,
while the skewness of the distribution grows as z decreases.
Values in the range [0.5, 1] are symmetric to the case con-
sidered here. Figure 1 illustrates how the log Bayes fac-
tor (cf. Eq. 7) increases from negative values (implying
independence) to positive values (suggesting dependence)
as the empirical distribution becomes increasingly skewed
(i.e., as z decreases). Figure 1 shows that, as the ESS-value
increases, the degree of skewness of ˆp has to diminish as
to prevent the log Bayes factor from implying dependence.
As an aside, note that the curves are quite ﬂat for large and
small ESS-value where either one of the distributions domi-
nates the sum; in contrast, the maximum at z = 0 is reached
for the ESS-value α = N = 100, i.e., when both distribu-
tions are weighted equally.

00.10.20.30.40.5−3−2−10123zlog Bayes factoralready close to the optimum with respect to the marginal
likelihood.

Bayesian network model:

The main contribution of the remainder of this paper is to
derive an analytical approximation for the optimal α ∗ in
the second update-step.

4.2 OPTIMAL ESS FOR GIVEN GRAPH

This section provides an understanding of the most impor-
tant properties of the data that affect the value of the op-
timal ESS. We derive an analytical approximation to the
optimal ESS-value for a ﬁxed graph G, cf. step 2 above.

Step 2 maximizes predictive accuracy in the prequen-
tial sense [Heckerman et al., 1995, Dawid, 1984], when the
data points are considered to arrive individually in a se-
quence. As this optimization problem is difﬁcult to solve,
we now replace it by a similar, but frequentist objective, de-
parting from Bayesian statistics: we minimize the test er-
ror, as is commonly done in cross-validation. Even though
this objective is not the same as the original one, it can
be expected to yield a sufﬁciently accurate approximation
in as far as we only aim to understand the difference be-
tween ESS-values of about 50 versus about 10 or lower for
the various data sets in [Silander et al., 2007]. Note that
this assumption and the following ones are validated ex-
perimentally on 20 UCI data sets in Section 4.3.

In the following, we combine two approximations to the
test error as to obtain an explicit approximation to the opti-
mal ESS-value α ∗. We carry out both approximations w.r.t.
a ‘reference point’ in the space of distributions. For conve-
nience, we choose this to be the distribution ˆp(X|G) im-
plied by the Bayesian network model with graph G and
maximum-likelihood parameter estimates ˆθ . The test er-
ror of this model with respect to the true distribution p(X)
reads

T [p(X), ˆp(X|G)] = −∑
x

p(x) log ˆp(x|G),

(12)

when using the log loss. As p is unknown, this cannot
be evaluated. As is well-known [Akaike, 1973], however,
the test error can be approximated by the training error,
−E ˆp(X)[log ˆp(X|G)], and a penalty term for model com-
plexity:

T [p(X), ˆp(X|G)] = −E ˆp(X)[log ˆp(X|G)] +

deff
G
N

+ O(

1
N2 ),
(13)

where

E ˆp(X)[log ˆp(X|G)] = ∑
x
n
∑
i=1

=

ˆp(x) log ˆp(x|G)

Nxi,πi

N

∑
xi

∑
πi

log

, (14)

Nxi,πi
Nπi

deff
G =

"

n
∑
i=1

∑
xi,πi

I(Nxi,πi) −∑
πi

#
I(Nπi)

,

(15)

where I(·) is the indicator function: I(a) = 1 if a > 0 and
I(a) = 0 otherwise.
If the data set is sufﬁciently large
so that all cell counts Nxi,πi are positive, then deff
G equals
the well-known number of parameters, which is given for
Bayesian networks by ∑i(|Xi| − 1)|Πi|, where | · | denotes
the number of (joint) states of the variable(s).

We obtain the second approximation to the test error in Eq.
12 as follows: we assume the (unknown) true distribution p
is well-approximated by the functional form of the regular-
ized parameter estimates in Eq. 3 using the optimal value
α ∗:

pxi,πi ≈ ˜pxi,πi,

(16)

where

˜pxi,πi =

= ˆp(xi, πi) +

Nxi,πi + α ∗ · qxi,πi

N + α ∗
(cid:18)

qxi,πi −

α ∗
N

(cid:19)

Nxi,πi

N

+ O((

)2).

(17)

α ∗
N

This ﬁrst-order Taylor expansion about the maximum-
likelihood estimate (our ‘reference point’) assumes α ∗ (cid:28)
N, which will be justiﬁed at the end of this section. Insert-
ing Eqs. 16 and 17 into Eq. 12, now results in our second
approximation,

T [p(X), ˆp(X|G)] ≈ −E ˆp(X)[log ˆp(X|G)]
(cid:18)

(qx −

) log ˆp(x|G) + O

−

α ∗
N ∑

x

Nx
N

(cid:19)

(

α ∗
N

)2

Next we equate this approximation with the one in Eq. 13,
and ﬁnally arrive at an explicit approximation to the opti-
mal ESS-value,

+ O(

α ∗2
N

),

α ∗ ≈

deff
G

E ˆp(X)[log ˆp(X|G)] − Eq(X)[log ˆp(X|G)]

(18)
where Eq(X)[log ˆp(X|G)] is the expectation with respect to
the prior distribution q, analogous to Eq. 14; regarding
the robustness against zero cell counts, we use N+
=
max{Nxi,πi, 1} in place of Nxi,πi in practice.

xi,πi

Note that the denominator in Eq. 18 is indeed positive if
the prior distribution q has a larger entropy H than the em-
pirical distribution ˆp does, which is the case for the BDeu
score. This is obvious from

E ˆp(X)[log ˆp(X|G)] − Eq(X)[log ˆp(X|G)]
= H(q(X|G)) − H( ˆp(X|G)) + KL(q(X|G)|| ˆp(X|G))

equals the maximum log likelihood divided by the sample
size, and deff
G is the effective number of parameters of the

given that the Kullback-Leibler divergence and the entropy
H are non-negative.

Interpretation of Eq. 18: This explicit approximation of
the optimal ESS α ∗ provides an understanding of the main
properties of the data determining the optimal ESS-value:

Table 1: Experimental validation of our analytical approx-
imations in Section 4.2. See text for details.

• skewness and dependence: the denominator of Eq. 18
tends to increase when the entropy (or negative likeli-
hood) of the empirical distribution factored according
to the graph structure, ˆp(X|G), decreases: this is the
cases if there are strong dependencies along the edges,
or if the conditional distributions of the variables are
very skewed. For such data sets, one can hence expect
small values of α ∗. Conversely, data sets that do nei-
ther imply a skewed distribution nor extremely strong
dependencies will result in increased values of α ∗.

• sample size: Eq. 18 does not explicitly depend on the
sample size N. There is an indirect relation, however:
the effective number of parameters, which is a mea-
sure of model complexity, tends to increase with N; a
more complex optimal model also entails an increased
maximum likelihood, and hence this also affects the
denominator; one may hence expect both the enumer-
ator and the denominator to grow in a similar way as
N increases, which suggests a weak dependence of α ∗
on N. Apart from that, when N is sufﬁciently large, the
model complexity tends to approach its ‘true’ value
and not increase further. Thus, for a sufﬁciently large
data set, α ∗ becomes independent of N. Consequently,
our earlier assumption α ∗ (cid:28) N indeed holds for suf-
ﬁciently large N. Note that this behavior is also con-
sistent with the one expected in the asymptotic limit
(N → ∞), namely α ∗/N → 0, so that the effect of reg-
ularization vanishes.

• number of nodes: Eq. 18 implies that α ∗ can be ex-
pected to be unaffected by the number n of nodes in
the (sparse) graph on average because both the enu-
merator and denominator are additive w.r.t. the nodes,
and hence on average grow proportionally to each
other when adding additional nodes to the network.

4.3 EXPERIMENTS

As an additional conﬁrmation of our assumptions under-
lying the approximations in Section 4.2, we determined
the optimal value α ∗ based on our iterative algorithm us-
ing Eq. 18 on 20 UCI data sets [Hettich & Bay, 1999].
We used the same pre-processed data as was used in
[Silander et al., 2007] (imputation of missing values, dis-
cretization of continuous variables), and compared to their
exact results.

Table 1 summarizes the results, and conﬁrms the valid-
ity of our approximation.
It is obvious that our ap-
proximate α ∗ agrees very well with the exact results of
[Silander et al., 2007], which were obtained under heavy
computational costs there (in our Table 1, α M is the exact

Data
balance
iris
thyroid
liver
ecoli
abalone
diabetes
post op
yeast
cancer
shuttle
tictac
bc wisc
glass
page
heart cl
heart hu
heart st
wine
adult

N
625
150
215
345
336
4177
768
90
1484
286
58000
958
699
214
5473
303
294
270
178
32561

n
5
5
6
7
8
9
9
9
9
10
10
10
11
11
11
14
14
14
14
15

α I

1...100
1...3
2...2
3...6
7...10
6...6
3...5
3...5
1...6
6...10
1...3
51...62
7...15
5...6
3...3
13...16
5...6
7...10
8...8
48...58

α M α ∗
48
44
2
2
3
2
3
4
8
8
7
6
3
4
3
3
6
6
7
8
3
3
60
51
8
5
6
6
3
3
9
13
5
5
10
10
7
8
50
49

k
1
2
5
3
3
3
3
3
2
2
2
2
3
4
2
3
3
4
3
3

solution of the maximization problem in Eq. 11, and α I is
the range of ESS-values that all yield the same MAP graph
as α M does [Silander et al., 2007]): while our approxima-
tion α ∗ does not always agree precisely with the exact value
α M, it correctly identiﬁes the data sets where the optimal
ESS is about 50 as opposed to about 10 or lower for the
remaining data sets. This suggests that our analytical ap-
proximation, and the underlying assumptions, capture the
main effects that inﬂuence the optimal ESS-value. More-
over, note that the sample size N varies between about 100
and 60, 000, and the number of variables n ranges from 5 to
15. Table 1 shows that both N and n have no obvious effect
on the optimal ESS-value, as expected from our analytical
approximation (see discussion in Section 4.2).

Moreover, note that the results of our two contributions are
very similar to each other (see Sections 3 and 4): the de-
cisive properties of the data are the implied dependencies,
or—in case of independence—the skewness of the implied
distribution.

With this insight, it is not surprising that an increase in the
optimal ESS value is related to a reduction in the maximum
number of edges in the graph attainable in the experiments
of [Silander et al., 2007]: the data sets with a large optimal
ESS-value of about 50 are exactly the data sets for which
the maximum number of edges in the graph (achieved by
increasing the ESS-value) is less than 80% of the edge-
count of the complete graph (see the column ‘range’ in Ta-

ble 1 of [Silander et al., 2007]). Both have the same cause,
namely a data set that implies neither strong dependencies
nor skewness.

Smets, P., & Bonissone, P. (eds), Proceedings of the
7 th Conference on Uncertainty in Artiﬁcial Intelligence.
Morgan Kaufmann.

As an aside, note that our iterative algorithm (cf. Section
4.1 using the approximation in Eq. 18) is computationally
efﬁcient, as it converges within a small number of itera-
tions, cf. k in Table 1; as a convergence criteria we required
|α ∗

k−1| < 0.1.

k − α ∗

5 CONCLUSIONS

This paper presents two contributions that shed light on
how the learned graph is affected by the value of the equiv-
alent sample size (ESS). First, we analyzed theoretically
the case of large but ﬁnite ESS values, which complements
the results for small values in the literature. Among other
results, it was surprising to ﬁnd that the presence of an edge
in a Bayesian network is favoured over its absence even if
both the Dirichlet prior and the data imply independence,
as long as the conditional empirical distribution is notably
different from uniform. Our second contribution provides
an understanding of which properties of the given data de-
termine the optimal ESS value in a predictive sense (when
considered as a free parameter). Our analytical approxima-
tion (which we also validated experimentally) shows that
the optimal ESS-value is approximately independent of the
number of variables in the (sparse) graph and of the sam-
ple size. Moreover, the optimal ESS-value is small if the
data implies a skewed distribution or strong dependencies
along the edges of the graph. Interestingly, this condition
concerning the optimal ESS-value is very similar to the
one derived for large but ﬁnite ESS-values. Finally, hav-
ing shown the crucial effect of the ESS value on the graph
structure that maximizes the Bayesian score, this suggests
that a similar effect can be expected concerning the poste-
rior distribution over the graphs, and hence for Bayesian
model averaging. If one embarks on this popular Bayesian
approach, one hence has to choose the prior with great care.

Acknowledgements

I am grateful to R. Bharat Rao for encouragement and sup-
port of this work, to Tomi Silander and Petri Myllym¨aki
for providing me with their pre-processed data, and to the
anonymous reviewers for valuable comments.

References

[Akaike, 1973] Akaike, H. 1973.

Information Theory
and an extension of the maximum likelihood principle.
Pages 267–81 of: Petrox, B. N., & Caski, F. (eds), Sec-
ond International Symposium on Information Theory.

[Buntine, 1991] Buntine, W. 1991. Theory reﬁnement on
Bayesian networks. Pages 52–60 of: D’Ambrosio, B.,

[Chickering, 1996] Chickering, D. M. 1996. Learning
Bayesian networks is NP-complete. Pages 121–30 of:
Proceedings of the International Workshop on Artiﬁcial
Intelligence and Statistics.

[Cowell et al., 1999] Cowell, R. G., Dawid, A. P., Lau-
ritzen, S. L., & Spiegelhalter, D. J. 1999. Probabilistic
Networks and Expert Systems. Springer.

[Dawid, 1984] Dawid, A. P. 1984. Statistical Theory. The
prequential approach. Journal of the Royal Statistical
Society, Series A, 147, 277–305.

[Haughton, 1988] Haughton, D. M. A. 1988. On the
choice of a model to ﬁt data from an exponential family.
The Annals of Statistics, 16(1), 342–55.

[Heckerman et al., 1995] Heckerman, D., Geiger, D., &
Chickering, D. M. 1995. Learning Bayesian networks:
The combination of knowledge and statistical data. Ma-
chine Learning, 20, 197–243.

[Hettich & Bay, 1999] Hettich, S., & Bay, D. 1999. The

UCI KDD archive. http://kdd.ics.uci.edu.

[Koivisto & Sood, 2004] Koivisto, M., & Sood, K. 2004.
Exact Bayesian structure discovery in Bayesian net-
works. Journal of Machine Learning Research, 5, 549–
73.

[Rissanen, 1978] Rissanen, J. 1978. Modeling by shortest

data description. Automatica, 14, 465–71.

[Schwarz, 1978] Schwarz, G. 1978. Estimating the dimen-
sion of a model. The Annals of Statistics, 6(2), 461–64.

[Silander & Myllym¨aki, 2006] Silander, T., & Myllym¨aki,
P. 2006. A Simple Approach for ﬁnding the globally
optimal Bayesian network structure. Pages 445–52 of:
Proceedings of the Conference on Uncertainty in Artiﬁ-
cial Intelligence.

[Silander et al., 2007] Silander, T., Kontkanen, P., & Myl-
lym¨aki, P. 2007. On Sensitivity of the MAP Bayesian
Network Structure to the Equivalent Sample Size Pa-
rameter. In: Proceedings of the Conference on Uncer-
tainty in Artiﬁcial Intelligence.

[Steck & Jaakkola, 2002] Steck, H., & Jaakkola, T. S.
2002. On the Dirichlet Prior and Bayesian Regulariza-
In: Advanves in Neural Information Processing
tion.
Systems (NIPS) 15.

[Tsallis, 2000] Tsallis, C. 2000. Entropic Nonextensiv-
arXiv:cond-

ity: a possible measure of complexity.
mat/0010150v1.

