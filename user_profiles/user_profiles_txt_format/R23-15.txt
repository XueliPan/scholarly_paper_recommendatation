3D Dynamic Grouping For Guided Stylization

Hedlena Bezerra

Elmar Eisemann Xavier D´ecoret∗

Jo¨elle Thollot

Grenoble University/ARTIS†INRIA

(a)

(b)

(c)

(d)

Figure 1: An example illustrating our 3D clustering to drive two diﬀerent stylizations: (a) In a meadow of ﬂowers, we want a butterﬂy to
inﬂuence surrounding objects. (b) We cluster the scene according to position. (c) Grouping information is used to drive a line rendering style
where the butterﬂy makes its cluster appear black; (d) The same grouping information can be used to guide a completely diﬀerent style: the
shape of the clusters determines the size and orientation of the stylized strokes in this painterly rendering.

Abstract

In art, grouping plays a major role to convey relationships of ob-
jects and the organization of scenes.
It is separated from style,
which only determines how groups are rendered to achieve a visual
abstraction of the depicted scene. We present an approach to inter-
actively derive grouping information in a dynamic 3D scene. Our
solution is simple and general. The resulting grouping information
can be used as an input to any “rendering style”.

We provide an eﬃcient solution based on an extended mean-shift
algorithm customized by user-deﬁned criteria. The resulting sys-
tem is temporally coherent and real-time. The computational cost
is largely determined by the scene’s structure rather than by its ge-
ometric complexity.

1 Introduction

One fundamental question in expressive rendering (also called Non-
Photorealistic rendering, or NPR) is: how to emphasize certain ob-
jects of the depicted scene? Answering this question allows the
creation of eﬃcient visual representations of a scene that commu-
nicate a speciﬁc message or are easier to understand.

Artists have done this for centuries. Depending on the medium, the
artist has access to diﬀerent methods of stylizations to produce such
a representation. If the artist uses a camera, she can set the focus
so that certain objects will be blurred (e.g. in the background). If
she uses a pen-and-ink technique, she may decide to simplify the
silhouette of certain objects, and to detail that of others with precise

∗X.D´ecoret is now working at Phoenix Interactive
†ARTIS, is part of the LJK research laboratory and of INRIA Rhˆone-
Alpes. LJK is UMR 5524, a joint research laboratory of CNRS, INRIA and
Grenoble University.

Copyright © 2008 by the Association for Computing Machinery, Inc. 
Permission  to  make  digital  or  hard  copies  of  part  or  all  of  this  work  for  personal  or 
classroom  use  is  granted  without  fee  provided  that  copies  are  not  made  or  distributed 
for  commercial  advantage  and  that  copies  bear  this  notice  and  the  full  citation  on  the 
first page. Copyrights for components of this work owned by others than ACM must be 
honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on 
servers,  or  to  redistribute  to  lists,  requires  prior  specific  permission  and/or  a  fee. 
Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail 
permissions@acm.org. 
NPAR 2008, Annecy, France, June 09–11, 2008. 
© 2008 ACM 978-1-60558-150-7/08/0006 $5.00 

89

strokes. Furthermore, color can be used to make certain objects less
visible by desaturation or averaging.

All these stylization techniques abstract or simplify the visual rep-
resentation of the scene according to what we call a level of abstrac-
tion (LOA). Such an LOA is determined for each object before ap-
plying the stylization technique. The choice of LOA depends on the
artist’s goal and often the viewpoint. When dealing with interactive
scenes, specifying the LOA can be very tedious. Our motivation is
to address this problem with a system that accounts for dynamically
varying scenes while allowing intuitive controls.

One possibility to derive an LOA uses the distance from the ob-
server. The aerial perspective eﬀect seen in nature is one example
of this, where contrast decreases with distance. It is also common
in artistic composition to give greater detail to foreground elements.
These considerations motivated the use of depth in many previ-
ous techniques for abstraction and stylization [Barla et al. 2006;
Kowalski et al. 2001; Markosian et al. 2000]. Although depth is a
very common criterion for abstraction, it is not the only one that
is involved in clustering processes. Information like normals, col-
ors [Kolliopoulos et al. 2006], or region of interests according to
the viewer [DeCarlo and Santella 2002] are combined to drive the
process.

In this paper, we present a real-time technique to cluster a dynamic
3D scene. The user can devise clustering strategies taking into ac-
count any attribute of the scene. Once the clustering is done, a tem-
porally coherent LOA is assigned to each group and used to drive
the stylization. We show how to achieve an automatic, yet control-
lable output that can then be used with any rendering style. This
stylization can be a function of the criteria used to cluster, but also
of other object information available from the cluster’s members.
Figure 1 shows an example where clustering, LOA and stylization
are based on diﬀerent attributes.

In the following sections we review related work, describe the main
steps of our algorithm in a simple situation and show how we can
extend it to handle complex behavior. We conclude by discussing
our method and show comparisons with previous approaches.

Figure 2: Overview: The ﬁrst step is done in a preprocess, all the others are dynamic. Finally, any stylization can be used for rendering.

Kolliopoulos et al.’s [2006] segmentation technique is similar to our
method in that they also use clustering. They segment a rendered
3D scene in image space while taking into account 3D information
(e.g. depth), and user provided object IDs. The major limitation of
this method is that it does not take hidden geometry into account.
Therefore regions that get disconnected by a nearby occluder can
cause severe problems. Moreover the information of segmentation
is not given at an object level, and thus it is harder to assure contin-
uous evolution over time. Our work shares the same goal, and by
using a fast object-space clustering, we are able to do this interac-
tively and let the user design her styles according to any information
available in the scene (including occluded geometry).

3 Overview

Figure 2 shows an overview of our approach. The input of our al-
gorithm is an interactively manipulated 3D scene. The clustering is
done on representative points that sample the scene and are selected
in a preprocess (Sec 4.1). At run time, these points are clustered us-
ing a mean-shift algorithm (Sec 4.2) in a feature space speciﬁed
by the user (Sec 6). Information from the clustered representative
points is then remapped and interpolated for any point of the scene.
Finally, the clustering information can be used as input to any ren-
derer to obtain the ﬁnal abstracted image (Sec 5).

4 Grouping

We begin by describing the clustering process in a simple case: how
to cluster according to the position of objects in camera space. This
is the most popular grouping in art because it takes into account
position in x, y and depth of the projected scene. We demonstrate
later that our method allows to cluster based on other attributes.

4.1 Representative Points

2 Related work

Abstraction has been involved in many NPR works and often in
form of a particular style deﬁnition. An exhaustive overview would
be well beyond the scope of this paper. We concentrate here on
previous methods addressing the determination of levels of abstrac-
tion.

When dealing with images (either stills or videos), abstraction is
usually done by segmenting the image into regions. This clustering
in 2D can be used to control stylization. Examples can be found
in [Wen et al. 2006] for color sketches, [Bousseau et al. 2000] for
watercolor, and [Wang et al. 2004b; Winnemoller et al. 2006] for
video stylization. In general, the level of abstraction is chosen man-
ually. Several approaches provide ways to compute LOAs automat-
ically. De Carlo and Santella [2002] use eye tracking, Lecot and
Levy [2006] use a saliency map to guide the LOA computation.
Bangham et al. [Bangham et al. 2003] associate LOAs based on
the distance to the center of attention, which is estimated using a
scale space approach. Orzan et al. [Orzan et al. 2007] also use a
scale-space analysis to abstract a photograph according to a mea-
sure of importance. Although these approaches give interesting re-
sults, they do not lead to information about what parts constitute an
object, or a group of objects, and they work on static data.

If the input is a 3D scene, one typical way to guide abstraction is to
rely on depth. The farther an object is away from the viewpoint, the
less detailed it is. Examples of usage can be found in [Markosian
et al. 2000] based on graftals, or in [Barla et al. 2006] for toon shad-
ing. This type of simpliﬁcation is classical in Computer Graphics
when level of details are involved [Luebke et al. 2003]. Cole et
al. [2006] present a temporally coherent system that relies on focal
points or planes to deemphasize parts of a stylized 3D scene based
on the distance in image or world space. This eﬀectively guides the
attention of an observer to important areas of the image. In contrast
to a uniform distance, our approach can provide a scene adapted
shape of the focus area and seeks to account for general criteria.

Several works rely on handmade segmentations of a 3D scene.
In [Luft and Deussen 2004] geometric proximity is used to deﬁne
bounding shapes for trees, where the size of the bounding volumes
is chosen by the user. When several trees are present in the scene,
meaningful groups are deﬁned by hand. Balzer and Deussen [2007]
present a clustering algorithm for graph visualization. Kowalski
et al. [2001] show various techniques for abstraction, and use the
notion of manually deﬁned groups to produce convincing results.
They also rely on the heuristic that objects far away should be
grouped together. The user is able to modify the default behav-
ior by adding some semantic importance information in the system.
Our work extends this approach by oﬀering an automatic clustering
that could be taken as an input of their system. It generalizes the
approach by giving more ﬂexibility to the grouping strategies.

For most of these approaches, once the clustering is established, it
does not evolve over time, since it is handmade or given as an input.
In our approach, we concentrate on dynamic grouping.

Clustering scenes with many points is a time consuming process.
To maintain interactive framerates, we work on a subset of scene
points called representative points, computed during a preprocess.

Figure 3: Sampling (left), representative points (middle), weights
on mesh (right).

90

SceneRepresentative Points (RP)Feature SpaceClustersClustered RPLOAStylized Image MappingPreprocessClusteringInverse MappingInterpolationLOA ComputationRenderingClustered sceneThe selection of these points is a trade-oﬀ between speed (directly
related to the number of points) and accuracy of the ﬁnal clustering.
We propose to use a sampling method that is independent of the
geometry (not related to the tessellation), and controlled by a single
parameter. We select points on the objects surface using a Poisson
sphere sampling (related to Poisson sphere and disc distributions
[Cook 1986; Lagae and Dutr´e 2006]).

Starting with a radius r of the size of the object, we add samples
until any additional sample would break the constraint that its dis-
tance to all other samples is at least r. Then we decrease the radius,
and continue the process until we reach r ≤ (cid:15). The resulting sam-
ples are our representative points. The single parameter (cid:15) controls
the precision of the sampling.

The clustering itself is performed on representative points only. Af-
ter grouping, the value vP of a given attribute at a point P is ob-
tained via a weighted interpolation of the attribute’s values vi at the
representative points:

vP :=

(cid:80)
(cid:80)

w(P, Ri)vi
w(P, Ri)

, with w(P, R) := e−||P−R||2

(1)

where w is a weighting function that measures the proximity be-
tween two points (Figure 3, right).

Evaluating these weights at run-time would be quite costly. Our so-
lution is to store the weights for each vertex in texture coordinates
that can be used to eﬃciently evaluate the inﬂuence of each rep-
resentative point. Currently, 32 RGBA texture coordinates can be
used, allowing 128 weights per vertex, which proved largely suﬃ-
cient in practice. Very large objects can be subdivided until 128 is
enough for each sub-object.

4.2 Clustering

There exist many clustering algorithms in Computer Graphics.
Many of them have been devised for acceleration purposes, such
as octrees, KD-trees, regular grids, etc. They impose a mostly uni-
form structure, and cannot handle arbitrarily shaped clusters. Their
goal is typically to divide the geometry in a balanced way to speed
up hierarchical geometric tests. In our case, the goal of the clusters
is to reveal the inherent structure of the scene. Thus, the result-
ing number of clusters cannot be known in advance, and should be
derived for a given scale. Clusters of arbitrary shape should be han-
dled. Furthermore, we will see that it is advantageous to be able to
derive a cluster center. One possible approach that meets the above
constraints is the mean shift algorithm [Comaniciu and Meer 2002].
We chose it because of its simplicity and potential to be accelerated
in our context.

The mean-shift algorithm clusters points in a feature space. Thus,
it is capable of ﬁnding groups of points that share suﬃcient simi-
larity. Even though there is a more general formulation introduced
in [Wang et al. 2004a] that represents an anisotropic extension, we
focus on the original formulation.
Given n data points xi ∈ Rd, the original paper [Comaniciu and
Meer 2002] deﬁnes a density function f . Based on a kernel func-
tion K, that describes the likelihood for points to cluster with their
surrounding and a scope H determining the scale on which clus-
ters will be established. Intuitively, it is a superposition of local
weighting functions around sample points. Formally, it is given by:

f (x) :=

1
nHd

n(cid:88)

i=1

(cid:19)

(cid:18) x − xi
H

K

Even though several conditions are imposed on K, there is a vast

variety of choices [Wand and Jones 1995]. We use a radially sym-
metric function:

K(x) := (2π)−d/2e− 1

2 ||x||2

(3)

In that case, H corresponds to the variance of the Gaussian.

The mean-shift moves each point iteratively along f ’s steepest
ascent to a local maximum, called the cluster leader (or cluster
mode). Interestingly, a locally weighted mean (depending on K) re-
sults in a position that lies along this gradient direction. An iterative
process can thus be used, where the current position is replaced by
the mean value of the neighborhood (hence the name mean-shift).
The convergence of this method has been proved in [Comaniciu
and Meer 2002]. Advanced solutions for a faster evaluation ex-
ist [Paris and Durand 2007] but are out of the scope of this article.

The mean-shift leads to clusters whose number does not have to be
speciﬁed in advance, revealing the clustering information inherent
to the scene at a given scope H. It can be seen as the scale at which
we try to ﬁnd clusters in the scene. Finding the right scale is a
decision we leave to the user, because it is a semantic deﬁnition.
An automatic approach to derive a reasonable size could be possible
by exploring scale space techniques [Ter Haar Romeny 2003], but
would be costly and might disagree with the artistic choice.

The advantage of performing clustering in feature space is that
this deﬁnition is completely independent of the scene or animation.
This is an elegant solution that has rather predictable behavior with-
out further user interaction.

a) Pixel-based

b) Object center-based

c) Voxel grid

d) Our result

Figure 4: Comparison of our clustering to naive approaches.

Figure 4 shows a comparison between our clustering and several
naive strategies. The scene is the one shown Figure 1 with a circu-
lar arrangement of ﬂowers. A pixel-based (a), as well as an object
center-based distance measure (b), do not isolate the circular ar-
rangement. Clustering according to a grid (c) causes artiﬁcial sepa-
rations between neighboring elements. Our approach (d) produces
clusters that are more natural because they relate to the scene’s
structure.

Figure 5 shows an example inspired by Kolliopoulos et al. [2006]
in order to compare our object space approach to their image space
method. The style we use is similar to Kolliopoulos’ cartoon style:
the color of a cluster is the mean color of the objects contained in
the cluster. Using an image space method creates two clusters in
the background due to the occlusion. The chosen style has a large
impact on the ﬁnal image. With our approach the background trees
stay clustered whatever may happen in the foreground.

(2)

91

Figure 5: Comparison of our clustering to an image-based approach: (Left) original scene; (Middle) our result; (Right) using only visible
points creates two distinct clusters in the background.

Nevertheless, including invisible geometry may also produce unde-
sirable results. For example, if a house contains people, should it
behave diﬀerently in the clustering process than an empty house?
There is no automatic way to predict what an artist would desire.
The strength of our approach is to give the user the possibility to
have more control over the grouping behavior. In the aforemen-
tioned example, one could exclude the people inside the house as
long as its door remains shut and add them into the clustering pro-
cess once the door opens. We present diﬀerent ways to vary the
inﬂuence of sample points in section 7.

4.3 Temporal coherence

It is important to note that, in general, the mean-shift clustering is
not stable: a slight perturbation can lead to a diﬀerent classiﬁca-
tion. Figure 6 shows this situation. There are two small clusters
in blue and red. The white point in the middle does not provide
enough density to create its own cluster, neither does it change the
global density function in a way that would fuse the red and the blue
cluster. This point can go either way, or even stay unclustered. A
tiny perturbation can lead to a leap towards a rather distant position
resulting in popping artifacts.

Nevertheless, ensuring temporal coherence in our case is relatively
straightforward. We work in object space so we have information
about all representative points and their history independently of
their visibility. For image-based approaches, this step represents an
important challenge. Our solution is to integrate a smoothing pro-
cess with exponential decay. The problem of Figure 6 occurs only at
local minima of the density function where the gradient vanishes.
To ensure that a point does not directly jump from one cluster to
the other without passing through an intermediate state, we exam-
ine whether it is located near a minimum. This can be done based
on the computed mean-shift value. It is proportional to the gradient
and we simply test whether the initial displacement is small enough
to be neglected. In practice, a small constant value performs well as
pointed out in [Wang et al. 2004b]. We used 10−5 throughout this
paper, but it could be chosen according to the scene’s total density.

Figure 6: Slight perturbations can lead to a diﬀerent clustering.
The white point could join the red or the blue cluster.

5 Level of abstraction and stylization

As mentioned in the overview, once the grouping is established,
for each cluster, a level of abstraction (LOA) is derived that guides

92

the stylization. This value (vector or scalar) is a set of signiﬁcant
attributes, necessary to determine the ﬁnal rendering. Its choice is
application-dependent and thus left to the user.

Here, we present two strategies to derive LOAs along with a corre-
sponding stylization example.

LOA computation using the cluster leader A simple way to
compute the LOA is to use the cluster leader. Indeed, this feature
point best represents the attributes of the group. It also lies inside
the cluster deﬁned by the density function. This is not necessarily
the case for an average.

Figure 7: The original scene is clustered using the position in cam-
era space as feature space; The LOA is computed from the depth of
the cluster leader; The style is cartoon-like with aerial perspective.

Figure 7 shows an example that uses this strategy. The trees are
clustered according to their camera space projections. The LOA of
a cluster is the z coordinate of its leader. Two simple styles are used:
color is desaturated according to the LOA and an outline is drawn
around the clusters based on the discontinuities of the cluster LOA
in image space.

LOA computation using a speciﬁc scene attribute A sec-
ond strategy is to consider a speciﬁc attribute and merge it inside
a cluster. A standard choice is to compute the average or the max-
imum. One direct application is to emphasize particular elements
of the scene to attract the observer’s attention. One way to achieve
this is by attaching a special attribute to each object indicating its
importance. The LOA of a cluster is then chosen based on whether
it contains an important element.

Figure 8 shows an example. The butterﬂy is important and colorizes
surrounding elements of the scene. This importance is encoded in
an attribute (1 for the butterﬂy, 0 elsewhere). Clustering is per-
formed in camera space. Due to the view space projection, objects
farther away will create larger clusters, whereas objects near the
viewpoint will be treated individually. This makes the butterﬂy act
on larger groups at a distance. The LOA is given by the maximum
of the importance value inside a cluster. A non-zero value indicates
that color should be applied whereas zero results in a gray-scale
output.

cut through density functiondensity for blue pointsdensity for red pointsdensity for white pointsdensity for Mean-ShiftAbstractionOriginal7 Mean-Shift Extension

To allow more ﬂexibility in the clustering process, we propose a
slight modiﬁcation of the original mean-shift to include weighting
of points in the process. It adds two degrees of freedom. First, it
makes the scale H vary with each point xi. Second, a weight ωi is
deﬁned for each point. These modiﬁcations are driven by the fact
that the user should be able to specify that certain elements, with
a particular semantics, are more likely to yield separate clusters, or
will more or less “attract” their neighbors in feature space. This is
typically useful in a scene that has elements at diﬀerent scales. On a
ﬁeld with cows, each ﬂower might be grouped with its neighboring
ﬂowers, but the scale is intuitively smaller than the level at which
cows are grouped. Some elements might be important and imply
the use of a larger radius; a king that attracts the attention of its
servants earlier than a simple knight. We refer to the video for
further examples of how these parameters can be applied.

The reformulated density function is:

f (x) := 1/n

ωiK(

n(cid:88)

i=1

x − xi
Hi

)

(4)

With these weighted instead of classical means the method still con-
verges. A proof is given in Appendix A.

Figure 10 shows an example for the inﬂuence of the weight. Here,
the color of the cluster corresponds to the color of the leader de-
ﬁned by a ramp from green to red along the x-axis of the image.
The only considered attribute is position and the top image shows
the result when all characters have similar weights: we obtain one
cluster with the leader in its center. The bottom image shows the
result of increasing only the weight of the left giraﬀe. All the gi-
raﬀes are clustered again but the leader is centered on the left. This
mechanism is very intuitive and easily manipulable via an inter-
face where the user selects an object and moves a slider to assign a
weight. The modiﬁcations are directly visible allowing an interac-
tive adjustment.

Figure 8: In this example the LOA is based on the presence of
the butterﬂy in the cluster. Using screen position as a feature, the
clustering evolves according to the viewpoint. Camera zoom un-
derneath the upper arrow: a ﬂower bush when seen from far away
is separated into individual ﬂowers when zooming.

6 Feature space

Although we illustrated the mean shift clustering in the context of
position, it works on a feature space of arbitrary dimension. This al-
lows our system to consider any attributes, or their combination, to
deﬁne the clustering. For example, we can take color into account.
Let’s assume that feature points have (x, y, z, r, g, b) attributes. We
can map them to a feature space using a perspective projection for
position and LUV for color.

Combining attributes like color and position in a clustering process
might not seem very intuitive, but by deﬁning a mapping function,
the clustering behavior can be predicted. For example, if color is
supposed to be relatively more important than position, a scaling of
the color dimensions takes this into account.

Figure 9 shows an example. Here, we use a simple style where the
object’s color is computed as the average of the objects colors in
the cluster. Using only color leads to two clusters (red and orange
on Figure 9-(b)). Similarly if only position is taken into account,
three clusters are naturally obtained (Figure 9-(c)). When using
both color and position, we end up with a compromise where a
yellow apple among the red apples will neither be clustered with
close apples (due to color), nor other yellow apples (due to distance)
(Figure 9-(d)). This example demonstrates that multiple features
can easily be taken into account in a controllable way.

Figure 9: (a) A scene with diﬀerent colors, is clustered using dif-
ferent attributes: (b) color, (c) position, (d) color and position.

Figure 10: The user can modify the weight of certain objects to
inﬂuence the clustering. Black points indicate the cluster leader
positions. Halos indicate stronger weights.

8 Discussion

Our algorithm performs often well, running from 90 fps for the but-
terﬂy to 160 fps for the apple scene. The computation time depends

93

After camera close upButterfly shares importance with clusterOriginal sceneClusteredbypositionClusteredbycolorClusteredbyposition and colora.b.c.d.on the number of representative points (we used up to around 500
points at real-time rates), the scale and the scene structure that can
be more or less adapted to our acceleration grid. Of course, the the-
oretical performance of the mean-shift is expected O(n2) and this
could be the case, but in practice, the grid data structure makes the
algorithm fast enough for many complex scenes.

Our technique allows the user to control the clustering via several
parameters. The ﬁrst choice is the feature space that enables our
system to decide which attributes have to be taken into account and
in which proportion. It describes the similarity measure used for
the clustering. Second, the user has to select the global scale of
the clustering and can modify weights and scale per object. These
parameters are accessible via a slider and can be changed interac-
tively for easy tuning. In the future, we plan to explore learned-
by-example clustering, and a system that automatically suggests
“good” parameters to assist the user.

The LOA computation and stylization are currently implemented
using shaders, letting the user make his own simple programs. But
nothing prevents the creation of more intuitive user interfaces for
given types of rendering that would use our clustering as a ﬁrst step.
We have shown that with simple styles and simple LOA strategies
we can obtain quite complex behaviors.

Currently, representative points are chosen in a preprocess. This
“sampling” is necessary to allow a fast clustering by treating a
smaller set of points. However it is an approximation and might cre-
ate artifacts for deformable objects because the samples may lose
the uniformity of their initial distribution. We plan to investigate a
real-time sampling method to address this issue.

In comparison to image based approaches like [Kolliopoulos et al.
2006], our algorithm shows strengths as well as weaknesses. Tem-
poral coherence becomes simpler and invisible geometry can be
treated (we discussed the implications in section 4.2). Another
important point is that, usually,
there are fewer representative
points than pixels, accelerating the computations but also leading
to coarser clusters. Image solutions directly take texture into ac-
count and our preprocess becomes unnecessary, which allows the
easy integration of deforming objects. A combination of both tech-
niques is a promising direction of future work.

9 Conclusion

We have presented a generic clustering approach that can be used to
derive the inherent structure of a scene with applications to styliza-
tion. This is a common mechanism in the artistic production that we
transfer in the context of dynamic 3D scenes. Our solution usually
runs in real-time since it is independent of the scene complexity and
it allows temporally coherent results. The user can control the clus-
tering via interactive and intuitive handles thanks to our mean-shift
extension.

The presented approach can be applied in contexts other than NPR.
For example, we plan to use it to perform calculations for groups
instead of each individual. The idea would be to perform a detailed
computation for the cluster leader and coarse approximations for
other elements, that are then combined to determine the complete
simulation. Another application that our clustering is useful for, is
to steer group behavior, where the style is animation. The accom-
panying video shows some of our results. All these possibilities
are uniﬁed in one approach.
In the future, we plan to apply the
framework to artiﬁcial intelligence to help make decisions: based
on group size, special members, distances to other clusters, etc.

Finally we believe that our decomposition of the image creation
process opens new research avenues. We see such a computational

model as a step towards a better formalization of the abstraction
process leading to powerful drawing systems.

Acknowledgements We thank the reviewers for their helpful sug-
gestions that improved the paper. Special thanks go to A. Orzan, K.
Subr, and P.-E. Landes for their comments and help on the paper.
We would also like to thank L. Boissieux for the giraﬀe models.
The ﬁrst author is supported by CAPES, Brazil.

References

B, M.,  D, O. 2007. Level-of-detail visualization of
clustered graph layouts. In Asia-Paciﬁc Symposium on Visuali-
sation 2007.

B, J., G, S.,  H, R. 2003. The art of scale-
In Proceedings of British Machine Vision Conference

space.
2003, 569–578.

B, P., T, J.,  M, L. 2006. X-toon: An ex-

tended toon shader. In Proceedings of NPAR 2006, 127–132.

B, A., K, M., T, J.,  S, F. X. 2000. In-
teractive watercolor rendering with temporal coherence and ab-
straction. In Proceedings of NPAR 2006, 141–149.

C, F., DC, D., F, A., K, K., M, K., 
S, A. 2006. Directing gaze in 3d models with stylized
focus. In Proceedings of Eurographics Symposium on Rendering
2006, 377–387.

C, D.,  M, P. 2002. Mean shift: A robust approach
IEEE Transactions on Pattern

toward feature space analysis.
Analysis and Machine Intelligence 24, 5 (May), 603–619.

C, R. L. 1986. Stochastic sampling in computer graphics. In

Proc. of SIGGRAPH, ACM Press, 51–72.

DC, D.,  S, A. 2002. Stylization and abstraction
of photographs. In Proceedings of SIGGRAPH 2002, 769–776.

K, A., W, J. M.,  H, A.

2006.
Segmentation-based 3d artistic rendering. In Proceedings of Eu-
rographics Symposium on Rendering 2006, 361–370.

K, M. A., H, J. F., R, C.,  O, J. 2001. User-
guided composition eﬀects for art-based rendering. In Proceed-
ings of Symposium on Interactive 3D Graphics 2001, 99–102.

L, A.,  D´, P. 2006. Poisson sphere distributions. In Vi-
sion, Modeling, and Visualization, IEEE Computer Society. Ac-
cepted.

L, G.,  L, B. 2006. Ardeco: Automatic region detection
and conversion. In Proceedings of Eurographics Symposium on
Rendering 2006, 349–360.

L, D., R, M., C, J., V, A., W, B., 
H, R. 2003. Level of Detail for 3D Graphics. Morgan-
Kaufmann, Inc.

L, T.,  D, O. 2004. Watercolor illustrations of plants
using a blurred depth test. In Proceedings of NPAR 2004, 11–20.

M, L., M, B., K, M. A., H, L. S.,
N, J. D.,  H, J. F. 2000. Art-based rendering
with continuous level of details. In Proceedings of NPAR 2000,
59–66.

O, A., B, A., B, P.,  T, J.

2007.
Structure-preserving manipulation of photographs. In Proceed-
ings of NPAR 2007, 103–110.

94

P, S.,  D, F. 2007. A topological approach to hierar-
chical segmentation using mean shift. In Proceedings of IEEE
conference on Computer Vision and Pattern Recognition 2007.

T H R, B. M. 2003. Front-End Vision and Multi-Scale
Image Analysis: Multi-Scale Computer Vision Theory and Ap-
plications, Written in Mathematica. Springer.

W, M. P.,  J, M. 1995. Kernel Smoothing. Chapman

and Hall.

W, J., T, B., X, Y.,  C, M. F. 2004. Image and
video segmentation by anisotropic mean shift. In Proceedings of
European Conference on Computer Vision 2004, 238–249.

W, J., X, Y., S, H.-Y.,  C, M. F. 2004. Video

tooning. In Proceedings of SIGGRAPH 2004, 574–583.

W, F., Q.L, L, L., X, Y.-Q.,  S, H.-Y. 2006.
Color sketch generation. In Proceedings of NPAR 2006, 47–54.

W, H., O, S. C.,  G, B. 2006. Real-time
video abstraction. In Proceedings of SIGGRAPH 2006, 1221–
1226.

A Convergence

In the paper [Comaniciu and Meer 2002], the proof for the iterative
scheme was given for the original formulation. In our case, we can
apply almost the same reasoning. Therefore we will mostly focus
on the diﬀerences. Like in [Comaniciu and Meer 2002], we take a
k(x) such that K(x) = k(x2) and g(x) = −k(cid:48)(x). We can rewrite the
gradient of f as:

∇ f (x) =

(cid:32)

cig

||

n(cid:88)

i=0

x − xi
Hi

||2

(cid:33) 


(cid:80)n
i=0 xicig
(cid:16)
(cid:80)n
i=0 cig

(cid:16)

|| x−xi
Hi
|| x−xi
Hi

||2

(cid:17)

||2
(cid:17) − x



 (5)

where we denote the grouped constants ci (including ωi). Thus
moving along the gradient is related to replacing our position by
the weighted mean:

mean(x) :=

(6)

(cid:17)

(cid:16)

(cid:80)n
i=0 xicig
(cid:16)
(cid:80)n
i=0 cig

|| x−xi
Hi
|| x−xi
Hi

||2
(cid:17)

||2

As in [Comaniciu and Meer 2002], yi+1 := mean(yi) should con-
verge for any y0. The only property we need to apply the original
proof, is that the functions k(x2/H2
i ) are all convex and cig(||x −
xi||2/H2
i ) is bounded. This is the case because of the choice of the
kernel function K.

95

96

