Data Min Knowl Disc
DOI 10.1007/s10618-008-0087-0

Automatically countering imbalance and its empirical
relationship to cost

Nitesh V. Chawla · David A. Cieslak ·
Lawrence O. Hall · Ajay Joshi

Received: 11 November 2006 / Accepted: 8 January 2008
Springer Science+Business Media, LLC 2008

Abstract Learning from imbalanced data sets presents a convoluted problem both
from the modeling and cost standpoints. In particular, when a class is of great inter-
est but occurs relatively rarely such as in cases of fraud, instances of disease, and
regions of interest in large-scale simulations, there is a correspondingly high cost for
the misclassiﬁcation of rare events. Under such circumstances, the data set is often
re-sampled to generate models with high minority class accuracy. However, the sam-
pling methods face a common, but important, criticism: how to automatically
discover the proper amount and type of sampling? To address this problem, we pro-
pose a wrapper paradigm that discovers the amount of re-sampling for a data set
based on optimizing evaluation functions like the f-measure, Area Under the ROC
Curve (AUROC), cost, cost-curves, and the cost dependent f-measure. Our analysis
of the wrapper is twofold. First, we report the interaction between different evaluation
and wrapper optimization functions. Second, we present a set of results in a cost-
sensitive environment, including scenarios of unknown or changing cost matrices. We

Responsible editor: Gary M. Weiss.

N. V. Chawla (B) · D. A. Cieslak
Department of Computer Science and Engineering, University of Notre Dame, Notre Dame,
IN 46556, USA
e-mail: nchawla@cse.nd.edu

D. A. Cieslak
e-mail: dcieslak@cse.nd.edu
L. O. Hall · A. Joshi
Department of Computer Science and Engineering, University of South Florida, Tampa,
FL 33620-5399, USA

L. O. Hall
e-mail: hall@cse.usf.edu

A. Joshi
e-mail: ajoshi@cse.usf.edu

123

N.V. Chawla et al.

also compared the performance of the wrapper approach versus cost-sensitive learn-
ing methods—MetaCost and the Cost-Sensitive Classiﬁers—and found the wrapper
to outperform the cost-sensitive classiﬁers in a cost-sensitive environment. Lastly, we
obtained the lowest cost per test example compared to any result we are aware of for
the KDD-99 Cup intrusion detection data set.

Keywords Classiﬁcation · Unbalanced data · Cost-sensitive learning

1 Introduction

Imbalance in class distribution is pervasive in a variety of real-world applications,
including but not limited to telecommunications, WWW, ﬁnance, biology, and med-
icine. The minority or positive class is often of interest and also accompanied with a
higher cost of making errors. A typical example of this problem is fraud detection. The
instances of fraud in the population are generally a very small proportion (often in the
neighborhood of 2%). However, it is quite important to be able to detect a fraudulent
transaction. At the same time, it is also important to minimize false positives because
these result in investigation costs and can also result in losing a customer. Thus, there is
a distribution of costs associated with both false positives and false negatives. Another
example is large-scale simulation. While, there are not “dollar” costs attached with
errors like there may be with fraud detection, there is a cost attached to time spent
in poring over “uninteresting regions” in a simulation. It is known that the regions
of interest in such simulations are typically rare (Bowyer et al. 2000; Banﬁeld et al.
2005). Hence, an intelligent tool should be accurate in identifying interesting regions,
without too many false alarms.

Weiss and Provost (2003) observed that the naturally occurring distribution is not
always optimal. Thus, one needs to modify the data distribution, conditioned on an
evaluation function. Re-sampling, by adding to the minority (positive) class or remov-
ing the majority (negative) class of a given data set, has become a de facto standard to
counter the curse of imbalance in various domains. There have been numerous papers
and case studies exemplifying their advantages (Chawla et al. 2003, 2004; Kubat and
Matwin 1997; Ling and Li 1998; Weiss and Provost 2003; Ferri et al. 2004; Dietterich
et al. 2000; Kubat et al. 1998; Zadrozny and Elkan 2001; Batista et al. 2004; Maloof
2003; Drummond and Holte 2003). However, the one common critique of various
works is: how does one effectively identify the potentially optimal sampling technique
and parameters for a given data set? An accompanying question is: can the techniques
for imbalance generalize across cost-sensitive scenarios?

Driven by these important and pertinent questions and appreciative of the prob-
lem that sampling will be highly data dependent, we propose a wrapper framework
for empirically discovering the sampling parameters. We ﬁll an important hole in the
application of sampling methods to counter the problem of class imbalance by com-
prehensively demonstrating the efﬁcacy of the wrapper paradigm. Our goal was to
empirically evaluate the usefulness of popular techniques for countering class imbal-
ance by tuning the parameters on a validation set before reporting the performance on
the testing set. All the work, thus far, has reported the best performance on the testing
set by applying different sampling strategies. Thus, there is no empirical estimate of

123

Automatically countering imbalance

generalization capacity. In addition, we evaluate the generalization performance of
the classiﬁers, learned on such re-sampled data sets, in a large variety of experimental
scenarios. We want to analyze the ﬂexibility of learned models in a dynamic envi-
ronment since misclassiﬁcation costs are often unknown (Weiss et al. 2007) or may
change. We empirically investigated the relationship between choosing the “best”
sampling strategy via a wrapper method and its relationship to classiﬁcation in a cost-
sensitive environment, especially when the costs are unknown or can change. Thus,
the over-arching question is: Can we utilize measures and methods that counter class
imbalance and do well in a cost-sensitive framework?

Contributions. Our main contributions in this paper are centered around the following
research questions.

• Can we effectively discover sampling strategies for a given classiﬁer and domain?
• Does the proposed wrapper paradigm for countering imbalance, guided by the
cost-insensitive criteria such as the Area Under the Receiver Operator Character-
istic Curve AUROC and the f-measure, perform well in a cost-sensitive testing
environment? How does it compare to using cost directly in the wrapper mode to
guide the search? How do changing costs or unknown costs effect the performance
of the wrapper?

• In a cost-sensitive environment, are there potential advantages in accounting for
the imbalance ﬁrst by applying sampling strategies as compared to MetaCost
(Domingos 1999) and Cost-Sensitive Classiﬁer (Zadrozny et al. 2003)?

The remainder of this article is organized as follows. Section 2 introduces the
wrapper framework to discover the coupling of a sampling method and corresponding
parameters with a classiﬁer and domain. Section 3 presents the experimental frame-
work. Our empirical analyses ﬁrst illustrate the efﬁcacy of the wrapper framework
across data sets using cost-transparent evaluation measures in Sect. 4. Then in Sect.
5, the impact of introducing costs during testing is demonstrated using a variety of
cost-ratios. Finally, Sect. 6 discusses the results.

2 Wrapper paradigm

Re-sampling is a popular solution to the class imbalance problem. However, one per-
sistent limitation of the sampling methods has been automatically discovering the
amount and type of sampling to apply to a given data set. To address this limitation,
we propose a comprehensive wrapper infrastructure that applies cross-validation to
ﬁrst discover the best amounts of undersampling and oversampling. For oversampling,
we use the Synthetic Minority Over-Sampling TEchnique (SMOTE) (Chawla et al.
2002), which oversamples the minority class by introducing synthetic examples along
the line segments joining any/all of the k minority class nearest neighbors. Depending
upon the amount of over-sampling required, neighbors from the k nearest neighbors
are randomly chosen. Synthetic samples are generated in the following way: take the
difference between the feature vector (sample) under consideration and its nearest
neighbor. Multiply this difference by a random number between 0 and 1, and add it
to the feature vector under consideration. This causes the selection of a random point

123

1: Input: List of minority classes M inorList
2: List of majority classes M ajorList
3: Number of cross-validation folds N umF olds
4: Output: Wrapper selected UnderSampling percentages for majority classes; Wrapper

selected SMOTE percentages for minority classes

N.V. Chawla et al.

U nderSampleList[M Class] = 100

5: for all M Class in M ajorList do
6:
7: end for
8: for all M Class in M inorList do
9:
10: end for
11: for F old ← 1 to N umF olds do
12:

SmoteList[M Class] = 0

Build classiﬁer on training fold F old and evaluate on validation set
Update BestM inorM etricV alues and BestM ajorM etricV alues

13:
14: end for
15: if M ajorList is not empty then
16: W RAP P ER U N DERSAM P LE(U nderSampleList)
17: end if
18: if M inorList is not empty then
19: W RAP P ER SM OT E(SmoteList(cid:1) U nderSampleList)
20: end if
21: Output U nderSampleList and (SmoteSampleList)

Fig. 1 Wrapper Undersample SMOTE Algorithm

along the line segment between two speciﬁc features. This approach effectively forces
the decision region of the minority class to become more general.

Obviously, searching the entire space of undersampling and SMOTE combinations
can quickly become intractable, so we proceed in a step-wise fashion. This strategy
removes the “excess” examples of the majority classes, which reduces the size of
the training data set. This also makes learning time more tractable. Then SMOTE
is used to add synthetic examples of the minority classes and increase the general-
ization performance of the classiﬁer over the minority classes. Figure 1 shows the
Wrapper_UnderSample_SMOTE algorithm, which can extend to multiple majority
and minority classes. The metric values in the pseudo-code indicate the performance
criterion, which will be discussed in subsequent sections. We will analyze the cross-
validation approach used in Sect. 2.1, outline the wrapper approach to selecting sam-
pling levels in Sects. 2.2 and 2.3, and survey the wrapper search evaluation metrics in
Sect. 2.4.

2.1 Using cross-validation to guide sampling

The wrapper approaches are optimized on an independent validation set to avoid any
bias in performance on the testing data. To that end, we construct a careful cross-
validation framework. We ﬁrst split the data set into ten partitions. Each partition is
used once as a testing fold, with the remaining 90% as the training fold. This results
in ten pairs of training and testing folds (10-fold cross-validation), forming the basis
for our comparison of the different methods. We further split each of the training
folds, independently, into ﬁve partitions for an internal ﬁvefold cross-validation. The

123

Automatically countering imbalance

wrapper applies this independent validation stage to each fold to discover the appro-
priate percentages of sampling for a given method and classiﬁer combination. Once
these percentages are discovered, the classiﬁer is re-learned on the original training
fold using the discovered percentages and tested on the corresponding testing fold.
Thus, the purpose of internal ﬁvefold cross-validation is only to guide an independent
wrapper stage, keeping the original testing fold separate for an unbiased assessment of
the performance. Note that this procedure is repeated for each of the 10-folds. Further-
more, due to the inherent random nature of undersampling and SMOTE, the process
of training and testing with wrapper selected undersampling and SMOTE percentages
was done a total of ﬁve times to get an averaged (more stable) performance measure.
To summarize, for ﬁve different runs, the 10-folds were constructed and on each
fold a ﬁvefold cross-validation was applied to discover the amounts of SMOTE and
undersampling. The ﬁnal reported performances are the averages over the 50 sets (5
runs × 10 folds). We will now explore the heuristic functions used to optimize the
sampling levels.

2.2 Wrapper-based algorithm to select undersampling percentages

This algorithm uses a wrapper approach to perform the search through the parameter
space of undersampling percentages for the majority class(es), using the chosen learn-
ing algorithm as a part of the evaluation function for a ﬁvefold cross-validation on the
training data. The purpose is to search for the sampling level in the parameter space
with the highest evaluation score guided by some heuristic function. This search in
the parameter space has to be restricted to the training data to avoid any estimation
biases. Thus, we utilized a ﬁvefold cross-validation on the training set to discover the
amount of undersampling.

The wrapper starts with no undersampling for all majority classes and obtains
baseline results on the training data. Then it traverses through the search space of
undersampling percentages in decrements of Sample Decrement (in this case 10%), in
a greedy iterative fashion, to increase performance over the minority classes without
sacriﬁcing performance on the majority class. We start with 100% undersampling,
implying that all of the majority class examples are retained. And then we remove the
majority class examples 10% at a time. This search process continues as long as it does
not hamper the performance of any minority class, Minor Metric Value, or drop the
performance for any majority class, Major Metric Value, by more than some amount
speciﬁed by (1-Increment Min). We used Increment Min = 5% for our experiments.
The algorithm terminates when the performance threshold is violated for any class
speciﬁed in Major List or Minor List. The details are presented in Fig. 2.

Note that Minor Metric Value and Major Metric Value are dependent on the objec-

tive evaluation function guiding the wrapper.

2.3 Wrapper-based algorithm to select SMOTE percentages

The data set is undersampled by the amounts discovered in the previous step. We
choose to undersample ﬁrst, as we want to ﬁrst eliminate any majority class
examples that did not add any improvement in the evaluation metric. Moreover, it also

123

N.V. Chawla et al.

1: SampleDecrement = 10%
2: IncrementM in = 5%
3: for all M Class in M ajorList do
4:
5: end for
6: repeat
7:

for all M Class in M ajorList do

StopU nderSamplingF lag[M Class] = f alse

8:
9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

if StopU nderSamplingF lag[M Class] = f alse then

U nderSampleList[M Class]
SampleDecrement
for F old ← 1 to N umF olds do

=

U nderSampleList[M Class] −

OutputT rainingData(F old(cid:1) U nderSampleList)
OutputT estingData(F old)
Build classiﬁer on the undersampled training set and evaluate on validation
set

end for
Update M inorM etricV alues & M ajorM etricV alues
if (Average(M inorM etricV alues) < Average(BestM inorM etricV alues))
OR
(Average(M ajorM etricV alues)
Average(BestM ajorM etricV alues)) then

(1. 0 − IncrementM in) ×

<

U nderSampleList[M Class] −

U nderSampleList[M Class]
SampleDecrement
StopU nderSamplingF lag[M Class] = true

=

Update BestM inorM etricV alues & BestM ajorM etricV alues

else

end if

end if
end for

23:
24: until StopU nderSamplingF lag for at least one M Class = true

Fig. 2 Wrapper Undersample Algorithm

reduces the size of the training set, speeding up learning. The performance estimate
on the minority or positive class (Minor Metric Value) obtained by undersampling
becomes the new benchmark or baseline to guide the wrapper. We only decide to
SMOTE if it improves performance over this baseline.

The Wrapper SMOTE algorithm evaluates different amounts of SMOTE at steps
of 100%.1 This is a greedy search, and at each step the new performance estimates
become the new baseline. That is, the initial baseline is the performance obtained
via the Wrapper Undersample. If SMOTE = 100% improves the performance over
that baseline by some margin Increment Min (set at 5% for our experiments; see line
17 in the algorithm), then the performance achieved at SMOTE = 100% becomes the
new baseline. The amount of SMOTE is then incremented by Sample Increment, and
another evaluation is performed to check if the performance increase at new SMOTE
amount is at least greater than Increment Min. This process repeats, greedily, until no
performance gains are observed. The amount of SMOTE is added at increments of

1 These steps are denoted by Sample Increment in the algorithm.

123

Automatically countering imbalance

100%. The ﬁvefold cross-validation, as previously described, is used for guiding the
wrapper.

However, there is an important caveat to the search to avoid being trapped in a
local maximum. If the average does not improve by 5% then to verify that we have
not settled on a local maximum, we look ahead two more steps at increasing amounts
of SMOTE. If the look-ahead does not result in an improvement in performance, then
the amount of SMOTE is reset to the value discovered prior to the look-aheads. This
is done to allow SMOTE to introduce additional examples with the aim of improving
performance. However, if the addition of examples does not help, then we go back to
using the lesser amount of SMOTE discovered prior to the look-ahead. The pseudo
code for the wrapper SMOTE algorithm is presented in Fig. 3.

We do not check for the Major Metric Value, as we are primarily interested in
optimizing the performance on the minority class in this stage, with the premise that
the true positives are more important than false positives.

1: SampleIncrement = 100
2: LookupAheadV alue = 3
3: for all M Class in M inorList do
4:

StopSmoteF lag[M Class] = f alse
LookupAhead[M Class] = 1

5:
6: end for
7: repeat
8:

for all M Class in M inorList do

if StopSmoteF lag[M Class] = f alse then

9:
10:

11:

12:

13:

14:

15:

16:

17:

18:

19:
20:

21:

22:

23:

24:

25:

26:

27:

28:

SmoteList[M Class] = SmoteList[M Class] + SampleIncrement
for F old ← 1 to N umF olds do

OutputT rainingData(F old, U nderSampleList, SmoteList)
OutputT estingData(F old)
Build classiﬁer on the sampled training set and evaluate on validation set

end for
Update M inorM etricV alues
if Average(M inorM etricV alues)
<
Average(BestM inorM etricV alues) then

if LookupAhead[M Class] < LookupAheadV alue then
LookupAhead[M Class] = LookupAhead[M Class] + 1

(1.0 + IncrementM in) ×

SmoteList[M Class] = SmoteList[M Class] −( LookupAhead[M Class] ×
SampleIncrement)
StopSmoteF lag[M Class] = true

else

end if

else

end if

end if
end for

Update BestM inorM etricV alues
LookupAhead[M Class] = 1

29:
30: until StopSmoteF lag for at least one M Class = true

Fig. 3 Wrapper SMOTE Algorithm

123

N.V. Chawla et al.

2.4 Evaluation metrics

The only way to determine optimal sampling levels is through empirical analysis.
Cross-validation is required because optimality of the sampling methods must be deter-
mined on the training data only. One question remains: given our sampling space, how
does one determine the “best” sampling levels? Thus, we will explore four classiﬁer
evaluation criteria which were deployed and empirically tested.

2.4.1 Cost

When optimizing sampling levels to improve overall cost, a logical evaluation criteria
is the cost itself. Cost is calculated as shown in Eq. 1 when we assume C(+|+) =
C(−|−) = 0.

cost = FN rate × C(−|+) + FP rate × C(+|−)

(1)

Therefore, once classiﬁcation occurs in the cross-validation phase, the wrapper calcu-
lates the validation cost and uses this information to select optimal sampling levels.
This approach is dependent on a priori knowledge of the cost relationship between
classes. Different cost ratios may cause selection of the different optimal sampling
levels and thus generate very different classiﬁers.

In the subsequent discussion, we will refer to the wrapper approach using cost as

wrapper-cost.

2.4.2 F-measure

The f-measure tends to be used in information retrieval. It is a composite metric based
on precision and recall (Dumais et al. 1998; Mladenic and Grobelnik 1999; Lewis and
Ringuette 1994; Cohen 1995b), where precision and recall are deﬁned as follows:

precision =

TP

TP + FP

r ecall =

TP

,

TP + FN

where TP = True Positives, FP = False Positives, TN = True Negatives, and FN =
False Negatives.

As a composite statistic, f-measure is then calculated from precision and recall to

summarize the effects of the two types of errors.

f measure = 2 × precision × recall
recall + precision

It is desirable to increase recall without a sacriﬁce in precision. Therefore, an effective
wrapper strategy is to determine the sampling levels which maximize the classiﬁer’s

123

(2)

(3)

(4)

Automatically countering imbalance

f-measure. This reduces error on the relatively expensive minority class while main-
taining accuracy on the majority class. This wrapper evaluation function has an advan-
tage over the cost-based wrapper in that a priori knowledge of the cost matrices is not
required. The generated classiﬁer can be consistently applicable throughout alterna-
tive unseen class distributions and cost relationships. In the subsequent discussion, we
refer to the wrapper approach using the f-measure as wrapper-f measure.

2.4.3 β varied f-measure

Another wrapper evaluation criterion is based on an alternate derivation of the pre-
viously outlined f-measure. Here, a separate β factor is incorporated to introduce an
element of relative importance between recall and precision:

f measureβ =

(1 + β2) × precision × recall

β2 × recall + precision

As β decreases, the importance of precision diminishes. To adapt this behavior to a
cost-based framework the β value must depend on the cost ratio. Therefore, β will be
set as:

(5)

(6)

β = C(+|−)
C(−|+)

As misclassifying minority examples becomes more costly relative to majority exam-
ples, improving recall affects the f-measure more heavily than precision. Therefore, the
minority class accuracy becomes more important as the costs become more divergent.
Like the cost-based wrapper, this metric is dependent on a predeﬁned cost matrix. In
the subsequent discussion we refer to the wrapper approach using β varied f-measure
as wrapper-β.

2.4.4 Area Under the ROC Curve (AUROC)

A Receiver Operating Characteristic curve (or ROC curve) is obtained by modifying
the parameters of the learning algorithm to get different true positive/false positive
percentages for two class problems. For classiﬁers which produce class probabilities,
a varying threshold on the probability of belonging to a class can be applied to get the
different “operating” points (e.g., a threshold of 0.3 would mean all examples classi-
ﬁed with probability greater than 0.3 or class A are considered to have label A). After
obtaining a set of points, they can be graphed. The points can be connected by lines
to approximate a curve. The area under this curve provides a method for comparing
classiﬁers (AUROC). An AUROC of one indicates a classiﬁer, which gets all of the
true positives correct and has no false positives. An AUROC of 0.5 would indicate that
for every true positive a classiﬁer generates a false positive. The greater the AUROC,
the better the classiﬁer is at obtaining true positives with fewer false positives. Like
the f-measure wrapper, this metric is cost independent and a classiﬁer constructed
in these terms should be ﬂexible through a wide range of costs and distributions. In

123

N.V. Chawla et al.

the subsequent discussion, we will refer to the wrapper approach using AUROC as
wrapper-AUROC.

2.5 A scalable implementation

The Achilles heel of any wrapper system is the computation time due to the explo-
ration of different parameters. However, the proposed wrapper framework for under-
sampling and SMOTE lends itself very well to a distributed computing paradigm.
This also offers an optimal utilization of idle machines in a department or institution.
We used the Condor framework (Thain et al. 2005) to enable distributed computation
on the University of Notre Dame machines resulting in significantly reduced time to
obtain a ﬁnal classiﬁer.2 Figure 4 shows the general implementation framework and
is discussed below.

1. First, partition the data into 10-folds. Distribute each of the folds to a different

computer.

2. Further partition each training fold into ﬁvefolds for the wrapper. Subsequently,

each of the ﬁvefolds can also be distributed to ﬁve different computers.

3. The sampling parameters were used in each of the ﬁvefolds simultaneously, and
an average performance estimate was computed. First, the search for undersam-
pling parameters was undertaken. Once the “best” undersampling parameters were
found the SMOTE phase was started. Again, the SMOTE parameters were applied
to each of the ﬁvefolds simultaneously. The performance estimates were aggre-
gated from each of the ﬁvefolds, and once the best combination of undersampling
and SMOTE parameters were found, the search was stopped.

We exploited this inherent parallelism in the parameter search, and were able to
achieve on the order of (average) 30 times speedups as discussed in Sect. 5.1. It is
worth noting that our largest data set contains just over 11,000 examples. Using larger
data sets may affect the parallel speedup, either negatively or positively.

2.6 Classiﬁers

We used two different machine learning algorithms for our experiments as base classi-
ﬁers—a decision tree learner, C4.5 release 8 (Quinlan 1993) and a rule learner, Ripper
(Cohen 1995a). We converted the leaf frequencies produced by C4.5 decision trees to
probability estimates after applying Laplace smoothing (Provost and Domingos 2003).
For Ripper, the probabilities were generated based on rule conﬁdence: the proportion
of a single class versus the total examples covered. As with C4.5, this was modiﬁed
with Laplace smoothing by introduction of a 1/C class prior probability.

We also considered class prediction by setting a probability threshold. Elkan (2001)
describes a system for generating optimal predictions in a two class problem based
on a known cost matrix. Here, we are interested in the probability threshold for

2 Condor can be downloaded from the University of Wisconsin Condor website.

123

Automatically countering imbalance

Fig. 4 Distributed wrapper implementation

predicting one class over another. Therefore, we consider the conditions under which
the expected costs for such a decision are equivalent:

P(y = −|x)C(+|−) + P(y = +|x)C(+|+) = P(y = +|x)C(−|+)

+ P(y = −|x)C(−|−)

(7)

Given that p∗ = P(y = +|x) and that P(y = +|x) = 1 − P(y = −|x) under the

two class assumption:

(1 − p

∗)C(+|−) + p

∗

C(+|+) = p

C(−|+) + (1 − p

∗

∗)C(−|−)

(8)

Rearranging this equation yields:

∗ =

p

C(+|−) − C(−|−)

C(+|−) − C(+|+) + C(−|+) − C(−|−)

Under our assumptions, C(+|+) = C(−|−) = 0 and we may therefore drop these

terms from consideration.

∗ =

p

C(+|−)

C(+|−) + C(−|+)

Therefore, when we observe a probability estimate p on a testing example, an opti-
mal decision is generated by comparing p to the threshold p∗. When p > p∗, the
example is labeled as the positive class, otherwise it is predicted as belonging to the
negative class.

(9)

(10)

123

N.V. Chawla et al.

3 Experimental framework

We used a variety of data sets for our study, including the KDD-99 cup data set. Eight
data sets have no associated costs and we call them unﬁxed cost data sets. These are
summarized in Table 1. Four of the data sets are from the UCI repository, and the
other four are real-world examples that we acquired from different sources. To estab-
lish a context, we will brieﬂy introduce the eight unﬁxed cost data sets. The ﬁxed cost
KDD-99 cup data set will be discussed separately in Sect. 5.

1. The Phoneme data set is used to distinguish between nasal and oral sounds based
on ﬁve features, and is from the ELENA project. The distribution is 3818 nasal
and 1586 oral samples.

2. The Segment data set (Blake et al. 1998) originally consisted of six equally dis-
tributed classes from an image segmentation problem. We transformed it into a
two class problem by considering one class of size 330 examples as the minority
class, and the rest of classes as a single majority class with 1980 examples.

3. The E-state data set (Chawla et al. 2002) consists of electro-topological state
descriptors for a series of compounds from the National Cancer Institute’s Yeast
AntiCancer drug screen. The activity classes are either active—at least one single
yeast strain was inhibited more than 70%, or inactive—no yeast strain was inhib-
ited more than 70%. The data set has 5,322 samples with 635 samples of active
compounds.

4. The Page data set (Esposito et al. 1994) consists of classifying all the blocks of
a page layout of a document that has been detected by a segmentation process.
This data set originally had ﬁve classes, and we have converted it to two classes:
4,913 text examples and 560 non-text examples.

5. The Satimage data set (Blake et al. 1998) originally contained six classes. For
our purposes, we used the two-class conversion outlined in (Provost et al. 1998),
where the smallest class receives one label and the others are part of the larger
class. This produced a data set with 5,809 majority class examples and 626 minor-
ity class examples.

Table 1 Data set distributions, in an increasing order of class imbalance

Data set

Examples

Features

Class balance

5,404

2,310

5,322

5,473

Phoneme

Segment

E-state

Page

Satimage

Compustat

Oil

123

6,435
Training = 10,358; Testing = 3,258
937

Mammography

11,183

5

19

12

10

36

20

49

6

79:21

86:14

88:12

90:10

90:10

96:4

96:4

98:2

Automatically countering imbalance

6. Compustat North America is a database of U.S. and Canadian fundamental and
market information on various active and inactive publicly held companies. It
provides more than 300 annual Income Statement, Balance Sheet, Statement of
Cash Flows, and supplemental data items. We extracted various ﬁnancial ratios
from the database, and after ﬁltering companies based on the rating we had a total
of 10,358 in the training set and 3,258 in the testing set. To make the problem
resemble a real-world scenario, the training set comprised data from 1995 to 2001
and the testing set comprised data from 2002 to 2004. The interested reader can
acquire the Compustat data set from the authors.

7. The Oil data set was provided by Kubat et al. (1998). This data set has 41 oil slick

samples and 896 non-oil slick samples.

8. The Mammography data set (Woods et al. 1993) has 11,183 total examples with
260 calciﬁcations. Using a trivial classiﬁer would yield 97.68% default accuracy.
However, such a classiﬁer would most likely yield a poor total cost, particularly
when the cost ratio is very skewed.

We used ﬁve random runs of 10-fold cross-validation framework for all but the
Compustat data set since it was already associated with a testing set. We wanted to
retain the complexity of the original Compustat testing set, as it was derived from a
different time period.

While these data sets are devoid of any known testing costs, we injected a range of
cost ratios to carefully establish the generality of the wrapper framework and compare
to the cost-sensitive learning methods. As a convention, we will use the notation of

UCI Datasets with

Experimental Cost Values

SMOTE

and Undersampling

C4.5

Ripper

MetaCost
− C4.5

MetaCost
− Ripper

CSC
− C4.5

CSC

−Ripper

Apply wrapper and use metrics to
select optimal sampling levels.

Beta

Cost

f−measure

ROC

Static misclassification costs generated for
scalar comparison. Performance also evaluated
on generated Cost Curves.

Fig. 5 Experiments overview

123

N.V. Chawla et al.

(FN:FP) to indicate the cost ratio for making a false negative error to a false positive
error. For instance a ratio of (2:1) indicates that it is twice as costly to make a false
negative than to make a false positive. This aligns with the premise that it is more
costly to call a positive (minority) class negative than to call a negative (majority)
class positive.

We compare the wrapper and cost-based learners under several cost scenarios.
First, we consider a case where costs are known at training and testing times. Then,
we investigate a situation where costs will remain unknown at both training and testing
times. Finally, we study the generalization of classiﬁers through the cost space. We
also compare the impact of different evaluation criteria in the wrapper mode. The C4.5
and Ripper classiﬁers learned on the resampled data sets (after Wraper) are compared
with MetaCost (Domingos 1999) and Cost-Sensitive Classiﬁer (CSC) (Zadrozny et al.
2003) using the same base C4.5 and Ripper classiﬁers. We used the Weka implemen-
tations of MetaCost and Cost-Sensitive Classiﬁer (Witten and Frank 2005). Figure 5
provides an overview of our experiments.

4 Results: unﬁxed cost data sets

We will ﬁrst discuss the analysis using AUROC and f-measure and then present the
cost-based results.

4.1 AUROC and f-measure analysis

Our ﬁrst experiment considered the effect of using AUROC and f-measure for guiding
the wrapper search under a (1:1) cost environment. This experiment allowed us to
observe the interaction between an objective function for the wrapper, sampling level,
and an evaluation metric on the testing set. Tables 2 and 3 summarize the results with
C4.5 and Ripper as base classiﬁers, respectively. The results are averaged over the 50
runs (ﬁve different random trials and 10-fold CV during each trial). The standard devi-
ations are also shown beside the averaged performance measures. The cost column
represents the cost of making errors at (1:1); thus, it is essentially the total number
of errors. For instance, for the Mammography data set if AUROC is used with the
wrapper, the amount of SMOTE is 720%, 14% of the majority class is removed, and
the testing set f-measure is 0.619 and AUROC is 0.907.

We found that SMOTE and undersampling generally help the base classiﬁers
learned on the different data sets. The results carry more significance for Ripper
(see Table 3). Irrespective of the wrapper evaluation function, the sampling methods
always result in an improved AUROC over the base classiﬁer. In all but two cases, the
f-measure is also improved. The two deviations from the norm are the Page and Mam-
mography data sets. While Page had marginal deterioration over the base classiﬁer,
the wrapper-AUROC on the Mammography data set results in a 2.5% drop over the
base classiﬁer for the f-measure. However, AUROC is always improved. We believe
this to be an artifact of a higher FPrate as we focus on the minority class being less
detrimental.

123

Automatically countering imbalance

Table 2 Average sampling levels and 10-fold performance measures for a (1:1) cost matrix (equal costs
of errors) with the C4.5 classiﬁer, including the standard deviations

Wrapper-evaluation

S

US

Cost

f-measure

AUROC

Phoneme

Base

Segment

Base

f-meas

AUROC

f-meas

AUROC

Base

f-meas

AUROC

Base

f-meas

AUROC

f-meas

AUROC

Base

f-meas

AUROC

f-meas

AUROC

E-State

Page

Oil

Satimage

Base

Mammo.

Base

–

212

562

–

460

420

640

1030

–

–

200

690

–

622

644

–

260

300

–

200

720

13

–

20

21

–

6

2

–

8

–

3

3

–

6

4

–

2

–

2

15

14

364 ± 50
415 ± 53
436 ± 51
9 ± 6
5 ± 3
7 ± 4
319 ± 5
1176 ± 297
1298 ± 276
78 ± 19
86 ± 22
94 ± 22
259 ± 19
275 ± 27
258 ± 16
25 ± 6
21 ± 7
22 ± 8
76 ± 15
92 ± 19
120 ± 29

0.772 ± 0.032
0.778 ± 0.025
0.772 ± 0.024
0.973 ± 0.018
0.985 ± 0.012
0.977 ± 0.012
0.003 ± 0.009
0.227 ± 0.018
0.242 ± 0.021
0.857 ± 0.039
0.856 ± 0.033
0.847 ± 0.033
0.569 ± 0.029
0.609 ± 0.036
0.628 ± 0.026
0.314 ± 0.256
0.467 ± 0.119
0.518 ± 0.133
0.644 ± 0.101
0.672 ± 0.063
0.619 ± 0.058

0.905 ± 0.018
0.922 ± 0.017
0.921 ± 0.019
0.957 ± 0.011
0.966 ± 0.008
0.966 ± 0.008
0.495 ± 0.006
0.580 ± 0.030
0.597 ± 0.027
0.959 ± 0.013
0.972 ± 0.004
0.972 ± 0.006
0.900 ± 0.019
0.919 ± 0.010
0.926 ± 0.014
0.631 ± 0.119
0.722 ± 0.110
0.736 ± 0.101
0.863 ± 0.031
0.896 ± 0.035
0.907 ± 0.038

The results are averaged over the ﬁve different random runs. Wrapper evaluation indicates the evaluation
function used for guiding the wrapper mode. The SMOTE and Undersample columns indicate the amounts
of SMOTE and undersampling discovered by the wrapper. The subsequent columns indicate the evalu-
ation function used on the testing set. Base indicates the base classiﬁer without any sampling. The best
performance is shown in bold

We note that there is not a distinct positive correlation between the f-measure in the
wrapper-mode and corresponding improvements in the ﬁnal evaluation. For instance,
C4.5 improved the f-measure on four data sets when using AUROC as the wrapper
evaluation metric rather than the f-measure. The question then becomes, why does
this happen with the f-measure and not with AUROC? We attribute this to the nature
of the two metrics. The f-measure is tuned to a ﬁxed threshold of 0.5. This means that
the ﬁxed point threshold of 0.5 can become unstable. We believe this results in more
generalized performances when using AUROC as the wrapper evaluation function.
Essentially, our goal was to identify which one was less prone to reaching a local
maximum in the wrapper scheme.

Both wrapper-AUROC and wrapper-f measure generally result in different amounts
of SMOTE and undersampling, as they optimize different properties of the classiﬁer.
As expected using different classiﬁers also results in different levels of sampling. We

123

N.V. Chawla et al.

Table 3 Average sampling levels and 10-fold performance measures for a (1:1) cost matrix (equal costs
of errors) with the Ripper classiﬁer

Wrapper-evaluation

SM

US

Cost

f-measure

AUROC

Phoneme

Base

Segment

Base

E-State

Page

Oil

Satimage

Base

f-meas

AUROC

f-meas

AUROC

Base

f-meas

AUROC

Base

f-meas

AUROC

f-meas

AUROC

Base

f-meas

AUROC

f-meas

AUROC

Mammo.

Base

Same convention as Table 2 holds

–

150

162

–

300

460

–

520

660

–

190

680

–

400

488

310

570

–

–

180

240

–

9

6

–

5

9

–

–

5

–

6

7

–

6

–

5

9

24

18

14

11

385 ± 45
491 ± 39
481 ± 59
9 ± 4
6 ± 4
7 ± 5
318 ± 4
1501 ± 372
1480 ± 160
78 ± 16
84 ± 16
99 ± 25
226 ± 23
263 ± 47
287 ± 48
24 ± 8
21 ± 9
21 ± 10
78 ± 16
81 ± 14
92 ± 22

0.748 ± 0.032
0.753 ± 0.017
0.759 ± 0.025
0.971 ± 0.014
0.982 ± 0.015
0.979 ± 0.016
0.000 ± 0.000
0.236 ± 0.018
0.235 ± 0.012
0.859 ± 0.026
0.862 ± 0.024
0.845 ± 0.036
0.571 ± 0.055
0.645 ± 0.047
0.633 ± 0.037
0.270 ± 0.249
0.521 ± 0.180
0.572 ± 0.155
0.648 ± 0.096
0.699 ± 0.047
0.675 ± 0.061

0.823 ± 0.022
0.899 ± 0.015
0.902 ± 0.020
0.956 ± 0.007
0.968 ± 0.006
0.967 ± 0.007
0.493 ± 0.000
0.579 ± 0.029
0.574 ± 0.018
0.906 ± 0.017
0.966 ± 0.011
0.969 ± 0.006
0.720 ± 0.029
0.918 ± 0.017
0.925 ± 0.013
0.529 ± 0.122
0.695 ± 0.092
0.723 ± 0.110
0.756 ± 0.052
0.869 ± 0.023
0.874 ± 0.026

posit that the amount of SMOTE or undersampling is not dependent on the class skew,
but on the properties of the feature space. Consider the E-state data set, which has
the largest amount of SMOTE and undersampling and yet it is certainly not the most
unbalanced data set. If we look at the statistics, both C4.5 and Ripper have a very poor
performance on E-state. However, there is a significant improvement offered by the
sampling methods. We believe that a wrapper method can allow one to empirically
discover the relevant amounts of sampling, as it is certainly intrinsically tied in with
the data properties. Lastly, as expected, the base classiﬁer achieves the lowest cost at
(1:1) when both FP and FN are equally costly.

Table 4 shows the average time (in seconds) taken on a data set for a single fold. We
show the time taken by both the sequential and distributed versions of the wrap-
per framework. As a representative of all the wrappers, we show the timings of
the wrapper-AUROC method, especially given that this results in larger amounts of

4.2 Timing

123

Automatically countering imbalance

Table 4 Time in seconds by different approaches

Data set

Wrapper-AUROC

Wrapper-AUROC

Wrapper-AUROC

Wrapper-AUROC

SEQ (C4.5)

DIST (C4.5)

SEQ (Ripper)

DIST (Ripper)

Compustat

Estate

Mammography

Oil

Page

Phoneme

Satimage

Segment

Average

1,061

3,028

181

30

75

1,816

2,797

726

1,214.25

22

71

7

2

9

87

105

33

42

SEQ: Sequential version; DIST: Distributed version

3,353

727

308

45

276

673

3,010

120

1,064

89

22

14

2

16

33

81

5

32

sampling than the wrapper-f measure. Exactly the same underlying code-base is part
of the sequential and distributed runs. The distributed implementations provide a sig-
nificant improvement in timing, making the wrapper a very competitive approach. On
an average, it takes only 42 s with C4.5 and 32 s with Ripper to complete the wrapper
search, which is very encouraging especially in the light of performance improve-
ments.

We note that the wrapper with Ripper is faster than the wrapper with C4.5 because
of the relatively lower amounts of undersampling and SMOTE with Ripper as the
base classiﬁer. The time taken for the entire wrapper paradigm is largely a function of
the number of minority class examples and the associated learning complexity, which
drives the search for the SMOTE amount.

4.3 Comparison to cost-sensitive classiﬁers

We wanted to examine an empirical relationship between learning from imbalanced
data sets and cost-sensitive classiﬁcation. Can we effectively handle the problem of
imbalance and achieve lower costs during testing than cost-sensitive classiﬁers?

We compared the wrapper-induced classiﬁers with MetaCost (Domingos 1999) and
CostSensitiveClassiﬁer (CSC) (Zadrozny et al. 2003). MetaCost wraps a “meta-learn-
ing” stage around an error-based classiﬁer to effectively minimize cost. It estimates
the conditional probability distributions of a data set via bagging (Breiman 1996)
Combining probability estimates with a simple expected-cost framework, MetaCost
j P(y = j|x)C(i| j). This essentially relabels a class in
selects the class as argmini
the training data such that the expected cost is minimized. A classiﬁer is then learned
on the relabeled data and evaluated on the testing set.

(cid:1)

The CSC method draws a new training set in accordance with each example’s mis-
classiﬁcation cost. However, when costs are highly skewed, overﬁtting tends to occur
as a limited number of examples are resampled heavily. To counteract this, rejection

123

N.V. Chawla et al.

sampling is used. The newly drawn training set keeps a selected example with a
probability of c
Z where c is the examples’ misclassiﬁcation cost and Z is a selected
constant greater than the maximum cost of the original training set.

We show comparisons using two illustrations. First, we use cost trends where the
y-axis indicates the total cost as the cost ratio varies on the x-axis. Second, we use the
cost curves (Drummond and Holte 2006).

4.3.1 Cost trends

Here, we compared the wrapper framework for countering class imbalance with the
MetaCost and Cost-Sensitive Classiﬁer (CSC). We considered various cost ratios of
(FN:FP)—(2:1), (5:1), (10:1), and (20:1). We used these cost ratios, as these have
been previously considered in other studies (Domingos 1999; Drummond and Holte
2006).

If the classiﬁers were built without utilization of a cost-matrix, then the generated
posterior probabilities were thresholded at the corresponding cost ratios for the testing
set. This scenario can be used when the costs are completely unknown during training.
The classiﬁers generate a posterior probability distribution on the testing set. Depend-
ing on the cost environment during the testing stage, the probabilities can effectively
be thresholded to reﬂect the cost distributions. That is the threshold for decision mak-
ing can be derived from the operating cost ratio (see Eq. 10). However, the classiﬁers
are not re-learned. The base classiﬁer, wrapper-f measure, and wrapper-AUROC fall
under this umbrella. This allows us to directly compare the effectiveness of dealing
with imbalance without incorporating costs during the training phase. We would like
to point the reader to Appendix A, which shows the beneﬁts from thresholding to
reﬂect the costs after applying SMOTE and undersampling. On the other hand, if
the classiﬁers incorporated costs during learning, such as wrapper-cost, wrapper-β,
MetaCost and CSC, then no thresholding was performed. This is under the assumption
that the classiﬁers have already been optimized on an objective function attuned to
a particular cost. Thus, we generated multiple such classiﬁers for the different cost
ratios.

Figures 6 and 7 show the range of costs obtained across the eight data sets, for dif-
ferent cost ratios. The x-axis in the ﬁgures show the different data sets and the y-axis
shows the normalized costs. We normalized the costs between 0 and 1 to enable a
better display of results.

The results show an interesting trend. At (2:1), the cost-sensitive classiﬁers gener-
ally resulted in lower costs than the wrapper-based methods. However, as we increased
the cost ratios, particularly at (10:1) and (20:1), the wrapper based methods begin to
result in lower costs than the cost-sensitive classiﬁers that are directly trained on the
corresponding costs. The wrapper-AUROC and wrapper-f measure make a very com-
pelling case at higher costs; without using costs during training, they are able to often
outperform the cost-sensitive classiﬁers. The results hold with both C4.5 and Ripper.
Thus, a combination of the sampling approaches for countering class imbalance and
thresholding is effective in overcoming the cost distributions. In fact we found that the

123

Automatically countering imbalance

Fig. 6 Cost trends with C4.5 as base classiﬁer. Beta is wrapper-β and Cost is wrapper-cost

wrapper-AUROC and wrapper-f measure are quite effective across the cost ranges.
We conjecture this to be an outcome of an improvement in the quality of estimates,
thus resulting in the reduced overall costs (Cieslak and Chawla 2006). The effective
treatment of imbalance without using costs during training can result in robust poster-
ior probability estimates applicable to a variety of cost scenarios during testing. This
is certainly desirable, especially when the costs are unknown or can change during
testing. Between C4.5 and Ripper, the former is more effective as a classiﬁer in reduc-
ing the costs during testing. Lastly, all the methods result in lower costs over the base
classiﬁer at cost ratios > (2:1). Thus, treating for imbalance or changing the objective
function to be cost-sensitive is always preferred over the thresholded estimates from
a base classiﬁer.

Zhou and Liu (2006) found that there might be a disconnect between learning from
imbalanced data sets and cost-sensitive learning, which is contrary to our observations.
However, we believe the differences in their conclusions can be attributed to a lack of
sufﬁcient search in the sampling parameter space. They resort to “default” sampling
parameters, but, as we point out, there are really no “default” sampling parameters for
a data set and a classiﬁer. We show that using wrapper-induced sampling methods on a
multi-class data set, we were able to achieve lower costs than any other known method.
Lastly, their cost functions were constrained to 1:10, while we extended our study to
1:100. However, our comprehensive experiments on real-world data sets, including
the to be discussed, multi-class KDD-99 Cup data set, demonstrate that treating a
data set for imbalance ﬁrst is often beneﬁcial when compared with cost-sensitive
classiﬁers.

123

N.V. Chawla et al.

Fig. 7 Cost trends with Ripper as base classiﬁer. Beta is wrapper-β and Cost is wrapper-cost

4.3.2 Cost curves

Conditioning on a particular TP rate or FP rate cut-off may not immediately establish
the expected generalization capacity of a classiﬁer. It may be more appropriate to
gauge the performance of classiﬁers across a range of TP rate and FP rate. Thus, cost
curves (Drummond and Holte 2006) founded on expected cost may yield a clearer
understanding of the classiﬁers within that operating environment. These curves plot
performance (expected cost normalized between 0 and 1) on the Y -axis. This function
is given by:

NE[C] = (1 − TP rate − FP rate) × PCF(+) + FP

(11)

The X -axis combines cost and class distribution in the following manner:

PCF(+) =

p(+)C(−|+)

p(+)C(−|+) + p(−)C(+|−)

(12)

where C(−|+) is the cost of misclassifying a positive example as negative, C(+|−)
is the cost of misclassifying a negative example as positive, p(+) is the probability
of a positive example and p(−) = 1 − p(+) is the probability of a negative example.
This is a multi-faceted tool which enables sophisticated analysis of classiﬁer perfor-
mance under ﬂuid conditions. Overall, cost curves allow visualization of the interplay
of classiﬁers as the weighted importance of two classes wax and wane. There is a
correspondence of a point in ROC space to a line in cost space. A single (TP rate,
FP rate) pair from the ROC space gets translated to a line segment in the cost space.

123

Automatically countering imbalance

Thus, different operating points from the ROC curve will represent different lines in
the cost space.

For clarity in presentation, we show the lower envelopes of the cost curves of
the following four methods: wrapper-f measure, wrapper-AUROC, CSC, and Meta-
Cost. We wanted to draw our comparisons among classiﬁers optimized on cost-
insensitive measures and the cost-sensitive classiﬁers that incorporate costs during
learning. We constructed the cost curves as follows. The cost-sensitive classiﬁers–
MetaCost and CSC—were trained with the different cost ratios, drawn from the
following FN:FP – 1:1, 2:1, 5:1, 10:1, 20:1, 30:1, 40:1, . . .100:1. This resulted
in 13 pairs of (TP rate, FP rate) for each of the above methods or a total of 91
(13 × 7) cost curves. After plotting all the possible cost curves from different meth-
ods, we only retained their corresponding lower envelopes to reﬂect their best operating
ranges.

Figures 8 and 9 show the cost curves for C4.5 and Ripper, respectively. The x-axis
in the ﬁgures represents the Probability Cost Function (PCF) and the y-axis represents
Normalized Expected Cost (NEC). As seen in the ﬁgures, the methods start-off gen-
erally overlapping with each other or the cost-sensitive classiﬁers resulting in lower
envelopes, but as the PCF increases the classiﬁers begin to separate. At lower PCF,
the cost-sensitive classiﬁers result in slightly lower costs than the wrapper classiﬁers
for some data sets. As the PCF increases, the advantage of treating the data sets for
class imbalance stands out. Based on the comparisons across all the data sets, we ﬁnd
that treating the data sets for imbalance using the wrapper mode results in the most
effective sets of classiﬁers across the range of PCF. If a classiﬁer produces the lowest
envelope it is the most optimal across the entire range of PCF’s. Our observations
corroborate the ﬁndings in the previous subsection. The lower envelopes of the cost-
insensitive measures generally dominate over the cost-sensitive classiﬁers, especially
at higher costs. Thus, using wrappers to ﬁrst counter the class imbalance problem was
generally more cost-effective than the cost-sensitive classiﬁers. We also found that
wrapper-AUROC was either comparable to or better than wrapper-f measure, and can
be the recommended optimization criterion. The results are quite compelling as they
imply that optimizing on class imbalance, without incorporating costs, is often a more
optimal strategy. Between MetaCost and CSC, we found that MetaCost resulted in
lower costs using C4.5 as the base classiﬁer, but this trend got reversed with Ripper.
We believe MetaCost relies heavily on the instability of the base classiﬁer, given the
bootstraps.

5 Fixed cost KDD cup intrusion data set

We considered the intrusion detection data set from 1999 KDD Cup. This data set not
only provides actual costs for making different types of errors, but also demonstrates
the proposed wrapper framework on a multi-class data set. A cost matrix was used in
the scoring of the competition as shown in Table 5 (Elkan 1999).

We pre-processed the data set as follows. There were many duplicate examples in
the original 5 million example training set. All duplicate examples were removed. We
also undersampled both the normal and neptune (dos) class by removing examples

123

N.V. Chawla et al.

Fig. 8 C4.5 cost curves

which occurred only once. For Training Data 1 as in Table 5, we undersampled the
normal class, and for the Training Data 2 we undersampled both the normal and nep-
tune classes. Note that for both of these sets of experiments, the test set remained
unchanged. Table 6 shows the results.

It can be seen that our approach with RIPPER as the classiﬁer produced the low-
est cost per example after undersampling for both the normal and neptune classes

123

Automatically countering imbalance

Fig. 9 Ripper cost curves

(Training Data 2), and applying 100% SMOTE to the u2r class, while keeping the
r2l class unchanged. This was better than the winner of the contest and better than
the subsequent results from the literature. Also using C4.5 as the base classiﬁer with
SMOTE (200% u2r and 300% for r2l) resulted in lower cost than the other published
techniques. Note that the respective amounts of SMOTE and undersampling were
discovered via the Wrapper.

123

Table 5 Cost matrix used for
scoring entries in KDD-99 CUP
competition

N.V. Chawla et al.

Actual/predicted

dos

u2r

r2l

Probe

Normal

dos

u2r

r2l

Probe

Normal

0

2

2

2

2

2

0

2

2

2

2

2

0

2

2

1

2

2

0

1

2

3

4

1

0

Table 6 Comparison of results obtained on the original KDD CUP 99 test data

Winning Strategy (Elkan 1999)

Decision Tree (Amor et al. 2004)

Naive Bayes (Amor et al. 2004)

Multi-classiﬁer (Sabhnani and Serpen 2003)

Using C4.5 on Training Data 1 u2r (200)—r2l (0)

Using RIPPER on Training Data 1 u2r (100)—r2l (0)

Using C4.5 on Training Data 2 u2r (200)—r2l (300)

Cost per test example

0.2331

0.2371

0.2485

0.2285

0.2478

0.2444

0.2051

Using RIPPER on Training Data 2 u2r (100)—r2l (0)

0.2049 (Chawla et al. 2005)

The numbers beside u2r and r2l indicate the SMOTE percentage utilized for the experiments

6 Conclusions

In this work, a wrapper approach was utilized to determine the percentage of minority
examples to add to the training set and the percentage for undersampling the majority
class examples. We used a variety of data sets with different class distributions and
characteristics, including a number of real world data sets. The wrapper approach
works by performing a guided search of the parameter space. The evaluation function
was applied with a ﬁvefold cross-validation on the training set. Once the best percent-
ages for undersampling and SMOTE are found they can be used to build a classiﬁer on
the updated training set and applied on the unseen testing set. We have demonstrated
the ability to optimize sampling levels for an evaluation function, resulting in effective
generalization performance.

We also wished to view the impact of using a threshold function on the probabilities
produced by a classiﬁer to generate optimal decisions in a cost-based problem. As we
note in Appendix A, cost thresholding works particularly well at reducing the total
cost of a classiﬁer, particularly when operating at a higher cost ratio. Thus, we may
assume that sampling has allowed classiﬁers to develop a calibration of probabilities
better suited for higher cost ratios.

Our conclusions, connected to the questions posited in Sect. 1, can be summarized

as follows.

123

Automatically countering imbalance

• The proposed wrapper-based paradigm, when using different evaluation functions,
was effective in automatically discovering the potentially optimal amounts of sam-
pling for a data set. This was effective in countering the cost-imbalance during
training and testing as well. We showed that the classiﬁers learned on sampling
parameters discovered when using the cost-transparent metrics, such as AUROC
and f-measure, retain their effectiveness when costs were introduced during the
testing stage. Both the metrics were very effective compared to the base classiﬁer
as demonstrated by the AUROC and f-measure on the testing sets. Thus, chang-
ing the data distribution is important to learning the classiﬁers in an imbalanced
domain.

• We also compared our approaches to cost-sensitive learners. In an analysis of raw
total cost, we found a strong tendency for wrapper-AUROC and wrapper-f mea-
sure to generally outperform the other wrapper evaluation functions. Moreover, at
higher cost ratios they also dominated over CSC and MetaCost.
Another set of comparisons among the classiﬁers was done using cost curves. Using
the lower envelope formed by multiple classiﬁers learned at different cost ratios,
it was also possible to examine each evaluation metric and the cost-based learners
throughout cost
section, we have noted that wrapper-
AUROC tends to produce classiﬁers that result in more effective generalization
in the PCF space, even better than the cost-sensitive classiﬁers optimized at dif-
ferent cost ratios.

In this

space.

• We also showed that a combination of undersampling and SMOTE offers a sig-
nificant advantage over several cost-based classiﬁers in a number of realistic cost
environments,
including champion level performance on a KDD Cup
Challenge.

Acknowledgements We are grateful to Robert Holte for providing the Cost Curves software and Oil data
set. David Cieslak was partially supported by the the Arthur J. Schmitt Fellowship. We are very thankful
to the reviewers and the guest editors for their helpful comments.

Appendix A: cost comparison between thresholded and unthresholded
C4.5 leaf estimates

Table A1 contains cost comparison between C4.5 decision trees learned using the
wrapper-AUROC and wrapper-f measure, called AUROC and f-measure in the table,
respectively. AUROC and f-measure are followed by unthresholded and thresholded.
Unthresholded is the decision at the leaf based on the default 0.5 threshold. Thres-
holded follows the technique outlined in Sect. 2.6 that uses the posterior probabilities
and the known cost ratio to generate a threshold during testing. The subsequent col-
umns in the table indicate the total misclassiﬁcation costs at different cost-ratios. The
classiﬁers are learned in the same manner; however, they differ in the method of eval-
uation on unseen examples. We note that thresholdeded predictions tend to be better at
higher cost ratios, but weaker in some cases on lower cost ratios. Better performance
on higher cost ratios is desirable, so we have elected to use the thresholded decision
process throughout this article.

123

Table A1 Cost comparison between thresholded and unthresholded C4.5 classiﬁers

(1:1)

(2:1)

(5:1)

(10:1)

(20:1)

N.V. Chawla et al.

E-State

AUROC thresholded

Phoneme

AUROC thresholded

Segment

AUROC thresholded

AUROC unthresholded

f-measure thresholded

f-measure unthresholded

AUROC unthresholded

f-measure thresholded

f-measure unthresholded

AUROC unthresholded

f-measure thresholded

f-measure unthresholded

AUROC unthresholded

f-measure thresholded

f-measure unthresholded

AUROC unthresholded

f-measure thresholded

f-measure unthresholded

AUROC unthresholded

f-measure thresholded

f-measure unthresholded

AUROC unthresholded

f-measure thresholded

f-measure unthresholded

AUROC unthresholded

f-measure thresholded

f-measure unthresholded

Page

AUROC thresholded

Satimage

AUROC thresholded

Compu.

AUROC thresholded

Oil

AUROC thresholded

Mammo.

AUROC thresholded

3495

3758

3325

3567

75

79

50

61

12985

12985

11765

11765

945

1055

865

976

2330

2763

2480

2902

1330

1330

1200

990

220

290

215

277

1200

1451

925

1165

4395

4285

4165

4169

105

105

70

82

16510

14095

15930

13215

1305

1259

1260

1224

3505

3679

3815

3852

2285

1690

1950

1375

405

387

370

383

1895

1805

1550

1528

6005

5866

5865

5975

195

183

160

145

20590

17425

20655

17565

1860

1871

1750

1968

5880

6427

5970

6702

4075

2770

3815

2530

710

678

770

701

3465

2867

2860

2617

7410

8501

7280

8985

220

313

220

250

22920

22975

22960

24815

2315

2891

2265

3208

7580

11007

7675

11452

4770

4570

4680

4455

1080

1163

1140

1231

4940

4637

4205

4432

9125

13771

9615

15005

405

573

370

460

24085

34075

23645

39315

3370

4931

2915

5688

9705

20167

9805

20952

6025

8170

6505

8305

1630

2133

1560

2291

7840

8177

6835

8062

Amor NB, Benferhat S, Elouedi Z (2004) Naive bayes vs. decision trees in intrusion detection systems. In:

Proceedings of the ACM symposium on applied computing, pp 420–424

Banﬁeld RE, Hall LO, Bowyer KW, Kegelmeyer WP (2005) Ensembles of classiﬁers from spatially disjoint
data. In: Proceedings of the sixth international conference on multiple classiﬁer systems, pp 196–205
Batista GEAPA, Prati RC, Monard MC (2004) A study of the behavior of several methods for balancing

machine learning training data. SIGKDD Explorations 6(1):20–29

References

123

Automatically countering imbalance

Blake CL, Newman DJ, Hettich S, Merz CJ (1998) UCI repository of machine learning databases. URL:

http://www.ics.uci.edu/~mlearn/MLRepository.html

Bowyer KW, Hall LO, Chawla NV, Moore TE (2000) A parallel decision tree builder for mining very
large visualization datasets. In: Proceedings of the IEEE International conference on systems, man
and cybernetics

Breiman L (1996) Bagging predictors. Machine Learn 24(2):123–140
Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP (2002) SMOTE: synthetic minority over-sampling

technique. J Artif Intel Res 16:321–357

Chawla NV, Hall LO, Joshi A (2005) Wrapper-based computation and evaluation of sampling methods for

imbalanced datasets. In: KDD workshop: utility-based data mining

Chawla NV, Japkowicz N, Kołcz A (eds) (2003) Proceedings of the ICML’2003 workshop on learning from

imbalanced data sets

rations 6(1):1–6

Chawla NV, Japkowicz N, Kolcz A (2004) Editorial: learning from imbalanced datasets. SIGKDD Explo-

Cieslak D, Chawla NV (2006) Calibration and power of PETs on unbalanced datasets. TR 2006-12,

Department of Computer Science and Engineering, University of Notre Dame

Cohen WW (1995a) Fast effective rule induction. In Prieditis A, Russell S (eds) 12th International confer-

ence on machine learning, Morgan Kaufmann, Tahoe City, CA, pp 115–123

Cohen WW (1995b) Learning to classify English text with ILP methods. In: 5th International workshop on

Dietterich T, Margineantu D, Provost F, Turney P (eds) (2000) Proceedings of the ICML’2000 workshop

Domingos P (1999) MetaCost: a general method for making classiﬁers cost-sensitive. In: Knowledge dis-

inductive logic programming, pp 3–24

on cost-sensitive learning

covery and data mining, pp 155–164

Drummond C, Holte R (2003) C4.5, class imbalance, and cost sensitivity: Why under-sampling beats

over-sampling. In: Proceedings of the ICML’03 workshop on learning from imbalanced data sets

Drummond C, Holte RC (2006) Cost curves: an improved method for visualizing classiﬁer performance.

Machine Learn 65(1):95–130

Dumais S, Platt J, Heckerman D, Sahami M (1998) Inductive learning algorithms and representations for
text categorization. In: Seventh international conference on information and knowledge management,
pp 148–155

Elkan C (1999) Results of the KDD’99 classiﬁer learning contest. http://www.cse.ucsd.edu/~elkan/clresults.

html

Intel 8:33–84

Elkan C (2001) The foundations of cost-sensitive learning. In: Proceedings of the seventeenth international

joint conference on artiﬁcial intelligence, pp 973–978

Esposito F, Malerba D, Semeraro G (1994) Multistrategy learning for document recognition. Appl Artif

Ferri C, Flach P, Orallo J, Lachice N (eds) (2004) First workshop on ROC analysis in AI. ECAI
Kubat M, Holte RC, Matwin S (1998) Machine learning for the detection of oil spills in satellite radar

images. Machine Learn 30(2–3):195–215

Kubat M, Matwin S (1997) Addressing the curse of imbalanced training sets: one sided selection. In: Pro-
ceedings of the fourteenth international conference on machine learning. Morgan Kaufmann, Nash-
ville, Tennesse, pp 179–186

Lewis D, Ringuette M (1994) A comparison of two learning algorithms for text categorization. In: 3rd

Annual symposium on document analysis and information retrieval, pp 81–93

Ling C, Li C (1998) Data mining for direct marketing problems and solutions. In: Proceedings of the fourth
international conference on knowledge discovery and data mining (KDD-98). AAAI Press, New York,
NY, pp 73–79

Maloof M (2003) Learning when data sets are imbalanced and when costs are unequal and unknown. In:

Proceedings of the ICML’03 workshop on learning from imbalanced data sets

Mladenic D, Grobelnik M (1999) Feature selection for unbalanced class distribution and naive bayes. In:

ICML, pp 258–267

Provost FJ, Domingos P (2003) Tree induction for probability-based ranking. Machine Learn 52(3):199–215
Provost FJ, Fawcett T, Kohavi R (1998) The case against accuracy estimation for comparing induction

algorithms. In: Fifteenth international conference on machine learning, pp 445–453

Quinlan JR (1993) Programs for machine learning. Morgan Kaufmann

123

N.V. Chawla et al.

Sabhnani MR, Serpen G (2003) Application of machine learning algorithms to KDD intrusion detection
dataset with misuse detection context. In: Proceedings of the international conference on machine
learning: models, technologies, and applications, pp 209–215

Thain D, Tannenbaum T, Livny M (2005) Distributed computing in practice: the condor experience. Concur

Comput Pract Exp 17:323–356

Weiss G, McCarthy K, Zabar B (2007) Cost-sensitive learning vs. sampling: which is best for handling

unbalanced classes with unequal error costs? In: DMIN, pp 35–41

Weiss G, Provost F (2003) Learning when training data are costly: the effect of class distribution on tree

induction. J Artif Intel Res 19:315–354

Witten IH, Frank E (2005) Data mining: practical machine learning tools and techniques. Morgan Kaufmann
Woods K, Doss C, Bowyer KW, Solka J, Priebe C, Kegelmeyer WP (1993) Comparative evaluation of
pattern recognition techniques for detection of microcalciﬁcations in mammography. Int J Pattern
Recog Artif Intel 7(6):1417–1436

Zadrozny B, Elkan C (2001) Learning and making decisions when costs and probabilities are both unknown.
In: Proceedings of the sixth ACM SIGKDD international conference on knowledge discovery and data
Mining, pp 204–213

Zadrozny B, Langford J, Abe N (2003) Cost-sensitive learning by cost-proportionate example weighting.

In: ICDM, pp 435–442

Zhou Z, Liu X (2006) Training cost-sensitive neural networks with methods addressing the class imbalance

problem. IEEE Trans Knowledge Data Eng 18(1):63–77

123

