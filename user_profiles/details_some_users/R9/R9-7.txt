Analyzing PETs on Imbalanced Datasets When Training

and Testing Class Distributions Differ

David Cieslak and Nitesh Chawla

University of Notre Dame, Notre Dame IN 46556, USA

{dcieslak,nchawla}@cse.nd.edu

Abstract. Many machine learning applications like ﬁnance, medicine, and risk
management suffer from class imbalance: cases of interest occur rarely. Further
complicating these applications is that the training and testing samples might dif-
fer signiﬁcantly in their respective class distributions. Sampling has been shown
to be a strong solution to imbalance and additionally offers a rich parameter space
from which to select classiﬁers. This paper is concerned with the interaction
between Probability Estimation Trees (PETs) [1], sampling, and performance
metrics as testing distributions ﬂuctuate substantially. A set of comprehensive
analyses is presented, which anticipate classiﬁer performance through a set of
widely varying testing distributions.

1 Introduction

Finance, medicine, and risk management form the basis for many machine learning ap-
plications. A compelling aspect of these applications is that they present several chal-
lenges to the machine learning community. The common thread among these challenges
persists to be class imbalance and cost-sensitive application, which has been a focus of
signiﬁcant recent work [2, 3]. However, the common assumption behind most of the
related works is that the testing data carries the same class distribution as the training
data. This assumption becomes limiting for the classiﬁers learned on the imbalanced
datasets, as the learning usually follows a prior sampling stage to mitigate the effect of
observed imbalance. This is, effectively, guided by the premise of improving the predic-
tion on the minority class as measured by some evaluation function. Thus, it becomes
important to understand the interaction between sampling methods, classiﬁer learning,
and evaluation functions when the class distributions change.

To illustrate, a disease may occur naturally in 15% of a North American population.
However, an epidemic condition may drastically increase the rate of infection to 45%,
instigating differences in P (disease) between the training and testing datasets. Thus,
the class distribution between negative and positive classes changes signiﬁcantly. Scalar
evaluations of a classiﬁer learned on the original population will not offer a reasonable
expectation for performance during the epidemic. A separate, but related problem oc-
curs when the model trained from a segment of North American population is then
applied to a European population where the distribution of measured features can po-
tentially differ signiﬁcantly, even if the disease base-rate remains at the original 15%.
This issue becomes critical as the learned classiﬁers are optimized on the sampling dis-
tributions spelled out during training to increase performance on minority or positive

T. Washio et al. (Eds.): PAKDD 2008, LNAI 5012, pp. 519–526, 2008.
c(cid:2) Springer-Verlag Berlin Heidelberg 2008

520

D. Cieslak and N. Chawla

class, as measured by some evaluation function. If sampling is the strong governing
factor for the performance on imbalanced datasets, can we guide the sampling to have
more effective generalization?

Contributions: We present a comprehensive empirical study investigating the effects
of changing distributions on a combination of sampling methods and classiﬁer learn-
ing. In addition, we also study the robustness of certain evaluation measures. We con-
sider two popular sampling methods for countering class imbalance: undersampling and
SMOTE [2,4]. To determine the optimal levels of sampling (under and/or SMOTE), we
use a bruteforce wrapper method with cross-validation that optimizes on different eval-
uation measures like Negative Cross Entropy (N CE), Brier Score (Brier), and Area
Under the ROC Curve (AU ROC) on the original training distribution. The former fo-
cuses on quality of probability estimates and the latter focuses on rank-ordering. The
guiding question here is – what is more effective – improved quality of estimates or
improved rank-ordering if the eventual testing distribution changes? We use the wrap-
per to empirically discover the potentially best sampling amounts for the given classi-
ﬁer and evaluation measure. This allows us to draw observations on the suitability of
popular sampling methods, in conjunction with the evaluation measures, on evolving
testing distributions. We restrict our study to PETs [1] given their popularity in the lit-
erature. This also allows for a more focused analysis. Essentially, we used unpruned
C4.5 decision trees [5] and considered both leaf frequency based probability estimates
and Laplace smoothed estimates. We also present an analysis of the interaction between
measures used for parameter discovery and evaluation. Is a single evaluation measure
more universal than the others, especially in changing distributions?

2 Sampling Methods

Resampling is a prevalent, highly parameterizable treatment of the class imbalance
problem with a large search space. Typically resampling improves positive class ac-
curacy and rank-order [6, 7, 8, 2]. To our knowledge, there is no empirical literature
detailing the effects of sampling on the quality of probability estimates; however, it is
established that sampling improves rank-order. This study examines two sampling meth-
ods: random undersampling and SMOTE [9]. While seemingly primitive, randomly
removing majority class examples has been shown to improve performance in class
imbalance problems. Some training information is lost, but this is counterbalanced by
the improvement in minority class accuracy and rank-order. SMOTE is an advanced
oversampling method which generates synthetic examples at random intervals between
known positive examples. [2] provides the most comprehensive survey and comparison
of current sampling methods.

We search a large sampling space via wrapper [10] using a heuristic to limit the
search space. This strategy ﬁrst removes “excess” negative examples by undersampling
from 100% to 10% in 10% steps and then synthetically adds from 0% to 1000% more
positive examples in 50% increments using SMOTE. Each phase ceases when the wrap-
per’s objective function no longer improves after three successive samplings. We use
Brier, N CE, and AU ROC [11, 12] as objective functions to guide the wrapper and
ﬁnal evaluation metrics. Figure 1 shows the Wrapper and Evaluation framework.

Analyzing PETs on Imbalanced Datasets

521

Training Data

Testing Data

10X

10X

Original Data Sample

Generate Varied Distributions

Generate Validation Data

Table 1. Dataset Distributions. Ordered in an increasing order of class imbalance.

5X

Optimize Sampling

Evaluate Classifiers

Validation Training Data

Validation Testing Data

Fig. 1. Wrapper and Evaluation Framework

Dataset

Examples Features Class Balance

Adult [13]
E-State [9]

Pendigits [13]
Satimage [13]

Forest Cover [13]

Oil [14]

48,840
5,322
10,992
6,435
38,500

937

Compustat [10]

10,358
Mammography [9] 11,183

14
12
16
36
10
49
20
6

76:24
88:12
90:10
90:10
93:7
96:4
96:4
98:2

3 Experiments and Results

potential

distributions

We consider performance on different samplings of the testing set to explore the range
of
which
P (+) = {0.02, 0.05, 0.1, 0.2, 0.3, ..., 0.9, 0.95, 0.98}. For example, suppose a given
dataset has 2000 examples from class 0 and 1000 examples of class 1 in the testing set.
To evaluate on P (+) = 0.5, 1000 class 0 examples are randomly removed from the
evaluation set. We experimented on eight different datasets, summarized in Table 1.

samplings

exploring

for

by

We explore visualizations of the trends in N CE and AU ROC as P (+) is var-
ied. Each plot contains several different classiﬁers: the baseline PET [1]; sampling
guided by Brier (called Frequency Brier Wrapper for frequency based estimates and
Laplace Brier Wrapper for Laplace based estimates); sampling guided by N CE; and
ﬁnally sampling guided by AU ROC (the latter two using similar naming convention as
Brier). In Figures 2 to 9, N CE and AU ROC are depicted as a function of increasing
class distribution, ranging from fully negative on the left to fully positive on the right.
A vertical line indicates the location of the original class distribution. Brier trends are
omitted as they mirror those of N CE.

522

D. Cieslak and N. Chawla

Baseline
Frequency Brier Wrapper
Frequency NCE Wrapper
Frequency ROC Wrapper
Laplace Brier Wrapper
Laplace NCE Wrapper
Laplace ROC Wrapper

Baseline
Frequency Brier Wrapper
Frequency NCE Wrapper
Frequency ROC Wrapper
Laplace Brier Wrapper
Laplace NCE Wrapper
Laplace ROC Wrapper

Natural Distribution

E
C
N

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

 

E
C
N

0.5

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

 

 

 

C
O
R
U
A

0.92

0.915

0.91

0.905

0.9

0.895

0.89

0.885

0.65

C
O
R
U
A

0.6

0.55

 

Fig. 3. E-State

Natural Distribution

50
P(+)

Natural Distribution

50
P(+)

10

20

30

40

60

70

80

90

100

10

20

30

40

60

70

80

90

100

Fig. 2. Adult

Natural Distribution

 

10

20

30

40

60

70

80

90

100

10

20

30

40

50

60

70

80

90

50
P(+)

P(+)

Figures 2 through 9 show the experimental N CE and AU ROC trends as the class
distribution varies. Despite the variety of datasets evaluated, some compelling general
trends emerge. Throughout, we note that wrappers guided by losses generally improve
N CE at and below the natural distribution of P (+) as compared to AU ROC wrap-
pers This implies that loss does well in optimizing N CE when the testing distribution
resembles the training conditions. It is notable that in some cases, such as Figures 2, 3,
5, 6, 7, 8, & 9, that the baseline classiﬁer actually produces better N CE scores than at
least the frequency AU ROC wrapper, if not both AU ROC wrappers. The frequency
AU ROC wrapper selected extreme levels of sampling. The reduction in N CE at low
P (+) indicates that using loss measures within the wrapper lowers the loss estimates
for the negative class examples. That is, while the loss from the positive class may
actually increase, the lower overall losses are driven by better calibrated estimates on
the predominantly occurring majority class. On the other hand, classiﬁers learned from
the AU ROC guided wrappers do not result in as well-calibrated estimates. AU ROC
favors the positive class rank-order, while Brier and N CE tend to treat both classes
equally, which in turn selects extreme sampling levels. Thus, if N CE optimization is
desired and the positive class is anticipated to occur as rarely or more rarely than in the
training data, sampling should be selected according to either Brier or N CE.

However, the environment producing the data may be quite dynamic, creating a shift
in the class ratio and causing the minority class to become much more prevalent. In a
complete paradigm shift, the former minority class might become larger than the for-
mer majority class, such as in an epidemic. Invariably, there is a cross-over point in

Analyzing PETs on Imbalanced Datasets

523

10

20

30

40

60

70

80

90

100

10

20

30

40

60

70

80

90

100

50
P(+)

50
P(+)

Natural Distribution

Fig. 4. Pendigits

0.98

Natural Distribution

 

E
C
N

0.1

0.08

0.06

0.04

0.02

0

 

E
C
N

0.4

0.3

0.7

0.6

0.5

0.2

0.1

0

 

Baseline
Frequency Brier Wrapper
Frequency NCE Wrapper
Frequency ROC Wrapper
Laplace Brier Wrapper
Laplace NCE Wrapper
Laplace ROC Wrapper

Natural Distribution

Baseline
Frequency Brier Wrapper
Frequency NCE Wrapper
Frequency ROC Wrapper
Laplace Brier Wrapper
Laplace NCE Wrapper
Laplace ROC Wrapper

Natural Distribution

 

 

C
O
R
U
A

1

0.998

0.996

0.994

0.992

0.99

0.988

0.986

0.984

1

C
O
R
U
A

0.96

0.94

0.92

0.9

0.88

 

Fig. 5. Satimage

10

20

30

40

60

70

80

90

100

50

P(+)

10

20

30

40

50

60

70

80

90

P(+)

each dataset after which one of the AU ROC wrappers optimizes N CE values. This is
logical, as AU ROC measures the quality of rank-order in terms of the positive class —
extra emphasis is placed on correctly classifying positive examples and is reﬂected by
the higher selected sampling levels. As the positive examples eventually form the ma-
jority of the evaluation set, classiﬁers producing on average higher quality positive class
probability estimates will produce the best N CE. Therefore, if a practitioner anticipates
an epidemic-like inﬂux of positive examples sampling methods guided by AU ROC are
favored.

Improvement to AU ROC under varied testing distributions is not as uniform. We
observe that at least one loss function wrapper generally produces better AU ROC val-
ues in Figures 2, 3, & 4, but that an AU ROC wrapper is optimal in Figures 6, 7, &
9. It is difﬁcult to declare a champion in Figures 5 & 8. It is of note that datasets with
naturally larger positive classes tend to beneﬁt (in terms of AU ROC) from a loss wrap-
per, while those with naturally smaller positive classes beneﬁt more from the AU ROC
wrapper. As seen before, AU ROC guides a wrapper to higher sampling levels than
Brier or N CE. In the cases of relatively few positive examples (such as Forest Cover,
Oil, and Mammography), a heavy emphasis during training on these few examples pro-
duces better AU ROC values. For the datasets with a larger set of positive examples (as
in Adult, E-State, and Pendigits) from which to naturally draw, this over-emphasis does
not produce as favorable a result. Therefore, in cases where there are very few positive
examples, a practitioner should optimize sampling according to AU ROC. Otherwise,
Brier or N CE optimization is sufﬁcient.

10

20

30

40

60

70

80

90

100

10

20

30

40

60

70

80

90

100

50
P(+)

50
P(+)

524

D. Cieslak and N. Chawla

Natural 
Distribution

 

Baseline
Frequency Brier Wrapper
Frequency NCE Wrapper
Frequency ROC Wrapper
Laplace Brier Wrapper
Laplace NCE Wrapper
Laplace ROC Wrapper

Natural
Distribution

Baseline
Frequency Brier Wrapper
Frequency NCE Wrapper
Frequency ROC Wrapper
Laplace Brier Wrapper
Laplace NCE Wrapper
Laplace ROC Wrapper

Natural Distribution

0.2

0.15

E
C
N

0.1

0.05

0

 

E
C
N

0.8

0.6

1.4

1.2

1

0.4

0.2

0

 

 

 

C
O
R
U
A

0.998

0.996

0.994

0.992

0.99

0.988

0.986

 

0.95

0.9

C
O
R
U
A

0.85

0.8

0.75

Fig. 6. Forest Cover

10

20

30

40

60

70

80

90

100

50
P(+)

10

20

30

50

60

70

80

40
P(+)

Natural Distribution

Fig. 7. Oil

The difference of characteristics between the trends in N CE and AU ROC is note-
worthy. The N CE trends appear stable and linear. By calculating the loss on each class
at the base distribution, it appears that one is able to project the N CE at any class distri-
bution using a weighted average. AU ROC trends are much more violent, likely owing
to the highly perturbable nature of the measure. Adding or removing a few examples
can heavily impact the produced ranking. As a measure, AU ROC is characteristically
less predictable than a loss function.

We also note that sampling mitigates the need of application of Laplace smoothing
at the leaves. We can see that the baseline classiﬁer beneﬁts from smoothing, as also
noted by other works. However, by treating the dataset for class imbalance ﬁrst, we
are able to counter the bias and variance in estimates arising from small leaf-sizes.
The wrapper essentially searches for the ideal training distribution by undersampling
and/or injecting synthetic minority class instances that lead to a reduction in loss or
improvement in ranking.

Throughout Figures 2 to 9, we also note that Brier and N CE loss wrappers tend
to perform similarly across measures and datasets. This is not surprising as the shape
of Brier and N CE values are similar. We observe that the optimal sampling levels
found by Brier and N CE are similar, certainly more similar than to those samplings
of AU ROC. In general, N CE maintains a slight performance edge. If in the interests
of time a practitioner may only experiment using one loss measure, then this study
recommends using N CE, although the results found here may not apply to all domains
and performance metrics.

Analyzing PETs on Imbalanced Datasets

525

10

20

30

40

60

70

80

90

100

10

20

30

40

60

70

80

90

100

50
P(+)

Natural Distribution

50
P(+)

Fig. 8. Compustat

 

 

C
O
R
U
A

1

0.95

0.9

0.85

0.8

0.75

C
O
R
U
A

0.96

0.95

0.94

0.93

0.92

0.91

0.9

0.89

0.88

Baseline
Frequency Brier Wrapper
Frequency NCE Wrapper
Frequency ROC Wrapper
Laplace Brier Wrapper
Laplace NCE Wrapper
Laplace ROC Wrapper

Natural Distribution

Baseline
Frequency Brier Wrapper
Frequency NCE Wrapper
Frequency ROC Wrapper
Laplace Brier Wrapper
Laplace NCE Wrapper
Laplace ROC Wrapper

Natural Distribution

E
C
N

0.8

0.6

1.4

1.2

1

0.4

0.2

0

 

E
C
N

0.4

0.8

0.7

0.6

0.5

0.3

0.2

0.1

0

 

4 Conclusions

10

20

30

40

60

70

80

90

100

10

20

30

40

60

70

80

90

100

50
P(+)

50
P(+)

Natural Distribution

Fig. 9. Mammography

The main focus of our paper was to empirically explore and evaluate the interaction be-
tween techniques for countering class imbalance, PETs, and corresponding evaluation
measures under circumstances where training and testing samples differ. In light of the
questions posited in the Introduction, we make the following key observations.

– We demonstrated that it is possible to identify potentially optimal quantities of
sampling by optimizing on quality of estimates or rank-order as calculated by AU-
ROC. Almost all the wrappers demonstrated signiﬁcant improvements in AUROC
and reductions in losses over the baseline classiﬁer, irrespective of the dataset. As
an evaluation measure, N CE is much more stable and predictable as compared
to AU ROC. We observe N CE to change almost linearly as a function of P (+),
while AU ROC tends to change as P (+) changes.

– There is a strong inter-play between undersampling and SMOTE. The wrapper de-
termines an interaction between both the approaches by searching undersampling
parameters before oversampling via SMOTE.

– It is much more difﬁcult to anticipate the effects of a class distribution shift on
AU ROC than it is on probability loss functions. When a dataset is highly im-
balanced, we recommend guiding sampling through AU ROC as this places the
necessary emphasis on the minority class. When class imbalance is much more
moderate, N CE tends to produce an improved AU ROC.

526

D. Cieslak and N. Chawla

– While Laplace smoothing has a profound effect in improving both the quality of
estimates and ranking for the baseline classiﬁer, the advantage diminishes with
sampling methods. The combination of SMOTE and undersampling improves the
calibration at the leaves and thus we observed that wrapper based sampling methods
are able to improve performance — lower losses and higher ranking — irrespective
of smoothing at the leaves.

References

1. Provost, F., Domingos, P.: Tree Induction for Probability-Based Ranking. Machine Learn-

ing 52(3), 199–215 (2003)

2. Batista, G., Prati, R., Monard, M.: A Study of the Behavior of Several Methods for Balancing

Machine Learning Training Data. SIGKDD Explorations 6(1), 20–29 (2004)

3. Chawla, N., Japkowicz, N., Kolcz, A.: Editorial: Special Issue on Learning from Imbalanced

Data Sets. SIGKDD Explorations 6(1), 1–6 (2004)

4. Estabrooks, A., Jo, T., Japkowicz, N.: A Multiple Resampling Method for Learning from

Imbalances Data Sets. Computational Intelligence 20(1), 18–36 (2004)

5. Quinlan, J.: C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo (1992)
6. Japkowicz, N.: The Class Imbalance Problem: Signiﬁcance and Strategies. In: ICAI 2000,

7. Ling, C.X., Li, C.: Data Mining for Direct Marketing: Problems and Solutions. In: KDD, pp.

pp. 111–117 (2000)

73–79 (1998)

8. Solberg, A., Solberg, R.: A Large-Scale Evaluation of Features for Automatic Detection of

Oil Spills. In: ERS SAR Images IEEE Symp. Geosc. Rem., vol. 3, pp. 1484–1486 (1996)

9. Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.: SMOTE: Synthetic Minority

Over-sampling Technique. JAIR 16, 321–357 (2002)

10. Chawla, N.V., Cieslak, D.A., Hall, L.O., Joshi, A.: Automatically countering imbalance and
its empirical relationship to cost. Utility-Based Data Mining: A Special issue of the Interna-
tional Journal Data Mining and Knowledge Discovery (2008)

11. Buja, A., Stuetzle, W., Sheu, Y.: Loss Functions for Binary Class Probability Estimation and

Classiﬁcation: Structure and Applications (under submission, 2006)

12. Caruana, R., Niculescu-Mizil, A.: An Empirical Comparison of Supervised Learning Algo-

rithms. In: ICML 2006, pp. 161–168 (2006)

13. Asuncion, A., Newman, D.: UCI machine learning repository (2007)
14. Kubat, M., Holte, R., Matwin, S.: Machine Learning for the Detection of Oil Spills in Satel-

lite Radar Images. Machine Learning 30, 195–215 (1998)

