Troubleshooting Thousands of Jobs on Production Grids

Using Data Mining Techniques

David A. Cieslak, Nitesh V. Chawla, and Douglas L. Thain

Department of Computer Science and Engineering, University of Notre Dame

Abstract

Large scale production computing grids introduce new
challenges in debugging and troubleshooting. A user that
submits a workload consisting of tens of thousands of jobs
to a grid of thousands of processors has a good chance of
receiving thousands of error messages as a result. How can
one begin to reason about such problems? We propose that
data mining techniques can be employed to classify failures
according to the properties of the jobs and machines in-
volved. We demonstrate this technique through several case
studies on real workloads consisting of tens of thousands
of jobs. We apply the same techniques to a year’s worth of
data on a 3000 CPU production grid and use it to gain a
high level understanding of the system behavior.

1 Introduction

Large scale production computing grids introduce new
challenges in debugging and troubleshooting. Consider the
user that submits ten thousands jobs to a computational grid,
only to discover that half of them have failed. The failures
might be due to a bug in the program on certain inputs, an
incompatibility with certain operating systems, a lack of re-
sources to run the job, or just bad luck in the form of a
widespread power failure. How can a user or administrator
of a large computing system diagnose such problems? It
does no good to examine one or even a handful of jobs in
detail, because they may not represent the most common or
the most signiﬁcant failure mode.

We propose that data mining techniques can be applied
to attack this problem. Computing grids of various kinds
already publish a large amount of structured information
about resources, users, and jobs. Given access to this data,
classiﬁcation techniques can be used to infer what proper-
ties of machines, jobs, and the execution environment are
correlated with success or failure. For example, it should be
possible for the user with 10,000 jobs to quickly discover
that 90 percent of all failures occurred on Linux machines
running kernel version 2.6.2 with less than 1 GB of RAM,
and the remainder were randomly distributed.

Of course, this kind of analysis does not immediately
diagnose the root cause of the problem, but it would be a
signiﬁcant help in allowing the user to focus their attention
on the situation or resource of greatest interest. In this case,
the information would allow the user to quickly identify the
correct environment in which to debug their program in de-
tail. Or, the user might simply direct the grid computing
system to avoid machines matching those properties.

In a previous short paper [4], we introduced the con-
cept of troubleshooting large production runs using data
mining techniques. We demonstrated the feasibility of this
approach by executing large synthetic workloads with in-
duced failure conditions dependent upon the properties of
execution machines. Our techniques were able to infer the
properties of machines correlated with failures, which were
closely related to the root cause.

In this paper, we apply our techniques to real workloads
in which trouble is suspected, but the root causes are un-
known and not easily diagnosed by the user. We consider
a number of workloads of tens of thousands of jobs each
submitted to a 500-CPU system at the University of Notre
Dame, showing examples of problems diagnosed by exam-
ining machine properties, job properties, and the relation-
ship between the two. We then apply these techniques to
data collected over the course of a year on 300,000 jobs
running on over 3000 machines on a large campus grid at
the University of Wisconsin. Throughout, we discuss the
strengths and limitations of this approach.

2 Troubleshooting Technique

We have implemented a prototype troubleshooting tool
that diagnoses workloads submitted to Condor [22] based
grids. Figure 1 gives an example of the input data to our
troubleshooting tool. Every job submitted to Condor is rep-
resented by a ClassAd [18] that lists the critical job proper-
ties such as the owner, executable, and constraints upon ma-
chines where it is willing to run. When the job is complete,
the exit code, resource consumption, and other statistics are
added to the ClassAd. Every machine participating in Con-
dor is also represented by a ClassAd that lists the available

978-1-4244-2579-2/08/$20.00 © 2008 IEEE

217

9th Grid Computing Conference

JobId
Owner
VirtOrg
Cmd
Args
ImageSize
ExitCode
ExitBySignal
Requirements
Rank

Name
OpSys
Arch
IpAddr
TotalDisk
TotalMemory
Mips
LoadAvg
KeyboardIdle
MachineGroup
Rank
Start

=
=
=
=
=
=
=
=
=
=

=
=
=
=
=
=
=
=
=
=
=
=

Job ClassAd

29269
”dthain”
”NWICG”
”mysim.exe”
”-f -s 5”
82400
1
FALSE
(Arch==”LINUX”)
(MachineGroup==”elements”)
Machine ClassAd
”aluminum.nd.edu”
”LINUX”
”INTEL”
”111.222.333.444”
3225112
1010
3020
1.5
272
”elements”
(UserGroup==”physics”)
(KeyboardIdle>300)

User Log

(29269) 05/02 09:35:53 Submitted 111.222.333.444
(29269) 05/02 09:44:13 Executing 111.222.333.555
(29269) 05/03 17:57:53 Evicted.
(29269) 05/03 17:59:01 Executing 111.222.333.666
(29269) 05/04 04:35:02 Exited normally, status 0

Figure 1. Sample Data for Troubleshooting

physical resources, the logical conﬁguration of the machine,
and constraints upon jobs that it is willing to accept. The
ClassAd language is schema-free, so the precise set of at-
tributes advertised by jobs and machines varies from instal-
lation to installation, depending on what policies the local
administrator constructs. However, within one Condor in-
stallation, there is a high degree of schema consistency.

A user log ﬁle gives the sequence of events in the life-
time of a workload. The log ﬁle states when each job is
submitted, when and where it begins executing, and when it
completes. It also records the following undesirable events
that are of great interest for troubleshooting. When a job
completes, it indicates whether it exited normally (i.e. com-
pleted main) with an integer exit code, or abnormally (i.e.
crashed) with a given signal number. A job can be evicted
from a machine based on local policy, typically a preemp-
tion in order to serve a higher priority user. A job can
suffer an exception which indicates a machine crash, a net-
work failure, or a shortage of system resources. A job can
be aborted by the user if it is no longer needed.
In this
work, we deﬁne failure as any non-zero completion, abnor-
mal completion, eviction, or exception.

Between these three data sources, we can observe what
jobs succeed, which fail and what the properties of the jobs
and machines involved are. Each of these data sources is

218

OpSys

WINNT

LINUX

TotalDisk

Arch

<1GB

>1GB

INTEL

AMD64

S: 90
F: 0

S: 120
F: 5

S: 1000
F: 10

S: 0
F: 250

Figure 2. Sample Decision Tree

available to end users without any assistance from the ad-
ministrator, so this technique can be widely applied. Users
may diagnose workloads that are incomplete. Both the job
and machine data are collected at the time tool is run; this
imposes some limitations we discuss below.

A challenge in this domain is that the failure and suc-
cesses are not constant — that is, the typical production grid
observes a varying degree of failures and successes over
time, which is largely driven by the addition/deletion of
users and applications. This also imposes the issue of high
class imbalance in the data [2]. An analysis of the work-
loads below shows that the failures and success are varying
over time signiﬁcantly. Thus, the primary challenges are to
be able to counter the issues of class imbalance, data distri-
bution, and model interpretability.

Standard decision tree algorithms can be very sensitive
to imbalance in class distributions [9], thus limiting their
use in our work. To that end, we implemented a decision
tree algorithm that utilizes the Hellinger distance as the ob-
jective function [5]. Hellinger distance is invariant to the
class skew; therefore, it is ideal for problems in which the
class ratios are unknown and potentially dynamic. This
algorithm is highly effective under class imbalance, trains
quickly compared to other imbalance solutions such as sam-
pling, and generates readable model.

Our prototype tool accepts the three data sources as in-
put, and then constructs a decision tree for both job and
machine properties, the Hellinger Distance Decision Tree
(HDDT) [5] algorithm. A sample tree is shown in Figure 2
The leaves of the tree represent disjoint subsets of jobs, a
certain number succeeding (S) and a certain number fail-
ing (F). The internal nodes of the tree represent job or ma-
chine properties, and the edges indicate constraints upon the
values of those properties. For example, the rightmost leaf
indicates the category of jobs that ran on machines where
OpSys=="LINUX" and Arch=="AMD64". 250 jobs in
that category failed, and none succeeded.

A decision tree from a real workload is much larger, and
may have hundreds of leaves. Such a data structure can be
difﬁcult for even the expert to parse. To accommodate the

Static Properties

(most relevant to success/failure)

OpSys
Arch
Subnet
TotalDisk
TotalMemory
KFlops
Mips
JavaVersion
CondorVersion
KernelVersion

=
=
=
=
=
=
=
=
=
=

”LINUX”
”INTEL”
”111.222.333”
3225112
1010
842536
3020
”1.6.0 05”
”7.0.2”
”2.6.18-53.1.13.el5”

Dynamic Properties

(only add noise to the result)
LoadAvg
CurrentTime
KeyboardIdle
Disk
Memory
VirtualMemory

1.5
1206568628
272
3225112
1010
2053
Artiﬁcial Labels

=
=
=
=
=
=

(can hide root causes)
Name
IpAddr
MachineGroup
IsDedicated
IsComputeCluster
ForDan

”aluminum.nd.edu”
”111.222.333.444”
”elements”
FALSE
TRUE
FALSE

=
=
=
=
=
=

Figure 3. Categories of Machine Labels

non-expert user, our tool simply emits the three most signif-
icant nodes of the decision tree as ClassAd expressions. In
our experience so far, we have found this to be effective.

In our initial attempts at applying this technique, the cre-
ated decision tree were enormous, very difﬁcult to read, and
contained a number of splits on what appeared to be irrele-
vant data. The problem arose becuase we collected machine
data by querying Condor after the workload completed, in
some cases weeks or months later. Figure 3 gives examples
of the kinds of data present in the machine ClassAds. Static
properties are labels created by Condor itself according to
the ﬁxed local hardware or operating system, which is not
likely to change over the course of weeks or months. Dy-
namic properties are observations of constantly changing
values, such as load average or available memory. Artiﬁcial
labels are properties assigned by administrators in order to
simplify policy enforcement or reporting.

In general, it is necessary to remove known dynamic la-
bels from the algorithm, because they only add noise to the
output. As we will show below, it is meaningful to classify
both static and artiﬁcial labels, but each produces a differ-
ent character of results. Our prototype tool allows for both
forms of analysis.

3 Single User Workloads

How do we evaluate the quality of a troubleshooting
technique? Quantitative performance evaluation does not

219

the techniques presented here run in seconds on
apply:
workloads of tens of thousands of jobs, and in minutes on
the whole-grid data presented later. We may judge a tech-
nique to be useful if it correctly diagnoses a problem that is
known but not understood, or reveals a problem that was en-
tirely unknown. Of course, no single troubleshooting tech-
nique is universally applicable. To evaluate this technique,
we present several case studies of real workloads, and use
them to describe the limits of the technique.

We begin by considering single-user workloads executed
on our local campus grid of 500 CPUs at the University
of Notre Dame. This grid is highly heterogeneous, con-
sisting of multiple dedicated research clusters and cycle-
scavenged desktop and classroom machines. Machine per-
formance and capacity varies widely: CPUs range from
500-6000 MIPS, disks range from 400MB to 500GB, and
RAM ranges from 243MB to 6GB. In such an environment,
diagnosis of incompatibilities is critical.

Our ﬁrst case study is set of 60,000 jobs that perform a
biometric image processing workload. The jobs are gen-
erated by a high level abstraction designed for non expert
users. The abstraction divides a very large problem into
a small number of very similar jobs, so any problems are
likely to be due to variations in available machines.

The user in question presented with the complaint that
the workload had a long tail. A number of jobs at the end
of the workload took far too long to complete, having been
evicted many times from multiple machines.

labels,

Figure 4 shows the output of the troubleshooting tool.
it produced a decision
When run on artiﬁcal
tree too large to present
the
most signiﬁcant rule immediately pinpointed the prob-
lem: these jobs were always evicted from machines where
MachineGroup=="ccl", a label indicating a cluster of
seven homogeneous machines.

in full here. However,

The results on static labels give a better sense of the root
cause. These machines are distinguished from others in the
pool by three properties: TotalDisk <= 125GB and
JavaMFlops <= 3.1 and TotalVirtualMemory
<= 1GB. Based on our knowledge of the workload and
machines, we know that the disk space is sufﬁcient and the
Java performance is irrelevant. However, the total virtual
memory seems unusually low: most machines are conﬁg-
ured with at least several GB of virtual memory. We hy-
pothesized that the job was crashing due to exhausting vir-
tual memory, and conﬁrmed this by manual testing. The
cluster was misconﬁgured when it was installed, but no-one
noticed until these particular jobs ran.

What may we observe from this exercise? First, the artif-
ical labels allowed for a compact, correct statement, but did
not suggest the root cause of the problem. The static labels
are more suggestive, less compact, and require the reader to
apply some domain knowledge to get to the root cause. But,

Feature mapping

Tree properties

Artiﬁcial Labels

Static Labels

height: 13
nodes: 301
leaves 166
height: 19
nodes: 301
leaves 153

Most Signiﬁcant Rule

( MachineGroup == ccl ) →
( completed:0 evicted:305 )

Machines Indicated

ccl00 – ccl07

( TotalDisk ≤ 125982176 ) &&
( JavaMFlops ≤ 3.152064 ) &&

( TotalVirtualMemory ≤ 1043856 )

→ ( completed:0 evicted:305 )

ccl00 – ccl07

cclbuild00 – 03

Figure 4. Decision Tree Results on a Biometric Workload

Feature mapping

Tree properties

Machine Labels

Most Signiﬁcant Rule

( TotalDisk ≤ 36669824 ) &&

Administrator Conclusion
The code generated very

( TotalVirtualMemory ≤ 4192800 ) → large output ﬁles, exceeding the
( completed: 8 ShadowException: 737 )

OS imposed 2GB limit.

Job Labels

( Parameter1 = 4 ) →

Setting Parameter1 to 4 makes

linear separation for SVM

height: 10
nodes: 34
leaves: 18
height: 2
nodes: 34
leaves: 31

( completed: 0 ShadowException: 393 )

training intractible.

Figure 5. Decision Tree Results on a Data Mining Workload

both forms create true statements that can be acted upon. In
both cases, the expression emitted by the troubleshooter can
be placed into the user’s submission ﬁle verbatim in order
to avoid machines with those properties, without even both-
ering to understand the root cause. In the case of the static
properties, the requirement even matches several additional
machines on which the job is likely to crash, but simply did
not happen to appear in the original workload.

Not all failures can be blamed on properties of machines.
In many cases, a particular job may have some property that
prevents it from executing successfully on any machine.
Can this technique diagnose problems with jobs?

Another user ran a workload of several hundred jobs,
performing a study of various data mining techniques on
several datasets. Much like the previous example, a large
portion of the workload ﬁnished quickly, but a few strag-
glers remained. This problem was debugged in two stages.
Figure 5 shows the output of the debugging tool. Ini-
tial analysis on the machine properties was not fruitful.
The most signiﬁcant branch indicated TotalDisk and
TotalVirtualMemory below critical values as associ-
ated with failure. Although that analysis was true, a lack of
resources was not the root cause as it was above.

Analysis on job properties was much more fruitful. For-
tunately, this user made a habit of putting logical informa-
tion about the job into the ClassAd for bookkeeping pur-
poses. First, our tool indicated that all the failures were oc-
curing to jobs using the dataset oil. A manual test of jobs
on this dataset revealed that, because of the large dataset
size, the algorithm was producing an output debug stream
that exceeded 2GB, the maximum ﬁle size for processes us-
ing the 32-bit interface. The user disabled the debug output
and re-ran the workload.

This improved the workload, but still stragglers re-
mained. Again, the tool was applied, and this time the

result was much clearer.
Jobs on multiple datasets with
Parameter1 == 4 were always evicted and never com-
pleted. In this particular case,Parameter1 controlled the
complexity of the algorithm; setting it to 4 yielded a run-
time so high that it could not complete within the 8-12 hour
window available on desktop machines. By redirecting the
jobs to dedicated machines, they eventually completed.

This exercise offers the following lessons. First, that
troubleshooting via data mining is not a silver bullet.
In
the case of the 2GB ﬁle problem, the association with the
oil dataset narrowed the search space for the problem,
but did not point directly to the root cause. Second, man-
ual annotation of the job properties can be of signiﬁcant
value. By placing runtime parameters directly into the Clas-
sAd (instead of just in the Arguments property), the trou-
bleshooter was able to isolate those associated with failure.

4 Multi User Workloads

So far, we have examined several examples of debugging
via data mining. In each case, a user with a large coherent
workload suspects a problem, and explicitly invokes the de-
bugging tool to investigate. Can we also use the same tech-
niques to study a large production computing system with
many different users over a long period of time?

To answer this question, we applied the same tech-
niques to data collected from a large production grid of the
course of one year. The Grid Lab of Wisconsin (GLOW)
is a campus-scale grid of about four thousand machines.
The grid is physically partitioned across multiple campus
departments, each running Condor, all ”ﬂocked” together
for mutual load sharing. GLOW is also accessible from
the Open Science Grid via a Globus GRAM [6] interface,
which can submit jobs to any of the Condor pools in GLOW.
A remote user would typically use an agent such as Condor-

220

CPUs Busy - Daily Average
Queue Length - Daily Max

y
s
u
B
 
s
U
P
C

 900
 800
 700
 600
 500
 400
 300
 200
 100
 0

 3500

 3000

 2500

 2000

 1500

 1000

 500

t

h
g
n
e
L

 

e
u
e
u
Q

 0

Jan

Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov

Dec

Figure 6. Timeline of CPU Utilization in the GLOW

Jobs Submitted
Jobs Complete
Jobs Failed
Jobs Rolled Back
Jobs Aborted
Goodput (CPU-days)
Badput (CPU-days)

Jan
14382
13980
365
13671
37
5268
523

Feb
18469
14873
1736
5530
1367
3329
1993

Mar
8719
8693
23
8196
382
4139
732

Apr May
1529
7872
1527
7679
0
277
821
2366
35
0
36
2196
207
5

Jun
9234
8230
1
1822
8
3575
152

Jul
68236
28404
34325
16827
5050
14391
2557

Aug
78474
19937
56707
83729
1120
8034
2251

Sep
25528
23314
774
65634
2768
8523
1524

Oct
15595
16127
161
88914
79
5034
1288

Nov
12828
11869
51
104606
202
8617
1061

Dec
36372
35420
286
170560
523
6374
3724

Figure 7. Number of Events per Month in GLOW

Virt Org
cmsprod
cdf
samgrid
nanohub
engage
uscms01
mis

Submitted
122422
119185
34132
2012
1998
1672
252

Completed
102623
64203
17878
1225
1563
893
210

Failed
16126
56635
11449
770
392
734
50

Aborted
5698
902
4848
23
52
48
0

Exception
14043
13992
4808
1596
953
12
0

Evicted
105786
338252
11563
5889
7976
2962
524

Figure 8. Number of Events per Virtual Organization in GLOW

G [11] to manage job submissions into GLOW.

GLOW is already instrumented as follows. Each indi-
vidual job that arrives via the Globus GRAM interface is
submitted to Condor. Each job produces a user log ﬁle that
records the events in the lifetime of the job. Upon com-
pletion of the job, GLOW automatically archives the user
log ﬁle and the ﬁnal ClassAd of the job.
In this section,
we present results from all the data recorded archived 2007.
Note that this data includes information about all jobs sub-
mitted to GLOW from the Open Science Grid, but it does
not include jobs submitted from submitters inside GLOW
itself. In addition, the machine properties were not recorded
at the time of execution, so we made a single observation of
machine data in April 2008.

This data is different from ordinary Condor pools in two
respects. First, the ”users” of the system are not recorded
as individuals, but as virtual organizations. Second, the ma-
chines in the system had about thirty additional labels added
in order to enforce local policy.

Figure 6 gives an overview of the load placed on this sys-
tem over time. The ﬁlled boxes indicate the average daily

CPU consumption of grid jobs. The dark line indicates the
maximum queue length on each day.
(Again, the actual
utilization of the resources was higher; our data only re-
ﬂects load placed on the system by remote users.) It can be
seen that both load and utilization are highly bursty. Small
workloads of hundreds of jobs are submitted on a weekly
or monthly basis from January to May. From May to July,
the system is very quiet, perhaps due to the academic cal-
endar. From July through January, batches of thousands of
jobs arrive and utilization is much higher.

Figure 7 gives more detail about the success and failure
of jobs on a monthly basis. Jobs Submitted is the number of
jobs arriving through the GRAM interface. Jobs Completed
is the number of jobs running to completion and indicating
success with an exit code of zero. Jobs Failed is the num-
ber of jobs running to completion, but exiting with a signal
or a non-zero exit code. Jobs Rolled Back is the number
of times in which a job is removed from an execution ma-
chine, either because it is removed by the local resource
manager, preempted by the global scheduler, or fails initial-
ize properly on that machine. In each of these cases, the job

221

Feature Mapping

Top Split

Artiﬁcal Labels

(FileSystemDomain = hep.wisc.edu)

→ ( C:104656 SE:22759 E:76056)

Most Signiﬁcant

(FileSystemDomain = hep.wisc.edu) &&
(TotalVirtualMemory > 10481808) &&

Static Labels

(Mips > 2925)

(TotalVirtualMemory > 10481656) &&

→ ( C:167094 SE:27732 E:152889)

(Mips ≤ 3455) →

( C:513 SE:11822 E:670)

(Mips > 2925) &&

(KFlops ≤ 857509) →
( C:513 SE:11822 E: 670)

Figure 9. Decision Tree Analysis for All of GLOW.

Feature Mapping
Artiﬁcial Labels

Static Labels

Jan.
0.212
0.223

Feb.
0.312
0.266

Mar.
0.558
0.268

Apr.
0.407
0.303

May
0.407
0.153

June
0.402
0.144

July
0.717
0.346

Aug.
0.526
0.425

Sep.
0.772
0.358

Oct.
0.531
0.300

Nov.
0.627
0.344

Dec
0.551
0.347

Feature Mapping
Artiﬁcal Labels

Static Labels

cdf
0.510
0.387

cmsprod

0.621
0.395

engage
0.723
0.258

glow
0.204
0.063

ligo
0.141
0.198

mis
0.208
0.154

nanohub

0.321
0.285

usatlas1
0.190
0.053

usmcs01

0.412
0.182

Figure 10. Comparison of Month and User Slices to Global Data via Rand Index

is placed back in the queue and will have an opportunity
to run elsewhere. A job could be rolled back many times
in its lifetime. Jobs Aborted indicates the number of jobs
explicitly removed by the submitting user. Goodput is the
total amount of CPU time consumed that results in a suc-
cessfully completed job. Badput is the total amount of CPU
time that is consumed with any other result.

This table reveals more about the workload. First, it is
very clear that the sytem goes through very large cycles of
success and failure. In January, over 14,000 jobs are sub-
mitted, and most complete successfully with less than 10
percent badput observed. In February, failures and badput
signiﬁcantly increase, but then tail off until the summer. In
July, there is an enormous increase: 68,236 jobs are submit-
ted, 34,000 fail, and 5,000 are aborted. This trend contin-
ues in August, but failures drop signiﬁcantly in September.
This suggest that the initial attempts of users to harness the
grid do not go well, but after repeated attempts, the proper
formula is found, and a workload proceeds smoothly. For
our purposes, this data conﬁrms earlier observations of large
numbers of failures in production grids.

Now, suppose that we assume the role of an administra-
tor overseeing this large computing system. If we apply our
data mining techniques, can we learn more about these fail-
ures, and use that knowledge to improve the system? This
problem is more potentially challenging because multiple
different users and workloads are all mixed together, and
may have varying rates of failure represented with different
weights according to the number of events.

That said, does it make sense to apply our classiﬁcation
algorithm to the entire data set? The analysis of the en-
tire year completes in several minutes using non-optimized
code running on a conventional laptop, so the technique is
quite feasible for perform on a regular basis. The results are
shown in Figure 9. Using artiﬁcial labels, we observe that

Info Gain
Label
0.5599
FileSystemDomain
0.4272
CkptServer
0.3329
UidDomain
0.3132
Mips
0.3130
Arch
0.3079
HasAFS Atlas
TotalVirtualMemory
0.2893
IsGeneralPurposeVM 0.2856
IsGeneralPurposeSlot
0.2836
0.2757
TotalMemory

Figure 11. Most Signiﬁcant Machine Labels

failure is highly correlated with FilesystemDomain
== "hep.wisc.edu". This property indicates the set
of ﬁlesystems available from a machine. A job submitted
from one ﬁlesystem domain will not have access to the data
it needs, unless the user manually speciﬁes needed data.
This is a common usability problem across many Condor in-
stallations, and appears to be a signiﬁcant operational prob-
lem in GLOW. Using static labels, we can observe that fail-
ure is also correlated with fast machines (Mips > 2925).
This is a superset of the ”hep.wisc.edu” machines, so the
observation is correct, but not very helpful in diagnosis.

What happens if we perform the same analysis on dif-
ferent subsets? To answer this, we partitioned the data by
month and by VO and generated decision trees for each.
Rather than showing all those results, we compared each
tree against the global tree using an Adjusted Rand Index,
a measure of indicating the coverage similarity on a scale
from zero to one [15]. As can be seen in ﬁgure 10, there
is considerable variance across the dataset.
In July and
September and with the Engage VO, the results are quite
similar. Other slices results in different top-level diagnoses.
To present these results compactly, we show the top
ten properties that are most signiﬁcant with respect to fail-

222

ure by computing information gain within the decision
tree. The top ﬁve properties are all deﬁned by the stan-
dard Condor conﬁguration. The top three are administra-
tive properties that describe the available ﬁlesystems, the
nearest checkpoint server, and the user database. To a cer-
tain extent, all are correlated with administrative cluster-
ings. It is somewhat surprising that Mips and Arch have
equal signiﬁcance. One would expect that Arch would
have higher signiﬁcance, because it describes the compat-
ibility of programs with processors. Our hypothesis is
that these properties are accidentally correlated with the
more signiﬁcant administrative properies, as seen in Fig-
ure 9. HasAFS Atlas, IsGeneralPurposeVM, and
IsGeneralPurposeSlot are artiﬁcal labels signiﬁcant
to the local system. This suggests that there is much to gain
be deﬁning custom properties within Condor.

Can we establish what workload or user is experiencing
this problem most heavily? To answer this, we adjust the
tool to alernate between job and machine properties in or-
der to successively reﬁne the source of failures. First, we se-
lect the subset of machines where FilesystemDomain
== "hep.wisc.edu", then select the set of jobs that
failed on those machines, and then analyze the proper-
ties of those jobs. Despite the VO problem mentioned
earlier,
the most signiﬁcant property of those jobs is
X509UserProxySubject, which identiﬁes the actual
person owning the GSI [10] credentials for the job. Sum-
ming up the number of jobs, we observe:

Failing Jobs
X509UserProxySubject
10998
/DC=gov/DC=fnal/O=Fermilab/CN=X
669
/DC=org/DC=doegrids/OU=People/CN=Y
/DC=org/DC=doegrids/OU=People/CN=Z
145
/DC=org/DC=doegrids/OU=People/CN=W 10

To sum up, this technique, when applied to a year’s worth
of data from a production grid, immediately diagnosed the
label most associated with failure, which also turned out
to be the root cause. With guidance from the operator, the
tool also identiﬁed the end user directing the workload that
(depending on your perspective) suffered the most failures,
or caused the most trouble.

5 Related Work

Previous authors have observed the very high rates of
user-visible failures in grid computing systems. Grid3 [13]
(a predecessor of the Open Science Grid) observed a fail-
ure rate of thirty percent for some categories of jobs, often
due to ﬁlled disks. Iosup [16] observe that availability can
have a signiﬁcant impact upon system throughput. In our
work, this is addressed in the sense that evictions (transition
to non-availability) are a kind of failures that can be system-
atically identiﬁed and avoided. Schopf [21] observed from
informal discussions with end users that troubleshooting is
a signiﬁcant obstacle to usability.

Others have performed systematic techniques for trou-
bleshooting grids. Gunter et al. [14] describe a system for
collecting large amounts of data from a grid and a tech-
nique for observing outliers in continuous valued proper-
ties. Palatin et al [17] discover misconﬁgured machines by
correlating performance with labelled features, and select-
ing outliers. These have the common working assumption
that performance outliers are likely to represent bugs, an
approach suggested by Engler[8].

We have generally approached this problem as a matter
of diagnosing past failures. Others have suggested predict-
ing future failures. Duan et al [7] have sketched a technique
for predicting future faults. Fu and Xu [12] demonstrate a
technique for predicting physical failures in clusters base on
time and space correlation.

Another way of troubleshooting complex systems is to
study structure rather than labels. For example, DeWiz [20]
examines message passing patterns between components of
a distributed system to infer causal relationships. A similar
idea is black box debugging [1, 19], which identiﬁes perfor-
mance bottlenecks. Pinpoint [3] tags data ﬂowing through
complex systems and applies data mining techniques to sug-
gest individual components that are failing.

6 Conclusion

In this work, we have made the following observations:

• Production computing grids do exhibit a large number
failures. From ﬁrsthand experience, we believe this is
due to iteratively submitting workloads and diagnosing
failures until an acceptable performance is achieved.
Troubleshooting via data mining can reduce the time
to successful execution.

• By constructing a decision tree and selecting the most
signiﬁcant decision points, we may easily draw cor-
relations that are strongly suggestive of root causes.
Both physical and logical labels are suitable for this
purpose, each leading to a different perspective on the
results.

• The same technique can be applied to large multi-user
workloads, but the troubleshooter must have additional
tools to subset and explore the data in order to draw
meaningful conclusions.

And, we offer the following recommendations:

• Other sites on the Open Science Grid should adopt the
logging behavior currently performed on GLOW. Al-
though there are logs at other layers of the system, they
do not contain the detail about intermediate failures
stored in the user job logs.

• User log ﬁles should record the complete ClassAd of
releavant machines at the time each event occurs. As

223

we have observed, machine properties can change be-
tween the time of execution and the time of observa-
tion, thus limiting the set of properties available for
troubleshooting.

• Both job submitters and machine owners are ad-
vised to include logical information directly in clas-
sads, even if it has no immediate functional pur-
pose. As shown in Figure 4 and 5, the non-functional
labels MachineGroup and Parameter1 yielded
succinct, intuitive results.

This work was supported in part by National Science
Foundation grant CNS-0720813. We are grateful to Dan
Bradley at the University of Wisconsin; Preston Smith
at Purdue University; Ryan Lichtenwalter, Christopher
Moretti, Tanya Peters, and Karen Hollingsworth at the Uni-
versity of Notre Dame for participating in this study.

References

[1] M. Aguilera, J. Mogul, J. Wiener, P. Reynolds, and
A. Muthitacharoen. Performance debugging of black-
box distributed systems. In ACM Symposium on Op-
erating Systems Principles, October 2003.

[2] N. V. Chawla, N. Japkowicz, and A. Kolcz. Editorial:
Learning from Imbalanced Datasets. SIGKDD Explo-
rations, 6(1):1–6, 2004.

[3] M. Chen, E. Kiciman, E. Fratkin, E. Brewer, and
A. Fox. Pinpoint: Problem determination in large, dy-
namic, internet services. In International Conference
on Dependable Systems and Networks, 2002.

[4] D. Cieslak, D. Thain, and N. Chawla. Short paper:
Data mining-based fault prediction and detection on
the grid. In IEEE High Performance Distributed Com-
puting, 2006.

[5] D. A. Cieslak and N. V. Chawla. Learning Decision
Trees for Unbalanced Data. In European Conference
on Machine Learning, 2008.

[6] K. Czajkowski, I. Foster, N. Karonis, C. Kesselman,
S. Martin, W. Smith, and S. Tuecke. Resource man-
agement architecture for metacomputing systems. In
IPPS/SPDP Workshop on Job Scheduling Strategies
for Parallel Processing, pages 62–82, 1998.

[7] R. Duan, R. Prodan, and T. Fahringer. Short paper:
Data mining-based fault prediction and detection on
the grid. In IEEE High Performance Distributed Com-
puting, 2006.

[8] D. Engler, D. Chen, S. Hallem, A. Chaou, and
B. Chelf. Bugs as deviant behavior: A general ap-
In ACM
proach to inferring errors in system code.
Symposium on Operating Systems Principles, October
2001.

[9] P. A. Flach. The Geometry of ROC Space: Under-
standing Machine Learning Metrics through ROC Iso-
metrics. In ICML, pages 194–201, 2003.

[10] I. Foster, C. Kesselman, G. Tsudik, and S. Tuecke. A
security architecture for computational grids. In ACM
Conference on Computer and Communications Secu-
rity Conference, 1998.

[11] J. Frey, T. Tannenbaum, I. Foster, M. Livny, and
S. Tuecke. Condor-G: A computation management
In IEEE High
agent for multi-institutional grids.
Performance Distributed Computing, pages 7–9, San
Francisco, California, August 2001.

[12] S. Fu and C.-Z. Xu. Exploring event correlation for
failure prediction in coalitions of clusters. In Super-
computing, 2007.

[13] R. Gardner and et al. The Grid2003 production grid:
In IEEE High Performance

Principles and practice.
Distributed Computing, 2004.

[14] D. Gunter, B. L. Tierney, A. Brown, M. Swany,
J. Bresnahan, and J. M. Schopf. Log summarization
and anomaly detection for troubleshooting distributed
systems. In IEEE Grid Computing, 2007.

[15] L. Hubert and P. Arabie. Comparing partitions. Jour-

nal of Classiﬁcation, pages 193–218, 1985.

[16] A. Iosup, M. Jan, O. Sonmez, and D. Epema. On the
dynamic resource availability in grids. In IEEE Grid
Computing, 2007.

[17] N. Palatin, A. Leizarowitz, A. Schuster, and R. Wolff.
Mining for misconﬁgured machines in grid systems.
In International Conference on Knowledge Discovery
and Data Mining, 2006.

[18] R. Raman, M. Livny, and M. Solomon. Matchmaking:
Distributed resource management for high throughput
computing. In IEEE Symposium on High Performance
Distributed Computing, July 1998.

[19] P. Reynolds, J. Wiener, J. Mogel, M. Aguilera, and
A. Vahdat. WAP5: black box performance debugging
for wide area systems. In Proceedings of the WWW
Conference, 2006.

[20] C. Schaubschlager, D. Kranzlmuller, and J. Volk-
ert. Event-based program analysis with de-wiz.
In
Workshop on Automated and Algorithmic Debugging,
pages 237–246, Septmber 2003.

[21] J. M. Schopf and S. J. Newhouse. Grid user require-
ments 2004: A perspective from the trenches. Cluster
Computing, 10(3), September 2007.

[22] D. Thain, T. Tannenbaum, and M. Livny. Condor and
the grid. In F. Berman, G. Fox, and T. Hey, editors,
Grid Computing: Making the Global Infrastructure a
Reality. John Wiley, 2003.

224

