A Robust Decision Tree Algorithm for Imbalanced Data Sets

Wei Liu∗

Sanjay Chawla∗

David A. Cieslak†

Nitesh V. Chawla†

Abstract

We propose a new decision tree algorithm, Class Conﬁdence
Proportion Decision Tree (CCPDT), which is robust and
insensitive to size of classes and generates rules which are
statistically signiﬁcant.

In order to make decision trees robust, we begin by
expressing Information Gain, the metric used in C4.5, in
terms of conﬁdence of a rule. This allows us to immediately
explain why Information Gain, like conﬁdence, results in
rules which are biased towards the majority class. To
overcome this bias, we introduce a new measure, Class
Conﬁdence Proportion (CCP), which forms the basis of
CCPDT. To generate rules which are statistically signiﬁcant
we design a novel and efﬁcient top-down and bottom-up
approach which uses Fisher’s exact test to prune branches
of the tree which are not statistically signiﬁcant. Together
these two changes yield a classiﬁer that performs statistically
better than not only traditional decision trees but also trees
learned from data that has been balanced by well known
sampling techniques. Our claims are conﬁrmed through
extensive experiments and comparisons against C4.5, CART,
HDDT and SPARCCC.

1

Introduction

While there are several types of classiﬁers, rule-based classi-
ﬁers have the distinct advantage of being easily interpretable.
This is especially true in a “data mining” setting, where the
high dimensionality of data often means that apriori very lit-
tle is known about the underlying mechanism which gener-
ated the data.

Decision trees are perhaps the most popular form of
rule-based classiﬁers (such as the well-known C4.5 [15]).
Recently however, classiﬁers based on association rules have
also become popular [19] which are often called associa-
tive classiﬁers. Associative classiﬁers use association rule
mining to discover interesting and signiﬁcant rules from the
training data, and the set of rules discovered constitute the
classiﬁer. The canonical example of an associative classi-
ﬁer is CBA (classiﬁcation based on associations) [14], which

∗Centre for Distributed and High Performance Computing, School of
Information Technologies, the University of Sydney, Sydney NSW 2006,
Australia. {weiliu, chawla}@it.usyd.edu.au

†University of Notre Dame, Notre Dame IN 46556, USA. dcies-

lak@cse.nd.edu, nchawla@nd.edu

uses the minimum support and conﬁdence framework to ﬁnd
rules. The accuracy of associative classiﬁers depends on the
quality of their discovered rules. However, the success of
both decision trees and associate classiﬁers depends on the
assumption that there is an equal amount of information for
each class contained in the training data. In binary classiﬁ-
cation problems, if there is a similar number of instances for
both positive and negative classes, both C4.5 and CBA gen-
erally perform well. On the other hand, if the training data
set tends to have an imbalanced class distribution, both types
of classiﬁer will have a bias towards the majority class. As
it happens, an accurate prediction is typically related to the
minority class – the class that is usually of greater interest.

One way of solving the imbalance class problem is to
modify the class distributions in the training data by over-
sampling the minority class or under-sampling the majority
class. For instance, SMOTE [5] uses over-sampling to
increase the number of the minority class instances, by
creating synthetic samples. Further variations on SMOTE
[7] have integrated boosting with sampling strategies to
better model the minority class, by focusing on difﬁcult
samples that belong to both minority and majority classes.

Nonetheless, data sampling is not the only way to deal
with class imbalanced problems: some speciﬁcally designed
“imbalanced data oriented” algorithms can perform well on
the original unmodiﬁed imbalanced data sets. For example,
a variation on associative classiﬁer called SPARCCC [19]
has been shown to outperform CBA [14] and CMAR [13] on
imbalanced data sets. The downside of SPARCCC is that
it generates a large number of rules. This seems to be a
feature of all associative classiﬁers and negates many of the
advantages of rule-based classiﬁcation.

In [8], the Hellinger distance (HDDT) was used as the
decision tree splitting criterion and shown to be insensitive
towards class distribution skewness. We will compare and
discuss CCPDT and HDDT more extensively in Section 3.4.
Here it will be sufﬁce to state that while HDDT is based on
likelihood difference, CCPDT is based on likelihood ratio.

In order to prevent trees from over-ﬁtting the data,
all decision trees use some form of pruning. Traditional
pruning algorithms are based on error estimations - a node
is pruned if the predicted error rate is decreased. But
this pruning technique will not always perform well on
imbalanced data sets. [4] has shown that pruning in C4.5 can
have a detrimental effect on learning from imbalanced data

766Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.sets, since lower error rates can be achieved by removing the
branches that lead to minority class leaves. In contrast our
pruning is based on Fisher’s exact test, which checks if a path
in a decision tree is statistically signiﬁcant; and if not, the
path will be pruned. As an added advantage, every resulting
tree path (rule) will also be statistically signiﬁcant.

Main Insight The main insight of the paper can be summa-
rized as follows. Let X be an attribute and y a class. Let
X → y and ¬X → y be two rules, with conﬁdence p and q
respectively. Then we can express Information Gain (IG) in
terms of the two conﬁdences. Abstractly,

IGC4.5 = F (p, q)

where F is an abstract function. We will show that a splitting
measure based on conﬁdence will be biased towards the
majority class. Our innovation is to use Class Conﬁdence
Proportion (CCP) instead of conﬁdence. Abstractly CCP of
the X → y and ¬X → y is r and s. We deﬁne a new
splitting criterion

Table 1: An example of notations for CBA analysis

y
¬y

X
a
c

b
d

Σ Attributes

a + c

b + d

a + b
c + d

n

¬X

Σ Instances

2.1 CBA The performance of Associative Classiﬁers de-
pends on the quality of the rules it discovers during the train-
ing process. We now demonstrate that in an imbalanced set-
ting, conﬁdence is biased towards the majority class.

Suppose we have a training data set which consists of n
records, and the antecedents (denoted by X and ¬X) and
class (y and ¬y) distributions are in the form of Table 1.
The rule selection strategy in CBA is to ﬁnd all rule items
that have support and conﬁdence above some predeﬁned
thresholds. For a rule X → y, its conﬁdence is deﬁned as:

(2.1)

Conf (X → y) =

Supp(X ∪ y)

Supp(X)

=

a

a + c

IGCCP DT = F

(r, s)

(cid:48)

“Conf ” and “Supp” stand for Conﬁdence and Support.

Similarly, we have:

The main thrust in the paper is to show that IGCCP DT
is more robust to class imbalance than IGC4.5 and behave
similarly when the classes are balanced.

1

The approach of replacing a conventional splitting mea-
sure by CCP is a generic mechanism for all traditional deci-
sion trees that are based on the “balanced data” assumption.
It can be applied to any decision tree algorithm that checks
the degree of impurity inside a partitioned branch, such as
C4.5 and CART etc.

The rest of the paper is structured as follows. In Section
2, we analyze the factors that causes CBA and C4.5 perform
poorly on imbalanced data sets. In Section 3, we introduce
CCP as the measure of splitting attributes during decision
tree construction. In Section 4 we present a full decision tree
algorithm which details how we incorporate CCP and use
Fisher’s Exact Test (FET) for pruning. A wrapper framework
utilizing sampling techniques is introduced in Section 5.
Experiments, Results and Analysis are presented in Section
6. We conclude in Section 7 with directions for research.

2 Rule-based Classiﬁers

We analyze the metrics used by rule-based classiﬁers in the
context of imbalanced data. We ﬁrst show that the ranking
of rules based on conﬁdence is biased towards the majority
class, and then express information gain and Gini index as
functions of conﬁdence and show that they also suffer from
similar problems.

1IGCCP DT is not Information Gain which has a speciﬁc meaning.

(2.2)

Conf (X → ¬y) =

Supp(X ∪ ¬y)

Supp(X)

=

c

a + c

Equation 2.1 suggests that selecting the highest conﬁ-
dence rules means choosing the most frequent class among
all the instances that contains that antecedent (i.e. X in this
example). However, for imbalanced data sets, since the size
of the positive class is always much smaller than the nega-
tive class, we always have: a + b (cid:28) c + d (suppose y is the
positive class). Given that imbalanced data do not affect the
distribution of antecedents, we can, without loss of general-
ity, assume that Xs and ¬Xs are nearly equally distributed.
Hence when data is imbalanced, a and b are both small while
c and d are both large. Even if y is supposed to occur with
X more frequently than ¬y, c is unlikely to be less than a
because the positive class size will be much smaller than the
negative class size. Thus, it is not surprising that the right-
side term in Equation 2.2 always tends to be lower bounded
by the right-side term in Equation 2.1. It appears that even
though the rule X → ¬y may not be signiﬁcant, it is easy
for it to have a high conﬁdence value.

In these circumstances, it is very hard for the conﬁdence
of a “good” rule X → y to be signiﬁcantly larger than that
of a “bad” rule X → ¬y. What is more, because of its low
conﬁdence, during the classiﬁer building process a “good”
rule may be ranked behind some other rules just because they
have a higher conﬁdence because they predict the majority
class. This is a fatal error, since in an imbalanced class
problem it is often the minority class that is of more interest.

767Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.2.2 Traditional decision trees Decision trees such as
C4.5 use information gain to decide which variable to split
[15]. The information gain from splitting a node t is deﬁned
as:

(2.3)

Inf oGainsplit = Entropy(t) −

Entropy(i)

(cid:88)

i=1,2

ni
n

where i represents one of the sub-nodes after splitting
(assume there are 2 sub-nodes), ni is the number of instances
in subnote i, and n stands for the total number of instances.
In binary-class classiﬁcation, the entropy of node t is deﬁned
as:

(2.4)

Entropy(t) = −

p(j|t) log p(j|t)

(cid:88)

j=1,2

where j represents one of the two classes. For a ﬁxed
training set (or its subsets), the ﬁrst term in Equation 2.3
is ﬁxed, because the number of instances for each class
(i.e. p(j|t) in equation 2.4) is the same for all attributes.
To this end, the challenge of maximizing information gain
in Equation 2.3 reduces to maximizing the second term
− (cid:80)

i=1,2
If the node t is split into two subnodes with two corre-
sponding paths: X and ¬X, and the instances in each node
have two classes denoted by y and ¬y, Equation 2.3 can be
rewritten as:

ni
n Entropy(i).

Figure 1: Approximation of information gain in the formula
formed by Conf (X → y) and Conf (¬X → y) from Equa-
tion 2.7. Information gain is the lowest when Conf (X → y)
and Conf (¬X → y) are both close to 0.5, and is the highest
when both Conf (X → y) and Conf (¬X → y) reaches 1
or 0.

either 0 or 1, and is minimized when Conf (X → y) and
Conf (¬X → y) are both close to 0.5. Note that when
Conf (X → y) is close to 0, Conf (X → ¬y) is close to 1;
when Conf (¬X → y) is close to 0, Conf (¬X → ¬y) is
close to 1. Therefore, information gain achieves the highest
value when either X → y or X → ¬y has the highest
conﬁdence, and either ¬X → y or ¬X → ¬y also has the
highest conﬁdence.

(2.5)

−

−

n1
n
n2
n

(2.6)

Inf oGainsplit = Entropy(t)

[−p(y|X) log(y|X) − p(¬y|X) log p(¬y|X)]

Inf oGainsplit = Entropy(t) +

Entropy(i)

(cid:88)

i=1,2

ni
n

[−p(y|¬X) log(y|¬X) − p(¬y|¬X) log p(¬y|¬X)]

=Entropy(t) +

[p log p + (1 − p) log(1 − p)]

n1
n

Note that the probability of y given X is equivalent to

(2.7)

the conﬁdence of X → y:

[q log q + (1 − q) log(1 − q)]

∝

[p log p + (1 − p) log(1 − p)]

n2
n

+

n1
n

+

n1
n

∝

[q log q + (1 − q) log(1 − q)]

n2
n
log pp(1 − p)1−p +

log qq(1 − q)1−q

n2
n

p(y|X) =

=

p(X ∩ y)

Support(X ∪ y)

p(X)

Support(X)

= Conf (X → y)

Then if we denote Conf (X → y) by p, and denote
Conf (¬X → y) by q (hence Conf (X → ¬y) = 1 − p and
Conf (¬X → ¬y) = 1 − q), and ignore the “ﬁxed” terms
Entropy(t) in equation 2.5, we can obtain the relationship in
Equation 2.7.

The ﬁrst approximation step in Equation 2.7 ignores
then the second approximation

the ﬁrst term Entropy(t),
transforms the addition of logarithms to multiplications.

Based on Equation 2.7 the distribution of information
gain as a function of Conf (X → y) and Conf (¬X → y)
is shown in Figure 1. Information gain is maximized when
Conf (X → y) and Conf (¬X → y) are both close to

Therefore, decision trees such as C4.5 split an attribute
whose partition provides the highest conﬁdence. This strat-
egy is very similar to the rule-ranking mechanism of asso-
ciation classiﬁers. As we have analyzed in Section 2.1, for
imbalanced data set, high conﬁdence rules do not necessarily
imply high signiﬁcance in imbalanced data, and some signif-
icant rules may not yield high conﬁdence. Thus we can assert
that the splitting criteria in C4.5 is suitable for balanced but
not imbalanced data sets.

We note that it is the term p(j|t) in Equation 2.4 that
is the cause of the poor behavior of C4.5 in imbalanced
situations. However, p(j|t) also appears in other decision

768Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Table 2: Confusion Matrix for the classiﬁcation of two
classes

All instances
Actual positive
Actual negative

Predicted positive
true positive (tp)
false positive (fp)

Predicted negative
false negative (fn)
true negative(tn)

tree measures. For example, the Gini index deﬁned in CART
[2] can be expressed as:

(2.8)

Gini(t) = 1 −

(cid:88)

p(j|t)2

j

Thus decision tree based on CART will too suffer from the
imbalanced class problem. We now propose another measure
which will be more robust in the imbalanced data situation.

3 Class Conﬁdence Proportion and Fisher’s Exact Test

Having identiﬁed the weakness of the support-conﬁdence
framework and the factor that results in the poor performance
of entropy and Gini index, we are now in a position to
propose new measures to address the problem.

3.1 Class Conﬁdence Proportion As previously ex-
plained, the high frequency with which a particular class y
appears together with X does not necessarily mean that X
“explains” the class y, because y could be the overwhelming
majority class. In such cases, it is reasonable that instead of
focusing on the antecedents (Xs), we focus only on each
class and ﬁnd the most signiﬁcant antecedents associated
with that class. In this way, all instances are partitioned ac-
cording to the class they contain, and consequently instances
that belong to different classes will not have an impact on
each other. To this end, we deﬁne a new concept, Class Con-
ﬁdence (CC), to ﬁnd the most interesting antecedents (Xs)
from all the classes (ys):

(3.9)

CC(X → y) =

Supp(X ∪ y)

Supp(y)

The main difference between this CC and traditional
conﬁdence is the denominator: we use Supp(y) instead of
Supp(X) so as to focus only on each class.

In the notation of the confusion matrix (Table 2) CC can

be expressed as:

CC(X → y) =

T rueP ositiveInstances
ActualP ositiveInstances

=

tp

tp + f n

(3.10)

(3.11)

CC(X → ¬y) =

F alseP ositiveInstances
ActualN egativeInstances

=

f p

f p + tn

While traditional conﬁdence examines how many
predicted positive/negative instances are actually posi-

(a) Classes are balanced

(b) Classes are imbalanced (1:10)

Figure 2: Information gain from original entropy when a data
set follows different class distributions. Compared with the
contour lines in (a), those in (b) shift towards the top-left and
bottom-right.

tive/negative (the precision), CC is focused in how many ac-
tual positive/negative instances are predicted correctly (the
recall). Thus, even if there are many more negative than pos-
itive instances in the data set (tp+f n (cid:28) f p+tn), Equations
3.10 and 3.11 will not be affected by this imbalance. Con-
sequently, rules with high CC will be the signiﬁcant ones,
regardless of whether they are discovered from balanced or
imbalanced data sets.

However, obtaining high CC rules is still insufﬁcient for
solving classiﬁcation problems – it is necessary to ensure
that the classes implied by those rules are not only of high
conﬁdence, but more interesting than their corresponding
alternative classes. Therefore, we propose the proportion
of one CC over that of all classes as our measure of how
interesting the class is – what we call the CC Proportion
(CCP). The CCP of rule X → y is deﬁned as:

(3.12)

CCP (X → y) =

CC(X → y)

CC(X → y) + CC(X → ¬y)

A rule with high CCP means that, compared with its
alternative class, the class this rule implies has higher CC,
and consequently is more likely to occur together with this
rule’s antecedents regardless of the proportion of classes
in the data set. Another beneﬁt of taking this proportion
is the ability to scale CCP between [0,1], which makes it
possible to replace the traditional frequency term in entropy
(the factor p(j|t) in Equation 2.4) by CCP. Details of the
CCP replacement in entropy is introduced in Section 4.

3.2 Robustness of CCP We now evaluate the robustness
of CCP using ROC-based isometric plots proposed in Flach
[10] and which are inherently independent of class and
misclassiﬁcation costs.

The 2D ROC space is spanned by false positive rate
(x-axis) and the true positive rate (y-axis). The contours
mark out the lines of constant value, of the splitting criterion,

769Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.conditioned on the imbalanced class ratio. Metrics which are
robust to class imbalance should have similar contour plots
for different class ratios.

In Figure 2, the contour plots of information gain are
It is
shown for class ratios of 1:1 and 1:10, respectively.
clear, from the two ﬁgures, that when the class distributions
become more imbalanced, the contours tend to be ﬂatter
and further away from the diagonal. Thus, given the same
true positive rate and false positive rate, information gain for
imbalanced data sets (Figure 2b) will be much lower than for
balanced data sets (Figure 2a).

Following the model of relative impurity proposed Flach
in [10], we now derive the deﬁnition for the CCP Impurity
Measure. Equation 3.12 gives:

(a) Classes are balanced

(b) Classes are imbalanced (1:10)

Figure 3:
Information gain from CCP-embedded entropy
when a data set follows different class distributions. No
contour line shifts when data sets becomes imbalanced.

(3.13)

CCP (X → y) =

tp

tp+f n

tp

tp+f n + f p

f p+tn

tpr

=

tpr + f pr

(3.16)

where tpr/fpr represents true/false positive rate. For
each node-split in tree construction, at least two paths will be
generated, if one is X → y, the other one will be ¬X → ¬y
with CCP:

(3.14)

CCP (¬X → ¬y) =

tn

f p+tn

tn

f p+tn + f n

tp+f n

=

1 − f pr

2 − tpr − f pr

The relative impurity for C4.5 proposed in [10] is:

(3.15)

Inf oGainC4.5 =Imp(tp + f n, f p + tn)

− (tp + f p) ∗ Imp(

− (f n + tn) ∗ Imp(

tp + f p

tp + f p

tp

f n

f p

tn

)

)

,

,

f n + tn

f n + tn

where Imp(p,n)=-plogp-nlogn. The ﬁrst term in the
right side represents the entropy of the node before splitting,
while the sum of the second and third terms represents
the entropy of the two subnodes after splitting. Take the
tp+f p ) as an example,
second term (tp + f p) ∗ Imp(
the ﬁrst frequency measure
tp+f p is an alternative way of
interpreting the conﬁdence of rule X → y; similarly, the
tp+f p is equal to the conﬁdence
second frequency measure
of rule X → ¬y. We showed in Section 2.2 that both terms
are inappropriate for imbalanced data learning.

tp+f p ,
tp

f p

f p

tp

To overcome the inherent weakness in traditional deci-
sion trees, we apply CCP into this impurity measure and thus
rewrite the information gain deﬁnition in Equation 3.15 as
the CCP Impurity Measure:

Inf oGainCCP = Imp(tp + f n, f p + tn)
f pr

tpr

− (tpr + f pr) ∗ Imp(

,

)

tpr + f pr

tpr + f pr

− (2 − tpr − f pr) ∗ Imp(

1 − tpr

1 − f pr

,

2 − tpr − f pr

2 − tpr − f pr

)

where Imp(p,n) is still “-plogp-nlogn”, while the origi-

nal frequency term is replaced by CCP.

The new isometric plots, with the CCP replacement, are
presented in Figure 3 (a,b). A comparison of the two ﬁgures
tells that contour lines remain unchanged, demonstrating that
CCP is unaffected by the changes in the class ratio.

3.3 Properties of CCP If all instances contained in a
node belong to the same class, its entropy is minimized
(zero). The entropy is maximized when a node contains
equal number of elements from both classes.

By taking all possible combinations of elements in the
confusion matrix (Table 2), we can plot the entropy surface
as a function of tpr and fpr as shown in Figure 4. Entropy
(Figure 4a) is the highest when tpr and fpr are equal, since
“tpr = fpr” in subnodes is equivalent to elements in the
subnodes being equally split between the two classes. On the
other hand, the larger the difference between tpr and fpr, the
purer the subnodes and the smaller their entropy. However,
as stated in Section 3.2, when data sets are imbalanced, the
pattern of traditional entropy will become distorted (Figure
4b).

Since CCP-embedded “entropy” is insensitive to class
skewness, its will always exhibit a ﬁxed pattern, and this
pattern is the same as traditional entropy’s balanced data
situation. This can be formalized as follows:

By using the notations in the confusion matrix, the fre-
quency term in traditional entropy is ptraditional = tp
tp+f p ,
while in CCP-based entropy it is pCCP = tpr
tpr+f pr . When
classes in a data set are evenly distributed, we have tp+fn =

770Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.(a) Traditional entropy on balanced data sets

(c) CCP-embedded entropy on any data sets

(b) Traditional entropy on imbalanced data sets.
(Positive:Negative = 1:10)

Figure 4: The sum of subnodes’ entropy after splitting. When a data set is imbalanced, the entropy surf (b) is “distored”
from (a); but for CCP-embedded “entropy” (c), the surface is always the same independent of the imbalance in the data.

fp+tn, and by applying it in the deﬁnition of CCP we obtain:

pCCP =

tpr

tpr + f pr

=

tp

tp+f n

tp

tp+f n + f p

f p+tn

tp

=

tp + f p

= ptraditional

Thus when there are same number of instances in each
class, the patterns of CCP-embedded entropy and traditional
entropy will be the same. More importantly, this pattern
is preserved for CCP-embedded entropy independent of the
imbalance the data sets. This is conﬁrmed in Figure 4c which
is always similar to the pattern of Figure 4a regardless of the
class distributions.

3.4 Hellinger Distance and its relationship with CCP
The divergence of two absolutely continuous distributions
can be measured by Hellinger distance with respect to the
parameter λ [17, 11], in the form of:

Figure 5: The attribute selection mechanisms of CCP and
Hellinger distances. This example illustrates a complemen-
tary situation where, while Hellinger distance can only pri-
oritizes B and C, CCP distinguishes only A and B.

dH (P, Q) =

(cid:115)(cid:90)

Ω

√
(

P − (cid:112)Q)2dλ

In the Hellinger distance based decision tree (HDDT)
the distribution P and Q are assumed to
technique [8],
be the normalized frequencies of feature values (“X” in
our notation) across classes. The Hellinger distance is
used to capture the propensity of a feature to separate the
classes.
In the tree-construction algorithm in HDDT, a
feature is selected as a splitting attribute when it produces
the largest Hellinger distance between the two classes. This
distance is essentially captured in the differences in the
relative frequencies of the attribute values for the two classes,
respectively.

The following formula, derived in [8], relates HDDT

with the true positive rate (tpr) and false positive rate (fpr).
(3.17)

ImpurityHD =

(cid:113)

((cid:112)tpr − (cid:112)f pr)2 + ((cid:112)1 − tpr − (cid:112)1 − f pr)2

This was also shown to be insensitive to class distribu-
tions in [8], since the only two variables in this formula are
tpr and fpr, without the dominating class priors.

Like the Hellinger distance, CCP is also just based on
tpr and fpr as shown in Equation 3.13. However, there is a
signiﬁcant difference between CCP and Hellinger distance.
While Hellinger distance take the square root difference of
tpr and fpr (|
f pr |) as the divergence of one class
distribution from the other, CCP takes the proportion of tpr
and fpr as a measurement of interest. A graphical difference
between the two measures is shown in Figure 5.

tpr −

√

√

If we draw a straight line (Line 3) parallel to the diagonal
in Figure 5, the segment length from origin to cross-point
between Line 3 and the y-axis is | tpro − f pro | (tpro
and f pro can be the coordinates of any point in Line 3), is
f pr |).
proportional to the Hellinger distance (|

tpr −

√

√

771Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.From this point of view, HDDT selects the point on those
parallel lines with the longest segment. Therefore, in Figure
5, all the points in Line 3 have a larger Hellinger distance
than those in Line 4; thus points in Line 3 will have higher
priority in the selection of attributes. As CCP = tpr
tpr+f pr
can be rewritten as tpr = CCP
1−CCP f pr, CCP is proportional
to the the slope of the line formed by the data point and
the origin, and consequently favors the line with the highest
slope. In Figure 5, the points in Line 1 are considered by
CCP as better splitting attributes than those in Line 2.

tprB
f prB

= tprC
f prC

By analyzing CCP and Hellinger distances in terms of
lines in a tpr versus fpr reference frame, we note that CCP
and Hellinger distance share a common problem. We give
an example as follows. Suppose we have three points, A
(f prA, tprA), B (f prB, tprB) and C (f prC, tprC), where
A is one Line 1 and 3, B on Line 2 and 3, and C on
Line 2 and 4 (shown in Figure 5). Then A and B are
on the same line (Line 3) that is parallel to the diagonal
| f prA − tprA |=| f prB − tprB |), while B and
(i.e.
C are on the same line (Line 2) passing through the origin
). Hellinger distances will treat A and
(i.e.
B as better splitting attributes than C, because as explained
above all points in Line 3 has longer Hellinger distances
than Line 4. By contrast, CCP will consider A has higher
splitting priorities than both B and C, since all points in
Line 1 obtains greater CCP than Line 2. However, on
points in Line 3 such as A and B, Hellinger distance fails
to distinguish them , since they will generate the same tpr
vs. fpr difference. In this circumstance, HDDT may make
an noneffective decision in attribute selection. This problem
will become signiﬁcant when the number of attributes is
large, and many attributes have similar | tpr −f pr | (or more
precisely |
f pr |) difference. The same problem
occurs in the CCP measurement on testing points in Line 2
such as B against C.

tpr −

√

√

Our solution to this problem is straightforward: when
choosing the splitting attribute in decision tree construction,
we select the one with the highest CCP by default, and
if there are attributes that possess similar CCP values, we
prioritize them on the basis of their Hellinger distances.
Thus, in Figure 5, the priority of the three points will be
A>B>C, since Point A has a greater CCP value than Points
B and C, and Point B has higher Hellinger distance than
Point C. Details of these attribute-selecting algorithms are
in Section 4.

3.5 Fisher’s Exact Test While CCP helps to select which
branch of a tree are “good” to discriminate between classes,
we also want to evaluate the statistical signiﬁcance of each
branch. This is done by the Fisher’s exact test (FET). For a
rule X → y, the FET will ﬁnd the probability of obtaining
the contingency table where X and y are more positively
associated, under the null hypothesis that {X, ¬X} and

{y, ¬y} are independent [19]. The p value of this rule is
given by:

(3.18)

p([a, b; c, d]) =

min(b,c)

(cid:88)

i=0

(a + b)!(c + d)!(a + c)!(b + d)!
n!(a + i)!(b − i)!(c − i)!(d + i)!

During implementation, the factorials in the p-value def-
inition can be handled by expressing their values logarithmi-
cally.

A low p value means that the variable independence null
hypothesis is rejected (no relationship between X and y);
in other words, there is a positive association between the
upper-left cell in the contingency table (true positives) and
the lower-right (true negatives). Therefore, given a threshold
for the p value, we can ﬁnd and keep the tree branches that
are statistically signiﬁcant (with lower p values), and discard
those tree nodes that are not.

4 CCP-based decision trees (CCPDT)

In this section we provide details of the CCPDT algorithm.
We modify the C4.5 splitting criterion based on entropy and
replace the frequency term by CCP. Due to space limits,
we omit the algorithms for CCP-embedded CART, but the
approach is identical to C4.5 (in that the same factor is
replaced with CCP).

Algorithm 1 (CCP-C4.5) Creation of CCP-based C4.5
Input: Training Data: T D
Output: Decision Tree

1: if All instances are in the same class then
2:

Return decision tree with one node (root), labeled as the
instances’ class,

// Find the best splitting attribute (Attri),
Attri = MaxCCPGain(T D),
Assign Attri to the tree root (root = Attri),
for each value vi of Attri do

Add a branch for vi,
if No instance is vi at attribute Attri then

Add a leaf to this branch.

3: else
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end if

else

end if
end for

Add a subtree CCP − C4.5(T Dvi ) to this branch,

4.1 Build tree The original deﬁnition of entropy in deci-
sion trees is presented in Equation 2.4. As explained in Sec-
tion 2.2, the factor p(j|t) in Equation 2.4 is not a good crite-
rion for learning from imbalanced data sets, so we replace it
with CCP and deﬁne the CCP-embedded entropy as:

772Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Algorithm 2 (MaxCCPGain) Subroutine for discovering the
attribute with the greatest information gain
Input: Training Data: T D
Output: The attribute to be split: Attri
1: Let MaxHellinger, MaxInfoGain and Attri to 0;
2: for Each attribute Aj in T D do
3:
4:
5:
6:
7:

Calculate Hellinger distance: Aj.Hellinger,
Obtain the entropy before splitting: Aj.oldEnt,
Set the sum of sub-nodes’ entropy Aj.newEnt to 0,
for Each value V i

// |Tx,y| means the number of instance that have value x
and class y,

j of attribute Aj do

8:

9:

10:

11:
12:
13:
14:
15:
16:
17:
18:

tpr =

fpr =

,y=+

T

x=V i
j
+T

,y=+
T

x=V i
j
+T

,y(cid:54)=+

T

x=V i
j

T

x=V i
j

x(cid:54)=V i
j

,y=+

,y(cid:54)=+

x(cid:54)=V i
j

,y(cid:54)=+

,

,

Aj.newEnt += Tx=V i
tpr+f pr log

tpr+f pr ),

f pr

f pr

j

∗ (− tpr

tpr+f pr log

f pr

tpr+f pr −

end for
CurrentInfoGain = Aj.oldEnt − Aj.newEnt,
if MaxInfoGain < CurrentInfoGain then

Attri = j,
MaxInfoGain = CurrentInfoGain,
MaxHellinger = Aj.Hellinger,

else

if MaxHellinger < Aj.Hellinger AND MaxInfoGain ==
CurrentInfoGain then

Attri = j,
MaxHellinger = Aj.Hellinger,

end if

19:
20:
21:
end if
22:
23: end for
24: Return Attri.

if Leafi.parent is not the Root of DT then

Leafi.parent.pruneable = true, // ‘true’ is default
SetP runeable(Leafi.parent, pV T ),

Algorithm 3 (Prune) Pruning based on FET
Input: Unpruned decision tree DT , p-value threshold (pV T )
Output: Pruned DT
1: for Each leaf Leafi do
2:
3:
4:
end if
5:
6: end for
7: Obtain the root of DT ,
8: for Each child(i) of the root do
9:
10:
11:
12:
13:
14:
end if
15:
16: end for

if child(i).pruneable == true then

P runebyStatus(child(i)).

if child(i) is not a leaf then

Set child(i) to be a leaf,

end if

else

Node.pruneable = false,

if child(i).pruneable == f alse then

Algorithm 4 (SetPruneable) Subroutine for setting prune-
able status to each branch node by a bottom–up search
Input: A branch node N ode, p-Value threshold (pV T )
Output: Pruneable status of this branch node
1: for each child(i) if N ode do
2:
3:
end if
4:
5: end for
6: if Node.pruneable == true then
7:
8:
9:
10:
11: end if
12: if N ode.parent is not the Root of the full tree then
13:
14:
15: end if

Calculate the p value of this node: N ode.pV alue,
if N ode.pV alue < pV T then

Node.parent.pruneable = true // ‘true’ is default
SetPruneable(Node.parent, pVT),

Node.pruneable = false,

end if

Algorithm 5 (PrunebyStatus) Subroutine for pruning nodes
by their pruneable status
Input: A branch represented by its top node N ode
Output: Pruned branch

Set N ode as a leaf,

1: if Node.pruneable == true then
2:
3: else
4:
5:
6:
7:
8:
9: end if

for Each child(i) of Node do
if child(i) is not a leaf then
PrunebyStatus(child(i)),

end if
end for

(4.19)
EntropyCCP (t) = −

(cid:88)

j

CCP (X → yj)logCCP (X → yj)

Then we can restate the conclusion made in Section 2.2
in CCP-based decision trees, IGCCP DT achieves the
as:
highest value when either X → y or X → ¬y has high
CCP, and either ¬X → y or ¬X → ¬y has high CCP.

The process of creating CCP-based C4.5 (CCP-C4.5)
is described in Algorithm 1. The major difference between
CCP-C4.5 and C4.5 is the the way of selecting the candidate-
splitting attribute (Line 5). The process of discovering the
attribute with the highest information gain is presented in
the subroutine Algorithm 2. In Algorithm 2, Line 4 obtains
the entropy of an attribute before its splitting, Lines 6 – 11
obtain the new CCP-based entropy after the splitting of that
attribute, and Lines 13 – 22 record the attribute with the
highest information gain. In information gain comparisons
of different attributes, Hellinger distance is used to select the
attribute whenever InfoGain value of the two attributes are

773Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.equal (Lines 18–21), thus overcoming the inherent drawback
of both Hellinger distances and CCP (Section 3.4).

In our decision tree model, we treat each branch node
as the last antecedent of a rule. For example, if there are
three branch nodes (BranchA, BranchB, and BranchC) from
the root to a leaf LeafY, we assume that the following rule
exists: BranchA ∧ BranchB ∧ BranchC → Leaf Y . In
Algorithm 2, we calculate the CCP for each branch node;
using the preceding example, the CCP of BranchC is that of
the previous rule, and the CCP of BrachB is that of rule
BranchA ∧ BranchB → Leaf Y , etc.
In this way, the
attribute we select is guarenteed to be the one whose split
can generate rules (paths in the tree) with the highest CCP.

4.2 Prune tree After the creation of decision tree, Fisher’s
exact test is applied on each branch node. A branch node
will not be replaced by a leaf node if there is at least one
signiﬁcant descendant (a node with a lower p value than the
threshold) under that branch.

Checking the signiﬁcance of all descendants of an entire
branch is an expensive operation. To perform a more efﬁ-
cient pruning, we designed a two-staged strategy as shown
in Algorithm 3. The ﬁrst stage is a bottom–up checking pro-
cess from each leaf to the root. A node is marked “prune-
able” if it and all of its descendants are non-signiﬁcant. This
process of checking the pruning status is done via Lines 1–6
in Algorithm 3, with subroutine Algorithm 4. In the begin-
ning, all branch nodes are set to the default status of “prune-
able” (Line 3 in Algorithm 3 and Line 13 in Algorithm 4).
We check the signiﬁcance of each node from leaves to the
root. If any child of a node is “unpruneable” or the node it-
self represents a signiﬁcant rule, this node will be reset from
“pruneable” to “unpruneable”. If the original unpruned tree
is n levels deep, and has m leaves, the time complexity of
this bottom–up checking process is O(nm).

After checking for signiﬁcance, we conduct the second
pruning stage – a top-down pruning process performed ac-
cording to the “pruneable” status of each node from root to
leaves. A branch node is replaced by a leaf, if its “prune-
able” status is “true” (Line 10 in Algorithm 3 and Line 1 in
Algorithm 5).

Again, if the original unpruned tree is n levels deep and
has m leaves, the time complexity of the top–down pruning
process is O(nm). Thus, the total time complexity of our
pruning algorithm is O(n2).

This two-stage pruning strategy guarantees both the
completeness and the correctness of the pruned tree. The
ﬁrst stage checks the signiﬁcance of each possible rule (path
through the tree), and ensures that each signiﬁcant rule is
“unpruneable”, and thus complete; the second stage prunes
all insigniﬁcant rules, so that the paths in the pruned tree are
all correct.

5 Sampling Methods

Another mechanism of overcoming the imbalance class dis-
tribution is to synthetically delete or add training instances
and thus balance the class distribution. To achieve this goal,
various sampling techniques have been proposed to either re-
move instances from the majority class (aka under-sampling)
or introduce new instances to the minority class (aka over-
sampling).

We consider a wrapper framework that uses a combi-
nation of random under-sampling and SMOTE [5, 6]. The
wrapper ﬁrst determines the percentage of under-sampling
that will result in an improvement in AUC over the decision
tree trained on the original data. Then the number of in-
stances in majority class is under-sampled to the stage where
the AUC does not improve any more, the wrapper explores
the appropriate level of SMOTE. Then taking the level of
under-sampling into account, SMOTE introduces new syn-
thetic examples to the minority class continuously until the
AUC is optimized again. We point the reader to [6] for more
details on the wrapper framework. The performance of deci-
sion trees trained on the data sets optimized by this wrapper
framework is evaluated against CCP-based decision trees in
experiments.

6 Experiments

In our experiments, we compared CCPDT with C4.5 [15],
CART[2], HDDT [8] and SPARCCC [19] on binary class
data sets. These comparisons demonstrate not only the
efﬁciency of their splitting criteria, but the performance of
their pruning strategies.

Weka’s C4.5 and CART implementations [20] were em-
ployed in our experiments, based on which we implemented
CCP-C4.5, CCP-CART, HDDT and SPARCCC 2, so that we
can normalize the effects of different versions of the imple-
mentations.

All experiments were carried out using 5×2 folds cross-
validation, and the ﬁnal results were averaged over the ﬁve
runs. We ﬁrst compare purely on splitting criteria without
applying any pruning techniques, and then comparisons be-
tween pruning methods on various decision trees are pre-
sented. Finally, we compare CCP-based decision trees with
state-of-the-art sampling methods.

6.1 Comparisons on splitting criteria The binary-class
data sets were mostly obtained from [8] (Table 3) which were
pre-discretized. They include a number of real-world data
sets from the UCI repository and other sources. “Estate”
contains electrotopological state descriptors for a series of
compounds from the US National Cancer Institute’s Yeast
Anti-Cancer drug screen. “Ism” ([5]) is highly unbalanced

2Implementation source code and data sets used in the experiments can

be obtained from http://www.cs.usyd.edu.au/˜weiliu

774Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Table 3:
Information about imbalanced binary-class data
sets. The percentages listed in the last column is the pro-
portion of the minor class in each data set.

Instances

Attributes MinClass %

Data Sets
Boundary
Breast
Cam
Covtype
Estate
Fourclass
German
Ism
Letter
Oil
Page
Pendigits
Phoneme
PhosS
Pima
Satimage
Segment
Splice
SVMguide

3505
569
28374
38500
5322
862
1000
11180
20000
937
5473
10992
2700
11411
768
6430
2310
1000
3089

175
30
132
10
12
2
24
6
16
50
10
16
5
481
8
37
20
60
4

3.5%
37.3%
5.0%
7.1%
12.0%
35.6%
30.0%
2.3%
3.9%
4.4%
10.2%
10.4%
29.3%
5.4%
34.9%
9.7%
14.3%
48.3%
35.3%

and records information on calciﬁcation in a mammogram.
“Oil” contains information about oil spills; it is relatively
small and very noisy [12]. “Phoneme” originates from the
ELENA project and is used to distinguish between nasal
“Boundary”, “Cam”, and “PhosS” are
and oral sounds.
biological data sets from [16].
“FourClass”, “German”,
“Splice”, and “SVMGuide” are available from LIBSVM [3].
The remaining data sets originate from the UCI repository
[1]. Some were originally multi-class data sets, and we
converted them into two-class problems by keeping the
smallest class as the minority and the rest as the majority.
The exception was “Letter”, for which each vowel became a
member of the minority class, against the consonants as the
majority class.

As accuracy is considered a poor performance measure
for imbalanced data sets, we used the area under ROC curve
(AUC) [18] to estimate the performance of each classiﬁer.

In our imbalanced data sets learning experiments, we
only wanted to compare the effects of different splitting
criteria; thus, the decision trees (C4.5, CCP-C4.5, CART,
CCP-CART, and HDDT) were unpruned, and we used
Laplace smoothing on leaves. Since SPARCCC has been
proved more efﬁcient in imbalanced data learning than CBA
[19], we excluded CBA and included only SPARCCC. Table
4 lists the “Area Under ROC (AUC)” value on each data
set for each classiﬁer, followed by the ranking of these
classiﬁers (presented in parentheses) on each data set.

We used the Friedman test on AUCs at 95% conﬁdence
level to compare among different classiﬁers [9].
In all
experiments, we chose the best performance classiﬁer as
the “Base” classiﬁer. If the “Base” classiﬁer is statistically

Table 4: Splitting criteria comparisons on imbalanced data
sets where all trees are unpruned.
“Fr.T” is short for
Friedman test. The classiﬁer with a (cid:88) sign in the Friedman
test is statistically outperformed by the “Base” classiﬁer.
The ﬁrst two Friedman tests illustrate that CCP-C4.5 and
CCP-CART are signiﬁcantly better than C4.5 and CART,
respectively. The third Friedman test conﬁrms that CCP-
based decision trees is signiﬁcantly better than SPARCCC.

Area Under ROC

Data Sets

C4.5 CCP-C4.5 CART CCP-CART HDDT SPARCCC
0.533(4) 0.595(2) 0.529(5) 0.628(1) 0.594(3) 0.510(6)
Boundary
Breast
0.919(5) 0.955(2) 0.927(4) 0.958(1) 0.952(3) 0.863(6)
0.707(3) 0.791(1) 0.702(4) 0.772(2) 0.680(5) 0.636(6)
Cam
0.928(4) 0.982(1) 0.909(5) 0.979(3) 0.982(1) 0.750(6)
Covtype
0.601(1) 0.594(2) 0.582(4) 0.592(3) 0.580(5) 0.507(6)
Estate
Fourclass
0.955(5) 0.971(3) 0.979(1) 0.969(4) 0.975(2) 0.711(6)
0.631(4) 0.699(1) 0.629(5) 0.691(3) 0.692(2) 0.553(6)
German
0.805(4) 0.901(3) 0.802(5) 0.905(2) 0.990(1) 0.777(6)
Ism
0.972(3) 0.991(1) 0.968(4) 0.990(2) 0.912(5) 0.872(6)
Letter
0.641(6) 0.825(1) 0.649(5) 0.802(2) 0.799(3) 0.680(4)
Oil
0.906(5) 0.979(1) 0.918(4) 0.978(2) 0.974(3) 0.781(6)
Page
0.966(4) 0.990(2) 0.966(4) 0.990(2) 0.992(1) 0.804(6)
Pendigits
0.824(5) 0.872(3) 0.835(4) 0.876(2) 0.906(1) 0.517(6)
Phoneme
0.543(4) 0.691(1) 0.543(4) 0.673(3) 0.677(2) 0.502(6)
PhosS
0.702(4) 0.757(3) 0.696(5) 0.758(2) 0.760(1) 0.519(6)
Pima
0.774(4) 0.916(1) 0.730(5) 0.915(2) 0.911(3) 0.706(6)
Satimage
0.981(5) 0.987(1) 0.982(4) 0.987(1) 0.984(3) 0.887(6)
Segment
Splice
0.913(4) 0.952(1) 0.894(5) 0.926(3) 0.950(2) 0.781(6)
SVMguide 0.976(4) 0.989(1) 0.974(5) 0.989(1) 0.989(1) 0.924(6)
Avg. Rank
1.6
Fr.T (C4.5) (cid:88)9.6E-5 Base
Fr.T (CART)
Fr.T (Other)

0.0896 (cid:88)1.3E-5

(cid:88)9.6E-5

Base

Base

3.95

5.65

4.15

2.09

2.4

signiﬁcantly better than another classiﬁer in comparison (i.e.
the value of Friedman test is less than 0.05), we put a “(cid:88)”
sign on the respective classiﬁer (shown in Table 4).

The comparisons revealed that even though SPARCCC
performs better than CBA [19], its overall results are far less
robust than those from decision trees. It might be possible
to obtain better SPARCCC results by repeatedly modifying
their parameters and attempting to identify the optimized
parameters, but the manual parameter conﬁguration itself is
a shortcoming for SPARCCC.

Because we are interested in the replacement of original
factor p(j|t) in Equation 2.4 by CCP, three separate Fried-
man tests were carried out:
the ﬁrst two between conven-
tional decision trees (C4.5/CART) and our proposed deci-
sion trees (CCP-C4.5/CCP-CART), and the third on all the
other classiﬁers. A considerable AUC increase from tradi-
tional to CCP-based decision trees was observed, and statis-
tically conﬁrmed by the ﬁrst two Friedman tests: these small
p values of 9.6E-5 meant that we could conﬁdently reject the
hypothesis that the “Base” classiﬁer showed no signiﬁcant
differences from current classiﬁer. Even though CCP-C4.5

775Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Table 5: Pruning strategy comparisons on AUC. “Err.ESt” is
short for error estimation. The AUC of C4.5 and CCP-C4.5
pruned by FET are signiﬁcantly better than those pruned by
error estimation.

Table 6: Pruning strategy comparisons on number of leaves.
The leaves on trees of C4.5 and CCP-C4.5 pruned by FET
are not signiﬁcantly more than those pruned by error estima-
tion.

C4.5

CCP-C4.5

C4.5

CCP-C4.5

Data set

Err.Est
0.501(3)
0.954(1)
0.545(3)
0.977(4)
0.505(3)
0.964(3)
0.708(3)
0.870(3)
0.985(3)
0.776(4)
0.967(4)
0.984(4)
0.856(4)
0.694(1)
0.751(4)
0.897(4)
0.987(3)
0.954(1)
0.982(4)

Boundary
Breast
Cam
Covtype
Estate
Fourclass
German
Ism
Letter
Oil
Page
Pendigits
Phoneme
PhosS
Pima
Satimage
Segment
Splice
SVMguide
Avg.Rank
Fr.T (C4.5) (cid:88) 0.0184
Fr.T (CCP)

3.0

FET

0.560(2)
0.951(3)
0.747(2)
0.979(2)
0.539(2)
0.961(4)
0.715(2)
0.891(1)
0.993(1)
0.791(3)
0.975(1)
0.986(3)
0.860(2)
0.649(3)
0.760(1)
0.912(2)
0.987(3)
0.951(4)
0.985(1)

2.15
Base

Err.Est
0.500(4)
0.953(2)
0.513(4)
0.979(2)
0.505(3)
0.965(2)
0.706(4)
0.848(4)
0.982(4)
0.812(2)
0.969(3)
0.988(2)
0.858(3)
0.595(4)
0.758(2)
0.907(3)
0.988(1)
0.954(1)
0.984(2)

2.65

FET

0.613(1)
0.951(3)
0.755(1)
0.980(1)
0.595(1)
0.969(1)
0.719(1)
0.887(2)
0.989(2)
0.824(1)
0.973(2)
0.989(1)
0.868(1)
0.688(2)
0.755(3)
0.917(1)
0.988(1)
0.953(3)
0.984(2)

1.55

Data set

Boundary
Breast
Cam
Covtype
Estate
Fourclass
German
Ism
Letter
Oil
Page
Pendigits
Phoneme
PhosS
Pima
Satimage
Segment
Splice
SVMguide
Avg.Rank
Fr.T (C4.5)
Fr.T (CCP)

Err.Est
2.7(2)
7.7(4)
54.5(2)
156.0(1)
2.2(2)
13.9(3)
41.3(3)
19.8(1)
30.2(1)
8.0(1)
29.4(2)
35.1(3)
32.6(4)
68.2(2)
15.5(4)
83.7(1)
8.3(3)
21.3(3)
14.5(3)
2.3
Base

FET
33.9(3)
6.4(2)
445.9(3)
175.3(3)
5.2(4)
13.0(1)
40.0(1)
26.3(3)
45.1(3)
10.7(3)
28.8(1)
37.8(4)
29.7(2)
221.5(3)
12.7(2)
107.8(3)
8.4(4)
18.0(1)
12.3(2)
2.45
0.4913

Err.Est
1.2(1)
7.0(3)
7.9(1)
166.9(2)
2.0(1)
14.1(4)
47.1(4)
21.9(2)
34.3(2)
8.5(2)
32.6(3)
31.5(1)
30.1(3)
37.7(1)
13.3(3)
94.6(2)
6.4(1)
22.6(4)
15.3(4)
2.25

FET
87.6(4)
6.2(1)
664.6(4)
189.0(4)
4.6(3)
13.3(2)
40.1(2)
31.0(4)
47.4(4)
12.0(4)
34.0(4)
32.8(2)
27.0(1)
311.4(4)
11.8(1)
119.2(4)
7.2(2)
18.4(2)
11.9(1)
2.7

(cid:88) 0.0076

Base

Base

0.2513

was not statistically better than HDDT, the strategy to com-
bine CCP and HDDT (analyzed in Section 3.4) provided an
improvement on AUC with higher than 91% conﬁdence.

6.2 Comparison of Pruning Strategies In this section,
we compared FET-based pruning with the pruning based on
error estimation as originally proposed in C4.5 [15]. We
reuse the data sets from previous subsection, and apply error-
based pruning and FET-based pruning separately on the trees
built by C4.5 and CCP-C4.5, where the conﬁdence level of
FET was set to 99% (i.e.
the p-Value threshold is set to
0.01). Note that the tree constructions differences between
C4.5 and CCP-C4.5 are out of scope in this subsection;
we carried out separate Friedman test on C4.5 and CCP-
C4.5 respectively and only compare the different pruning
strategies. HDDT was excluded from this comparison since
it provides no separate pruning strategies.

Table 5 and 6 present the performance of the two pairs
of pruned trees. The numbers of leaves in Table 6 are not
integers because they are the average values of 5 × 2 –
fold cross validations. Statistically, the tree of C4.5 pruned
by FET signiﬁcantly outperformed the same tree pruned
by error estimation (Table 5) without retaining signiﬁcantly
more leaves (Table 6). The same pattern is found in CCP-
C4.5 trees, where FET-based pruning sacriﬁced insigniﬁcant
more numbers of leaves to obtain a signiﬁcantly larger
AUC. This phenomenon proves the completeness of the
FET pruned trees, since error estimation inappropriately cuts
signiﬁcant number of trees paths, and hence always has
smaller number of leaves and lower AUC values.

6.3 Comparisons with Sampling Techniques We now
compare CCP-based decision trees against sampling based
methods discussed in Section 5. Note that the wrapper
is optimized on training sets using 5 × 2 cross validation
to determine the sampling levels. The algorithm was then
evaluated on the corresponding 5 × 2 cross validated testing
set.

The performances of three pairs of decision trees are
shown in Table 7. The ﬁrst pair “Original” has no modiﬁ-
cation on either data or decision tree algorithms. The sec-
ond pair “Sampling based” uses the wrapper to re-sample
the training data which is then used by original decision tree
algorithms to build the classiﬁer; and “CCP based” shows
the performance of CCP-based decision trees learned from
original data. Friedman test on the AUC values shows that,
although the “wrapped” data can help to improve the per-
formance of original decision trees, using CCP-based al-
gorithms can obtain statistically better classiﬁers directly
trained on the original data.

7 Conclusion and future work

We address the problem of designing a decision tree algo-
rithm for classiﬁcation which is robust against class imbal-
ance in the data. We ﬁrst explain why traditional decision
tree measures, like information gain, are sensitive to class
imbalance. We do that by expressing information gain in
terms of the conﬁdence of a rule. Information gain, like con-
ﬁdence, is biased towards the majority class. Having iden-
tiﬁed the cause of the problem, we propose a new measure,
Class Conﬁdence Proportion (CCP). Using both theoretical

776Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Table 7:
Performances comparisons on AUC gener-
ated by original, sampling and CCP-based techniques.
“W+CART/W+C4.5” means applying wrappers to sample
training data before it’s learned by CART/C4.5. In this table,
CCP-based CART decision tree is signiﬁcantly better than
all trees in “Original” and “Sampling based” categories.

Original

Sampling based

CCP based

Data set

CART

C4.5 W+CART W+C4.5 CCP-CARTCCP-C4.5
0.613(3)
0.951(5)
0.755(2)
0.980(2)
0.595(2)
0.969(3
0.719(1)
0.887(4)
0.989(1)
0.824(1)
0.973(3)
0.989(2)
0.868(4)
0.688(2)
0.755(2)
0.917(1)
0.988(1)
0.953(2)
0.984(4)

Boundary 0.514(5) 0.501(6) 0.582(4) 0.616(2) 0.631(1)
0.928(6) 0.954(2) 0.955(1) 0.953(4) 0.954(2)
Breast
Cam
0.701(3) 0.545(6) 0.660(5) 0.676(4) 0.777(1)
0.910(6) 0.977(4) 0.974(5) 0.980(2) 0.981(1)
Covtype
Estate
0.583(3) 0.505(6) 0.560(5) 0.580(4) 0.598(1)
Fourclass 0.978(1) 0.964(5) 0.943(6) 0.965(4) 0.971(2)
German
0.630(6) 0.708(2) 0.668(5) 0.690(4) 0.691(3)
0.799(6) 0.870(5) 0.905(3) 0.909(1) 0.906(2)
Ism
0.964(6) 0.985(4) 0.977(5) 0.989(1) 0.989(1)
Letter
0.666(6) 0.776(5) 0.806(3) 0.789(4) 0.822(2)
Oil
Page
0.920(6) 0.967(5) 0.970(4) 0.978(1) 0.978(1)
Pendigits 0.963(6) 0.984(4) 0.982(5) 0.987(3) 0.990(1)
Phoneme 0.838(6) 0.856(5) 0.890(2) 0.894(1) 0.871(3)
0.542(6) 0.694(1) 0.665(5) 0.670(4) 0.676(3)
PhosS
Pima
0.695(6) 0.751(4) 0.742(5) 0.755(2) 0.760(1)
Satimage 0.736(6) 0.897(4) 0.887(5) 0.904(3) 0.914(2)
Segment
0.980(5) 0.987(2) 0.980(5) 0.982(4) 0.987(2)
0.885(5) 0.954(1) 0.829(6) 0.942(3) 0.940(4)
Splice
SVMguide 0.973(6) 0.982(5) 0.985(3) 0.987(2) 0.989(1)
Avg.Rank
Fr.T

(cid:88) 9.6E-5(cid:88) 0.0076 (cid:88) 5.8E-4 (cid:88) 0.0076

1.75
Base

0.3173

4.15

3.85

5.05

2.3

2.7

and geometric arguments we show that CCP is insensitive
to class distribution. We then embed CCP in information
gain and use the improvised measure to construct the deci-
sion tree. Using a wide array of experiments we demonstrate
the effectiveness of CCP when the data sets are imbalanced.
We also propose the use of Fisher exact test as a method for
pruning the decision tree. Besides improving the accuracy,
an added beneﬁt of the Fisher exact test is that all the rules
found are statistically signiﬁcant.

Acknowledgements

The ﬁrst author of this paper acknowledges the ﬁnancial
support of the Capital Markets CRC.

References

[1] A. Asuncion and D.J. Newman. UCI Machine Learning

[2] L. Breiman. Classiﬁcation and regression trees. Chapman &

Repository, 2007.

Hall/CRC, 1984.

[3] C.C. Chang and C.J. Lin. LIBSVM: a library for support
Software available at http://

vector machines, 2001.
www.csie.ntu.edu.tw/˜cjlin/libsvm.

[4] N.V. Chawla. C4. 5 and imbalanced data sets: investigating
the effect of sampling method, probabilistic estimate, and
decision tree structure. 2003.

[5] N.V. Chawla, K.W. Bowyer, L.O. Hall, and W.P. Kegelmeyer.
SMOTE: synthetic minority over-sampling technique. Jour-
nal of Artiﬁcial Intelligence Research, 16(1):321–357, 2002.
[6] N.V. Chawla, D.A. Cieslak, L.O. Hall, and A. Joshi. Auto-
matically countering imbalance and its empirical relationship
to cost. Data Mining and Knowledge Discovery, 17(2):225–
252, 2008.

[7] N.V. Chawla, A. Lazarevic, L.O. Hall, and K.W. Bowyer.
SMOTEBoost: Improving prediction of the minority class in
boosting. Lecture notes in computer science, pages 107–119,
2003.

[8] D.A. Cieslak and N.V. Chawla. Learning Decision Trees
for Unbalanced Data. In Proceedings of the 2008 European
Conference on Machine Learning and Knowledge Discovery
in Databases-Part I, pages 241–256. Springer-Verlag Berlin,
Heidelberg, 2008.

[9] J. Demˇsar. Statistical comparisons of classiﬁers over multiple
data sets. The Journal of Machine Learning Research, 7:30,
2006.

[10] P.A. Flach. The geometry of ROC space: understanding ma-
chine learning metrics through ROC isometrics. In Proceed-
ings of the Twentieth International Conference on Machine
Learning, pages 194–201, 2003.

[11] T. Kailath. The divergence and Bhattacharyya distance mea-
sures in signal selection. IEEE Transactions on Communica-
tion Technology, 15(1):52–60, 1967.

[12] M. Kubat, R.C. Holte, and S. Matwin. Machine Learning for
the Detection of Oil Spills in Satellite Radar Images. Machine
Learning, 30(2-3):195–215, 1998.

[13] W. Li, J. Han, and J. Pei. CMAR: Accurate and Efﬁcient
Classiﬁcation Based on Multiple Class-Association Rules.
In Proceedings of the 2001 IEEE International Conference
on Data Mining, pages 369–376. IEEE Computer Society
Washington, DC, USA, 2001.

[14] B. Liu, W. Hsu, Y. Ma, A.A. Freitas, and J. Li. Integrating
Classiﬁcation and Association Rule Mining. IEEE Transac-
tions on Knowledge and Data Engineering, 18:460–471.

[15] J.R. Quinlan. C4.5: Programs for Machine Learning. Mor-
gan Kaufmann Publishers Inc. San Francisco, CA, USA,
1993.

[16] P. Radivojac, N.V. Chawla, A.K. Dunker, and Z. Obradovic.
Classiﬁcation and knowledge discovery in protein databases.
Journal of Biomedical Informatics, 37(4):224–239, 2004.

[17] C.R. Rao. A review of canonical coordinates and an alter-
native to correspondence analysis using Hellinger distance.
Institut d’Estad´ıstica de Catalunya, 1995.

[18] J.A. Swets. Signal detection theory and ROC analysis in
psychology and diagnostics. Lawrence Erlbaum Associates,
1996.

[19] F. Verhein and S. Chawla. Using Signiﬁcant, Positively Asso-
ciated and Relatively Class Correlated Rules for Associative
Classiﬁcation of Imbalanced Datasets. In Seventh IEEE Inter-
national Conference on Data Mining, 2007., pages 679–684,
2007.

[20] I.H. Witten and E. Frank. Data mining: practical machine
learning tools and techniques with Java implementations.
ACM SIGMOD Record, 31(1):76–77, 2002.

777Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.