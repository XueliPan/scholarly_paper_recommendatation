Proﬁling Users in a 3G Network Using Hourglass

Co-Clustering

Ram Keralapura

Narus Inc.

Antonio Nucci

Narus Inc.

570 Maude Ct, Sunnyvale,

570 Maude Ct, Sunnyvale,

CA, USA

CA, USA

rkeralapura@narus.com

anucci@narus.com

Zhi-Li Zhang

University of Minnesota

Minneapolis, MN 55455, USA
zhzhang@cs.umn.edu

Lixin Gao

University of Massachusetts
Amherst, MA 01003, USA
lgao@ecs.umass.edu

ABSTRACT
With widespread popularity of smart phones, more and more users
are accessing the Internet on the go. Understanding mobile user
browsing behavior is of great signiﬁcance for several reasons. For
example, it can help cellular (data) service providers (CSPs) to im-
prove service performance, thus increasing user satisfaction. It can
also provide valuable insights about how to enhance mobile user
experience by providing dynamic content personalization and rec-
ommendation, or location-aware services.

In this paper, we try to understand mobile user browsing be-
havior by investigating whether there exists distinct “behavior pat-
terns” among mobile users. Our study is based on real mobile net-
work data collected from a large 3G CSP in North America. We
formulate this user behavior proﬁling problem as a co-clustering
problem, i.e., we group both users (who share similar browsing
behavior), and browsing proﬁles (of like-minded users) simultane-
ously. We propose and develop a scalable co-clustering methodol-
ogy, Phantom, using a novel hourglass model. The proposed hour-
glass model ﬁrst reduces the dimensions of the input data and per-
forms divisive hierarchical co-clustering on the lower dimensional
data; it then carries out an expansion step that restores the origi-
nal dimensions. Applying Phantom to the mobile network data, we
ﬁnd that there exists a number of prevalent and distinct behavior
patterns that persist over time, suggesting that user browsing behav-
ior in 3G cellular networks can be captured using a small number
of co-clusters. For instance, behavior of most users can be classi-
ﬁed as either homogeneous (users with very limited set of brows-
ing interests) or heterogeneous (users with very diverse browsing
interests), and such behavior proﬁles do not change signiﬁcantly at
either short (30-min) or long (6 hour) time scales.

Categories and Subject Descriptors: J.0 [Computer Applications]:
General

General Terms: Algorithms, Measurement.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
MobiCom’10, September 20–24, 2010, Chicago, Illinois, USA.
Copyright 2010 ACM 978-1-4503-0181-7/10/09 ...$10.00.

Keywords: Hierarchical Co-clustering, Hourglass Model, Phan-
tom Bi-Clustering.

1.

INTRODUCTION

Understanding user proﬁles (i.e., the web browsing behavior of
users) is critical to cellular service providers (CSP) for implement-
ing dynamic content recommendation systems and web personal-
ization. These content recommendation systems help CSPs to pre-
dict user behavior, and thus recommend links that users are most
likely to be interested in. The accuracy of such recommendation
systems helps in providing a personalized web browsing experi-
ence for users that not only increases the satisfaction of existing
users but will also attract new users. To thrive in today’s market, a
CSP is expected to understand its users’ needs better than its com-
petitors to offer appropriate location-based services.

In this paper, we try to understand the browsing behavior of mo-
bile users by investigating whether there exists distinct “behavior
patterns”. We are interested in understanding how similar or differ-
ent their behavior is, and studying this behavior by grouping users
and browsing proﬁles into meaningful clusters. We conduct this
study using real mobile network data collected from a large 3G
CSP in America.

The user proﬁling problem has been addressed in the past as a
clustering problem where the objective was to group users who ex-
hibit similar behavior [3]. Grouping entities of similar kind (like
“users” in user proﬁling) based on a chosen similarity metric is
termed as one-way clustering. However, in this work, we argue that
the user proﬁling problem should be treated as a two-way clustering
problem with an objective to cluster both users and browsing pro-
ﬁles simultaneously. This will capture the grouping among users
induced by the browsing proﬁles, and the grouping among brows-
ing proﬁles induced by users simultaneously. Such a grouping is
essential since both users and their browsing proﬁles are inﬂuenced
by each other and are not independent. This two-way clustering is
called co-clustering or bi-clustering.

Another way to think about this is in terms of a matrix. The
input for user proﬁling can be viewed as a matrix with users on
the rows and websites (or URLs) on the columns, and each entry
in the matrix representing the number of times the user visited the
website. A set of websites constitutes a browsing proﬁle that deter-
mines the interests of one or more users. The goal of user proﬁling
is not just to group rows or columns that are similar to each other,
but to ﬁnd sub-matrices that exhibit certain cohesive properties (we
explain this in detail later). These sub-matrices group both rows

341and columns of the matrix in such a way that the users and web-
sites in each group will exhibit a high association with each other
while they exhibit low associations with other groups. These sub-
matrices are termed as co-clusters.

Compared to clustering, co-clustering techniques are relatively

new. Few approaches like coupled two-way clustering (CTWC) [13],
iterative signature clustering [4], statistical algorithmic method [17],
fully automatic cross-associations [5], METIS [1], GRACLUS [7],
and spectral co-clustering [8, 9] have been proposed for gene ex-
pression data analysis [6, 18, 16] and text mining. In this work,
we focus on spectral co-clustering [8]. Spectral co-clustering tech-
niques show that simple linear algebra (singular value decomposi-
tion of matrices) can provide a solution to the real relaxation of the
discrete optimization (i.e., co-clustering) problem by considering
the input matrix as a bipartite graph with the objective of minimiz-
ing the cut used to form the co-clusters. Finding a globally optimal
solution to such a graph partitioning problem is NP-complete; how-
ever the authors in [8] show that the left and right singular values of
the normalized original data matrix can be used to ﬁnd the optimal
solution of the real relaxation of the co-clustering problem.

Although theoretically well studied, existing co-clustering algo-
rithms typically have some/all of the following issues: (i) For an
input matrix with m rows and n columns, the time complexity in
the order of m × n. In real-life applications, the number of rows
and columns of the input data matrix is usually very large. For ex-
ample, in user proﬁling we can see several hundred thousand users
and URLs (columns). Current algorithms make an implicit assump-
tion that the whole data matrix is held in main memory, since the
original data matrix needs to be accessed constantly during the ex-
ecution of the algorithm. (ii) Existing techniques assume that we
know the number of co-clusters that the input data matrix contains.
This assumption is often unrealistic for the majority of real-time
applications as it is hard to understand the hidden relationships that
the input data matrix may contain. (iii) Most of the proposed tech-
niques are designed for “hard" co-clustering of the input data ma-
trix, i.e., a row or a column belongs to one and only one cluster.
However, in reality, a given row or column can belong to one or
more co-clusters.

While some of the techniques like [13] and [8] have all the above
issues, other approaches like [1], [7] and [4] require a-priori knowl-
edge of the precise number of co-clusters, and support only hard co-
clustering. In [5], the authors address the ﬁrst two issues (scalabil-
ity and a-priori knowledge of the number of clusters) and come up
with a completely automated approach for co-clustering. However
this approach also has other disadvantages when applied to our cur-
rent problem: (i) It works only with binary matrices, (ii) A fully
automated approach does not give the user any control regarding
the quality of the output co-clusters, and (iii) Every row/column
always belongs to a single co-cluster and cannot be shared.

To address these limitations, we propose a novel co-clustering
algorithm called Phantom, designed for large data matrices. Phan-
tom leverages the spectral graph partitioning formulation in [8] to
alleviate the above limitations. We collect 1-day long trace from a
nationwide 3G CSP, and use this data to show how Phantom can be
effectively used for user proﬁling. Our contributions are:
• Hourglass Model: To make Phantom scalable to large-size data
matrices, we introduce a coarsening and un-coarsening model that
ﬁrst aggregates several rows and/or columns, performs the coclus-
tering, and then subsequently disaggregates the rows and/or columns
to restore the original matrix1.
In the context of user proﬁling,
Phantom ﬁrst groups the hundreds of thousands of distinct columns

1A coarsening/uncoarsening approach has been used in the mul-
tilevel algorithms like [1] and [7] where, unlike our semantic ap-

into a few tens of “categories" that are content-wise similar to each
other. For example, two distinct URLs such as amazon.com and
ebay.com are collapsed into one single category named E-Commerce.
The new lower-dimensional matrix is then used to co-cluster users
and categories. In the next step, we expand the categories in each
co-cluster into URLs and subsequently run Phantom again to yield
co-clusters of users and URLs. The sequence of operations starting
from dimensionality reduction of original matrix, co-clustering of
the reduced-size matrix, expansion of each co-cluster, and the sub-
sequent co-clustering of the expanded matrices is referred in this
paper as Hourglass model. We show that this model can deal with
very large matrices.
• Number of Co-Clusters: Phantom employs a recursive hierar-
chical partitioning of data to automatically determine the number of
co-clusters in the input matrix. It starts with the entire matrix, with
all users and URLs, as one single co-cluster. In the second step,
this single co-cluster is split into two children co-clusters. If both
of the children co-clusters meet a speciﬁc optimality criteria, then
the split is declared legitimate and each child co-cluster is further
processed in the next step; otherwise, the split is deleted and the the
parent node is declared as the leaf co-cluster and no further parti-
tioning is executed. At the end of this process, the cardinality of the
leaf co-clusters represents the total number of co-clusters found by
Phantom. We show that Phantom is not only able to automatically
determine the number of co-clusters in the input matrix, but is also
able to obtain co-clusters that have much better “quality” (in terms
of the metrics deﬁned later) compared to the existing techniques.
• Hard and Soft Co-Clustering: Phantom can partition a matrix
by operating in one of the two modes: hard co-clustering or soft
co-clustering. In hard co-clustering mode, every row and column
belongs to only one leaf co-cluster. However, if the property of a
user or URL is such that the entity cannot be grouped into only one
co-cluster, then hard co-clustering mode yields incorrect results.
To address this, Phantom can operate in a soft co-clustering mode
where rows and columns can be borrowed between multiple co-
clusters, thus allowing multiple leaf clusters to contain the same
row/column. We show that soft co-clustering will result in better
grouping than hard co-clustering.
• User Proﬁling: We collect one day long data trace from one of
the largest 3G CSPs in North America. To the best of our knowl-
edge, our current work is the ﬁrst one to explore such a data trace
from a 3G cellular network for proﬁling users. 2 In this mobile en-
vironment, we ﬁnd that there are both homogeneous (i.e., users with
very limited set of browsing interests) and heterogeneous (users
with very diverse browsing interests) co-clusters. The number of
users who belong to homogeneous co-clusters are far greater than
the number of users in heterogeneous co-clusters. Also, the number
of co-clusters required to capture 500K users is around 10 showing
that user behavior in a cellular networks can be captured using a few
browsing proﬁles. We comprehensively analyze the user behavior
over time and ﬁnd that user proﬁles do not change signiﬁcantly at
both a micro timescale (30 min intervals) and a macro timescale (6
hour intervals).

2. USER PROFILING PROBLEM

In this section, we present the user proﬁling problem formula-

tion, and challenges with the existing solutions.

proach, the coarsening/uncoarsening of the graph is based on graph
properties.
2Although, the authors in [19] also experiment with 3G CSP data,
they study whether applications accessed by users are location-
dependent and focus on user mobility-based hot-spots.

3422.1 Problem Formulation

The input to the user proﬁling problem is a set of users and web-
sites they visit along with the access frequency. This input can be
viewed in two equivalent perspectives: (i) As a matrix with users
on the rows and websites on the columns, and every entry in the
matrix represents the access frequency. (ii) As a bipartite graph
with users and websites representing the two sets of vertices, and
the access frequencies are the weight of the links from the user
nodes to website nodes. We use both these notations equivalently
to derive a solution to the problem. The solution in one perspective
can be translated into the other perspective using linear algebra and
graph theory.

A graph G = (V, E) has a set of vertices, V = {1, 2, ..., |V |},
and a set of edges, {i, j}, with edge weight Eij. The adjacency
matrix, M , is deﬁned as:

Mij =

(cid:26) Eij,

0,

if edge {i,j} exists
otherwise.

(1)

i∈V1,j∈V2

Given a partitioning of the vertex set, V , into k subsets, V1, ..., Vk,
we deﬁne cut as the sum of weights of all links that were cut
to form the partitioning. Mathematically, cut(V1, V2, ..., Vk) =
(cid:80)

i<j cut(Vi, Vj) where cut(V1, V2) = (cid:80)
We represent the bipartite graph for user proﬁling as a triple G =
(U, W, E), where U = {u1, u2, ..., um} is the set of user vertices,
W = {w1, w2, ..., wn} is the set of website (URL) vertices, and
E is the set of edges {{ui, wj} : ui ∈ U, wj ∈ W }. Note that
there are no edges between users or between websites. An edge
signiﬁes an association between a user and a website and the weight
on the edge captures the strength of this association. In this paper,
we use normalized access frequencies as the edge weights: Eij =
, where sij is the number of times user ui accesses
(cid:80)

Mij.

sij

j=1,...,m sij

the website wj. Note that (cid:80)

j∈1,...,m Eij = 1.

Consider the user-website matrix, D, with m users and n web-
sites. Every element in this matrix, Dij, represents edge weight
Eij. The adjacency matrix, M , of the bipartite graph:

M =

(cid:18) 0 D
0

DT

(cid:19)

(2)

Note that the vertices in M are now ordered such that the ﬁrst
m vertices represent the users while the last n vertices represent
websites.

A basic premise in this work is that there exists a duality be-
tween users and websites clustering. In other words, user clustering
induces website clustering while website clustering induces user
clustering. Now, given a set of disjoint website clusters c(w)
, ..., c(w)
k ,
the corresponding user clusters c(u)
can be determined as
follows. A given user ui belongs to the user cluster cu
if its asso-
l
ciation with the website cluster c(w)
is greater than its association
with any other website cluster.

1 , ..., c(u)

k

1

l

c(u)
l = {ui :

Dij ≥

Dij, ∀h = 1, ..., k}

(3)

(cid:88)

j∈c

(w)
l

(cid:88)

(u)
i∈c
l

(cid:88)

j∈c

(w)
h

(cid:88)

i∈c

(u)
h

Similarly for a website wj,

c(w)
l = {wj :

Dij ≥

Dij, ∀h = 1, ..., k}

(4)

Observe that the above formulation is recursive in nature since
webpage clusters determine user clusters, which in turn determine

webpage clusters. In every iteration we ﬁnd better user and web-
site clusters. Clearly, the “best” user and webpage clustering corre-
sponds to a graph partitioning such that the sum of weights of edges
between partitions have the minimum value. This is naturally ac-
complished when the cut for the partitions is minimized. That is,

cut(c(u)

1

(cid:91)

c(w)
1

, ..., c(u)

k

(cid:91)

c(w)
k ) = min

V1,...,Vk

cut(V1, ..., Vk)

(5)

where V1, ..., Vk is any k-partitioning of the bipartite graph.

2.2 Spectral Graph k-Partitioning

Although there are several heuristics (like KL [15] and FM [11]
algorithms) to solve the above k-partitioning problem, the spectral
graph heuristic that was introduced in [14, 10, 12] typically results
in a better global solution. The spectral graph heuristic proves that
the eigenvectors corresponding to the eigenvalues other than the
largest eigenvalue of the generalized eigenvalue problem, Lz =
λW z, where L is the Laplacian matrix (L = A − M , where A is
the diagonal degree matrix such that Aii = (cid:80)
k Eik ) and W is
diagonal matrix of vertex weights (explained below), will represent
the partition vectors that can be used to determine the membership
of the nodes into different clusters by minimizing the following
objective function:

Q(V1, V2, ..., Vk) =

(6)

(cid:88)

cut(V1, ..., Vk)

weight(Vi)

i

The eigenvectors are the real relaxation of the optimal gener-
alized partitioning vectors. W can be deﬁned in several different
ways, but the most commonly used deﬁnition is Wii = weight(Vi) =
cut(V1, V2, .., Vk) + within(Vi), where within(Vi) is the sum of
the weights of edges with both ends in Vi. Using this in Eqn. 6
leads to the normalized-cut objective function.

It is important to note that the normalized-cut objective func-
tion accomplishes a very similar objective as our original objective
function in Eqn 5. In [8], the authors show that the above spectral
k-partitioning algorithm can also be posed in terms of the singular
value decomposition of the (normalized) original matrix, D. We
refer the reader to [8] for more details, but the main idea is the
following:
• Given D, compute Dn = Y −1/2
matrices; Y1(i, i) = (cid:80)
• Compute h = (cid:100)log2k(cid:101) singular vectors of Dn, u2, ..., uh+1
and v2, ..., vh+1. Form, X T = [D−1/2
V ] where U =
1
[u2, ..., uh+1] and V = [v2, ..., vh+1].
• Run k-means on the h-dimensional data X to obtain the desired
k-partitioning.

j Dij and Y2(j, j) = (cid:80)

; Y1, Y2 are diagonal

DY −1/2

U D−1/2

i Dij.

1

2

2

In the rest of this paper, we refer to the k-partitioning algorithm

as SpectralGraph-k-Part.

2.3 Challenges

Although SpectralGraph-k-Part algorithm works reasonably well,
it has several limitations. In this section, we highlight these limi-
tations. Although we focus on SpectralGraph-k-Part, these limita-
tions hold for other co-clustering algorithms as well.

The ﬁrst limitation comes from the fact that input matrices in
real-life are usually very large. For example, the input matrix for
user proﬁling problem in large CSPs can easily have up to several
thousand users (rows) and webpages (columns) even when aggre-
gated every hour. If we consider a larger timescale, the size of the
input matrix signiﬁcantly increases. Such high dimensions limit the
applicability of the SpectralGraph-k-Part algorithm. Furthermore,

343the SpectralGraph-k-Part algorithm implicitly makes the assump-
tion that the entire data matrix is loaded into memory, since the
original data matrix needs to be accessed constantly during the ex-
ecution of the algorithm.

The second limitation is that the number of clusters in to which
the input data matrix has to be partitioned should be known a-priori.
This assumption is often unrealistic for the majority of applications
as it is hard to understand the hidden relationships that the input
data may contain. For example, in the user proﬁling problem, the
number of user proﬁles that exists in the data is unknown, and thus
it is impossible to guess the number of clusters ahead of time.

The ﬁnal limitation is that these algorithms have been designed
for hard co-clustering, i.e, a row or column in the input matrix
always belongs to one and only one of the several (one or more)
output clusters. However, in reality, in several applications we like
to allow a row or column to belong to one or more output clus-
ters. We refer to such a co-clustering approach as soft co-clustering
of the input data matrix. There are three reasons why soft co-
clustering is important: (i) Hard co-clustering completely removes
the association between users and websites if they belong to dif-
ferent clusters. For example, consider three users u1, u2 and u3
and three websites w1, w2 and w3. Let the website access pat-
terns be as follows: (0.5 × w1, 0.5 × w2) for users u1 and u3,
and (0.1 × w1, 0.2 × w2, 0.7 × w3) for user u2. Using hard co-
clustering, we obtain two co-clusters, c1 = ((u1, u3), (w1, w2))
and c2 = ((u2), (w3)). The information about the user u2 access-
ing websites w1 and w3 for 30% of the time is lost. Since soft co-
clustering allows a website to belong to multiple clusters, we can
perform clustering with minimal loss of information. Using soft
co-clustering for the above example will also result in two clusters,
c1 = ((u1, u3), (w1, w2)) and c2 = ((u2), (w1, w2, w3)), that
clearly reﬂects the different access behaviors of users. (ii) Hard
co-clustering is very sensitive to noisy data, and thus may produce
incorrect partitions. In fact, hard co-clustering always ﬁnds clus-
ters even in random data. The standard procedure to deal with this
is to use robust noise pre-ﬁltering techniques. However, such a ﬁl-
tering mechanism is completely problem dependent. On the other
hand, soft co-clustering is more resilient to noisy data because of
its ability to differentiate the cluster strength and thus, avoid detect-
ing “random” clusters. (iii) A recursive or iterative co-clustering
hard co-clustering algorithm tends to accumulate errors, if any, at
each iteration. The impact of these errors become bigger as the al-
gorithm goes through several iterations and there is no easy way to
ﬁx them. For example, consider the user proﬁling problem. If a
hard co-clustering algorithm erroneously assigned a set of website
to a set of users, this mistake is propagated to its children clusters
in the next iterations, leading to cumulative errors.

3. PHANTOM ALGORITHM

We address the above limitations and design Phantom, a co-
clustering algorithm that leverages the SpectralGraph-2-Part algo-
rithm with several modiﬁcations. We ﬁrst present the core ideas of
Phantom at a high level, and then present the detailed algorithm.

3.1 The Hourglass Model

The input matrix for several real-world co-clustering applica-
tions can be very large. Such large matrices pose a computational
hazard even with the today’s technological advancement. In this
context, we propose a novel dimensionality reduction and expan-
sion model called Hourglass model. The main idea here is to ﬁrst
reduce the dimensions of the input matrix, perform the co-clustering
on a lower dimensional matrix, and then subsequently restore the
original matrix to perform co-clustering again to give us the clusters

that we needed. For the user proﬁling problem in large CSPs, the
input data matrix, D, can have several thousands (or even millions)
of rows and columns. Using our Hourglass model, we ﬁrst reduce
the dimensions of D. Note that we can choose to reduce both row
and column dimensions or reduce either one of the dimensions. In
this work, we choose to reduce only the column (websites) dimen-
sion. To accomplish this we ﬁrst group semantically similar web-
sites into few categories. For instance, websites such as Facebook,
Twitter, MySpace, Orkut, etc. will be grouped together into one
single category called social-networking. We present more details
about the rules we use to build such mapping between websites and
categories in Section 3.4.

Consider we have l categories to group our websites. As a con-
sequence, D will be transformed into a lower dimensional ma-
trix, Dr, of size m × l. Next, we perform divisive hierarchical
clustering (see Section 3.2) on Dr by iteratively dividing clusters
into smaller clusters (while retaining certain cluster characteristics)
to yield the ﬁnal co-clusters of users and categories. This will
generate k co-clusters, C (u,l) = {c(u,l)
}, where
each co-cluster c(u,l)
groups users (with similar category brows-
ing pattern) and categories (with similar users visiting them). Also,
c(u,l)
represent the
i
set of users and categories that belong to the co-cluster c(u,l)

, i ∈ 1, ..., k, where c(u)

, ..., c(u,l)

and c(l)
i

= c(u)

(cid:83) c(l)

, c(u,l)

k

1

2

.

i

i

i

i

i

i

i

| × |c( (cid:98)w)

After obtaining k user-category co-clusters, each cluster c(u,l)

is
expanded so that the categories in c(l)
are mapped back to the orig-
i
inal websites. This generates the expanded matrix, De(i) of size
|c(u)
|, where ( (cid:98)w) represents the new space obtained when
considering the websites associated with the categories in |c(l)
|.
i
Note that the cardinality of users belonging to each of these clus-
ters is usually much smaller than the original number of users in D,
i.e., |c(u)
| (cid:28) |U |. Each expanded matrix, De(i), i ∈ 1, ..., k, is
then used as the input to Phantom to generate the ﬁnal user-website
co-clusters that we are interested in.

i

i

The main advantage of the Hourglass model is that we always
process low dimensional matrices, thus making it computationally
feasible to process large input matrices in terms of memory require-
ments and processing ability. Also, the Hourglass model can be
recursively applied to a given input. We refer this recursive model
as Onion-Peel Hourglass model. For example, for a input matrix
we can apply, say, two data reduction steps, perform the ﬁrst co-
clustering step, then carry out the ﬁrst expansion step, subsequently
perform the second co-clustering step, then the second expansion
step, and ﬁnally perform the last co-clustering step. Such a model
will help when: (i) Domains that lend themselves to multiple lev-
els of grouping. In our user proﬁling problem, we can use multi-
ple levels of categories (say macro-categories that contain several
micro-categories which in turn contains several websites). (ii) We
require data reduction in both the row and column dimensions.

A disadvantage of this Hourglass model is that it requires domain
speciﬁc knowledge to carry out dimension reduction. The Hour-
glass model cannot be applied if a problem does not lend itself to
such dimension reduction step.

3.2 Divisive Hierarchical Co-Clustering

Determining the number of clusters in input data before using
any clustering algorithm to process it is an extremely difﬁcult prob-
lem. In Phantom, we adopt a divisive hierarchical approach to au-
tomatically determine the ideal number of clusters that exists in the
data. This iterative algorithm builds a binary tree with the ﬁnal
clusters left behind as the leaves of this tree. It works as follows.
• Step 1: Starts with the entire input matrix as a single cluster. At

344Table 1: Classifying URLs into Categories

Category

Sample Keywords

Category

Sample Keywords

Dating

Music3

News

E-Commerce

Social netw.

dating, harmony, personals

single, chemistry, match

song, mp3, audio,

music, track, pandora

magazine, tribune,
news, journal, times
amazon, ebay, buy,
market, craigslist
facebook, blog
myspace, twitter

Mail

Maps

Photo

Search

Travel

mail

virtualearth

maps

gallery, picture,

photo, ﬂickr

google,

yahoo, msn

vacation, hotel
expedia, travel

Category
Gaming

Sample Keywords
poker, blackjack,

Category
Download

Sample Keywords

download

MMS

Ringtones

game, casino

mms

tones

Weather

weather

Books

Video

video

CDN

Porn

cdn,akamai,

cacheﬂy,internap

porn,adult

xxx
book

every iteration, partition each of the clusters obtained in the previ-
ous iteration into two child clusters by applying the SpectralGraph-
2-Part co-clustering algorithm [8].
• Step 2: After every co-clustering step, both the child clusters are
examined to determine their goodness. For this we deﬁne a cluster
cohesiveness metric that captures how self-contained a the child
cluster is. If cluster cohesiveness of both children is greater than a
pre-speciﬁed threshold (say T ), then the partitioning is deemed to
be legitimate and the child clusters are accepted. Both the children
clusters are considered as parent clusters in the next iteration and
the process is repeated.
• Step 3: If the cluster cohesiveness metric of either of the two
children is less than T , then the partitioning is considered to be
of low quality and the children are rejected. The parent cluster is
declared as a leaf and is not subject to further partitioning.
• Step 4: The divisive process above is repeated until no cluster
can be partitioned further. At the end, the ﬁnal clusters will form
the leaves of the binary tree, and the number of leaves represents
the number of clusters c in the input data.

The above approach avoids having a-priori knowledge about the
number of clusters, which has been a major limitations of the sev-
eral algorithms including [8]. Compared to the prior algorithms,
the input to our algorithm has changed from the number of clus-
ters, k, to the threshold, T , for cluster cohesiveness. We will show
later, why T as a parameter works better in practice than k. We
will present the details about the cluster cohesiveness metric later
in this section.

3.3 Soft Co-Clustering Paradigm

As described in Section 2.3, hard co-clustering of input data may
result in poor quality clusters. We deal with this issue by designing
a soft co-clustering approach that allows borrowing of row/s and/or
column/s between two child clusters while partitioning the parent
cluster. This approach works with a small modiﬁcation to Step 3
presented in Section 3.2:
• If cluster cohesiveness of both the children is less than T , then
the partitioning is rejected and the parent cluster is declared as a
leaf cluster.
• If one the two child clusters has a cluster cohesiveness value that
is less than T , while the other child cluster has a value greater than
T , then we trigger greedy borrowing. The child with a lower cluster
cohesiveness value starts borrowing columns, one at a time, from
the other child starting from the column that will increase its own
cohesiveness value the most. Note that when a column is borrowed
from one child by another child, the column will exist in both the
children. This column borrowing continues until the cohesiveness
of both the children is greater than T . After this borrowing process
some of the columns may belong to both child clusters. We then
declare the partition as legitimate and accept the child clusters.

FUNCTION: PHANTOM(U, W, D, L, T )
1: /* Initialization */
2: ∆ = {}, θ = 0
3: /* Dimensionality Reduction: Dr is of size m × l with l (cid:28) n */
4: Dr = ReduceDim(D);
5: /* Co-Cluster Executed on Low-Dimensionality Matrix Dr */
6: (C(u,l), k)=KernelPhantom(U, L, Dr, T );
7: for j=1 to k do
8:

∈ C(u,l) are expanded to con-

/* Dimensionality Expansion: c(u,l)
sider webpages */
c(u, (cid:98)w)
j

= ExpandDim(c(u,l)

j

;

j

j

)

/* Co-Cluster Executed on each De(j) independently */
(C(u,w)(j), θ(j))=KernelPhantom(c(u)

9:
10: De(j) ← c(u, (cid:98)w)
11:
12:
, De(j), T );
13: /* Output Preparation: Construction of set ∆ of cardinality θ */
14: ∆ = (cid:83)
15: θ = (cid:80)k
16: return (∆, θ);

j=1,...,k C(u,w)
j=1 θ(j);

, c( (cid:98)w)

j

j

j

;

Figure 1: Hourglass Model

3.4 Algorithm

3.4.1 Notations and Deﬁnitions

First, we remind the reader about the notations that we use in this
section. The input matrix, D (of size m × n) has m users (U =
{u1, ..., um}) and n websites (W = {w1, ..., wn}). Each element,
Dij = sij/(cid:80)
j=1,...,m sij where sij represents the access fre-
quency of user ui to website wj. Furthermore, let L = {l1, ..., ll}
represent the l categories that covers the entire website space such
that each website belongs to one and only one category4. The inter-
mediate and ﬁnal outputs of Phantom are user-category and user-
website co-clusters. We use C (u,l) = {c(u,l)
} to repre-
sent the user-category co-clusters, while C (u,w) = {c(w,l)
, ..., c(w,l)
for user-website co-clusters. Every user-category co-cluster c(u,l)
consists of users, c(u)
j ∈ L. Similarly,
every co-cluster c(u,w)
j ∈ U and websites,
c(w)
j ∈ W .
Using the above notation, we deﬁne the cluster cohesiveness
metric used to evaluate the goodness of a co-cluster. Let c(a,b)
be a parent co-cluster, while {c(a,b)
}, i ∈ 1, ..., q be the child co-
clusters. Here we assume that we partition the parent co-cluster
into q child co-clusters. The cluster cohesiveness metric, γ(a,b)
, of

j ∈ U and categories, c(l)

consists of users, c(u)

, ..., c(u,l)

k

k

1

1

j

j

i

i

}

4Note that a website can belongs to multiple categories and Phan-
tom can deal with such a situation. However for ease of exposition,
we assume that a website belongs to one and only one category

345a child co-cluster c(a,b)
sociations between the sets c(a)
of all the associations of c(a)

i

i

i
. Mathematically,

, is the ratio of the sum of the weights of as-
to the sum of the weights

and c(b)

i

(cid:80)

(cid:80)

γ(a,b)
i

=

(cid:80)

(cid:80)

Dhk

(b)
k∈c
i
k∈C(b) Dhk

h∈c

(a)
i

h∈c

(a)
i

(7)

i

where C (b) = {c(b)
i }, i ∈ 1, ..., q is the entire space of b for all the
sibling co-clusters of c(a,b)
(including itself). Cluster cohesiveness
represents how well the child co-clusters (from the same parent)
are separated from each other. The higher the value of this metric,
the better is the separation. We use the cohesiveness threshold pa-
rameter, T , to let the user specify the degree cohesiveness of the
co-clusters during the co-clustering process. A high value of T
ensures that the leaf co-clusters are very well separated from each
other.

3.4.2 Hourglass Algorithm

Figure 1 shows the algorithm for the Hourglass model in Phan-
tom. First, the algorithm (line 4) reduces the dimensions of the
input matrix, D (m × n), to generate Dr (m × l). It maps the large
number of websites into a few categories (l (cid:28) n). This is accom-
plished by semantically grouping similar websites into the same
category using a simple keyword mining over the website URL. A
complete list of the categories and the sample keywords used for
these categories are shown in Table 1. Some keywords like google,
yahoo, and msn represent portals from where users can access dif-
ferent services (e-mail, IM, search, etc). Hence, in order to distin-
guish between e-mail, IM, and search we apply all the non-search
rules ﬁrst and use the keywords for search as the last step.

1

k

, ..., c(u,l)

Next, Phantom co-clusters Dr using the KernelPhantom method
(line 6). The input to this method are the two sets of entities it
has to co-cluster (user set U and category set L), their associations
(Dr), and the cluster cohesiveness threshold T that drives the co-
clustering process. The function then returns k co-clusters of users
and categories, C (u,l) = {c(u,l)
}. Each co-cluster c(u,l)
is then processed independently. First, Phantom builds the ex-
panded matrix, De(j), that maps each category in c(l)
to the web-
j
sites. Thus, the user-category co-cluster, c(u,l)
, is transformed in to
a user-website co-cluster c(u, (cid:98)w)
(line 9). Note that De(j) = c(u, (cid:98)w)
(line 10). This matrix, De(j), is then processed by the KernelPhan-
tom method (line 12). Although this phase increases the number
of columns of De, these columns only correspond to websites be-
longing to one co-cluster, and hence its size is much smaller when
compared to the D. Each of the co-clusters, c(u, (cid:98)w)
, j = 1, ..., k
results in more user-website co-clusters. In lines 14-16, all these
ﬁnal co-clusters are aggregated and results are returned.

j

j

j

j

j

We refer to the co-clustering of Dr as the outerloop and co-

clustering of De as the innerloop of Phantom.

3.4.3 Hard and Soft Co-Clustering Algorithm

The main goal of KernelPhantom (Figure 2) is to ﬁnd all cohe-
sive co-clusters either in hard or soft co-clustering modes. This
method does not require the user to specify the number of co-
clusters to be used. The choice of the parameter T will determine
this number - the larger is the value of T the lesser is the ﬁnal num-
ber of co-clusters and vice-versa.

Figure 2 shows the KernelPhantom pseudo-code. It starts with
an initialization phase (lines 2-5). The variable S contains the set of
parent co-clusters that includes both examined and to-be-examined
co-clusters. The index r keeps track of which co-cluster in S is

Figure 2: Soft and Hard Co-Clustering Algorithm

currently being processed. Note that r separates the already exam-
ined co-clusters from the to-be-examined co-clusters in S. The set
L contains all the leaf co-clusters that output by this algorithm.

The main loop of the algorithm (lines 6-34) performs divisive
hierarchial co-clustering to construct a binary tree until the vari-
able stop is set to true. In every iteration, the variable c(a,b) is
initialized to the current parent co-cluster to be processed (line 7).
This parent co-cluster is used to extract the corresponding asso-
ciations matrix, Fr from the original associations matrix, F (line
8). This parent co-cluster, c(a,b), is now partitioned into two child
co-clusters, c(a,b)
, using the SpectralGraph-2-Part algo-
rithm (line 9). The cluster cohesiveness of the children co-clusters
are then computed as per Eqn 7 (line 10). If the cohesiveness metric
of both the children is less than T , then the parent node is declared
as the leaf (lines 11-12). Similarly, if any of the children have no
elements of either one of the two entities (i.e., either no users or
no websites/categories), then the parent node is declared as the leaf
(lines 13-16). If KernelPhantom is set to soft co-clustering mode,
then the cohesiveness of each of the two children are examined to

and c(a,b)

1

2

346see if there is a need to borrow columns. If required, the columns
are borrowed from the sibling co-cluster until the cluster cohesive-
ness is at least equal to T (lines 20-22). In lines 23-27, we examine
if any of the children co-clusters contain just one element of an
entity (i.e. one user or one website/category). If this is the case,
then we will be unable to perform further co-clustering on this co-
cluster; thus we declare it as a leaf. Lines 29-37 deal with the hard
co-clustering case, which is similar to the soft co-clustering case
except that we now declare the parent co-cluster as a leaf if one of
the two children have a cluster cohesiveness value less than T . We
repeat the above steps until we have processed all possible parent
co-clusters in the binary tree (lines 39-40), and return all the leaf
co-clusters, L (line (41)).

4. DATA TRACE

Figure 3: Number of users and webpages in the micro and
macro intervals

To address the user proﬁling problem, we collect an entire day
of traces on April 16, 2008 from one of the largest CSP networks
in North America. Our data collectors are located in a central lo-
cation and receives RADIUS [2] authentication and data sessions
from the CSP’s data network. Note that our collectors do not mon-
itor the CSP’s voice network. From the RADIUS authentication
sessions we identify the ip-address allocation to a user at the cur-
rent time. Note that the ip-address allocation for a user can change
several times in a day due to mobility, reconnection, etc. We then
use this information to identify all the data sessions that belong to a
particular user. Finally, after all the processing we obtain all details
about a user’s browsing activity including the URL of the website
and the time when the website was visited. Here we only consider
the top-level domain name in the URL.

The data trace collected above contains users in different time
zones. The default timestamp in our traces is the time when the
collector sees the authentication/data session. However, given that
our goal is not only to analyze user behavior at a given point in
time, but over the course of a day, we convert all the timestamps to
the local time stamps of the users. After this step all the timestamps
in our trace are between 0 and 24 hours. To make this point clearer,
let us consider two users, one in San Francisco (PST) and the other
in New York (EST). We are interested in analyzing the behavior
of these two users, say, in the evening between 6pm and 7pm. A
session between 6pm and 7pm from New York user will be seen a
few hours earlier by our collector than a session from San Francisco
user (also between 6pm and 7pm). So we need to consider local
timestamps for these two users in their own time zone instead of
the collector timestamp.

For our analysis, we split the trace in two ways: 30-min and 6-
hour windows. Both of these start at 12 midnight on the day of our
collection. We have 48 30-min traces and 4 6-hour traces (Morning
(6am-12pm), Afternoon (12pm-6pm), Evening (6pm-12am), and
Night (12am-6am)). We refer to our 30-min window analysis as
micro-analysis and 6-hour window analysis as macro-analysis. The
reason for the two time windows is to analyze both dynamic (short
timescales) and stable (longer timescales) user behavioral trends.

Figure 3 shows the number of users and websites that we see in
our 1-day trace for both the time window sizes. We can see that
on a average there are 22K users and 7K websites in 30-minute
windows. These numbers increase to 150K and 30K for 6-hour
windows. Overall, in the entire day we see about 0.5 million users
and 150K URLs.

5. EXPERIMENTAL EVALUATION

5.1 Walking through an Example

We now consider a 30-minute data set (8:00 am - 8:30 am) de-
scribed in Section 4. Note that in this 30-minute data, there are
around 18, 000 users and 8, 000 URLs. Running on a machine
with 2.66GHz processor and 4GB of memory size, the Phantom
algorithm ﬁnished processing (both the outer and inner loops) this
data matrix in under 20 seconds.

We now show how the Phantom framework can be used to solve
the problem. Using the URL to category mapping shown before,
we map all the URLs into 20 categories - 19 categories listed in
Table 1 and an “Unknown” category that contains all URLs that we
could not decisively map to one of the 19 categories. About 9%
of the URLs get mapped to the unknown category. In the rest of
this work, we ignore the URLs in the unknown category. Using the
URL-category mapping we obtain a lower dimensional matrix, Dr,
and use this matrix as input to Phantom. We set the cohesiveness
threshold parameter, T , to be 0.99 to ensure that we get very co-
hesive clusters while trying to form as many clusters as possible.
To keep things simple, we set our algorithm to perform hard co-
clustering, i.e., we do not allow borrowing of categories. For this
30-minute trace, Phantom resulted in 6 clusters (as shown in Fig-
ure 7(b)). Most of the clusters were homogeneous clusters - clusters
with just one category in them - while one of them was a heteroge-
neous cluster - a cluster with several different categories grouped
together. We will focus on homogeneous clusters in this subsection
and dive in to heterogeneous clusters in the next subsection.

Consider the homogeneous cluster with the “Music” category.
Fig. 4(a) shows this homogeneous cluster with the category mapped
back to its URLs. The small circles on the periphery of the ﬁgure
represents users and the rectangular boxes represents URLs. The
links between users and URLs represent the fact that the user visits
the URL. We can see that users clearly prefer some websites over
others, and hence we have several seemingly disconnected compo-
nents in this graph. Running the innerloop of Phantom co-clusters
these users along with the speciﬁc websites that are part of their
browsing proﬁle. Figures 4(b) and 4(c) show two of the several co-
clusters formed by the innerloop of Phantom. Comparing Figures
4(b) and 4(c) with Figure 4(a), we can clearly see that Phantom
extracts the dominant components in the graph in Figure 4(a) that
represent the group of users and URLs that form their browsing
proﬁle. Note that this is true even for heterogeneous clusters.

5.2 Reﬁning the Algorithm

If the cohesiveness threshold used for the outerloop of Phantom
is very high, then the resulting co-clusters are very cohesive, but
could be heterogeneous. We refer to co-clusters with more than 5

010203040024x 104Time Interval IDNumber of Users010203040050001000015000Time Interval IDNumber of URLs1234012x 105Time Interval IDNumber of Users1234024x 104Time Interval IDNumber of URLs30 min intervals30 min intervals6 hr intervals6 hr intervals347(a)

(b)

(c)

Figure 4: (a) The homogeneous music cluster expanded into URLs. (b) and (c) Two (of the eight) clusters formed by running the
inner-loop of Phantom.

Figure 6: Three sub-clusters of the giant cluster

the cohesiveness threshold value has fallen below the permissible
lower limit. In this example, we deﬁne the lower limit of the cohe-
siveness threshold as 0.95, the increment as 0.01, and T = 0.99.
This ensures that the resulting clusters are still very cohesive, but
gives Phantom a chance to form more clusters. This iterative step
of Phantom resulted in 6 additional co-clusters. Figures 6(a), 6(b)
and 6(c) show three of these clusters.

5.3

Phantom Vs SpectralGraph-k-Part

In this subsection, we compare the results obtained from the
SpectralGraph-k-Part algorithm with Phantom. We use the same
30-minute dataset as in the previous subsection, but only use the
user-category matrix since we were unable to run SpectralGraph-
k-Part on the user-websites matrix.

SpectralGraph-k-Part is a non-hierarchical approach that requires
the number of co-clusters to be formed as an input parameter. To
determine the ideal number of clusters in the input data, we follow a
brute-force approach. We vary the number of clusters to be formed
from 1 to 19 (the maximum possible clusters with at least one cate-
gory in each cluster). For each value of this parameter we compute
the average leaf cluster cohesiveness and standard deviation of leaf
cluster cohesiveness (see the black curve in Figure 8(b)). We can
see, by inspection, that the optimal value of the number of clus-
ters based on the average and standard deviation of the leaf cluster
cohesiveness values is 4. If the number of clusters increases be-
yond this value, then the average leaf cluster cohesiveness consider-
ably decreases (top graph in Figure 8(b)) and its standard deviation
considerable increases (bottom graph in Figure 8(b)). However a
lower value of the number of clusters does not affect the cohesive-

Figure 5: Example of a heterogeneous giant cluster

different categories grouped together as “giant” heterogeneous co-
clusters. Cluster-3 in Figure 7(b) (Figure 5) is one such co-cluster
with 15 categories. Such co-clusters with a disparate set of cate-
gories do not accurately represent the browsing proﬁle of individual
users that we are interested in. Hence we adopt the following strat-
egy to further split these giant heterogeneous clusters. We ﬁrst de-
ﬁne a lower limit and an increment for the cohesiveness threshold.
If we obtain any giant clusters in the ﬁrst round of co-clustering in
the outerloop of Phantom, we decrease the cohesiveness threshold
by the increment value and run the outerloop again on the giant
cluster. We repeat this process until there are no giant clusters or

sonymusicmobile.commspot.comemusic.comsongtouch.compassalong.compuretracks.comxmradio.compandora.commusicplaylist.combrew.sonymobile.comsonymobile.commuse.mspot.comr1n3.mspot.comr1n1.mspot.comVIDEOCDNNEWSSEARCHPORNDOWNLOADPHOTOINFOSOCIAL NETWORKINGINFOBOOKSPORNSOCIAL NETWORKINGDOWNLOADVIDEODATINGE-COMMERCEMAPSCDNPHOTONEWSMAILSEARCHBANKING348Figure 7: Clustering results from (a) SpectralGraph-k-Part Co-Clustering Algorithm (b) Phantom Hard Co-Clustering Algorithm (c)
Phantom Soft Co-Clustering Algorithm.

(a)

(a)

(b)

(b)

(c)

(c)

Figure 8: (a)Inﬂuence of cohesiveness threshold on Phantom hard co-clustering algorithm in terms of the number of clusters formed
and average cohesiveness of the clusters. (b) Phantom compared to the SpectralGraph-k-Part algorithm. (c) Inﬂuence of cohesiveness
threshold on Phantom soft co-clustering algorithm in terms of average and standard deviations of cluster cohesiveness.

ness as much. Hence, we use 4 as the ideal number of clusters for
SpectralGraph-k-Part in the rest of this subsection.

Figure 7 compares the clusters formed by SpectralGraph-k-Part
with the Phantom hard and soft co-clustering algorithms. We can
see that the results from the Phantom-hard algorithm are very simi-
lar to that of SpectralGraph-k-Part in terms of the average and stan-
dard deviation of leaf cluster cohesiveness. However, the Phantom-
hard algorithm results in two additional clusters compared to the
SpectralGraph-k-Part algorithm. One of the key beneﬁts of Phan-
tom is that it typically results in higher number of co-clusters than
the existing techniques without sacriﬁcing the cohesiveness of the
resulting co-clusters. Figure 7(c) shows the results for the Phantom-
soft algorithm, and we ﬁnd that it out-performs SpectralGraph-k-
Part in terms of all the metrics (number of clusters, average and
standard deviation of the leaf cluster cohesiveness).

The only parameter that need to be tuned in Phantom is the cohe-
siveness threshold, T . In Figure 8(a) we show the inﬂuence of this
parameter on the number of clusters that are formed and the leaf
cluster cohesiveness. As one can expect, higher values of T results
in lower number of more cohesive clusters. Depending on the par-
ticular application, this parameter can be set to the right value. In
this work we use T = 0.99. Figure 8(b) compares SpectralGraph-
k-Part with the Phantom-hard algorithm in terms of leaf cluster
cohesiveness for different values of the number of clusters. Note
that in SpectralGraph-k-Part, the number of clusters is a parame-
ter and hence every value of this parameter results in a value for
the leaf cluster cohesiveness. However, for the Phantom algorithm,
varying the parameter T , results in different values of the number
of clusters as shown in the top graph in Figure 8(a). We plot the
results from this on the same graph (Figure 8(b)) to compare the
two algorithms. We can clearly see that the Phantom outperforms
SpectralGraph-k-Part in terms of the average/standard deviation of
leaf cluster cohesiveness. Figure 8(c) shows the same metrics for

the Phantom-soft algorithm and we can see that the gain is signiﬁ-
cantly higher when compared to the Phantom-hard algorithm.

We refer to the set of categories that are grouped together in a
single cluster by Phantom as a category-level “browsing proﬁle”.
Intuitively, a category-level browsing proﬁle represents the inter-
ests of all users who belong to the same cluster. If we choose a
high value of cohesiveness threshold, T , then the browsing pro-
ﬁle captures “most” (or a large portion) of the browsing interests of
users in the clusters. Choosing a lower threshold implies that Phan-
tom will capture a smaller portion of users’ interests in each cluster.
The number of users in each cluster indicates the “strength” of the
browsing proﬁle represented by the categories in that cluster. The
higher is the strength of a browsing proﬁle, the more popular is the
proﬁle among users. A CSP can use the browsing proﬁles and their
strength to determine the diversity of services that can be offered
to customers. For instance, assume that Phantom ﬁnds a browsing
proﬁle with a large number of users that contains both Music and
Travel categories. Using this information, the CSP can now offer
bundled services that includes both music and travel to all users
since users with interest in music are most likely to be interested in
travel as well and vice versa..

The set of categories in the categories column in Figure 7 are the
category level browsing proﬁles of users in our 30-min data trace.
We split the giant heterogeneous co-cluster using the reﬁnement
presented in Section 5.2 to yield 6 more co-clusters. Thus only
11 different browsing proﬁles are sufﬁcient to capture the browsing
behavior of all 18000 users in this 30-min data trace.

Unlike the SpectralGraph-k-Part algorithm, Phantom automat-
ically ﬁnds the number of clusters in the data based on T . This
is a big advantage in several problems (including the current prob-
lem of ﬁnding browsing proﬁles of users), since it is much easier
to intuitively specify the required cohesion in clusters than the total
number of clusters. For example, in the context of our problem, our

SpectralGraph-k-PartCluster CohesivenessNumber of UsersCategoriesCluster-10.9910060Ringtones, Travel, MusicCluster-20.92545Weather,Photo, DatingCluster-30.961356Maps, Download, E-Commerce, Mail,CDN, Video, Gaming, News, Search, Porn, Books, Social NetworkingCluster-40.996062MMSAverageLeaf Cluster Cohesiveness = 0.969Std Deviation of Leaf Cluster Cohesiveness = 0.036Phantom (Hard)Cluster CohesivenessNumber of UsersCategoriesCluster-10.996064MMSCluster-20.92238MusicCluster-30.981596Maps,Download,E-Commerce,Mail,CDN, Dating, Video, Gaming, Photo,News, Search, Porn, Books, Social NetworkingCluster-40.97277WeatherCluster-50.989819RingtonesCluster-60.9029TravelAverageLeaf Cluster Cohesiveness = 0.963Std Deviation of Leaf Cluster Cohesiveness = 0.038Phantom (Soft)Cluster CohesivenessNumber of UsersCategoriesCluster-10.996064MMSCluster-20.981596Maps,Download,Trading,Mail,CDN,Gaming, Dating, Video,  Photo,News, Search, Porn, Books, Social NetworkingCluster-30.97277WeatherCluster-40.99238Ringtones,MusicCluster-50.9429Ringtones, TravelCluster-60.989819RingtonesAverageLeaf Cluster Cohesiveness = 0.983Std Deviation of Leaf Cluster Cohesiveness = 0.0180.80.850.90.9515101520Cohesiveness Threshold ValueOptimal Number of Clusters    0.790.810.830.850.870.890.910.930.950.970.99100.51Cohesiveness Threshold ValueLeaf Cluster Cohesiveness (avg/min/max)051015200.20.40.60.81Number of clusters (k−value)Average Leaf Cluster Cohesiveness  SpectralGraph−k−PartPhantom−Hard Clustering0510152000.20.40.60.8Number of clusters (k−value)Std Dev of Leaf Cluster Cohesiveness  SpectralGraph−k−PartPhantom−Hard Clustering0.80.850.90.9510.40.60.81Cohesiveness Threshold ValueLeaf ClusterCohesiveness(avg/min/max)0.80.850.90.95100.050.10.150.2Cohesiveness Threshold ValueLeaf ClusterCohesiveness (std. dev.)349Figure 9: Macro-Proﬁle in 6-hour intervals (a) Morning (b) Afternoon (c) Evening (d) Night

(a)

(b)

(c)

Figure 10: Users tend to stick to the same browsing proﬁle at the category level in (a) Morning (b) Afternoon (c) Evening

objective is to ﬁnd the number of browsing proﬁles that naturally
come out of the dataset rather than force the number of proﬁles to
have a certain value before performing clustering. In summary, T
can be used to specify the quality of clusters that are output by the
Phantom algorithm. If we need clusters that are very cohesive then
we can set a high value of T (for example, T = 0.99); we can sac-
riﬁce the cohesiveness (or quality) of clusters in favor of a larger
number of clusters by setting a lower value of T .

5.4 User proﬁles using Phantom

Our main goal in this section is to gain more insights about the
browsing behavior of users with the help of our novel Phantom
co-clustering algorithm. We use the Phantom-hard algorithm in
this section, but we would like to remind the reader that using
the Phantom-soft algorithm will only improve the results that we
present here. We use results from hard co-clustering mainly for the
ease of exposition of results in this section.

In this work, browsing proﬁle implies proﬁling all user-initiated
browsing activities. Two of the categories that we considered in the
previous section, MMS and Ringtones, contain both user-initiated
and network-initiated sessions. Network-initiated sessions are those
sessions that are pushed by the CSP to users. Examples of activi-
ties resulting in such sessions include mobile services, mms about
new services, free ringtone availability, etc. Hence, in this section,
we ignore these two categories, and consider only those categories
where the HTTP sessions are initiated by the user.

Figure 9 shows the macro-proﬁle of users (at 6-hour timescale).
The size of the pie represents the number of users that belong to the
cluster with the categories noted in the ﬁgure. We can see that sev-
eral clusters in all the four 6-hour intervals have the same categories
indicating that irrespective of the time of the day, certain users tend
to almost exclusively browse the URLs in these categories. For
example, Music, Social Networking and Mail are three categories
that belong to separate clusters in all intervals, showing that these
are strong browsing proﬁles that remain constant throughout the
day. However, some of the categories like news, cdn, and video get
grouped into the same cluster in all the intervals. These set of cate-
gories also form strong browsing proﬁle. One way to interpret this
grouping is that most of the news and video websites that users tend

to browse are served by CDN network content caching. Other cate-
gories like Search, get clustered differently in different intervals. In
the Morning, Evening, and Night, Search gets grouped with other
categories like News, Video, Maps, etc., however in the Afternoon
this category is grouped by itself. This shows that some users tend
use search URLs irrespective of the fact that they are browsing into
other categories or not. However, in the Afternoon a certain set of
users tend to predominantly use Search engines, thus resulting in a
separate cluster for itself.

Now that we have seen the various proﬁles of users in a macro-
timescale, the questions that we want to address are the following:
(i) Do users exhibit the same proﬁle even at smaller timescales? In
other words, does the dynamic behavior of users change consider-
ably from the stable behavior? If yes, then how long do they keep
their proﬁles? (ii) Do users show similar behavior when we con-
sider URLs instead of categories (referred to as URL-proﬁle)? If
yes, then how long do they keep their URL-proﬁle?

Figures 10, 11 and 12 try to address the above questions. For
each of the macro-proﬁles, we ﬁrst consider 30-min intervals that
make up the 6-hour interval, and then compute the micro-proﬁle of
users in this 30-min intervals. If the micro-proﬁle of a user matches
the macro-proﬁle of the user in the 6-hour time interval then we
say that the user has the same proﬁle in the 30-minute interval as
in the 6-hour interval. We compute such matches for all the twelve
30-min intervals in every 6-hour interval, and show the percentage
of users who tend to keep their proﬁle over this time interval in
Figure 10. The x-axis in these graphs represents the fraction of time
spent in the same proﬁle and the y-axis represents the CDF of the
fraction of users. The four different curves in each of these graphs
represent users who are active for a different amount of time. For
example, the curve for users who are active for more than 3 hours
includes all the users who appear in at least six 30-min intervals.

In all of the graphs in Figure 10, and all the curves in these
graphs, we can see that there is big spike between values of 0.9 and
1.0 on the x-axis. This spike shows that around 60-70% of the users
do not change their proﬁles (at the category level) in all the time in-
tervals where they are active. In other words, in our 3G traces, mu-
sic users predominantly stay as music users and social networking
users predominantly stay as social networking users for the entire

MUSICWEATHERTRAVEL,GAMINGMAILE−COMMERCEDATINGSOCIAL NETWORKINGPHOTO,PORNBOOKSMAPS,CDN,VIDEO,NEWS,SEARCHDOWNLOADMUSICWEATHERGAMINGTRAVELMAILDATINGE−COMMERCESOCIAL NETWORKINGPHOTO,PORNBOOKSDOWNLOAD,CDN,VIDEO,NEWSSEARCHMAPSMUSICWEATHERGAMINGMAILDATINGE−COMMERCESOCIAL NETWORKINGPHOTO,PORNBOOKSMAPS,CDN,VIDEO,NEWS,SEARCHDOWNLOADTRAVELMUSICWEATHERTRAVEL,GAMINGMAILDATINGPHOTO,PORNSOCIAL NETWORKINGBOOKSMAPS,CDN,VIDEO,NEWS,SEARCHDOWNLOADE−COMMERCE00.20.40.60.8100.20.40.60.81Percentage of time spent in a single profileCDF of the percentage of users  All UsersUsers active > 1.5 hrsUsers active > 3 hrsUsers active > 4.5 hrsTotal Time Window 6 hrs : 6am − 12pm00.20.40.60.8100.20.40.60.81Percentage of time spent in a single profileCDF of the percentage of users  All UsersUsers active > 1.5 hrsUsers active > 3 hrsUsers active > 4.5 hrsTotal Time Window 6 hrs : 12pm − 6pm00.20.40.60.8100.20.40.60.81Percentage of time spent in a single profileCDF of the percentage of users  All UsersUsers active > 1.5 hrsUsers active > 3 hrsUsers active > 4.5 hrsTotal Time Window 6 hrs : 6pm − 12am350duration that s/he is active. There are less than 20% of users who
tend to stay in their proﬁles for less than 40% of their active time.
That is, less than 20% of users exhibit a volatile (i.e., highly chang-
ing) browsing behavior. The take away point here is that majority
of users stick to their proﬁles irrespective of the timescale.

Figure 11 is very similar to Figure 10 except that we are now ex-
ploring the URL-proﬁle of users. Note that the y-axis is shown in
log scale. We can see that the pattern of users staying in the same
URL-proﬁle is even more dominant in Figure 11, than in Figure 10.
In other words, a majority of users not only tend to stay in the same
category, but they tend to stay with the same set of URLs. We fur-
ther explore if this behavior holds for every co-cluster individually,
and ﬁnd that users still tend to stick to their URLs (Figure 12).

Figure 11: Users tend to stick to the same browsing proﬁle at
the URL level. All users in all the clusters.

Figure 12: Users tend to stick to the same browsing proﬁle at
the URL level. Users in particular clusters.

6. CONCLUSIONS

In this paper we address user proﬁling in large CSPs. We formu-
lat the problem as a co-clustering problem with the goal of group-
ing both users and websites simultaneously. To overcome the lim-
itations of current techniques, we propose Phantom, a novel co-
clustering algorithm based on the Hourglass model that reduces the
dimensions of the input, carries out the co-clustering before ex-
panding the dimensions again. This model can be applied in onion-
peel mode where we can have several levels of dimension reduc-
tion and expansion. We showed that Phantom can deal with high
dimensional matrices and produce very cohesive co-clusters.

Phantom is a generic co-clustering framework that can be used
to solve any problem that lends itself into the co-clustering for-
mulation. It automatically ﬁnds the number of clusters in the data
based on a cohesiveness threshold, T , speciﬁed by the user. This
is a huge advantage, since it is much easier to intuitively specify
the required cohesion in clusters than the total number of clusters.
In this paper, we have used a constant value of T through out the
Phantom co-clustering process. However, it may be advantageous

to change this value as we build the binary tree. For example, in the
hard co-clustering mode errors tend to accumulate as we go down
the binary tree. If we increase the value of T with the level in the
binary tree, then the error accumulation can be reduced. This is
part of our future work.

A disadvantage of the Hourglass model is that we require do-
main speciﬁc knowledge to carry out the dimension reduction. The
Hourglass model cannot be applied if a problem does not lend itself
to such dimension reduction step.

7. REFERENCES
[1] METIS - Family of Multilevel Partitioning Algorithms.

http://glaros.dtc.umn.edu/gkhome/views/metis.

[2] RFC 2865. http://tools.ietf.org/html/rfc2865.
[3] P. Antonellis and C. Makris. XML Filtering Using Dynamic

Hierarchical Clustering of User Proﬁles. DEXA, 2008.

[4] S. Bergmann, J. Ihmels, and N. Barkai. Iterative signature

algorithm for the analysis of largescale gene expression data.
Phys Rev E Stat Nonlinear Soft Matter Physics, 2003.

[5] D. Chakrabarti, S. Papadimitriou, D. S. Modha, and

C. Faloutsos. Fully Automatic Cross-associations. KDD,
2004.

[6] Y. Cheng and G. Church. Biclustering of expression data.

Intelligent Systems for Molecular Biology, 2000.

[7] I. Dhillon, Y. Guan, and B. Kulis. A Fast Kernel-based

Multilevel Algorithm for Graph Clustering. Proceedings of
ACM SIGKDD, 2005.

[8] I. S. Dhillon. Co-clustering documents and words using

Bipartite Spectral Graph Partitioning. KDD, 2001.

[9] I. S. Dhillon, S. Mallela, and D. S. Modha.

Information-theoretic coclustering. KDD, 2003.

[10] W. E. Donath and A. J. Hoffman. Lower bounds for the

partitioning of graphs. IBM R&D Journal, 1973.

[11] C.M. Fiduccia and R.M. Mattheyses. A linear time heuristic

for improving network partitions. Technical Report
82CRD130, GE Corporate Research, 1982.

[12] M. Fiedler. Algebraic connectivity of graphs. Czecheslovak

Mathematical Journal, 1973.

[13] G. Getz, E. Levine, and E. Domany. Coupled two-way

clustering analysis of gene microarray data. Proceedings of
National Academy of Science, 2000.

[14] K. M. Hall. An r-dimensional quadratic placement algorithm.

Management Science, 1970.

[15] B. Kerninghan and S. Lin. An efﬁcient heuristic procedure
for partitioning graphs. The Bell system Technical Journal,
1970.

[16] Y. Kluger, R. Basri, J.T. Chang, and M. Gerstein. Spectral
Biclustering of Microarray Data: Coclustering Genes and
Conditions. Genome Research, CSH Press, 2003.

[17] A. Tanay, R. Sharan, M. Kupiec, and R. Shamir. Revealing
modularity and organization in the yeast molecular network
by integrated analysis of highly heterogeneous genomewide
data. Proceedings of National Academy of Science, 2004.

[18] A. Tanay, R. Sharan, and R. Shamir. Discovering Statistically

Signiﬁcant BiClusters in Gene Expression Data.
Bioinformatics, Oxford University Press, 2002.

[19] I. Trestian, S. Ranjan, A. Kuzmanovic, and A. Nucci.

Measuring serendipity: Connecting people, locations and
interest in a mobile 3G network. Proc. of ACM Internet
Measurement Conference, 2009.

0.10.20.30.40.50.60.70.80.9110−410−310−210−1100Percentage of time spent in a single url−profileCDF of the percentage of users  All UsersUsers active > 1.5 hrsUsers active > 3 hrsUsers active > 4.5 hrsTotal Time Window 6 hrs : 12am − 6am00.20.40.60.8110−410−310−210−1100Percentage of time spent in a single url−profileCDF of the percentage of users  Cluster−1Cluster−2Cluster−3Cluster−4Cluster−5Cluster−6All ClustersTotal Time Window 6 hrs : 12am − 6am351