A Platform to Evaluate the Technology for Service Discovery in the Semantic Web

Cecile Aberg and Johan Aberg and Patrick Lambrix and Nahid Shahmehri

Department of Computer and Information Science

Link¨opings universitet, Sweden

cecab, johab, patla, nahsh

@ida.liu.se

{

}

Abstract

Since the description of the Semantic Web paradigm in 2001,
technology has been proposed to allow its deployment and
use. However, there is not yet any large and widely deployed
set of semantically annotated Web resources available. As
a result, it is not possible to evaluate the use of the technol-
ogy in a real environment, and several assumptions about how
the Semantic Web should work are emerging. In order to fur-
ther investigate these assumptions and the related technology,
we propose a simulation and evaluation platform. The plat-
form provides tools to create Semantic Web simulations using
different technologies for different purposes, and to evaluate
their performance. In this paper we introduce the model of
the platform and describe the current implementation. The
implementation facilitates the integration of technology for
an essential operation on the Semantic Web, namely Seman-
tic Web service discovery. We illustrate the use of the plat-
form in a case study by implementing a Semantic Web where
the Jade multi-agent platform provides the framework to de-
scribe the agents, and a number of existing Semantic Web
technologies are embedded in agent behavior.

Introduction

The Web is a very popular source of information and com-
mercial services. However, as documented by (Nielsen
2006), ﬁnding a speciﬁc piece of information or service us-
ing current search engines is still a complex and time con-
suming task. The performance of current Web technology,
such as search engines, is limited by the fact that it is not
possible for a computer to fully understand the content of
Web resources. This is due to the fact that the Web re-
sources’ content is typically written by humans in languages
that humans can understand. To allow computers to un-
ambiguously understand the content of Web resources, Tim
Berners-Lee formulated the vision of the Semantic Web.
Speciﬁcally, “the Semantic Web is an extension of the cur-
rent Web in which information is given well-deﬁned mean-
ing, better enabling computers and people to work in co-
operation” (Berners-Lee, Hendler, & Lassila 2001). With a
Semantic Web, a lot of the manual effort done today to ﬁnd
and use Web resources can be automated, at least partially,
by having the user delegating tasks such as resource retrieval
to software agents (Hendler 2001).

Copyright c
(cid:13)
gence (www.aaai.org). All rights reserved.

2006, American Association for Artiﬁcial Intelli-

The need for a Semantic Web is underscored by the recent
appearance of numerous new applications that refer to the
Semantic Web vision and/or rely heavily on the technology
being developed for the Semantic Web. Inter-organizational
workﬂows (Patil et al. 2004; Dan et al. 2004), information
retrieval (Huynh, Mazzocchi, & Karger 2005), e-commerce
(Trastour, Bartolini, & Preist 2002) and bio-informatics
(Lambrix 2005) are examples of the research domains where
such new applications are proposed.

Each new proposed application tends to make its own as-
sumptions regarding three aspects of the Semantic Web: 1)
the use case, i.e. who are the users and resource providers,
what motivates them to use and provide resources, and what
are the social and business rules that govern their interac-
tion, 2) the available resources together with their semantic
annotations, and 3) the technologies available (e.g. descrip-
tion logic reasoners, rule engines, ontology managers, etc)
and how they are used. For approaches to service discov-
ery, such assumptions specify implicit requirements on the
scalability (e.g. in terms of response time, bandwidth use,
or cost) and the quality of the result (e.g. measured as pre-
cision and recall). However, since there is not yet any large
and widely deployed set of semantically annotated Web re-
sources available for experimentation, it is difﬁcult to know
when, and if, these requirements are satisﬁed.

For this reason we propose a common simulation and
evaluation platform for service discovery. In this platform
current and future Semantic Web technology can be inte-
grated and evaluated with suitable use cases and resource
sets. The integration of technology is facilitated by means
of an API for common components together with default
implementations of these components. The evaluation is fa-
cilitated by means of a monitoring tool where event listeners
can be employed for creating performance reports on simu-
lation runs.

This paper describes the model and implementation of
this simulation and evaluation platform. The use of the plat-
form is illustrated in a case study where service discovery
technology is integrated in a speciﬁc use case together with
a speciﬁc set of resources. The evaluation of the technol-
ogy is illustrated in terms of scalability and quality of result
measures, and we discuss lessons learned from the use of the
platform.

The rest of the paper is organized as follows. In the next

1253section we provide more background information about the
current Semantic Web technology for service discovery. We
then describe the model and implementation of the simula-
tion and evaluation platform. We continue by illustrating the
use of the platform and discuss the lessons learned from the
case study. We then compare our approach to related work.
Finally, we conclude and discuss directions for future work.

Background - Semantic Web

The Semantic Web can be seen as a set of semantically an-
notated Web resources. A Web resource may be a text, a
picture, a piece of software, a representation of an element
of the real world such as a person, etc. Semantic annota-
tions describe the semantic of the resources so that software
agents can reason about them in order to reach a predeﬁned
goal. The goals of the agents vary from application to ap-
plication, but they all rely on the operation of ﬁnding and
using the resources necessary to perform the goal. To allow
the deployment of the Semantic Web, technology is being
developed for representing semantic annotations, for ﬁnding
them, for reasoning about them and for using the resources
that they annotate. The technology provides:
Machine-understandable languages to describe the con-
tent of Web resources. RDF and OWL are such languages.
Semantic annotation description languages that provide
the set of language constructs for describing the proper-
ties, capabilities, and use rules of the Web resources in
an unambiguous, machine-understandable manner. Se-
mantic Web services is a category of semantic annota-
tions that comes with a speciﬁc management framework
as deﬁned in (Fensel & Bussler 2002). Semantic Web
services are programmable, machine-understandable in-
terfaces that can be attached to a Semantic Web resource
in order to provide the information necessary for software
agents to decide if they need to use the speciﬁc resource
or not. As pointed out in (Lara et al. 2003), Semantic
Web services are designed to support the operation of re-
source discovery, composition and interoperability. There
are several language propositions for Semantic Web ser-
vices, such as OWL-S1, WSMO2, and OWL-DTP3.

Semantic-aware tools that use and manage the semantic
annotations, as well as the ontologies4 that the annota-
tions may refer to. Examples of tools that use semantic
annotations are Semantically enhanced web browsers like
Piggy Bank (Huynh, Mazzocchi, & Karger 2005). Exam-
ples of ontology management tools are ontology editors
such as Prot´eg´e5, and ontology aligners and mergers such
as SAMBO6. Examples of management tools for the se-

1http://www.daml.org/services/owl-s/1.0
2http://www.wsmo.org/
3http://www.ida.liu.se/˜iislab/projects/SWSlanguages/OWL-

DTP/20051124/

4From (Neches et al. 1991): “An ontology deﬁnes the basic
terms and relations comprising the vocabulary of a topic area as
well as the rules for combining terms and relations to deﬁne exten-
sions to the vocabulary.”

5http://protege.stanford.edu/
6http://www.ida.liu.se/˜iislab/projects/SAMBO/

mantic annotations are automatic generators of semantic
annotations such as the one-click publishing process of
IRS III illustrated in (Domingue et al. 2004). Examples
of tools that use ontologies are logic reasoners. Currently
the most successful reasoners are using description log-
ics, and one of the most popular such reasoner is Racer
(Haarslev, M¨oller, & Wessel 1999 2006). Reasoners rely-
ing on other logics (e.g. F-logic (de Bruijn et al. 2005))
are also being proposed.

Semantic Web operations that include resource retrieval,
resource use, and Semantic Web management operations
such as handling the changes in the resources’ content.
When the semantic annotations are Semantic Web ser-
vices, the operation of resource retrieval is called service
discovery. These operations use semantic-aware tools.
The operations are complex, and solutions are just emerg-
ing for applications where semantic annotations are for-
mulated as Semantic Web services. WSMX7 and IRS
III (Domingue et al. 2004) apply the recommendation of
the Web service modeling framework (Fensel & Bussler
2002) to describe service discovery and service execution.
There are some attempts to describe operations that han-
dle changes in the state of resources on the Semantic Web.
However, the semantic-aware tools required for such tech-
nology are still under development. The work done in the
REWERSE network8 aims at providing such technology.

Platform Model

The simulation and evaluation platform must provide the
support to 1) generate simulations of service discovery op-
erations, and 2) generate evaluation reports.

In order to generate a simulation, we need a model of the
Semantic Web as well as support to instantiate this model
with respect to a set of speciﬁc assumptions about the use
case, resources, and technology used.

We propose a Semantic Web model with four compo-
nents: the Web resources, the machine-understandable data,
the language speciﬁcations, and the operations. The Web
resources provide some semantic content presented in a for-
mat that may not be machine understandable. The machine-
understandable data includes the semantic annotations of the
Web resources, the possible queries for resources, and the
ontologies available to describe and reason about the anno-
tations and the queries. The language speciﬁcations include
the machine-understandable languages and the Semantic an-
notation description languages mentioned in the previous
section. The operations are the different approaches to ser-
vice discovery. Furthermore, the Semantic Web allows for
modeling applications as a set of software agents with differ-
ent capabilities, which collaborate to perform speciﬁc goals
(Hendler 2001). Operations are such applications. They can
thus be represented by a set of agents that embed one or sev-
eral semantic-aware tool(s), and may collaborate with each
other by exchanging messages whose content is written in
one of the available languages.

7http://www.wsmx.org
8http://rewerse.net

1254Figure 1 illustrates the use of the platform where the
set of assumptions pictured in the ASSUMPTIONS box is
used by the components of the platform (represented in the
PLATFORM box) to generate a monitored Semantic Web
as sketched in the SIMULATION box. The support tools
provided by the platform use the assumptions about the re-
sources to 1) instantiate the Web resource component by
gathering the resource URIs in a single database, 2) gather
the semantic annotation in another database that refers to the
database of resource URIs, and 3) generate a set of service
provider agents in charge of advertising and managing the
resources. The instantiation of the language speciﬁcations
component of the platform requires the identiﬁcation of the
set of languages used in the use case, the technology, and the
data. The instantiation of the machine-understandable data
component requires the gathering of the semantic annota-
tions, the queries deﬁned by the use case, and the ontologies
to which the annotations and the queries refer. As illustrated
in the OPERATION box in ﬁgure 1, the instantiation of the
operation component requires the implementation of the ser-
vice discovery operations as multi-agent systems where each
agent packages speciﬁc uses of semantic-aware tools.

Further, to facilitate evaluation, the platform must allow
deﬁnition of different settings of the same simulation in
terms of, for example, the number of resources used or the
number of agents available with a speciﬁc behavior. This is
handled by a speciﬁc set of support tools represented by the
“Use Case Settings” in the PLATFORM box.

Finally, in order to evaluate a Semantic Web simulation,
a monitoring mechanism is required. We propose to adopt
an event listening approach where the different components
of the simulation can generate events. As a result, and as
illustrated with the “Evaluation support” in the PLATFORM
box, the platform provides the API for implementation of
speciﬁc monitoring behaviors that listen to speciﬁc events
and compute speciﬁc evaluation reports, and a monitoring
agent in charge of running parallel threads for each of these
behaviors.

Platform Implementation

As a ﬁrst step towards a full implementation of the support
tools provided by the platform model, we implemented the
evaluation support, some support for changing the settings
of the simulation, and some support for the operation com-
ponent. The operation component requires the most com-
plex support. Each operation requires identiﬁcation of the
categories of agents that will participate, the algorithms that
each agent will implement, and assurance that the agents es-
tablish coherent collaborations.
In the current implemen-
tation of the platform we provide the following support to
create service discovery operations:

•

The description of the different categories of agents that
typically take part in the operation of service discovery.
Concretely, the description consists of:

– A set of agent categories in natural language (see be-

low).

– An API description of each agent category in terms of
the minimal set of functions that they must provide, and

Legend:

refers to
written in
uses

generates

USE CASE

RESOURCES

TECHNOLOGY

Web resources

M.U. Language

M.U. Language

M.-U. Language

S. A. D. Language

S. A. D. Language

S. A. D. Language

S.-A. TOOL

S.-A. TOOL

S.-A. TOOL

S. W. Operation

S. W. Operation

S. W. Operation

Semantic annotations

ASSUMPTIONS

input

Support to instantiate:

Evaluation support

Use Case
Settings

Web resources component

M. U. data component

 Language spec. component

Operation component

Monitoring

Agent

Monitoring 
Behavior API

Predefined 
Predefined 
Predefined 
Monitoring 
Monitoring 
Monitoring 
Behavior
Behavior
Behavior

PLATFORM

Web resources

DATA

LANGUAGE 

LANGUAGE 
SPECIFICATION

LANGUAGE 
SPECIFICATION

SPECIFICATION

understands

ontology

ontology

ontology

SWSSWS
SW service

query
queryquery

events

Monitoring

Agent

S. A. TOOL

S. A. TOOL

S. A. TOOL

AGENT

AGENT

AGENT

sends

message

message

Evaluation report

Evaluation report

Evaluation report

OPERATION

SIMULATION

Figure 1: Platform Model

the minimal set of messages that each agent is expected
to be able to interpret.

– An illustrative implementation of one Semantic Web
simulation corresponding to the Travel scenario dis-
cussed in the case study in the next section.

With these tools, the potential users of the platform do not
have to identify their own agent categories, but can focus
on specifying the agent categories that are taking part in
the operation that they want to implement, and what mode
of collaboration they must adopt.

•

•

One default implementation for each agent category. This
is useful for users who do not wish to specify all the agent
behaviors, but only the speciﬁc ones corresponding to the
speciﬁc technology that they want to test.

A mechanism for supporting monitoring and an illustra-
tive monitoring tool that is able to compute the time to
get an answer to a speciﬁc request message. This allows
evaluation data.

When it comes to the actual implementation of these tools
we considered two facts. First, the implementation of the op-
erations requires integration of different technologies writ-
ten in different programming languages, possibly running on
different machines. Second, to allow for the comparison of
different technologies and different settings, the evaluation
platform should provide the means to minimize the effort
required by changing one or several technologies used by

1255the operations. By providing the possibility to describe ap-
plications whose architecture is strongly decoupled, multi-
agent systems as deﬁned by FIPA, provide an environment
that support these needs. We thus implemented the support
above in Jade9, a Java framework that provides the means
to describe FIPA multi-agent systems. The API is a set of
Java interfaces and the messages exchanged by the agents
are ACL messages whose content is written in a Semantic
Web language such as OWL. We further adopted the service
discovery solution of the multi-agent system introduced in
(Aberg, Lambrix, & Shahmehri 2005). As a result, the cur-
rent implementation of the support for instantiating the op-
eration component provides an API and a default implemen-
tation for the following set of agent categories:

A Requester is able to formulate a query for a speciﬁc ser-
vice, and to send it to the agent(s) able to start up the pro-
cess of service discovery, i.e. the Web service discovery
agents described next. A Requester may also be able to
enact a service once it is discovered.

A Web service discovery agent is able to ﬁnd the services
that match a given query for services. Web service dis-
covery agents may also be able to discover compositions
of services that match the query.

A Web service manager is a directory of Semantic Web
services. Web service managers are associated to one or
several Semantic Web service description languages such
as OWL-S, WSMO or OWL-DTP. A Web service man-
ager is able to answer queries for speciﬁc Web services.
A Web service manager does not perform composition of
services.

A Service provider sends service advertisements to Web
service managers. The service advertisements are formu-
lated as Semantic Web services.

An Ontology Agent (OA) is able to reason about a speciﬁc
domain (e.g. Travel, Car.) Any agent can delegate part
of their reasoning to ontology agents. OAs can answer
several types of queries such as “is A a subclass of B?” ,
“what are all the subclasses of class C?” or “what are all
the instances of class C?”

For each agent category above, the API speciﬁes the min-
imal set of behaviors that they must provide as well as the
minimal set of messages that they must understand. Each
of the agents’ behavior can embed one or several Seman-
tic Web technologies. For example, requester agents may
embed query editors, which in turn may refer to ontology
browsers. Semantic Web managers may typically embed Se-
mantic Web service matchmaking algorithms. Service dis-
covery agents can embed composition algorithms that may
refer to work done on choreography and orchestration. On-
tology agents embed ontology technologies such as editors,
aligning tools, and domain speciﬁc matchmakers. Service
providers may embed technology such as automatic gen-
erators of Semantic Web services (Domingue et al. 2004;
Ciravegna 2004).

9http://jade.cselt.it/

Moreover, the support also provides a java package of the
classes implementing the minimal set of messages and pro-
viding the methods to parse the content of the messages. In
order to allow the users of the platform to focus on the inte-
gration and monitoring of their own technology, the imple-
mentation of the platform also provides for a default behav-
ior for each agent. Finally, to satisfy the platform model’s
requirements on a monitoring agent, the current implemen-
tation of the platform provides a Jade Monitor agent which
can run several monitor behaviors in parallel. We also pro-
vide one MonitorAnswerTime behavior that observes the
time when a message is sent and when an answer is received
in order to compute the resulting answer time.

Illustration

To illustrate the use of the platform we show how service
discovery technology was integrated and evaluated for a
speciﬁc use case and with a speciﬁc set of Web resources.
Lessons learned from the case study with respect to the plat-
form’s ease of use are discussed in the next section.

Assumptions
Our initial assumptions with respect to the use case, the ser-
vice discovery technology, and the available Web resources
are as follows. For the use case, we assume the Seman-
tic Web to be an open world where requesters and ser-
vice providers can specify the kind of transaction that they
agree to participate in (e.g. buying, lending, acquiring for
free). The service providers provide travel itineraries and
requesters query for speciﬁc travel itineraries, and expect
to get answers at least as quickly as when they consult a
database of travel itineraries.

As for the service discovery technology, we assume that
the underlying architecture corresponds to the agent archi-
tecture introduced in (Aberg, Lambrix, & Shahmehri 2005)
where the Semantic Web service language is OWL-DTP and
the Web Service manager agent integrates the OWL-DTP
matchmaking algorithm introduced in (Aberg 2005). This
algorithm requires the use of description logic reasoning for
which the Racer system is used. The Jena-DIG interface is
used as a Java interface to Racer. The matchmaking algo-
rithm is implemented in a straightforward way that requires
each query to be matched against each service description.
Each operation of matching a query and a service, requires a
set of reasoning operations including some subsumption op-
erations. To implement the full service discovery operation,
we use the default agents provided by the platform and pack-
age our matchmaking algorithm as a Web Service manager
and a Travel ontology agent.

With respect to the Web resources available we consider
a set of 183 services providing travel itineraries. These ser-
vices correspond to those provided by the different Web sites
of the travel providers that the employees of our department
are authorized to use when planning work-related trips. We
also consider a set of 14 queries corresponding to real travel
queries expressed by some employees when planning their
trips. There are two categories of queries. Queries that will
require a composition of services (e.g. “Give me an itinerary

1256to go from Stanford University, Palo Alto, CA, USA, to
the Conference center in Chiba city, Japan”), and queries
for which service composition is not always necessary (e.g.
“Give me all the ﬂights to go from Heathrow Airport, Lon-
don, UK, to Kastrup Airport, Copenhagen, Denmark”).

Evaluation
Scalability We measure the scalability of the service dis-
covery approach with respect to the number of services and
the technical capabilities of the machines running the agents,
by measuring the average response time to the queries. To
do that we use the MonitorAnswerTime behavior provided
by the platform. Additionally, all the agents trigger an event
when they send or receive a message. We run the simula-
tion in different settings where the agents run on different
machines.

This ﬁrst set of evaluation runs teaches us that the triplet
Jena-DIG-Racer cannot run the required knowledge base on
a machine with too little CPU and RAM. Concretely, the
reasoner freezes if it uses the Travel ontology agent knowl-
edge base on a pc with the x96 Family 6 Model 1 stepping
9 processor (ca. 199 MHz) and 64 MB of RAM. Further,
the reasoner that uses both the knowledge base for the Web
service manager and the Travel ontology agent10 and runs
on a pc with an Athlon processor (1.33 GHz) and 512 MB
of RAM, freezes after treating a random number of queries
(ten, eleven or even forty). We identiﬁed one machine set-
ting that works well for our technology use:
the reasoner
that uses the Web service manager knowledge base runs on
a pc with an Athlon processor (1.33 GHz) and 512 MB of
RAM, and another reasoner that uses the Travel ontology
agent knowledge base runs on a pc with an Intel Pentium
M processor (ca. 1400 MHz) and 512 MB of RAM. Addi-
tionally, in the machine settings providing for the best aver-
age time for the set of 183 services, we obtain an average
response time to the queries of approximately 14 minutes.
This is clearly not an acceptable performance with respect
to the use case. Upon more detailed inspection we ﬁnd that
the reason for this great delay in response time is that the
current matchmaking approach performs approximately 300
subsumption operations per query. Most of these operations
are required to match the travel itineraries.

Given these observations we design a new matchmaking
algorithm such that the Web Service manager decomposes
the OWL-DTP representation in three components, and in-
dexes them at service advertisement time. The indexing of
the components referring to travel itineraries is performed
by the Travel ontology agent, which stores the generated in-
dexes in a database. The indexes are then used at query time.
We change the behavior of the Web Service manager and
Travel ontology agent to integrate the new algorithm. The
new algorithm requires two subsumption operations and one
SQL query to match a query with all the available services.
Running the simulation now provides an answer in 10 sec-
onds on average. This is a result that better ﬁts the use case
requirements with respect to time, even if there is still room

10Jena-DIG related note: both knowledge bases are deﬁned in

their own model.

for improvement.

The monitoring also provides the time to advertise the ser-
vices. With the straightforward algorithm, it takes ca. 28
seconds to advertise 183 services in one Web service man-
ager. With the second version of the algorithm, it takes
ca. 183 seconds to advertise 183 services in one Web ser-
vice manager. The preprocessing done at advertisement time
takes its toll. However, it is still a reasonable processing time
for advertisements since they need to be done only once per
service in this use case.

Result quality In order to measure the quality of the result
we measure precision and recall for each query. This is done
by implementing a monitoring behavior that compares the
set of services returned for each query, with a precompiled
ideal set of services. The results show that we obtain 100%
precision and recall for the 3 queries that request one spe-
ciﬁc travel leg (i.e. they correspond to one or several exist-
ing services), showing that the service description language
is suitable for the corresponding information needs. For the
other 11 queries that requested travel itineraries composed
of several legs, and thus requiring service composition, we
got 0% precision and recall. This result provides us with
a clear next step for the development of a complete service
discovery operation, namely to package a service composi-
tion algorithm as a Web Service discovery agent behavior
and evaluate how that would inﬂuence the precision and re-
call of the corresponding queries.

Summary
We have illustrated how the platform was used in a case
study. We showed how service discovery technology was
evaluated and analyzed, in terms of scalability and result
quality, and reﬁned, based on assumptions in the use case.
This analysis helped us narrow down the main performance
bottleneck of the technology. After ﬁne-tuning the match-
making algorithm the platform also facilitated the compari-
son with the previous version, while indicating the unwanted
side effect of increased advertisement time that the new al-
gorithm implied. All in all, the platform helped us maintain
a high-level view of the service discovery problem, while
facilitating our work on the details.

Lessons Learned

When implementing the service discovery approaches de-
scribed above, we noticed three clear advantages of using the
platform. The ﬁrst advantage concerned time gain at design
time. When pondering how to implement the assumptions of
our case study in a service discovery operation, the platform
provided us with a clear model of the operation. We imme-
diately identiﬁed the need for a requester, a set of service
provider and a Web service manager agent. The platform
also made us consider the decomposition of the matchmak-
ing algorithm so that the travel-related part of the reasoning
would be delegated to a speciﬁc ontology agent. This is a
good design choice if we consider that we will later want to
extend the scope of services.

The second advantage concerned both debugging and the
integration of the second version of the matchmaking algo-

1257rithm. In both cases, because of the strongly decoupled ar-
chitecture of the implementation, including the different be-
haviors implemented by each agent, the code rewriting could
be done locally, requiring very little, if any, rewriting of the
code of other behaviors.

The third advantage is also connected to ease and rapidity
of implementation: the predeﬁned package of messages al-
lowed us to very quickly set up the communication between
agents.

All this allowed us to concentrate on the one task that was
really important to us: integrating the matchmaking algo-
rithm and evaluating its performance.

Related Work

The similarity of the paradigms of the Semantic Web and
multi-agent systems has been acknowledged by others.
However, most other work concentrates on providing an in-
terface between multi-agent systems and the Semantic Web.
Jade does go in the direction of supporting the integration of
the Web paradigm in multi-agent systems by providing the
possibility to use the HTTP protocol as the communication
protocol between agents. However, more advanced features
such as the management of Semantic Web resources are not
taken into account by any other agent approach that we know
of. IRS III (Domingue et al. 2004) and WSMX do provide
platforms to manage the life cycle of Semantic Web services
in terms of service discovery. However they force the use
of one Semantic Web service representation (i.e. WSMO),
which may not ﬁt all Semantic Web use cases. Further, they
do not provide any means to evaluate and compare different
approaches.

Conclusions and Future Work

We have highlighted the need for a platform to support the
integration of Semantic Web technology to build service dis-
covery operations and evaluate them with respect to scala-
bility and quality of the results generated. We have provided
the model and an implementation of such a platform, and il-
lustrated its use on a service discovery operation in the travel
domain. The platform allowed us to integrate service dis-
covery technology, identify their weaknesses, and limit the
effort to change parts of the implementation. Our work is a
ﬁrst step towards providing a full-ﬂedged platform for sim-
ulating and evaluating the Semantic Web. As for the future,
we plan to complete the current support for describing ser-
vice discovery, and provide support for the other operations
of service use and Semantic Web management as well.

References

Aberg, C.; Lambrix, P.; and Shahmehri, N. 2005. An
Agent-based Framework for Integrating Workﬂows and
Web Services. In IEEE WETICE workshop on Agent-based
Computing for Enterprise Collaboration, 27–32.
Aberg, C. 2005. Integration of Organizational Workﬂows
and the Semantic Web. Licentiate thesis, Link¨opings uni-
versitet.
Berners-Lee, T.; Hendler, J.; and Lassila, O. 2001. The
Semantic Web. Scientiﬁc American.

2004. Amilcare - adaptive IE tool.

Ciravegna, F.
http://nlp.shef.ac.uk/amilcare/ (Accessed 2006-02-13).
Dan, A.; Davis, D.; Kearney, R.; Keller, A.; King, R.;
Kuebler, D.; Ludwig, H.; Polan, M.; Spreitzer, M.; and
Youssef, A. 2004. Web services on demand: WSLA-driven
automated management. IBM Systems Journal 43(1):136–
158.
de Bruijn, J.; Polleres, A.; Lara, R.; and D.Fensel. 2005.
OWL DL vs. OWL Flight: Conceptual Modeling and Rea-
In the 14th International
soning for the Semantic Web.
World Wide Web Conference, 623–632.
Domingue, J.; Cabral, L.; Hakimpour, F.; Sell, D.; and
Motta, E. 2004. IRS-III: A Platform and Infrastructure for
Creating WSMO-based Semantic Web Services. In Work-
shop on WSMO Implementations.
Fensel, D., and Bussler, C. 2002. The Web Service Mod-
eling Framework WSMF. Electronic Commerce Research
and Applications 1(2):113–137.
Haarslev, V.; M¨oller, R.; and Wessel, M.
1999-2006.
Racer: Semantic Middleware for Industrial Projects Based
on RDF/OWL, a W3C Standard.
http://www.sts.tu-
harburg.de/ r.f.moeller/racer/ (Accessed 2004-12-08).
Hendler, J. 2001. Agents and the Semantic Web. IEEE
Intelligent Systems 16(2):30–37.
Huynh, D.; Mazzocchi, S.; and Karger, D. 2005. Piggy
Bank: Experience the Semantic Web Inside Your Web
Browser. In International Semantic Web Conference, 413–
430.
Lambrix, P. 2005. Towards a Semantic Web for Bioinfor-
matics using Ontology-based Annotation. In 14th IEEE In-
ternational Workshops on Enabling Technologies: Infras-
tructures for Collaborative Enterprises, 3–7. Invited Talk.
Lara, R.; Lausen, H.; Arroyo, S.; de Bruijn, J.; and Fensel:,
D. 2003. Semantic Web Services: description requirements
In International Workshop on
and current technologies.
Electronic Commerce, Agents, and Semantic Web Services,
In conjunction with the Fifth International Conference on
Electronic Commerce (ICEC 2003).
Neches, R.; Fikes, R.; Finin, T.; Gruber, T.; Patil, R.; Sen-
ator, T.; and Swartout, W. 1991. Enabling Technology for
Knowledge Sharing. AI Magazine 12(3):26–56.
Nielsen, J.
ary 6, 2006: Users
http://www.useit.com/alertbox/cross site behavior.html
(Accessed 2006-02-08).
Patil, A.; Oundhakar, S.; Sheth, A.; and Verma, K. 2004.
METEOR-S Web service Annotation Framework. In Inter-
national World Wide Web Conference, 553–562.
Trastour, D.; Bartolini, C.; and Preist, C. 2002. Seman-
tic Web Support for the Business-to-Business E-Commerce
Lifecycle. In International World Wide Web Conference,
89–98.

Jakob Nielsen’s Alertbox, Febru-
Interleave Sites and Genres.

2006.

1258