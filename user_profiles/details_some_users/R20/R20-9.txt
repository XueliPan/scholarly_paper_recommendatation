An Empirical Study of Human Web Assistants: Implications 

for User Support in Web Information Systems 

 

 

Johan Aberg and Nahid Shahmehri 

Dept. of Computer and Information Science 

Linköpings universitet, S-581 83 Linköping, Sweden 

+46 13 281465 

{johab,nahsh}@ida.liu.se

ABSTRACT 
User support is an important element in reaching the goal of 
universal  usability  for  web  information  systems.  Recent 
developments  indicate  that  human  involvement  in  user 
support  is  a  step  towards  this  goal.  However,  most  such 
efforts are currently being pursued on a purely intuitive basis. 
Thus,  empirical  findings  about  the  role  of  human  assistants 
are  important.  In  this  paper  we  present  the  findings  from  a 
field  study  of  a  general  user  support  model  for  web 
information  systems.  We  show  that  integrating  human 
assistants into web systems is a way to provide efficient user 
support. Further, this integration makes a web site more fun 
to  use  and  increases  the  user’s  trust  in  the site. The support 
also 
the  site  atmosphere.  Our  findings  are 
summarised  as  recommendations  and  design  guidelines  for 
decision-makers and developers of web systems. 

improves 

Keywords 
User  support,  web  information  systems,  universal  usability, 
attitude, efficiency, field study, design guidelines 

INTRODUCTION 
An important challenge for universal usability is to bridge the 
gap between what users know and what they need to know to 
successfully  interact  with  a  computer  system  [19].  Different 
users  have  different  needs  and  skills,  especially  for  web 
information  systems  (WISs)  [8].  WISs  often  serve  an 
international  user  community  aiding  people  with  various 
backgrounds.  Therefore,  there  is  a  strong  need  to  provide 
support  for  the  whole  range  of  users.  Ben  Shneiderman 
identified online help and customer service as two important 
approaches  in  dealing  with  this  challenge  and  called  for 
evaluations  and  design  guidelines  [19].  Earlier,  Carroll  and 
McKendree  urged  that  empirical  studies  on  advice  giving 
systems be done [6]. Online help means online support for the 
user  tasks  that  the  WIS  is  intended  for.  We  use  the  term 
customer  service  as  the  specialisation  of  online  help  to  the 

 
Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
SIGCHI’01, March 31-April 4, 2001, Seattle, WA, USA. 
Copyright 2001 ACM 1-58113-327-8/01/0003…$5.00. 

 
 

task  of  online  shopping.  In  this  paper  we  use  the  term  user 
support for these two approaches. 

It has been shown that user support has an influence on WISs 
such as electronic commerce sites. For example, the study by 
Vijaysarathy  and  Jones  [21]  suggests  that  customer  service 
for the pre-order part of the shopping process has a positive 
influence on user attitudes toward Internet catalog shopping. 

Still, the current state of practice in user support for WISs is 
limited and in need of improvements. In a study of consumer 
reactions to electronic commerce by Jarvenpaa and Todd [9], 
it is shown that user support is limited in electronic commerce 
systems.  In  the  study  by  Spiller  and  Lohse  [20]  several 
aspects  of  customer  service  were  analysed.  The  result  was 
that the customer service in the stores studied was limited. As 
further  motivation  for  studying  user  support  for  WISs,  we 
consider the Home Net field trial. The results from this study 
showed that much user support is needed for Internet usage in 
general [13]. 

In our previous work we have introduced a general model for 
user  support  [1].  The  model  features  a  combination  of 
computer-based  support  and  human  assistants.  Aspects  of 
universal  usability  such  as  technology  variety  and  user 
diversity  [19]  are  considered  in  the  model.  The  model 
proposes a flexible user interface where users can select how 
they  want  to  interact  with  the  support  system.  For  example, 
users  can  choose  whether  they  only  want  computer-based 
support or would prefer to chat with human assistants via text 
chat, voice chat, or other means of interaction. Our approach 
to  dealing  with  user  diversity  is  based  on  user  modelling, 
allowing personalised support. 

prototype 

implementation 

We have studied our proposed model in a two-step project. In 
step 1 we conducted an exploratory usability study based on a 
limited 
for 
communication  between  a  user  and  an  assistant.  We 
employed  the  REAL  (relevance,  efficiency,  attitude,  and 
learnability) model of usability [16]. Each aspect of usability 
was  evaluated  based  on  a  questionnaire  following  a 
controlled experiment with nine subjects and one assistant. 

designed 

In step 2, our main aim was to test the technical feasibility of 
the support model. To do this we implemented an instance of 
the full model and deployed it at an existing WIS for a three-

Papers CHI 2001 • 31 MARCH – 5 APRIL  Volume No. 3, Issue No. 1           CHI 2001       week period. The focus of this paper is on the usability study 
that was part of step 2. In this usability study we tested some 
of  the  indications  from  step  1.  We  hypothesised  that  our 
support  system  would  have  a  positive  influence  on  user 
attitudes  towards  the  WIS.  We  also  hypothesised  that 
efficient support would be provided. Furthermore, we wanted 
to study the work situation of human assistants in a real-world 
setting,  which  was  impossible  to  do  in  step  1  (due  to  the 
nature of controlled experiments in general). In summary, our 
focus  in  this  paper  is on the following questions: How does 
our support system affect users’ attitudes toward a WIS? Can 
our system provide efficient user support? What is the work 
situation for human assistants like? 

When  it  comes  to  user  attitude,  we  consider  the  issues  of 
trust, fun-factor, and atmosphere. Trust is an aspect of attitude 
that  has  recently  received  much  attention  in  the  literature. 
Evidence on the importance of trust for electronic commerce 
is presented in [7, 14, 17], and is discussed in the context of 
groupware  in  [18].  While  the  effect  of  trust  on  customer 
loyalty  has  been  studied  previously,  it  is  important  to  also 
study  how  trust  can  actually  be  improved.  Thus  it  is 
interesting  to  study  how  human  assistants  affect  users’  trust 
for a WIS. 

Fun-factor  and  the  atmosphere  of  a  WIS  are  highlighted  in 
[9].  The  study  by  Jarvenpaa  and  Todd  shows  that  users  of 
online shops miss the fun and store atmosphere of shopping 
in traditional stores. Thus it would be of interest to improve 
these attitude factors. 

Regarding  efficiency,  we  consider  issues  such  as  flexibility, 
quality  of  support,  feasibility  of  textual  chat,  and  the 
feasibility  of  having  users  waiting  in  a  queue  for  human 
assistance. Knowledge gained about these issues is important 
for the successful deployment of this kind of support system. 

It is also necessary to consider the work situation of assistants 
in order to know how to support them in the best way, and to 
know what is required of an assistant. 

User Support System 
Our implemented user support system is a question-answering 
system.  A  user  is  routed  through  an  initial  step  where  the 
natural  language  question  is  matched  against  a  frequently 
asked question file (FAQ). Closely-matching FAQ items are 
returned as potential answers to the user’s question [2]. If this 
step is not satisfactory, the user can request a help chat with a 
human assistant that has expertise in the particular area of the 
question. The system computes a set of the assistants that best 
match the question type based on assistant expertise profiles. 
If all the best-matching assistants are currently busy, the user 
is offered the option of waiting in a queue. Once an assistant 
is  available  the  user  and  the  assistant  are  connected  via 
textual chat, and can have a help conversation. 

A user model tool is used by the assistants in order to provide 
personalised  support  for  the  users.  The  user  model  contains 
personal information and skill-related information. Assistants 
take  an  active  role  in  acquiring  data  for  user  models  from 

help  conversations  by  collecting  relevant  information  and 
storing  it  in  the  user  models.  The  assistants  are  also 
responsible for continuously updating the FAQ as users come 
with new kinds of questions. 

Figure 1 illustrates the process of a user asking a question of 
the system. The system is illustrated from an assistant’s point 
of view by showing an example of system usage in Figure 2. 

Figure  1.  An  example  of  possible  question-answering 
sessions illustrated from a user's point of view. 

User
asks a
question

V iew
matched
FAQ
items

Answer
not
found

Wait in
queue
for
assistant

Assistant is
available

User

Answer
found

Tired of waiting in
queue

Stop

Stop

Chat

Answer
found

Stop

Figure  2.  An  example  of  a  question-answering  session 
illustrated from an assistant's point of view. 

V iew and update user
model

S ystem
requests user
chat

User
satisfied

Chat
w ith
user

Add
new
FAQ
item

Stop

Assistant

Related Work 
Since the initiation of our work, there have been commercial 
moves  in  the  directions  discussed  in  this  paper.  Companies 
such  as  LivePerson  (www.liveperson.com)  and  FaceTime 
(www.facetime.org)  now  offer  commercial  systems  for 
human  assistance 
the 
importance  of  the  kind  of  studies  reported  in  this  paper. 
While  the  efforts  by  FaceTime  and  LivePerson  are  well 
motivated, little is known about how users interact with these 
kinds  of  systems,  and  how  the  most  benefit  can  be  derived 
from the human assistants.  

in  WISs.  This 

increases 

trend 

to 

in  spirit 

The  systems  currently  offered  by  LivePerson  and  FaceTime 
are  clearly  similar 
the  system  we  have 
implemented.  Still,  there  are  some  important  differences. 
First,  user  modelling  seems  to  play  a  limited  part  in  the 
commercial  systems  when compared to our system. Second, 
the  user  support  provided  is  completely  centered  on  the 
human  support  agents.  Our  approach  with  routing  users 
through a computer-based question-answering system before 
connecting  users  with  human  assistants  seems to be a better 
use of resources [2]. 

In  [12] 
two  applications  for  web-based  collaborative 
customer  service  are  described.  The  applications  are  for  a 
banking  kiosk  setting  and  for  home  banking  over  Internet. 
The  banking  kiosk  application  uses  a  shared  browser 
approach with ink annotation for communication. The home 
banking application also uses a shared browser approach and 
has support for voice chat. The two applications are related to 

our  approach.  They  are,  however,  limited  when  it  comes  to 
providing  personalised  support.  The  descriptions  of  the 
systems  are  on  an  architectural  level,  and  no  evaluation  is 
presented. 

wanted  to  test  our  system  in  a  low  risk  environment  where 
unexpected  system  problems  would  not  have  large  financial 
consequences.  Based  on  these  requirements  we  considered 
Elfwood to be a good environment for the field study. 

The  Answer  Garden  (AG)  system  by  Ackerman  [3]  is  a 
question-answering system related to our work. Still, there are 
some important differences. There is no form of support for 
personalisation  in  AG  similar  to  our  user  modelling  system. 
Further,  where 
synchronous 
communication between users and assistants via textual chat, 
question  answering  by  an  expert  in  AG  consists  of  two 
asynchronous messages, the question and the answer. 

supports 

system 

our 

The AG system was later extended to a second version called 
Answer Garden 2 (AG2) [4]. AG2 features mainly two new 
functions  consisting  of  an  automatic  escalation  of  help 
sources and a form of collaborative refinement of the system 
database  taking  sources  other  than  user  questions  into 
consideration. These features could very well be incorporated 
into our system, and would be interesting subjects for future 
work. 

FIELD STUDY 
As  mentioned  previously,  our  overall  research  objective  in 
step  2  was  to  test  the  technical  feasibility  of  the  support 
model. Therefore a field study, where the system is tested in a 
real environment, is a natural research method. In this paper 
we  focus  on  two  aspects  of  usability,  namely  attitude  and 
efficiency.  We  also  consider  the  work  situation  of  human 
assistants. 

Environment 
The user support system has been attached to an existing WIS 
for  a  period  of  three  weeks.  The  WIS,  called  Elfwood,  is  a 
non-profit  WIS  in  the  art  and  literature  domain,  where 
amateur  artists  and  writers  can  exhibit  their  material.  At the 
time  of  this  writing  around  6700  artists  and  1300  writers 
exhibit  their  work  in  the  fantasy  and  science  fiction  genre. 
The  artists  and  writers  are  members  of  the  site,  and  have 
access to an extranet for updating their own exhibition areas. 
A  part  of  the  site  is  devoted  to  teaching  art,  and  includes  a 
large number of feature articles on different topics. 

Elfwood  has  around  14,500  daily  visitor  sessions  (many  of 
these  are  by  non-members),  where  each  session  averages 
approximately 35 minutes. About 60% of the sessions are by 
users  from  the  US.  The  remaining  users  are  mainly  from 
Europe and Canada.  

There were three main user tasks that we intended to support. 
1)  Member  activities,  such  as  uploading  new  art  and 
literature,  and the management of each member’s exhibition 
area.  2)  Searching  for  interesting  art  and  literature  at 
Elfwood.  3)  Learning  how  to  create  fantasy  and  science-
fiction related art and literature. 

We chose Elfwood as the environment for our study for two 
reasons. First, we wanted a site with a reasonable number of 
users and user traffic, and with a user community that would 
allow  the  recruitment  of  suitable  assistants.  Second,  we 

Participants 
Voluntary  assistants  participated  in  the  study  (without 
payment)  from  their  home or work environment. They were 
recruited some months before the field study began, and they 
were all Elfwood members. A recruiting message was posted 
on the site with a description of the role and requirements of 
an  assistant.  In  the  end  35  people  with  proper  expertise 
participated actively as assistants throughout the period of the 
field  study,  and  30  of  these  helped  users  (the  other  5  were 
only online at times when not so many users needed support). 
A  schedule  with  the  online  times  of  the  assistants  for  the 
entire field study period was established and published at the 
site. 

Before the field study started we advertised the existence of 
the  support  system  at  Elfwood  in  a  message  column  on  the 
main page, and by sending an email to all members. During 
the field study 636 users registered with the support system, 
and  129  of  these  users  worked  through  the  system  to  have 
help conversations with assistants. Once the study started, we 
had an eye-catching icon on the main page that worked as a 
link  to  the help system page. To further facilitate the use of 
the  online  help  system,  we  also  had  icons  that  linked  to  the 
help system from every page at the site.  

Data Collection 
We  chose  to  use  questionnaires  as  our  evaluation  tool.  We 
saw  questionnaires  as  appropriate  for  our  focus  on  the 
subjective  opinions  of  assistants  and  users.  The  use  of 
alternative  evaluation  tools  such  as  interviews  and  think-
aloud evaluations [11] were considered to be difficult, due to 
the wide geographical distribution of the users and assistants. 

We used three questionnaires, and in the design of these we 
considered the guidelines in [5]. 

User Questionnaire 1 
The  first  user  questionnaire  considered  users’  subjective 
opinions on aspects of attitude and efficiency. In order to get 
a  reasonably  high  response  rate  we  limited  the  number  of 
statements  and  questions  as  much  as  possible.  Regarding 
attitude, we considered trust, fun-factor, and atmosphere. All 
three aspects of attitude are mentioned as relevant in related 
literature  (as  discussed  in  the  introduction).  We  also 
considered how users would react if there were no assistants 
available when they needed it. While our study of efficiency 
in step 1 was limited to the issue of flexibility, we now take a 
more detailed approach. We consider ease of use, quality of 
support, the users’ view upon queuing for human support, the 
feasibility  of  textual  chat  as  a  communication  means,  users’ 
usage purpose, and general system flexibility. 

The  questionnaire  contained  a  number  of  statements  and 
questions  related  to  the  mentioned  aspects  of  efficiency  and 
attitude.  The  respondents  were  asked  to  express  their 

agreement  or  disagreement  with  each  statement  by  giving  a 
rating  on  a  scale  of  1  to  10.  The  questions  had  alternative 
answers  that  the  users  were  told  to  choose  from.  The 
respondents  were  asked  to  explain  their  answers  to  all 
statements and questions. 

The  respondents  were  asked  to  consider  the  concept  of  the 
support  system  in  general  and  not  focus  on  the  actual 
implementation.  The  questionnaire  was  sent  out  to  the  129 
users who had had help conversations with assistants. It was 
sent  via  email  just  after  the  field  study  was  finished.  We 
received  53  questionnaires  that  were  properly  filled  out 
(response rate of 41%). 

The  respondents had a distribution of 51% female and 49% 
male,  74%  came  from  North  America,  19%  were  from 
different  European  countries,  and  the  rest  came  from 
Australia, Israel, and Honduras. The age distribution was: 10-
19  (66%),  20-29  (23%),  30-39  (9%),  and  40-49  (2%).  The 
respondents  had  1.55  help  conversations  on  average.  Note 
that  these  data  are  not  quite  complete  as  5  respondents  are 
missing in the figures, due to loss of data. 

User Questionnaire 2 

During  the  course  of  the  field  study  we  noticed  that  many 
users had registered with the support system but never had a 
help conversation with an assistant. In order to investigate the 
reasons for this behavior, we designed a second questionnaire 
directed  towards  these  users.  Several  possible  reasons  were 
provided  together  with  the  option  of  creating  alternative 
reasons. The questionnaire was sent out to all 507 users who 
had registered but not participated in a help conversation. It 
was sent out directly after the field study ended. We received 
175 answers, a response rate of 35%. A slight majority of the 
respondents  were  female  (56%).  The  majority  came  from 
North  America  (75%),  followed  by  Europe  (17%)  and 
Oceania  (4%).  The  remaining  4%  came  from  Africa,  South 
America,  and  Asia.  The  age  distribution  was:  10-19  (60%), 
20-29 (28%), 30-39 (8%), 40-49 (3%), and 50-59 (1%). Due 

to  loss  of  data,  the  demographics  represent  98%  of  the 
respondents. 

Assistant Questionnaire 
The  questionnaire  that  was  sent  out  to  the  assistants 
considered  the  work  situation  for  assistants.  Several  open-
ended questions were used to address this issue. We chose to 
use open-ended questions for two reasons. First, we believed 
that  the  assistants  would  be  likely  to  spend  the  extra  time 
needed for these questions compared to, for example, rating 
statements.  Second,  we  were  looking  for  as  much  detailed 
information as possible, making open-ended questions a good 
alternative [11]. 

The questionnaire was sent out to the 30 assistants who had 
been  involved  in  helping  users  via  email  directly  after  the 
field study period had ended. We received 21 properly filled 
out questionnaires (response rate of 70%). 

In addition, the questionnaire also contained some questions 
of a demographic nature. From these questions we found that 
the  assistants  were  mainly  from  North  America  (62%)  and 
Europe  (29%).  Asia  and  Oceania  shared  the  remaining  9%. 
The assistants were from different age groups: 10-19 (38%), 
20-29  (38%),  30-39  (14%),  40-49  (10%).  The  gender 
distribution was rather even: 43% male, and 57% female. All 
assistants had at least two years of Internet experience, with 
one  exception  in  which  the  assistant  had  half  a  year  of 
Internet experience. Most assistants used a traditional dial-up 
modem  to  access  Internet  (67%),  while  some  used  cable 
modem  (19%),  and  the  rest  had  direct  cable  access.  On 
average,  a  responding  assistant  participated  in  6.7  help 
conversations. 

FINDINGS 
User Questionnaire 1 
The  results  for  the  statements  in  the  questionnaire  are 
presented  in  Table  1.  Apart  from  the  statements,  the 
questionnaire also contained two questions. The results from 
these questions are shown in Table 2 and Table 3. Notice that 

Questionnaire statements regarding efficiency 

Mean 

S. Dev. 

Table 1. Results from statements in user questionnaire 1 

To get help from this kind of assistant system is (1=complex, 10=straightforward) 

The flexibility of this kind of assistant system is (1=low, 10=high) 

Waiting in a queue for a human assistant is (1=useless, 10=worthwhile) 

The time I would be willing to wait in a queue for an assistant is (1=short (e.g. max 1 minute), 10=long (e.g. 
max 20 minutes)) 

The quality of support given by assistants in general is (1=bad, 10=good) 

Having a dialog with an assistant via a textual chat is (1=cumbersome, 10=straightforward) 

Questionnaire statements regarding attitude 

Getting support from this kind of assistant system is (1=boring, 10=fun) 

My trust in the provided support is (1=low, 10=high) 

The effect that human assistants have on my trust for a web site is (1=negative, 10=positive) 

The effect that human assistants have on the atmosphere of a web site is (1=negative, 10=positive) 

 

7.42 

7.40 

7.42 

5.51 

7.91 

8.32 

8.04 

8.58 

8.83 

9.08 

1.94 

1.68 

2.28 

2.33 

1.83 

1.85 

1.90 

1.56 

1.45 

1.37 

in  Table  2  some  users  saw  more  than  one  alternative  as 
appropriate. This is why the percentages sum up to more than 
100%.  Also,  concerning  the  results  in  Table  3,  two 
respondents did not find any of the alternatives fitting, which 
is why the percentages do not quite sum up to 100%. 

Table  2.  Results  on  reactions  to  a  system  with  no 
assistants logged in 

 Reactions to a system with no assistants logged in 

A. Try again later 

B. Never come back 

C. Make use of the FAQs 

D. Try to solve the problem on ones own 

Table 3. Results on different usage types 

Different kinds of usage 

A. To save time and energy 

B. To get help with something difficult 

C. Both of the alternatives (A and B) 

22 (42%) 

3 (6%) 

21 (40%) 

15 (28%) 

17 (32%) 

7 (13%) 

27 (51%) 

We summarise the results from user questionnaire 1 in light 
of  the  explanations  that  we  received.  The  following  method 
was used in analysing the respondents’ explanations. First, to 
get  a  good  overview,  we  created  a  file  for  each  statement 
where  all  explanations  were  stored  verbatim.  Second,  for 
each  statement  we  made  a  summary  of  all  issues  that  were 
mentioned in the explanations and noted the number of times 
that  they  were  mentioned.  Third,  issues  that  were  closely 
related  were  organised  into  categories  with  corresponding 
descriptive  terms, keeping track of the number of times that 
each category was mentioned. Note that this step sometimes 
required  interpretation.  Fourth,  for  the  sake  of  brevity  only 
the  most  informative  categories  were  selected  for  the 
presentation.  Still,  we  attempted  to  reflect  both  positive and 
negative  comments  and  provide  as  wide variety as possible. 
In  some  cases  we  found  that  an  explanation  described  a 
category  in  an  illustrative  way.  Some  of  these  explanations 
are  presented  as  quotes.  Observe  that  not  all  respondents 
explained  their  ratings.  Slightly  less  than  50%  of  the 
respondents  elaborated  on  their  ratings.  This  means  that  the 
explanations offered might not give a complete picture. Still, 
many interesting issues were raised. 

It is mostly straightforward to get help from the support 
system.  The  following  descriptive  terms  were  used  in 
association with positive rankings: quick help (9 users), ease 
of  use  (5  users).  On  the  negative  side,  the  following 
descriptive terms were used: slow system (3 users), no help (2 
users). One user giving a low score explained, “First wait for 
Java to start, then login, wait longer, enter a question, then 
(after  waiting  a  bit)  choose  category,  then  read  the  FAQ 
and/or chat with assistant…” 

Most  users  are  willing  to  wait  in  a  queue  for  human 
assistance, but not for long. Several descriptive terms were 
used  for  unwillingness  to  wait  long:  impatience  (8  users), 
online time is expensive (2 users), unreliable (2 users). Three 
users said that they were willing to wait for a long time since 
they could do many other things online while waiting. 

Human  assistants  provide  mostly  good  quality  support. 
The  following  descriptive  terms  were  used  in  explanations: 
sensibility  (10  users),  flexibility  (6  users),  friendliness  (6 
users),  making  an  effort  to  help  (3  users),  alternative 
solutions (1 user). 

Textual chat is a viable means for help conversations on 
most  topics.  Several  descriptive 
in 
explanations: simple (8 users), expressive (7 users), real-time 
(2  users).  On  the  negative  side,  the  following  terms  were 
used: unstructured (3 users), inexpressive (3 users). 

terms  were  used 

Human  assistants  make  the  use  of  a  WIS  more  fun  for 
most  users.  The  following  descriptive  terms  were  used  in 
explanations:  human  touch  (12  users),  making  an  effort  to 
help (5 users), sense of humor (2 users). Three users thought 
the assistants had no effect on the fun of using a system. Two 
other  users  said  that  the  system  was  too  slow  to  have  any 
effect on the fun. 

Most  users  have  a  high  level  of  trust  in  the  advice 
provided  by  human  web  assistants.  The  following 
descriptive  terms  were  used  in  explanations:  sensibility  (14 
users),  advice  is  easy  to  verify  (3  users).  Two  users 
commented  that  they  generally  wanted  to  verify  pieces  of 
advice before trusting it. 

Human  web  assistants  have  a  positive  influence on most 
users’ trust in a WIS. The following descriptive terms were 
associated with this influence in the explanations: a message 
of caring (8 users), helpfulness (6 users), and accountability 
(4 users). Two users said that they neither trust nor distrust a 
web site. 

this  effect 

terms  were  associated  with 

The  presence  of  human  web  assistants  has  a  positive 
influence  on  the  atmosphere  of  a  WIS.  The  following 
descriptive 
in 
explanations:  alive  and  interactive  (7  users),  friendly  (6 
users), warm (4 users), personal (3 users), and appealing and 
understandable  (3  users).  One  user  commented,  “It’s  great! 
Sometimes I feel a bit alone when surfing the web. But real 
people talking to you about your problems make you feel like 
you are part of a huge family. You are no longer alone out 
there.”  Another  user  said,  “It  gives  a  feeling  of  personal 
attention and makes the user feel important.” 

Most users are not put off by unavailability of assistants. 
However,  some  users  are  likely  to  never  come  back  if  they 
find the support system deserted. Their motivation is that they 
would believe that the system was not working, as is currently 
the case with many systems on the web. 

The  support  system  can  be  used  for  different  purposes. 
The  support  system  not  only  attracts  users  who  need  help 

with a task that they cannot solve on their own. The system is 
also of value for more casual purposes, such as saving time or 
effort. 

User Questionnaire 2 
The results from the second user questionnaire are presented 
in Table 4. The percentages are calculated over the number of 
respondents (175). Some users gave more than one reason for 
not  making  use  of  a  human  assistant.  Many  users  gave 
alternative  reasons  according  to  option  G.  Most  of  these 
indicated  that  the  users  had  registered  out  of  need  but  then 
never  got  the  time  to  actually  try  the  support  system  during 
the three week period of the field study. 

From  the  results,  we  note  the  importance  of  an  easy-to-use 
support system. A user who cannot figure out how to use the 
support system is unlikely to maintain a positive view of the 
WIS. The results also show that the FAQ system was useful 
in answering several users’ questions. 

Table 4. Results from user questionnaire 2 

Reasons for not using a human assistant 

A. The system didn’t work. 

B. The FAQs solved the problem. 

C. There were no assistants logged in. 

D. I registered just to check things out. 

E. I didn’t understand how to use the system. 

F. The system took too long to load. 

G. Other reasons. 

 

18 (10%) 

26 (15%) 

50 (29%) 

67 (38%) 

24 (14%) 

20 (11%) 

24 (14%) 

Assistant Questionnaire 
The  following  information  is  a  summary  of  the  most 
important issues that came out of the assistant questionnaire. 
The  points  that  are  raised  follow  issues  in  the  open-ended 
the 
questions.  Notice 
that 
subjective  opinions  of 
their 
experience  during  the  field  study.  In  our  analysis  of  the 
questionnaires we employed a method similar to the one we 
described for analysing the statements in user questionnaire 1 
above. 

the  presentation  expresses 
the  assistants,  based  on 

Users  differ  in  conversational  style  and  in  background 
knowledge.  The  assistants  need  to  be  sensitive  to  these 
differences  in  order  to  adapt  appropriately.  For  example, 
some  users  want  quick  answers  to  their  questions,  while 
others want to discuss alternative solutions. A few users can 
be impolite. 

Users  generally  come  straight  to  the  point  with  their 
questions.  Sometimes  however,  follow-up  questions  are 
necessary to fully understand a user’s problem. 

Users explicitly express their opinion on the given advice. 
They generally tell the assistant what they think, and whether 
they are satisfied or not. 

fluency  and 

An  assistant  does  not  need  to  speak  English  as  a  first 
language.  Decent 
the 
terminology  of  the  domain  seems  to  be  enough.  Observe 
though  that  this  is  likely  dependent  on  the  application 
domain.  For  example,  in  a  banking  application  the  situation 
could be different. 

familiarity  with 

Being an assistant can cause stress. The following sources 
of  stress  were  identified:  Hardware  and  software  problems 
during help sessions. Having to multi-task to find information 
for  the  user  while  chatting.  Having  a  slow  or  unreliable 
network.  Impolite  users.  Having  difficulty  expressing  a 
solution in writing. 

The  most  important  skills  or  characteristics  for  an 
assistant  are  patience,  domain  knowledge,  social  skills, 
and fast typing. About one third of the responding assistants 
mentioned these skills. Several assistants also mentioned that 
it was important to be friendly, to have a sense of humor, and 
to be able to explain things in written words. 

Textual  chat  is  sufficient  as  a  means  of  communication. 
Also,  textual  chat  has  an  advantage  over  voice  chat  as  the 
history of the chat is easily accessible in textual form, which 
means  that  the  participants  do  not  have  to  take  notes  while 
chatting.  However,  in  some  cases  the  ability  to  graphically 
demonstrate things was missed in textual chat (mainly for art-
related topics).  

Handling more than one simultaneous help-session can be 
difficult for some assistants. Eleven assistants thought they 
could  handle  several  simultaneous  help  sessions,  while  ten 
thought they could not. Most assistants agree that their typing 
speed,  the  users’  conversation  styles,  and  the  kinds  of 
questions  affect  their  ability  to  handle  simultaneous  help 
sessions. Most assistants thought that three was the maximum 
number of possible simultaneous sessions. 

LIMITATIONS 
A  field  study  is  expected  to  allow  findings  that  can  be 
generalised  from  the field setting to other similar real-world 
settings.  The  drawback  is  that  there  might  be  alternative 
explanations  for the findings that were not eliminated in the 
design,  due  to  the  generally  uncontrolled  nature  of  a  field 
study.  In  this  section  we  discuss  such  limitations,  and  our 
approach to dealing with these. 

Users in our study might have been loyal to the Elfwood site 
and thus only willing to give positive feedback. We did our 
best to discourage such behavior by clearly pointing out that 
the  support  system  was  only  a  limited  time  experiment, and 
that all kinds of feedback, whether positive or negative, were 
welcome.  The  assistants  were  volunteers  with 
limited 
experience  in  helping  others.  Thus,  they  might  not  be 
representative  of  assistants  with  proper  experience  and 
education.  We  tried  to  make  the  best  of  the  situation  by 
providing 
the  help 
conversations, and by only selecting the volunteers who had 
proper  skills  within  some  area  of  expertise.  We  should  also 
note  that  the  participating  users  and  assistants  were  all  self-

instructions  on  how 

to  handle 

selected.  Still,  since  the  support  system  was  operating  in  a 
real environment and the users used the system purely out of 
need  and  curiosity,  the  usage  likely  reflects  real  usage.  A 
further limitation is that we cannot know if the persons who 
did  not  respond  to  the  questionnaires  differ  systematically 
from the respondents. 

It  is  reasonable  that  the  results  can  be  generalised  to  WISs 
similar to Elfwood. Such WISs include information providing 
sites  such  as  Intranet  systems  and  so-called  web  presence 
sites  (“sites  that  are  marketing  tools  designed  to  reach 
consumers outside the firm” [8, page 78]). Our study in step 1 
was  performed  in  an  electronic  commerce  setting  and  gave 
indications  similar  to  the  findings  reported  here  for  user 
attitude  [1].  However,  it  is  not  obvious  that  all  our  recent 
results are valid for electronic commerce sites. For example, 
when  it  comes  to  trust,  the  situation  for  users  is  clearly 
different  when  trust  in  the  advice  might  have  financial 
consequences.  Further  studies  should  be  carried  out  in 
electronic  commerce  contexts  where  users  need  to  commit 
themselves to potentially risky decisions. 

DISCUSSION 
Achieving universal usability is a very well-motivated albeit 
difficult objective. From our findings it is clear that humans 
have an important role to play in user support for WISs, and 
in  the  long  run,  for  achieving  universal  usability.  However, 
we  are  just  at  the  beginning  of  this  research,  and  there  are 
ample opportunities for further interesting work. 

We  have  shown  that  human  assistants  have  a  positive 
influence on user attitudes towards a web site. Still, the direct 
influence  of  these  attitude  variables  needs  to  be studied, for 
example  on  site  loyalty  for  different  kinds  of  WISs  such  as 
web  presence  sites or electronic commerce sites. While it is 
suggested  that  trust  has  a  positive  influence  on  customer 
loyalty  for  electronic  commerce  sites  [14],  there  are  still 
many open questions. 

statements.  Future  work 

Trust is a complex concept [17] and it is difficult to interpret 
our  results  on  trust,  due  to  the  simple  nature  of  the 
questionnaire 
should  more 
thoroughly investigate the effect of human assistants on users’ 
trust 
is 
playfulness, which can have a positive influence on a user’s 
learning,  mood,  involvement,  and  satisfaction  [22].  Do 
human assistants evoke users’ playfulness? 

in  a  WIS.  Another  concept  worth  studying 

The  three-week  duration  of  our  field  study  is  another 
limitation.  It  is  possible  that  usage  patterns  would  change 
over a longer time period. Further work is needed to study the 
longitudinal effects on this kind of user support system. 

The  study  by  Lohse  and  Spiller  [15]  suggests  that  customer 
service influences neither site traffic nor sales. However, the 
variables used in the study to represent customer service are 
limited (e.g. presence of a FAQ section, email support, or a 
phone  number  to  a  customer  service  department).  Thus,  the 
results  cannot  be  generalised  to  more  advanced  forms  of 
customer service, and further studies are needed to see what 

effects  the  form  of  customer  service  that we propose in this 
paper would have on the variables mentioned. 

In  [10]  the  effect  of  communication  modality  on  trust  and 
cooperation in online environments is studied. The results do 
not indicate a statistically significant difference between “no 
communication”  and  communication  via  textual  chat,  while 
voice  chat  has  a  significantly  better  effect  on  trust  and 
cooperation. This indicates that users’ trust for the support of 
human  assistants  could  be  further 
if  voice 
communication were to be used instead of textual chat. 

increased 

IMPLICATIONS 
The  implications  of  this  study  are  twofold.  First,  our results 
about  user  attitudes  and  the  efficiency  of  the  support  have 
implications  for  decision-makers  of  information  providing 
WISs  in  general.  The  kind  of  user  support  system  we  have 
studied  should  be  considered  as  an  option  whenever  user 
satisfaction is of importance. While our study has shown the 
technical  feasibility,  the  economical  feasibility  remains  an 
open  question.  The  expected  gain  from  the  efficient  user 
support  and  the  improved  user  attitudes  must  be  weighed 
against the cost for each individual application. 

Second, our study revealed a number of design implications, 
and we conclude with the following summary: 
•  Textual  chat  is  a  viable  means  for  communication 
between assistants and users. Depending on the domain, 
a text chat system may need to be accompanied by other 
communication options for special cases where text does 
not have sufficient expressive power, as is the case of art. 
•  Some users do not want to wait in a queue for the chance 
to talk to an assistant. Optimally, there should be enough 
assistants  to  avoid  having  users  wait.  Still,  if  a  queue 
system 
that  users  are 
continuously  updated  with  their  queue  position  and 
expected waiting time via audio as well as visually. This 
is  important  since  many  users  do  other  things  on  their 
computers  while  waiting,  and  might  not  observe  visual 
update  information.  Furthermore,  some  users  might  not 
think of the possibility of doing other things online while 
waiting  in  a  queue  and  a  hint  at  this  possibility  could 
benefit them. 

is  necessary,  make  sure 

•  The  efficiency  of  this  kind  of  support  system  is closely 
correlated  to  the  efficiency  of  the  assistants.  Assistants 
need  to  be  knowledgeable,  patient,  have  good  social 
skills, and type fast. 

•  Users differ in their knowledge and conversational style. 
Assistants need to be aware of this in order to give each 
user proper support. 

•  This kind of user support can very well be used for web 
sites with an international user base. It also works well to 
have  assistants  from  different  countries,  as  long  as  the 
assistants  have  decent  fluency  in  the  communication 
language. 

ACKNOWLEDGEMENTS 
This  work  has  been  partially  supported  by  TFR.  We  would 
like  to  thank  Johan  Lövdahl,  Jonas  Almfeldt,  and  Thomas 
Abrahamsson for their help with the field study. 

11.  Karat, 

J.  User-Centered 

Software  Evaluation 
Methodologies.  In  Helander,  M.,  Landauer,  P.,  and 
Prabhu,  P. 
(eds.)  Handbook  of  Human-Computer 
Interaction. Elsevier Science B.V., (1997), 689-704. 

REFERENCES 
1.  Aberg,  J.,  and  Shahmehri,  N.  The  role  of  human  Web 
assistants  in  e-commerce:  an  analysis  and  a  usability 
study. 
Internet  Research:  Electronic  Networking 
Applications and Policy 10, 2 (2000), 114-125. 

2.  Aberg, J., and Shahmehri, N. Collection and Exploitation 
of  Expert  Knowledge  in  Web  Assistant  Systems,  in 
Proceedings of the 34th Hawaii International Conference 
on System Sciences (2001). 

3.  Ackerman,  M.S.  Augmenting 

Memory:  A  Field  Study  of  Answer  Garden, 
Proceedings of CSCW’94 (1994), 243-252. 

the  Organizational 
in 

12.  Kobayashi,  M.,  Shinozaki,  M.,  Sakairi,  T.,  Touma,  M., 
Daijavad,  S.,  and  Wolf,  C.  Collaborative  Customer 
Services  Using  Synchronous  Web  Browser  Sharing,  in 
Proceedings of CSCW’98 (1998), 99-108. 

13.  Kraut, R., Scherlis, W., Mukhopadhyay, T., Manning, J., 
and Kiesler, S. The Home Net Field Trial of Residential 
Internet  Services.  Communications  of  the  ACM  39,  12 
(1996), 55-63. 

14.  Lee,  J.,  Kim,  J.,  and  Moon,  J.Y.  What  makes  Internet 
Users  visit  Cyber  Stores  again?  Key  Design  Factors  for 
Customer Loyalty, in Proceedings of CHI’00 (2000), 305-
312. 

4.  Ackerman, M.S., and McDonald, D.W. Answer Garden 
2:  Merging  Organizational  Memory  with  Collaborative 
Help, in Proceedings of CSCW’96 (1996), 97-105. 

15.  Lohse,  G.L.,  and  Spiller,  P.  Quantifying  the  Effect  of 
User Interface Design Features on Cyberstore Traffic and 
Sales, in Proceedings of CHI’98 (1998), 211-218. 

5.  Bouchard Jr., T.J. Field research methods: Interviewing, 
questionnaires, 
systematic 
observation,  unobtrusive  measures.  In  Dunnette,  M.D. 
(ed.)  Handbook  of 
Industrial  and  Organizational 
Psychology. Rand McNally, (1976), 363-413. 

observation, 

participant 

6.  Carroll, J.M., and McKendree, J. Interface design issues 
for advice-giving expert systems. Communications of the 
ACM 30, 1 (1987), 14-31. 

7.  Hoffman,  D.L.,  Novak,  T.P,  and  Peralta,  M.  Building 
Consumer  Trust  Online.  Commmunications  of  the  ACM 
42, 4 (1999), 80-85. 

8. 

9. 

Isakowitz,  T.,  Bieber,  M.,  and  Vitali,  F.  Web 
Information Systems. Communications of the ACM 41, 7 
(1998), 78-80. 

Jarvenpaa, S.L., and Todd, P.A. Consumer Reactions to 
Electronic  Shopping  on 
the  World  Wide  Web. 
International  Journal  of  Electronic  Commerce  1,  2 
(1997), 59-88. 

10.  Jensen, C., Farnham, S.D., Drucker, S.M., and Kollock, 
P.  The  Effect  of  Communication  Modality  on 
Cooperation  in  Online  Environments,  in  Proceedings  of 
CHI’00 (2000), 470-477. 

16.  Löwgren, 

J. 

Human-computer 

interaction. 

Studentlitteratur, Lund, Sweden, 1993. 

17.  Ratnasingham,  P.  The  importance  of  trust  in  electronic 
commerce.  Internet  Research:  Electronic  Networking 
Applications and Policy 8, 4 (1998), 313-321. 

18.  Rocco, E. Trust Breaks Down in Electronic Contexts but 
Can Be Repaired by Some Initial Face-to-Face Contact, in 
Proceedings of CHI’98 (1998), 496-502. 

19.  Shneiderman,  B.  Universal  Usability.  Communications 

of the ACM 43, 5 (2000), 85-91. 

20.  Spiller, P., and Lohse, G.L. A Classification of Internet 
International  Journal  of  Electronic 

Retail  Stores. 
Commerce 2, 2 (1998), 29-56. 

21.  Vijaysarathy,  L.R,  and  Jones,  J.M.  Print  and  Internet 
catalog 
Internet  Research:  Electronic 
Networking Applications and Policy 10, 3 (2000), 191-
202. 

shopping. 

22.  Webster,  J.,  and  Martocchio,  J.J.  Microcomputer 
playfulness:  Development  of  a  measure  with  workplace 
implications. MIS Quarterly 16, 2 (1992), 201-226. 

 

 

