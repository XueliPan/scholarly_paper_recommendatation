Modeling and Rendering of Quasi-Homogeneous Materials

Xin Tong

Jiaping Wang(cid:3)

Stephen Lin

Baining Guo

Heung-Yeung Shum

Microsoft Research Asia

(cid:3)CAS Institute of Computing Technology

Abstract
Many translucent materials consist of evenly-distributed heteroge-
neous elements which produce a complex appearance under differ-
ent lighting and viewing directions. For these quasi-homogeneous
materials, existing techniques do not address how to acquire their
material representations from physical samples in a way that allows
arbitrary geometry models to be rendered with these materials. We
propose a model for such materials that can be readily acquired
from physical samples. This material model can be applied to geo-
metric models of arbitrary shapes, and the resulting objects can
be ef(cid:2)ciently rendered without expensive subsurface light transport
simulation. In developing a material model with these attributes,
we capitalize on a key observation about the subsurface scattering
characteristics of quasi-homogeneous materials at different scales.
Locally, the non-uniformity of these materials leads to inhomoge-
neous subsurface scattering. For subsurface scattering on a global
scale, we show that a lengthy photon path through an even distri-
bution of heterogeneous elements statistically resembles scattering
in a homogeneous medium. This observation allows us to represent
and measure the global light transport within quasi-homogeneous
materials as well as the transfer of light into and out of a mate-
rial volume through surface mesostructures. We demonstrate our
technique with results for several challenging materials that exhibit
sophisticated appearance features such as transmission of back illu-
mination through surface mesostructures.

Keywords: subsurface scattering, re(cid:3)ectance and shading models,
rendering

1 Introduction

Many non-homogeneous translucent materials are composed of
various substances that are evenly distributed throughout its vol-
ume. These quasi-homogeneous materials present a formidable
challenge in realistic rendering because of their complex spatially
variant subsurface scattering properties [Hanrahan and Krueger
1993]. Furthermore, their appearance is often complicated by sur-
face mesostructures [Koenderink and Doorn 1996] that not only
produce surface re(cid:3)ections, but also affect how light enters and ex-
its a material volume. That computer graphics has still not been
able to convincingly render a slice of bread, as notably mentioned
by the late Alain Fournier [Fiume 2001], can largely be attributed
to such intricacies in material structure and optical properties.

Quasi-homogeneous materials, or any other non-homogeneous
can be adequately described by the bidirectional
material,
scattering-surface
re(cid:3)ectance-distribution function (BSSRDF)
[Nicodemus et al. 1977]. Unfortunately, acquiring a BSSRDF is
extremely dif(cid:2)cult. While a technique is available [Jensen et al.

(cid:3)This work was done while Jiaping Wang was a visiting student at Mi-

crosoft Research Asia.
Copyright © 2005 by the Association for Computing Machinery, Inc. 
Permission to make digital or hard copies of part or all of this work for personal or classroom 
use  is  granted  without  fee  provided  that  copies  are  not  made  or  distributed  for  commercial 
advantage and  that  copies bear  this notice and  the  full citation on  the  first  page. Copyrights 
for  components of  this work owned by others  than  ACM  must  be honored.  Abstracting with 
credit is permitted.  To  copy otherwise,  to  republish,  to post on  servers, or  to  redistribute  to 
lists,  requires  prior  specific  permission  and/or  a  fee.  Request  permissions  from  Permissions 
Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. 
© 2005 ACM 0730-0301/05/0700- 1054 $5.00 

Figure 1: A slice of bread rendered using a quasi-homogeneous ma-
terial model acquired from a physical sample. The geometric model
of the rendered slice was designed by an artist. Composed of many
evenly distributed cavities and ingredients, bread is a common ex-
ample of a quasi-homogeneous material.

2001] for measuring the BSSRDF of homogeneous materials, it is
not clear how to extend this technique to non-homogeneous mate-
rials. Another approach to modeling quasi-homogeneous materials
is to create synthetic volumetric models with subsurface scattering
properties. This, however, has proven to be a highly non-trivial
task [Chen et al. 2004]. Synthetic volumetric models furthermore
require expensive light transport simulation for rendering [Dorsey
et al. 1999; Pharr and Hanrahan 2000].

In this paper, we present a technique for modeling and rendering
quasi-homogeneous materials that is based on a material represen-
tation which can be readily acquired from physical samples. This
material representation can be applied to geometric models of ar-
bitrary shape, and the resulting objects can be ef(cid:2)ciently rendered
without expensive subsurface light transport simulation. To em-
phasize its broad applicability to arbitrary geometric models, we
call this representation a material model to distinguish it from ob-
ject models which are typically used in image-based modeling and
rendering of real-world objects (e.g., [Goesele et al. 2004]). A ma-
terial model describes how light is scattered by the material, while
an object model captures the actual appearance of a speci(cid:2)c phys-
ical object. Unlike a material model, an object model measured
from a given translucent object cannot be applied to objects of other
shapes, because the appearance of each surface point is not a local
quantity but the result of light propagation within the whole object.
The key observation behind our material model is that the sub-
surface scattering characteristics of quasi-homogeneous materials
can be effectively analyzed through examination at two different
scales. At a local level, the heterogeneity of a volume leads to non-
homogeneous subsurface scattering. At a larger scale, because of
the even distribution of materials within a quasi-homogeneous vol-
ume, small neighborhoods centered at different points in the volume

1054

have a statistically similar material composition and distribution.
For global subsurface scattering at this larger scale, photon prop-
agation is similar to that in a homogeneous material composed of
particles whose scattering properties approximate the overall scat-
tering of these volume neighborhoods.

Based on this observation of local heterogeneity and global ho-
mogeneity, we derive a material representation that consists of four
components: a homogeneous subsurface scattering component for
global light transport, a local re(cid:3)ectance function, a mesostruc-
ture entrance function, and a mesostructure exiting function. The
last three components account for local scattering effects that in-
clude inhomogeneous local subsurface scattering, surface re(cid:3)ec-
tions from mesostructure, and locally inhomogeneous mesostruc-
ture effects on light entering and exiting the material. The four
components of our model can be acquired by capturing both laser
stripe and halogen lamp images of a physical sample under multiple
illumination and viewing directions, and then breaking down these
images into appropriate components.

We demonstrate results for several challenging materials that ex-
hibit sophisticated appearance features, including bread as shown
in Fig. 1. Although some of these materials have been measured
as bidirectional texture functions (BTFs) in the CUReT database
[Dana et al. 1999], a BTF exhibits global light transport speci(cid:2)c
to the shape and size of the imaged sample, and consequently does
not represent light scattering within a volume of arbitrary geometry.
Moreover, since a BTF does not measure the transmission of back-
light through materials, a translucent object synthesized with BTFs
will exhibit incorrect rendering results, especially in places where
a signi(cid:2)cant amount of light leaves the material volume through
mesostructures. This problem is addressed in our model by the
mesostructure entrance and exiting functions.

In the remainder of the paper, we present our modeling and ren-
dering technique for quasi-homogeneous materials as follows. Sec-
tion 2 reviews related work. Our material model is described in
Section 3, and the system for its acquisition is outlined in Section
4. The rendering method for the material model is presented in Sec-
tion 5. Results are reported in Section 6, followed by conclusions
and discussion of future work in Section 7.

2 Related Work

Material Models: The most widely used material model in com-
puter graphics is the bidirectional re(cid:3)ectance distribution function
(BRDF), which was (cid:2)rst derived in [Nicodemus et al. 1977] as a
simpli(cid:2)cation of the BSSRDF. Since this simpli(cid:2)cation excludes
light transport in a material volume, the BRDF is intended for
homogeneous opaque materials. To model from physical mate-
rial samples, numerous techniques have been presented for mea-
suring BRDFs (e.g., [Ward 1992; Matusik et al. 2003]). For non-
homogeneous opaque materials, spatially-variant BRDFs are gen-
erally used, such as in [Gardner et al. 2003]. The BTF introduced
in [Dana et al. 1999] furthermore handles surface mesostructures,
and can be measured in a variety of ways (e.g., [Dana 2001; Han
and Perlin 2003]). The polynomial texture map (PTM) [Malzben-
der et al. 2001] also deals with spatially-variant re(cid:3)ectance and
mesostructures but only for a (cid:2)xed viewpoint. All BRDFs, BTFs,
and PTMs are not suitable for translucent materials because these
material models do not provide suf(cid:2)cient information on subsur-
face scattering. For example, the transmission of backlight through
materials is not captured.

Subsurface scattering was introduced to computer graphics by
[Hanrahan and Krueger 1993]. For homogeneous translucent mate-
rials, a practical subsurface light transport model was proposed in
[Jensen et al. 2001]. Concurrent work [Koenderink and van Doorn
2001] also studied translucent materials using a diffusion approx-
imation. For homogeneous material models, some ef(cid:2)cient ren-
dering techniques have been presented [Jensen and Buhler 2002;

(cid:5)(cid:2)

(cid:4)(cid:2)

(cid:1)(cid:2)

(cid:1)(cid:3)

(cid:5)(cid:3)

(cid:1)(cid:3)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(cid:1)(cid:2)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 2: Light scattering geometry for quasi-homogeneous mate-
rials.

Mertens et al. 2003; Hao et al. 2003; Lensch et al. 2003].

For non-homogeneous materials, early techniques [Dorsey et al.
1999; Pharr and Hanrahan 2000] create synthetic volumetric mod-
els and employ expensive light transport simulations for render-
ing. The recently proposed shell texture function (STF) [Chen
et al. 2004] gains ef(cid:2)ciency by partially precomputing light trans-
port near the material surface and approximating the material deep
within objects as a homogeneous core. Capturing STFs from real-
world materials, however, remains very dif(cid:2)cult. In [Chen et al.
2004], a material sample is segmented into homogeneous sub-
volumes according to its CT data, and the optical properties of each
sub-volume are obtained from tables of measured homogeneous
materials [Jensen et al. 2001]. Segmenting CT data is a well-known
challenge in medical imaging that is tedious to perform. Further-
more, few measurement tables are available, and some materials
such as marble exhibit variations in albedo that cannot be deter-
mined from tables.
Object Models: Image-based models of objects are easier to ac-
quire than material models, and naturally include subsurface scat-
tering, mesostructures and other appearance features. Because of
the high dimensionality of viewing and lighting conditions, these
methods record only a subset of possible object appearances. Sur-
face light (cid:2)elds [Wood et al. 2000; Chen et al. 2002] capture the
appearance of an object from different viewing angles under a (cid:2)xed
lighting environment, while re(cid:3)ectance (cid:2)elds [Debevec et al. 2000;
Masselus et al. 2003] represent an object from a (cid:2)xed viewpoint
with different lighting directions. Matusik et al. [2002a] present a
system for capturing re(cid:3)ectance (cid:2)elds of distant lighting from ar-
bitrary viewpoints. For speci(cid:2)cally acquiring transparent or highly
translucent objects, environment matting techniques have been pre-
sented to capture light refraction [Matusik et al. 2002b; Zongker
et al. 1999; Chuang et al. 2000].

For inhomogeneous translucent objects, the DISCO acquisition
system [Goesele et al. 2004] recovers a 4D subsurface scattering
function with respect to incoming and outgoing surface points,
while disregarding the directional dependence on lighting and view-
point. Surface geometries are assumed to be smooth for accurate in-
terpolation of global light transport. Because of its smooth surface
restriction, DISCO does not address the issue of correctly capturing
how light enters and leaves an object through mesostructures.

To avoid object dependence, we recover from images a mater-
ial model that can be employed with objects of arbitrary geometry.
Additionally, the transmission of light through mesostructures is
measured.

3 Material Representation

The bidirectional scattering-surface re(cid:3)ectance-distribution func-
tion (BSSRDF) provides a general model for light scattering from
surfaces, and from it we present our representation for quasi-
homogeneous materials. The outgoing radiance L(xo;wo) at a sur-
face point xo in direction wo can be computed by integrating the
contribution of incoming radiance L(xi;wi) for all incident direc-
tions wi over the surface A,

L(xo;wo) =

S(xi;wi; xo;wo)L(xi;wi)dwidxi:

ZA ZW

1055

Ll(xo;wo) =

S(xi;wi; xo;wo)L(xi;wi)dwidxi

Figure 3: Capture device of the material measurement system

Lg(xo;wo) =

S(xi;wi; xo;wo)L(xi;wi)dwidxi;

(1)

ZBo ZW

the incoming radiance L(xi;wi) undergoes signi(cid:2)cant scattering and
we wish to approximate its impact at xo by an averaging effect as
follows:

material can be formulated as

L(xo;wo) =

Here, dxi = (n (cid:1) wi)dA(xi), where (n (cid:1) wi) is the cosine factor and
dA(xi) is a differential area at xi. We separate the integral into local
and global contributions

L(xo;wo) =

S(xi;wi; xo;wo)L(xi;wi)dwidxi

+

S(xi;wi; xo;wo)L(xi;wi)dwidxi

where the local contribution is integrated over a small disk Ao of
radius rs around xo and the global contribution is aggregated from
the rest of the surface Bo = A (cid:0) Ao, illustrated in Fig. 2. Since the
incident radiance L(xi;wi) may be regarded as locally uniform over
Ao, the local contribution Ll(xo;wo) can be written as

ZAo ZW

ZBo ZW

ZAo ZW

ZW

(cid:25)

R(xo;wi;wo)L(xi;wi)dwi

where

R(xo;wi;wo) =

S(xi;wi; xo;wo)dxi

ZAo

is the local re(cid:3)ectance function.

For the global contribution

1
pr2

ZBo ZW

s (cid:20)ZAi

S(xi + ri;wi; xo;wo)dri

L(xi;wi)dwidxi

(cid:21)

where Ai is a disk of radius rs around xi and L(xi;wi) is locally
uniform over Ai. We can further average over Ao and obtain

S(xi + ri;wi; xo + ro;wo)dridro

L(xi;wi)dwidxi:

(cid:21)

1

p2r4

ZBo ZW
Notice that

s (cid:20)ZAi ZAo

ZAi ZAo

is the radiance contribution from area Ai to area Ao. Since a quasi-
homogeneous material is homogeneous at a large scale and is opti-
cally thick, we can apply the dipole diffusion approximation to this
area-to-area contribution:

ZAi ZAo

S(xi + ri;wi; xo + ro;wo)dridroL(xi;wi)

(cid:25) p2r4

s Fo(Ao;wo)Rd(xi; xo)L(xi;wi) f (wi)

where Rd(xi; xo) is the diffuse re(cid:3)ectance given by the dipole ap-
proximation, Fo(Ao;wo) is the average outgoing Fresnel term over
Ao in direction wo, and f (wi) is the mesostructure entrance function
of the material, which essentially represents an average product of
the incoming Fresnel term and cosine foreshortening factor over the
surface area. The mesostructure entrance function is expressed in-
dependently of location since its average effect within a local region
Ai is taken to be uniform over the material surface.

Based on the above analysis for a local region Ao, we know that

Lg(xo;wo) is proportional to the dipole diffusion term

Lg(xo;wo) (cid:181)

Rd(xi; xo)L(xi;wi) f (wi)dwidxi:

ZBo ZW

The above relation can be rewritten as

Lg(xo;wo) =

fv(xo;wo)Rd(xi; xo)L(xi;wi) f (wi)dwidxi (2)

ZBo ZW

1056

(cid:10)(cid:6)(cid:11) (cid:9)(cid:3)(cid:6)(cid:5)(cid:12)(cid:3)(cid:13)

(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:5)(cid:12)(cid:3)(cid:13)

(cid:14)(cid:6)(cid:19)(cid:9)(cid:3)(cid:5)(cid:20)(cid:13)(cid:6)(cid:4)(cid:5)(cid:21)(cid:4)(cid:15)(cid:18)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:1)(cid:6)(cid:7)(cid:8)(cid:9)

(cid:22)(cid:6)(cid:8)(cid:23)(cid:16)(cid:9)(cid:4)(cid:5)

(cid:14)(cid:6)(cid:11) (cid:24)

by introducing a mesostructure exiting function fv(xo;wo) that
modulates the average diffuse contribution of the dipole diffusion
term according to the view-dependent effects at xo, the local varia-
tion within Ao, and the outgoing Fresnel factor at xo. From Eq. (1)
and Eq. (2), the mesostructure exiting function can be expressed as

fv(xo;wo) =

W S(xi;wi; xo;wo)L(xi;wi)dwidxi
W Rd(xi; xo)L(xi;wi) f (wi)dwidxi
R
R

Bo

R
Bo

R

:

(3)

In summary, the outgoing radiance from a quasi-homogeneous

W R(xo;wi;wo)L(xi;wi)dwi
R
Bo

R

+

W fv(xo;wo)Rd(xi; xo)L(xi;wi) f (wi)dwidxi:
(4)
R
Based on this equation, we propose a model for quasi-homogeneous
materials that consists of
re-
(cid:3)ectance function R(xo;wi;wo), the mesostrcuture exiting func-
tion fv(xo;wo), the mesostructure entrance function f (wi), and the
global dipole term Rd(xi; xo). As we shall see, all of these compo-
nents can be measured from real materials.

four components:

the local

In measuring a material model, a laser beam could be used to illu-
minate a material sample, but its highly concentrated illumination
results in large intensity differences between specular radiance and
global scattering radiance. An extensive dynamic range presents an
impediment to accurate radiance measurement, as noted in [Goe-
sele et al. 2004]. To diminish this problem, we separately capture
global and local scattering using different types of lighting. Since
global scattering does not contain specular re(cid:3)ections and exhibits
an exponential falloff with increasing scattering distance, we em-
ploy a laser to heighten its visibility. Speci(cid:2)cally, we use a laser
stripe instead of a laser beam to signi(cid:2)cantly accelerate the cap-
ture process and substantially reduce the amount of image data.
Although a laser stripe does not illuminate a local circular area
as shown for Ai in Fig. 2, it can nevertheless be used for global
scattering measurements. In acquiring local scattering, we take a
BTF measurement approach and use lamps instead of lasers be-
cause their uniform illumination produces a relatively narrow dy-
namic range and reduces obscuration of texture details by specular
re(cid:3)ections.

4.1 Capture Device
Device setup: Shown in Fig. 3, our capture device consists of
four components(cid:150) a turntable, an imaging array, a lighting array
and a laser scan unit(cid:150) that are each controlled by a computer. The
cameras, lights and laser unit are mounted on two concentric arcs

S(xi + ri;wi; xo + ro;wo)dridroL(xi;wi)

4 Measurement System

centered around the turntable, which can be rotated by a stepping
motor. On the (cid:3)at surface of the turntable is placed the material
sample to be measured.

The imaging array consists of eight Dragon(cid:3)y cameras evenly
mounted on a stationary arc. The bottom camera, as well as the
bottom lamp, are not used in the capture process, because the top
surface of a thick sample is occluded from them. From each of the
24-bit color cameras, images of resolution 1024 (cid:2) 768 are acquired
at 3.75 Hz using different shutter speeds and the maximum aperture
setting to create high dynamic range (HDR) images.

The lighting array is mounted on the right arc and consists of
eight halogen lamps, which provides nearly directional light for
samples on the turntable. The light arc is driven by another stepping
motor that rotates it around the turntable center.

The laser scan unit contains one of three interchangeable 10mw
lasers (red:650nm, green:532nm, blue:473nm) with a line lens at-
tached to produce a laser stripe. In our current implementation, only
one laser at a time can be attached to the laser control unit, so im-
age measurement is iterated three times for the red, green and blue
lasers. The laser scan unit can be manually (cid:2)xed at any position on
the light arc, and a controller adjusts the orientation of the laser to
sweep a stripe over the surface of the sample.

Device calibration: For geometric calibration, the origin of our
coordinate system is set at the center of the turntable, with the XY
plane aligned with the (cid:3)at surface. Before capture, we calibrate the
position of the light sources manually as in [Bouguet and Perona
1998]. The intrinsic and extrinsic parameters of the cameras are
calibrated using the method described in [Zhang 1999].

For photometric calibration, we (cid:2)rst determine the response
curves of the cameras using the method in [Mitsunaga and Nayar
1999]. Then for relative color calibration of the cameras, we place
a standard color pattern on the turntable and capture images from
each camera with light from a given lamp. To calibrate lamp illumi-
nation colors, we conversely capture an image from a given camera
for lamp.

To calibrate the laser stripes, we compute the angle between the
laser plane, de(cid:2)ned as the plane containing the laser stripe and laser
emission point, and the XY plane. This is done by projecting a laser
stripe onto two planes of different heights and calculating the offset
of the laser stripe in the two planes from two recorded laser scan
images. From this offset, we can derive the angle between the laser
plane and the XY plane. Since the distance of the laser emitter from
the turntable is much larger than the material sample diameter, we
regard all the laser planes in one scan as being parallel.

4.2 Material Model Acquisition
Image data is measured using the laser stripe and lamp illumina-
tion in the process outlined in Fig. 4. From the acquired data set,
appearance components that correspond to the elements of our ma-
terial model are separated to enable inverse rendering of dipole dif-
fusion parameters and measurement of the texture functions.
In
the laser stripe images, global subsurface scattering is clearly re-
vealed, but its appearance is in(cid:3)uenced by mesostructure entrance
and exiting functions. To decouple these re(cid:3)ectance components,
our technique takes advantage of their distinct directional depen-
dencies:
the mesostructure entrance function depending on light
direction, the mesostructure exiting function on view direction, and
dipole diffusion being directionally independent. Subtracting the
global components from the halogen lamp images leaves the lo-
cal re(cid:3)ectance function, which consists of local non-homogeneous
subsurface scattering and mesostructure surface re(cid:3)ections.

In the image capture procedure, all laser images are recti(cid:2)ed onto
the reference plane of the material model, de(cid:2)ned as the top surface
of the sample, which is taken to be globally (cid:3)at but can contain lo-
cal mesostructures. In the recti(cid:2)cation process, pixels on the laser

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:4)(cid:4)(cid:9)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:6)(cid:5)(cid:7)(cid:8)(cid:3)(cid:4)(cid:9)(cid:2)(cid:6)(cid:2)(cid:3)(cid:7)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:6)(cid:5)(cid:7)(cid:8)(cid:3)(cid:4)(cid:10)(cid:2)(cid:7)(cid:11)(cid:12)(cid:11)(cid:2)(cid:13)(cid:7)(cid:4)(cid:2)(cid:13)(cid:4)(cid:5)(cid:3)(cid:9)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:6)(cid:11)(cid:14)(cid:15)(cid:12)(cid:4)(cid:5)(cid:3)(cid:9)(cid:4)(cid:3)(cid:2)(cid:12)(cid:5)(cid:12)(cid:11)(cid:2)(cid:13)(cid:7)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:10)(cid:3)(cid:2)(cid:16)(cid:8)(cid:9)(cid:12)(cid:4)(cid:6)(cid:5)(cid:7)(cid:8)(cid:3)(cid:4)(cid:7)(cid:12)(cid:3)(cid:11)(cid:10)(cid:4)(cid:2)(cid:13)(cid:4)(cid:7)(cid:5)(cid:17)(cid:10)(cid:6)(cid:8)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:8)(cid:5)(cid:9)(cid:15)(cid:4)(cid:6)(cid:5)(cid:7)(cid:8)(cid:3)(cid:4)(cid:7)(cid:9)(cid:5)(cid:13)(cid:4)(cid:10)(cid:2)(cid:7)(cid:11)(cid:12)(cid:11)(cid:2)(cid:13)(cid:4)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:9)(cid:5)(cid:10)(cid:12)(cid:18)(cid:3)(cid:8)(cid:4)(cid:11)(cid:17)(cid:5)(cid:14)(cid:8)(cid:7)(cid:4)(cid:1)(cid:3)(cid:2)(cid:17)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:9)(cid:5)(cid:17)(cid:8)(cid:3)(cid:5)(cid:7)

(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:6)(cid:7)(cid:8)(cid:4)(cid:4)(cid:9)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:6)(cid:5)(cid:7)(cid:8)(cid:3)(cid:4)(cid:9)(cid:2)(cid:6)(cid:2)(cid:3)(cid:7)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:12)(cid:18)(cid:3)(cid:13)(cid:12)(cid:5)(cid:19)(cid:6)(cid:8)(cid:4)(cid:10)(cid:2)(cid:7)(cid:11)(cid:12)(cid:11)(cid:2)(cid:13)(cid:7)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:10)(cid:3)(cid:2)(cid:16)(cid:8)(cid:9)(cid:12)(cid:4)(cid:12)(cid:15)(cid:8)(cid:4)(cid:6)(cid:5)(cid:7)(cid:8)(cid:3)(cid:4)(cid:7)(cid:12)(cid:3)(cid:11)(cid:10)(cid:4)(cid:20)(cid:8)(cid:3)(cid:12)(cid:11)(cid:9)(cid:5)(cid:6)(cid:6)(cid:21)(cid:4)(cid:2)(cid:13)(cid:4)(cid:7)(cid:5)(cid:17)(cid:10)(cid:6)(cid:8)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:8)(cid:5)(cid:9)(cid:15)(cid:4)(cid:6)(cid:5)(cid:7)(cid:8)(cid:3)(cid:4)(cid:7)(cid:9)(cid:5)(cid:13)(cid:4)(cid:10)(cid:2)(cid:7)(cid:11)(cid:12)(cid:11)(cid:2)(cid:13)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:9)(cid:5)(cid:10)(cid:12)(cid:18)(cid:3)(cid:8)(cid:4)(cid:11)(cid:17)(cid:5)(cid:14)(cid:8)(cid:7)(cid:4)(cid:1)(cid:3)(cid:2)(cid:17)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:9)(cid:5)(cid:17)(cid:8)(cid:3)(cid:5)(cid:7)

(cid:16)(cid:17)(cid:2)(cid:3)(cid:15)(cid:6)(cid:7)(cid:8)(cid:4)(cid:4)(cid:9)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:6)(cid:11)(cid:14)(cid:15)(cid:12)(cid:4)(cid:5)(cid:3)(cid:9)(cid:4)(cid:3)(cid:2)(cid:12)(cid:5)(cid:12)(cid:11)(cid:2)(cid:13)(cid:7)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:6)(cid:5)(cid:17)(cid:10)(cid:7)(cid:4)(cid:2)(cid:13)(cid:4)(cid:12)(cid:15)(cid:8)(cid:4)(cid:5)(cid:3)(cid:9)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:12)(cid:18)(cid:3)(cid:13)(cid:12)(cid:5)(cid:19)(cid:6)(cid:8)(cid:4)(cid:3)(cid:2)(cid:12)(cid:5)(cid:12)(cid:11)(cid:2)(cid:13)(cid:7)
(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:9)(cid:5)(cid:10)(cid:12)(cid:18)(cid:3)(cid:8)(cid:4)(cid:11)(cid:17)(cid:5)(cid:14)(cid:8)(cid:7)(cid:4)(cid:1)(cid:3)(cid:2)(cid:17)(cid:4)(cid:5)(cid:6)(cid:6)(cid:4)(cid:9)(cid:5)(cid:17)(cid:8)(cid:3)(cid:5)(cid:7)

Figure 4: Pseudocode for image acquisition in the capture system.

stripe are identi(cid:2)ed according to a prescribed intensity threshold,
and to these pixels a line is (cid:2)t. We do not use HDR images in the
recti(cid:2)cation process. The obtained set of recti(cid:2)ed images I(p; v; l)
record for incoming (cid:3)ux from laser stripe l the measured radiance
in direction v from 3D location p(x; y) on the reference plane, where
p lies within a user-speci(cid:2)ed distance interval from l such that radi-
ance from p results from global scattering and has an intensity that
is suf(cid:2)ciently above the camera noise level.

The measured material sample is assumed to be optically thick
such that a negligible amount of light passes through the bottom
surface. As generally done in BTF capture systems, only a central
portion of the sample is measured to avoid edge effects such as light
leakage through the sides.

Mesostructure Entrance Function:
In the (cid:2)rst step of the
measurement process, we scan the sample with laser stripes emit-
ted from different light directions. As described in the (cid:2)rst pass
of Fig. 4, we mount the laser at several positions along the light
arc. For each position, we rotate the light arc to sampled angles and
capture the laser scan images at sampled viewing directions.

For a laser stripe projected from the vertical direction, we set its
mesostructure entrance function value to one. From other incident
light directions, the mesostructure entrance function is computed in
relation to the vertical direction as

f (wi) = (cid:229)
v

(cid:229)
p

(cid:229)
l2wi

I(p; v; l)

(cid:229)
v

(cid:229)
p

(cid:229)
l2w0

,

I(p; v; l)

where the numerator aggregates images with laser lines from direc-
tion wi, and the denominator sums images with laser lines from the
vertical direction w0. Although images from a single view are suf-
(cid:2)cient to solve for this function, we utilize several sparse viewing
samples (4 (cid:2) 4 in our implementation) for greater robustness.

Dipole Di(cid:11)usion Parameters: As described in the second pass
of Fig. 4, laser scan images from different viewing directions and
under vertically projected illumination are captured to compute the
dipole diffusion parameters. After we rotate the turntable to its de-
sired sampling position, the light arc is rotated accordingly so that
the sample is always scanned along the same direction. The appear-
ance of the sample is captured by all cameras simultaneously.

To (cid:2)t the global dipole model to this data set, we utilize the ap-
proximation given in [Jensen et al. 2001] which sets the refraction
index to 1.3 in dipole (cid:2)tting. Although this value may not be exact,
it also works well for the materials we captured. The global dipole
term can then be determined from two scattering properties:
the

1057

reduced albedo a0 and reduced extinction coef(cid:2)cient s0
t . Directly
(cid:2)tting these two parameters to measured data has proven to be ill-
conditioned [Jensen and Buhler 2002]. Instead, we (cid:2)rst measure
the total diffuse re(cid:3)ectance R of the material sample and derive the
reduced albedo a0. Then we (cid:2)t the reduced extinction coef(cid:2)cient
s0
t from the captured images.
In our case, the total diffuse re(cid:3)ectance is the ratio between the
outgoing (cid:3)ux of the sample surface and incoming (cid:3)ux of a vertical
laser stripe, expressed for stripe l from direction w0 as

I(p; v; l) = R

(cid:229)
p

(cid:229)
v

k1
k0

f

where k1 is the laser stripe length on the sample, f is the laser (cid:3)ux
measured from a reference surface of known re(cid:3)ectance with laser
stripe length k0. Also we assume the intensity of the laser strip to
be constant along the line. The effect of the mesostructure exiting
function is effectively removed by summing corresponding image
pixels from the different viewpoints. For a more robust estimate, we
capture multiple laser stripe projections at different positions and
then average the resulting diffuse re(cid:3)ectances. With the total diffuse
re(cid:3)ectance, we solve for the reduced albedo a0 of the material as in
[Jensen and Buhler 2002].

With the derived reduced albedo a0, the material’s reduced ex-
tinction coef(cid:2)cient s0
t can be solved by (cid:2)tting the dipole model
in [Jensen et al. 2001] to the measured data. In (cid:2)tting the dipole
model, we sum the same image set over the viewing directions to
eliminate mesostructure exiting function effects from the measured
data:

I(p; l) = (cid:229)
v

I(p; v; l) = (cid:229)
p02l

Rd(p; p0)

f
k0

:

We additionally integrate the outgoing radiance of all points pd

that are a distance d from the laser line to obtain the function:

(5)

(6)

I(d) = (cid:229)
pd

(cid:229)
p02l

Rd(pd; p0)

f
k0

:

Since the double summation in Eq. (6) has no closed-form solution,
we take the approach of [Gardner et al. 2003] and create a virtual
version of the laser stripe with the same incoming (cid:3)ux k1f
and ren-
k0
der ID(d) from it using Eq. (6) and s0
t values sampled from 0.0 to
10.0, which covers most of the materials in which we are interested.
Then we take the s0
t value whose solution ID(d) to Eq. (6) most
closely matches the measured I(d) according to kID(d) (cid:0) I(d))k2.
To ef(cid:2)ciently determine the s0
in our current implementation, a
t
coarse-to-(cid:2)ne search is employed. The effectiveness of this ap-
proach is demonstrated in Fig. 5, where the blue points indicate
I(d) values measured from a real sponge, and the red curve repre-
sents ID(d) values computed with the dipole approximation from
the recovered values of a0 and s0
t .

Mesostructure Exiting Function: From the image set cap-
tured for estimating the dipole parameters, the mesostructure ex-
iting function can be solved using the dipole model in Eq. (5) as

fv(p;wo) = (cid:229)
l

I(p; v; l)

I(p; l);

(cid:229)
l

,

which essentially represents the image residual after factoring out
global dipole diffusion. We note that since multiple scattering in
global light transport is directionally independent, Eq. (3) can in
practice be determined independently of incident light direction.

Local Re(cid:13)ectance Function:
In the last step, we capture halo-
gen lamp images of the material sample with different lighting di-
rections and viewpoints, as outlined in the third pass of Fig. 4. The

Figure 5: Comparison of dipole diffusion intensities measured from
sponge data (blue), and values computed from recovered scattering
values (red). The green bar represents the user-speci(cid:2)ed range used
to estimate the dipole diffusion parameters.

acquired images contain both local and global lighting effects. To
obtain an LRF that represents only local scattering, we compute
the global component from the determined dipole parameters and
mesostructure entrance/exiting functions, then subtract this global
component from the captured data:

R(xo;wi;wo) = I(xo;wi;wo)(cid:0) (cid:229)
xi6=xo

fv(xo;wo)Rd(xi; xo)L(xi;wi) f (wi):

Although the dipole diffusion model is not intended to model light
transport through inhomogeneous local volumes, we allow some
portion of the local scattering to be represented by it, instead of
being fully modelled in the LRF. This avoids the need to specify
the extent of area Bo in Eq. (4). Subtracting the dipole component
in the local area may lead to negative values, but these are not errors
and are used to obtain the correct rendering result. For any negative
values in the (cid:2)nal rendering result due to noise or measurement
errors, we simply truncate them to zero before tone mapping. Since
the sampled laser and lamp positions are not identical due to the
structure of our device, mesostructure entrance function values at
the lamp directions are bilinearly interpolated from the four nearest
laser directions.

5 Rendering

As a material model, the quasi-homogeneous material acquired
from real samples can be easily applied to arbitrary objects. For
this purpose, we reorganize the captured data as a 2D texture T (x; y)
de(cid:2)ned on the reference plane. Each texel T (xi; yi) consists of the
re(cid:3)ectance function values and mesostructure exiting function at
(xi; yi). Similar to the method described in [Liu et al. 2004], we syn-
thesize the texture T (x; y) onto the target surface mesh while main-
taining consistency in surface appearance under different lighting
and viewing directions. The scattering properties are directly as-
signed to the object for simulating global light transport within its
volume.

Quasi-homogeneous objects formed from this material model are
straightforward to render. To evaluate the radiance of a surface
point xo under a given lighting condition, we (cid:2)rst attentuate the in-
coming radiance on object surface according to the mesostructure
entrance function. The diffuse light transport beneath the surface
is then evaluated using the dipole approximation and modi(cid:2)ed by
the mesostructure exiting function texture mapped onto the surface.
For the local scattering effects, we directly integrate the local re-
(cid:3)ectance function on xo over the incoming radiance at xo. As a
result, both appearance variations over the object surface and the
scattering effects inside the object volume are evaluated with our
material model.

1058

In addition, our system was used to recover the scattering properties
of whole milk, for which it obtained measured values that are in
close agreement with those reported in [Jensen et al. 2001].

We integrated the quasi-homogeneous material model into a ray
tracer, where the global light transport effect is computed by the di-
pole approximation as in [Jensen and Buhler 2002]. The rendering
results of the different models under local and global illuminations
are shown in Figs. 9, 10, 11, and 12.

In Fig. 9, we compare our rendering of an acquired sponge block
model to renderings with BTF data and with a combination of BTF
data and a dipole diffusion model. A high dynamic range envi-
ronment map is used for illumination. We note that the appear-
ance of the sponge under backlighting is convincingly generated
by the quasi-homogenous material model but cannot be accurately
produced with a BTF. The sponge material model is also used in
rendering the bunny in Fig. 10 under different viewing and lighting
directions.

Fig. 11 displays rendering results of a teapot modelled with rice-
cake material under different lighting and viewing conditions. Note
that both the global light transport effects, which are exhibited on
the boundary of the teapot body and in the shadowed area, and local
lighting effects are well captured and rendered.

Fig. 12 shows the rendering result of bread slices under different
lighting and viewing directions. The bread is modelled with the ac-
quired material model, while the crust of the bread and the plate are
shaded with a conventional BRDF model and color texture maps.

7 Discussion and Conclusion

The basis of our quasi-homogeneous material model is its differ-
ence in subsurface scattering properties at local and global scales.
The complexity of local scattering due to material heterogeneity
leads to a couple of assumed conditions in rendering an object made
with the material. One is that the scale of mesh geometry variations
exceeds the scale of local material heterogeneity, such that all light
transport effects due to object shape are approximately modelled by
dipole diffusion. Another rendering condition is illumination uni-
formity in a local area, which arises from our BTF representation of
local re(cid:3)ectance functions. For illumination to be locally uniform,
the distance of illuminants from the object must be much larger
than the radius of a local heterogeneous area. With an assumption
of locally uniform illumination, sharp changes in incident illumina-
tion will cause some inaccuracy in our result. This problem could
be addressed by pre(cid:2)ltering irradiance on the surface, or measuring
point response functions within local areas.

An assumed condition in the capture process is that the material
sample is optically thick enough so that a negligible amount of light
exits through the bottom of the sample. In our current implemen-
tation, this transmitted light is absorbed with a black cloth beneath
the sample and is ignored. Since transmitted light is not accounted
for in our measurement system, it may lead to some inaccuracies in
our acquired material model.

In future work, we plan to investigate techniques for accelerating
our rendering method to obtain real-time performance. Several ef-
fective compression techniques have previously been presented for
texture data [Chen et al. 2002; Mueller et al. 2003; Liu et al. 2004],
and we also intend to examine how our material model data can be
compressed to facilitate its use.

Acknowledgements
The authors would like to thank Yanyun Chen and Kyros Kutulakos
for helpful discussions, and Kun Xu for his help in capturing and
processing data. The bread slice was modelled by Mindong Xie.
The authors are grateful to the anonymous reviewers for their help-
ful suggestions and comments.

Figure 6: Acquired physical samples. Top row: Lamp images; Mid-
dle row: Mesostructure exiting functions; Bottom row: Local re-
(cid:3)ectance functions.

To render the LRF, existing hardware-accelerated BTF rendering
methods (e.g., [Sloan et al. 2003; Suykens et al. 2003; Mueller et al.
2003; Liu et al. 2004]) can be utilized, while an ef(cid:2)cient dipole al-
gorithm such as in [Jensen and Buhler 2002; Lensch et al. 2003] can
be used to compute global light tranport. With these methods, the
local and global contributions can be rapidly computed and added
together to give the rendering result.

6 Experimental Results

We implemented the measurement system described in Section 4
and captured several material samples. System speci(cid:2)cations in
capturing the texture function components of a material model are
listed in Table 1. Data processing is performed on a PC with a
3.2GHZ Intel Xeon CPU and 4GB memory. Measurement of the
dipole term occurs in about one minute, and it takes about four
hours for image recti(cid:2)cation and HDR image construction.

Fig. 6(a) shows an image of all the acquired samples illuminated
by a halogen lamp, without color calibration. Figs. 6(b) and 6(c)
show their corresponding mesostructure exiting functions and lo-
cal re(cid:3)ectance functions under speci(cid:2)c viewing and lighting direc-
tions. We note that the mesostructure exiting function exhibits spa-
tial variations that correspond to the local re(cid:3)ectance function. The
scattering properties of the captured materials are shown in Table 2.
To validate our material model, we created a synthetic vol-
ume shown in Fig. 7 composed of two materials with the fol-
lowing scattering properties in the RGB color bands: a0 =
(0:4248; 0:4248; 0:99) and s0
t = (1:0; 1:5; 2:13)mm(cid:0)1 for one ma-
t = (1:5; 1:5; 1:5)mm(cid:0)1
terial, and a0 = (0:98; 0:98; 0:98) and s0
for the other. From rendered images of this synthetic volume,
we simulated the measurement process and recovered the follow-
ing dipole diffusion parameters: a0 = (0:8224; 0:8125; 0:9853) and
s0
t = (1:1701; 1:29605; 1:53456)mm(cid:0)1. With the acquired material
model, we generated its appearance under a new lighting condition
as shown in Fig. 7. Slight blurring exists in our rendered result
due to interpolation of the mesostructure exiting function, but our
model nevertheless provides a good approximation to direct render-
ing from the original synthetic volume.

For real materials whose images contain noise and registration
errors, the quasi-homogeneous material model can also be acquired
and rendered with accurate results, as shown for a sponge in Fig. 8.
Fig. 5 demonstrates the measurement performance for this example.

1059

Table 1: Capture System Speci(cid:2)cations

Mesostructure Mesostructure

Local

view resolution
light resolution
image resolution

(cid:2)nal data size

capturing time
processing time

entrance
function

N/A
12 (cid:2) 7

256 (cid:2) 256

1KB

3.0h
0.5h

exiting
function
12 (cid:2) 7
N/A

256 (cid:2) 256

63MB

1.2h
0.5h

re(cid:3)ectance
function
12 (cid:2) 7
12 (cid:2) 7

256 (cid:2) 256

5.2GB

2.0h
0.5h

Table 2: Captured Dipole Diffusion Parameters

Reduced albedo a0

Reduced extinction s0

t [mm(cid:0)1]

Sponge
Bread

RiceCake

(0.9985,0.9971,0.7577)
(0.9805,0.9569,0.9179)
(0.9986,0.9766,0.8400)

(1.6397, 1.5927, 1.3894)
(0.9175,0.9355,0.8961)
(0.8554, 0.8394, 0.7943)

References

BOUGUET, J.-Y., AND PERONA, P. 1998. 3d photography on your desk. In Proc. Int.

Conf. on Computer Vision, 43(cid:150)50.

CHEN, W.-C., BOUGUET, J.-Y., CHU, M. H., AND GRZESZCZUK, R. 2002. Light
(cid:2)eld mapping: Ef(cid:2)cient representation and hardware rendering of surface light
(cid:2)elds. ACM Trans. on Graphics 21, 3, 447(cid:150)456.

CHEN, Y., TONG, X., WANG, J., LIN, S., GUO, B., AND SHUM, H.-Y. 2004. Shell

texture functions. ACM Trans. on Graphics 23, 3, 343(cid:150)353.

CHUANG, Y.-Y., ZONGKER, D. E., HINDORFF, J., CURLESS, B., SALESIN, D. H.,
AND SZELISKI, R. 2000. Environment matting extensions: towards higher accu-
racy and real-time capture. In Proc. SIGGRAPH 2000, 121(cid:150)130.

DANA, K. J., VAN GINNEKEN, B., NAYAR, S. K., AND KOENDERINK, J. J. 1999.
Re(cid:3)ectance and texture of real-world surfaces. ACM Trans. on Graphics 18, 1,
1(cid:150)34.

DANA, K. J. 2001. Brdf/btf measurement device. In Proc. Int. Conf. on Computer

Vision, vol. 2, 460(cid:150)466.

DEBEVEC, P., HAWKINS, T., TCHOU, C., DUIKER, H.-P., SAROKIN, W., AND
In Proc.

SAGAR, M. 2000. Acquiring the re(cid:3)ectance (cid:2)eld of a human face.
SIGGRAPH 2000, 145(cid:150)156.

DORSEY, J., EDELMAN, A., LEGAKIS, J., JENSEN, H. W., AND PEDERSEN, H. K.
In Proc. SIGGRAPH 1999,

1999. Modeling and rendering of weathered stone.
225(cid:150)234.

FIUME, E. 2001. Dedication to Alain Fournier. In Proc. SIGGRAPH ’01, 10.

GARDNER, A., TCHOU, C., HAWKINS, T., AND DEBEVEC, P. 2003. Linear light

source re(cid:3)ectometry. Proc. SIGGRAPH ’03, 749(cid:150)758.

GOESELE, M., LENSCH, H. P. A., LANG, J., FUCHS, C., AND SEIDEL, H.-P. 2004.
Disco: acquisition of translucent objects. ACM Trans. on Graphics 23, 3, 835(cid:150)844.

HAN, J. Y., AND PERLIN, K. 2003. Measuring bidirectional texture re(cid:3)ectance with

a kaleidoscope. ACM Trans. on Graphics 22, 3, 741(cid:150)748.

HANRAHAN, P., AND KRUEGER, W. 1993. Re(cid:3)ection from layered surfaces due to

subsurface scattering. In Proc. SIGGRAPH 1993, 165(cid:150)174.

Figure 7: Ground truth comparison with a synthetic material. Left:
Rendered from a captured material model; Right: Rendered from
ground truth synthetic data.

Figure 8: Ground truth comparison with a real sponge. Left: Ren-
dered from a captured material model; Right: Actual photograph.

LIU, X., HU, Y., ZHANG, J., TONG, X., GUO, B., AND SHUM, H.-Y. 2004. Syn-
thesis and rendering of bidirectional texture functions on arbitrary surfaces. IEEE
Trans. on Visualization and Computer Graphics 10, 3, 278(cid:150)289.

MALZBENDER, T., GELB, D., AND WOLTERS, H. 2001. Polynomial texture maps.

Proc. SIGGRAPH 2001, 519(cid:150)528.

MASSELUS, V., PEERS, P., DUTR (cid:30)E, P., AND WILLEMS, Y. D. 2003. Relighting with

4d incident light (cid:2)elds. ACM Trans. on Graphics 22, 3, 613(cid:150)620.

MATUSIK, W., PFISTER, H., NGAN, A., BEARDSLEY, P., ZIEGLER, R., AND
MCMILLAN, L. 2002. Image-based 3d photography using opacity hulls. In Proc.
SIGGRAPH ’02, 427(cid:150)437.

MATUSIK, W., PFISTER, H., ZIEGLER, R., NGAN, A., AND MCMILLAN, L. 2002.
In Proc. Euro-

Acquisition and rendering of transparent and refractive objects.
graphics Workshop on Rendering, 267(cid:150)278.

MATUSIK, W., PFISTER, H., BRAND, M., AND MCMILLAN, L. 2003. A data-driven

re(cid:3)ectance model. ACM Trans. on Graphics 22, 3, 759(cid:150)769.

MERTENS, T., KAUTZ, J., BEKAERT, P., SEIDEL, H.-P., AND REETH, F. V. 2003.
In Proc. Eurographics

Interactive rendering of translucent deformable objects.
Workshop on Rendering, 130(cid:150)140.

MITSUNAGA, T., AND NAYAR, S. 1999. Radiometric self calibration.

In Proc.

Computer Vision and Pattern Recognition, 1374(cid:150)1380.

MUELLER, G., MESETH, J., AND KLEIN, R. 2003. Compression and real-time
rendering of measured BTFs using local PCA. In Proc. Vision, Modeling and Vis.

NICODEMUS, F. E., RICHMOND, J. C., HSIA, J. J., GINSBERG, I. W., AND
LIMPERIS, T. 1977. Geometrical Considerations and Nomenclature for Re-
(cid:3)ectance. National Bureau of Standards (US).

PHARR, M., AND HANRAHAN, P. M. 2000. Monte Carlo evaluation of non-linear
scattering equations for subsurface re(cid:3)ection. In Proc. SIGGRAPH 2000, 275(cid:150)286.

HAO, X., BABY, T., AND VARSHNEY, A. 2003. Interactive subsurface scattering for

SLOAN, P.-P., LIU, X., SHUM, H.-Y., AND SNYDER, J. 2003. Bi-scale radiance

translucent meshes. In ACM Symposium on Interactive 3D Graphics, 75(cid:150)82.

transfer. ACM Trans. on Graphics 22, 3, 370(cid:150)375.

JENSEN, H. W., AND BUHLER, J. 2002. A rapid hierarchical rendering technique for

translucent materials. In Proc. SIGGRAPH 2002, 576(cid:150)581.

SUYKENS, F., VOM BERGE, K., LAGAE, A., AND DUTR ·E, P. 2003.

Interactive

rendering with bidirectional texture functions. In Proc. Eurographics 03.

JENSEN, H. W., MARSCHNER, S. R., LEVOY, M., AND HANRAHAN, P. 2001. A
practical model for subsurface light transport. In Proc. SIGGRAPH 2001, 511(cid:150)518.

GRAPH ’92, 265(cid:150)272.

WARD, G. J. 1992. Measuring and modeling anisotropic re(cid:3)ection. In Proc. SIG-

KOENDERINK, J. J., AND DOORN, A. J. V. 1996. Illuminance texture due to surface

mesostructure. Journal of the Optical Society of America 13, 3, 452(cid:150)463.

KOENDERINK, J., AND VAN DOORN, A. 2001. Shading in the case of translucent

objects. Proceedings of SPIE 4299, 312(cid:150)320.

LENSCH, H. P. A., GOESELE, M., BEKAERT, P., KAUTZ, J., MAGNOR, M. A.,
LANG, J., AND SEIDEL, H.-P. 2003. Interactive rendering of translucent objects.
Computer Graphics Forum 22, 2, 195(cid:150)206.

WOOD, D., AZUMA, D., ALDINGER, W., CURLESS, B., DUCHAMP, T., SALESIN,
D., AND STUETZLE, W. 2000. Surface light (cid:2)elds for 3D photography. Proc.
SIGGRAPH 2000, 287(cid:150)296.

ZHANG, Z. 1999. Flexible camera calibration by viewing a plane from unknown

orientations. In Proc. Int. Conf. on Computer Vision, 666(cid:150)673.

ZONGKER, D. E., WERNER, D. M., CURLESS, B., AND SALESIN, D. H. 1999.

Environment matting and compositing. In Proc. SIGGRAPH 1999, 205(cid:150)214.

1060

Figure 9: A thin sponge with back illumination. Left: Using a quasi-homogeneous material model; Center: Using a BTF; Right: Using a
combination of a BTF and a dipole diffusion model for subsurface scattering.

Figure 10: A sponge bunny under different lighting and viewing directions.

Figure 11: A teapot made of rice cake, shown under different lighting and viewing directions. Global light transport is exhibited near the
silhouettes of the teapot and in the shadowed areas.

Figure 12: Slices of bread under different lighting and viewing directions. The bread is modelled with an acquired material model, while the
crust of the bread and the other objects in the scene are shaded with a conventional BRDF model and color texture maps.

1061

