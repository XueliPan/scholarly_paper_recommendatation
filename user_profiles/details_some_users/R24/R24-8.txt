Paciﬁc Graphics 2009
S. Lee, D. Lischinski, and Y. Yu
(Guest Editors)

Volume 28 (2009), Number 7

Edit Propagation on Bidirectional Texture Functions

Kun Xu1†

Jiaping Wang2

Xin Tong2

Shi-Min Hu1

Baining Guo2

1 Tsinghua National Laboratory for Information Science and Technology
and Department of Computer Science and Technology, Tsinghua University

2 Microsoft Research Asia

Abstract
We propose an efﬁcient method for editing bidirectional texture functions (BTFs) based on edit propagation
scheme. In our approach, users specify sparse edits on a certain slice of BTF. An edit propagation scheme is
then applied to propagate edits to the whole BTF data. The consistency of the BTF data is maintained by prop-
agating similar edits to points with similar underlying geometry/reﬂectance. For this purpose, we propose to use
view independent features including normals and reﬂectance features reconstructed from each view to guide the
propagation process. We also propose an adaptive sampling scheme for speeding up the propagation process.
Since our method needn’t any accurate geometry and reﬂectance information, it allows users to edit complex
BTFs with interactive feedback.

Categories and Subject Descriptors (according to ACM CCS): Computer Graphics [I.3.7]: Color, shading, shadow-
ing, and texture —

1. Introduction

Faithfully modeling and rendering surface appearance is
important for many graphics applications. For surfaces
with geometric details and material variations, Dana et. al.
[DvGNK99] proposed six dimensional Bidirectional Tex-
ture Functions (BTFs) that directly capture their appearances
with respect to 2D positions, 2D viewing directions and 2D
lighting directions. Nearly all surface appearance effects,
such as self-occlusion, self-shadowing and reﬂectance, can
be well modeled by BTF. Despite recent advances in BTF
capturing, synthesis and rendering, editing BTF data is still
a challenging task. Since points appear in different views
may correspond to the same point on the meso-structure sur-
face, to maintain appearance consistency in BTF editing, ed-
its on one point should be automatically propagated to those
points in other views which are essentially the same under-
lying meso-structure point. Without explicit geometry infor-
mation, it is a non-trivial task to do such propagation.

Inspired by recent edit propagation methods on spatially
varying materials [LFUS06, PL07, AP08], we propose a

This work was done when Kun Xu was an visiting student at Mi-

†

crosoft Research Asia.

c(cid:2) 2009 The Author(s)
Journal compilation c(cid:2) 2009 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

method for editing BTF data based on edit propagation
scheme. In our approach, users specify sparse edits on a slice
of BTF data under a certain view. These edits are then au-
tomatically propagated to other views. The consistency of
the BTF data is maintained by propagating similar edits to
BTF data with similar underlying geometry/reﬂectance. The
similarity between different points is deﬁned by view inde-
pendent features including normal and reﬂectance features
reconstructed from each view. Thus, points that correspond
to the same meso-structure surface point but appear in dif-
ferent views would have the same view-independent features
and thus receive the same edits. Since our method does not
rely on an explicit geometry, BTFs with complex geometric
structures, even non-height-ﬁeld geometry, could be edited
by our approach.

As in AppProp [AP08], we solve the propagation prob-
lem with a linear system deﬁned by a dense afﬁnity matrix
of millions of elements. We also apply the column sampling
technique to approximate afﬁnity matrix for efﬁciency. In-
stead of randomly sampling columns, we propose an adap-
tive column sampling method which is demonstrated to pro-
duce a better approximation.

Following prior works [LFUS06, PL07, AP08], we prop-
agate edit parameters rather than the ﬁnal appearance val-

Kun Xu, et. al. / Edit Propagation on Bidirectional Texture Functions

ues, which makes the propagation process independent of
the edit operator being applied. We have tested various BTF
edit operators proposed by Kautz et al. [KBD07], includ-
ing hue change, tone adjustment, angular blur, local frame
change and shadow removal. To preserve the rich effects of
BTF, we directly edit the raw BTF data rather than editing
an approximated representation of BTF data. Furthermore,
our framework is extensible. 3D position could be added as
an additional view independent feature if the geometry of
BTF is available, and new edit operators can also be easily
integrated in.

In the context of recent works on edit propagation [AP08,
PL07] and BTF editing [KBD07, MSK07], we see this work
make the following contributions:
• We propose a novel method for editing BTFs based on
view independent features and edit propagation scheme.
In our approach, users only need to specify sparse edits
and those edits would be automatically propagated to the
whole data based on edit propagation scheme, and appear-
ance consistency is automatically maintained during edit-
ing.

• Our method enables editing of BTFs with complex ge-
ometric structures, e.g. non-height-ﬁeld geometry, which
could not be achieved by previous BTF editing methods.
• We propose an adaptive column sampling method for ap-
proximating dense afﬁnity matrix which is demonstrated
to be more efﬁcient than randomly sampling.

2. Related Work

∗

∗

∗

∗

06,GTR
∗
06,GTR

Interactive Material Editing: With the advance of appear-
ance acquisition techniques, editing captured spatially vary-
ing materials receives more and more attention in recent
06,LBAD
06,PL07,AP08]. In partic-
years [WTL
∗
06] demonstrated editing time varying
ular, [WTL
effects of spatially varying BRDFs interactively. Lawrence
06] factorized spatially varying BRDFs into
et al. [LBAD
products of low dimensional functions in a hierarchical way,
each of which can be easily edited by user. Recently, opti-
mization based methods have been proposed for edit propa-
gation. In these methods, users specify rough or sparse edits
on some regions, then an optimization problem is set up and
solved to propagate those edits to the whole dataset. These
methods include image colorization [LLW04], tone adjust-
ment [LFUS06] and material editing [PL07, AP08]. All of
these propagation schemes work on an explicit domain (2D
image plane) that every point on the domain is essentially
different from each other. Differently, we propagate on the
domain of the underlying meso-structure surface that is im-
plicitly determined by reference plane location and viewing
directions of BTFs.

BTF Editing: A lot of works have been done for BTF
acquisition [DvGNK99, SSK03, KMBK03, HP03, ND06]
05], render-
,compression [SSK03, KMBK03, VT04, WWS
05] and synthesis
ing [SSK03, SVLD03, MMK04, WTS

∗

∗

∗

[TZL
02, KMBK03, HH05]. A comprehensive survey of
∗
05]. Recently, Kautz
these works could be found in [MMS
et al. [KBD07] proposed a set of edit operators for BTFs
and further developed an interactive BTF editing system.
They directly manipulated on the raw BTF data and uti-
lized an out-of-core scheme to achieve interactive perfor-
mance. To maintain appearance consistency, the height ﬁeld
reconstructed from the BTF data is applied to warp the BTF
data under different views. Unfortunately, it is difﬁcult to
apply this approach to BTF captured from non-height-ﬁeld
geometry. Based on texture synthesis techniques, Müller et
al. [MSK07] proposed a procedural method for editing the
spatial pattern of BTFs. Different from their method, we
focus on editing material property of BTFs. Besides, in-
stead of assuming a height-ﬁeld meso-structure geometry,
our method propose to use view independent features to pre-
serve appearance consistency. Since view independent fea-
tures can be easily and robustly recovered from BTFs, our
method can be conveniently applied for editing BTFs with
complex geometry.

3. Edit Propagation on BTF

The 6D BT F(x, o, i) is an image-based representation that
is deﬁned on reference plane location x, viewing direction
o and lighting direction i. Editing the surface reﬂectance of
the BTF is essentially changing the material properties of the
points on the underlying meso-structure surface. However,
for raw BTF data whose underlying geometry is unknown,
we cannot directly use the underlying meso-structure surface
as editing domain. Instead, we deﬁne our editing domain on
spatial-view domain (x, o) which combines both the refer-
ence plane location and viewing direction. The underlying
meso-structure surface can be represented by spatial-view
domain implicitly and recovery of the meso-structure ge-
ometry is thus avoided. The BTF data could be treated as
2D light reﬂectance ﬁeld b(i) deﬁned in 4D spatial-view do-
main:

(x, o) → bx,o(i) = BT F(x, o, i).

(1)

In our framework, user gives some edits on sparse sam-
ples in the spatial-view domain. Typically, the edits are di-
rectly speciﬁed on the top view. Then, the user given edits
are propagated to the whole spatial-view domain. However,
since points appear in different views may correspond to the
same underlying point on the meso-structure surface, their
appearances should keep consistent during propagating . An
naive approach is to deﬁne similarity based on the L2 dis-
tance between light reﬂectance ﬁelds b(i) and to use this
similarity to guide propagation. However, this approach fails
because light reﬂectance ﬁelds b(i) from the same meso-
structure surface point varies with viewing directions due to
specular reﬂection. Thus the similarity of light reﬂectance
ﬁelds doesn’t indicate the similarity of material property cor-
rectly. The appearance consistency in the BTF data may be

c(cid:2) 2009 The Author(s)
Journal compilation c(cid:2) 2009 The Eurographics Association and Blackwell Publishing Ltd.

Kun Xu, et. al. / Edit Propagation on Bidirectional Texture Functions

they should not change with viewing direction. For this pur-
pose, we choose normals and diffuse colors as view in-
dependent features. The view independent features are ex-
tracted as a pre-process step. Normal and diffuse colors
are recovered from the light reﬂectance ﬁelds at each point
in the spatial-view domain using the classical photometric
stereo method [RTG97]. In particular, we exclude the shad-
owed values and the specular values and use the remain-
ing values to recover normal and diffuse. Values are de-
termined as shadowed/specular if they are too dark/bright,
e.g. smaller/larger than a predeﬁned threshold. Note that
the classical photometric stereo method is robust for BTFs
with lambertain surfaces, but it cannot work well for BTFs
with highly specular reﬂectance, highly subsurface scatter-
ing or strong occlusion, since it breaks the lambertain re-
ﬂectance assumption. In addition, if the underlying 3D ge-
ometry could be recovered, e.g. as height-ﬁeld, we would
include the 3D position as an additional view independent
feature. In this case, we use the shape from shadow tech-
niques [DD98] to recover a height-ﬁeld geometry. Note that
the view independent features are only used to guide the
propagation process, the ﬁnal edits are directly applied to
the raw BTF data rather than an approximated representa-
tion deﬁned by view independent features. Therefore, we do
not need a perfectly accurate reconstruction of view inde-
pendent features.

For denotation simplicity, in the remaining parts of the pa-
per, we use subscript i, j to refer to points on the 4D spatial-
view domain(x, o). The similarity zi j between two points is
deﬁned as:

zi j = exp

−

(cid:2)

(cid:3)
(cid:3)ni − n j
σ2
n

(cid:4)

(cid:3)
(cid:3)2

(cid:2)

exp

−

(cid:4)

(cid:3)
(cid:3)2

(cid:3)
(cid:3)di − d j
σ2
d

(2)

It is the product of two Gaussian kernels of the distances
between normal ni and diffuse color di, controlled by de-
viation parameters σn and σd, respectively. How to set the
values of σn and σd depends on user intention. The smaller
σn(σd) is, the more precise propagation would depend on
normal(diffuse color). In our experiments, for high dynamic
range BTFs, σd is usually set around 0.2 to 0.4; σn is usually
set around 0.2 to 0.4 for normalized normals. Besides, if the
underlying geometry is available, we could add 3D position
as an additional view-independent feature to be accounted in
similarity computation between points. In this case, zi j could
be rewritten as:

zi j = exp

−

(cid:2)

(cid:3)
(cid:3)ni − n j
σ2
n

(cid:4)

(cid:3)
(cid:3)2

(cid:2)

exp

−

(cid:3)
(cid:3)di − d j
σ2
d

(cid:4)

(cid:3)
(cid:3)2

(cid:2)

exp

−

(cid:3)
(cid:3)pi − p j

(cid:3)
(cid:3)2

(cid:4)

(3)

σ2
p

Figure 1: The pipeline of our method. First, we extract view
independent features from the input BTF data; Next, we
propagate user given edit parameters in the spatial-view do-
main under the guidance of view independent features; Fi-
nally, we apply the desired edit operator according to prop-
agated edit parameters.

broken after editing. In our framework, we deﬁne similar-
ity on view independent features extracted from the light
reﬂectance ﬁelds. The view independent features, such as
diffuse color, indicate material property and do not change
with viewing direction. In this way, points that correspond
to the same meso-structure surface point but appear in dif-
ferent views would have the same view independent feature
and thus receive the same edits.

The basic pipeline of our work is illustrated in Figure 1.
Given the input BTF data, view independent features are
ﬁrst extracted (Section 3.1) as a pre-process. Then, user
could specify desired edits with our sketch-based user in-
terface at a certain slice of BTF data, e.g. top view. Af-
ter that, our system automatically propagate user edits to
the whole BTF data based on the similarity of view inde-
pendent features. The details of propagation algorithm are
described in Section 3.2. Following prior edit propagation
works [LFUS06,PL07,AP08], we propagate edit parameters
rather than the ﬁnal appearance values to make the propaga-
tion process independent of the certain edit operator being
applied.

3.1. View Independent Features

In our approach, the view independent features are used
to deﬁne similarity between points in the spatial-view do-
main. To maintain appearance consistency between views,

where pi is the position of point i, σp is the corresponding
deviation parameter. It could be set from 0.05 to 5.0 in our
experiments.

c(cid:2) 2009 The Author(s)
Journal compilation c(cid:2) 2009 The Eurographics Association and Blackwell Publishing Ltd.

Kun Xu, et. al. / Edit Propagation on Bidirectional Texture Functions

3.2. Edit Propagation

Following the AppProp edit framework on spatially varying
materials [AP08], given user speciﬁed edit parameters g, to
solve for the propagated edit parameters e, we formulize the
propagation process as an optimization problem minimizing
the following energy function:

∑i ∑ j w jzi j(ei − g j)2 + λ∑i ∑ j zi j(ei − e j)2

(4)

where the sums are calculated over all points on the 4D
spatial-view domain. The energy function consists of two
terms. λ controls the relative contributions of the two terms,
and is set to ∑ j w j/n to make the contributions of the two
terms almost same, and n is the total number of points on
the 4D spatial-view domain. The ﬁrst term accounts for the
constraint of user input, where g denotes user speciﬁed edit
parameters and w is edit strength which is one for user edited
points and zero for non-edited points. The second term ac-
counts for the propagation scheme that ensures similar edits
are applied to points with similar view independent features.
The similarity zi j between point i and j is deﬁned in Equa-
tion 2.

Our formulation is different from AppProp [AP08] in two
aspects. First, we propagate edits in the 4D spatial-view do-
main instead of a 2D image domain. Secondly, we use view-
independent features instead of appearance and image pixel
position to deﬁne similarity between points.

Algorithm: We utilize the method in AppProp [AP08] to
minimize the energy function in Equation 4. To make our
paper self-contained, we brieﬂy review AppProp’s solution.
Please refer to their original paper for detailed derivations.
Since the energy function in Equation 4 is quadratic, opti-
mizing it is equivalent to solving a linear system:

(D − Z)e = 1

2λ ZW g ⇒ e = 1
2λ

(D − Z)−1ZW g

(5)

where e,g are respectively the vectors of edit parameters
ei and user speciﬁed parameters gi, and Z is the afﬁn-
ity matrix of zi j, D is a diagonal matrix with Dii = di =
(1 + w j/(2λ))zi j, W is a diagonal matrix whose element
∑ j
is the edit strength Wii = wi.

The dense afﬁnity n × n matrix Z is further approximated
by a low rank matrix approximation. By selecting m columns
(m (cid:4) n) of matrix Z, denote U as the n×m matrix composed
of the m selected columns, and A as the m × m afﬁnity matrix
of the m selected points, we have:

Z ≈ UA

−1U T

With this approximation, the diagonal elements d of diago-
nal matrix D could be approximated by:

(cid:5)

d ≈

1
2λUA

(cid:6)

−1U T W +UA

−1U T

1n

(6)

(7)

where 1n is a length-n vector of ones. Finally, edit param-
eters e in Equation 5 could be approximated by applying

3. End.

Figure 2: Some results of propagated edit parameters. The
left column indicates user speciﬁed edit parameters. The 2-
th to 5-th columns show BTF slices and propagated edit
parameters under different views. The ﬁrst row edits on a
synthetic brick BTF with parameters σn = σd = 0.4; the
second row edits on a synthetic lawn BTF with parame-
ters σn = 0.2, σd = 0.4. the third row edits on a captured
peanut BTF with an additional position feature extracted
from reconstructed height-ﬁeld, and the used parameters are
σn = 1.5, σd = 0.25, σp = 0.1.

Woodbury Formula:

e ≈ 1

2λ (D
·(UA

−1 − D
−1U T )W g

−1U(−A +U T D

−1U)−1U T D

−1)

(8)
The bottleneck lies on the matrix-matrix calculation of
U T D

−1U whose complexity is O(m2n).

Adaptive Column Sampling: In AppProp [AP08], they
approximate matrix Z by stratiﬁed randomly sampling
columns. Intuitively, with a given column number m, a
more sophisticated column sampling scheme such as sin-
gular value decomposition would approximate Z better, but
would require the knowledge of the full matrix Z and need
much more computation. Noticing that matrix Z is an afﬁn-
ity matrix, which means that the i-th column and the j-th
column would be similar if zi j is close to 1, we propose a
more efﬁcient adaptive column sampling method:

1. Randomly select a column from Z, suppose it is the i-
th column. Remove all the j-th columns from Z if the
afﬁnity zi j is larger than a given threshold;

2. If the number of selected columns is already m or there is

no columns left, go to Step 3; otherwise go to Step 1;

c(cid:2) 2009 The Author(s)
Journal compilation c(cid:2) 2009 The Eurographics Association and Blackwell Publishing Ltd.

Kun Xu, et. al. / Edit Propagation on Bidirectional Texture Functions

the blending weight between original shadowed color and
ﬁlled color. We have applied this operator to the synthetic
block BTF. As shown in Figure 6(b), shadow is removed af-
ter editing.

Local Frame Change: At each spatial view point, we
linearly blend the target normal with the original normal
weighted by edit parameter ei to obtain a rotated local frame.
Next, we search in the 2D spatial slice of the rotated view
direction, and return the pixel position j with the best view-
independent feature match. Then, we modify the light re-
ﬂectance ﬁeld bi(i) by looking up original value in bj(i) us-
ing rotated light directions. We have applied this operator to
the synthetic dot BTF, as shown in Figure 8(c).

Angular Blur: At each spatial view point, we apply a
spherical Gaussian kernel G in light direction space to the
light reﬂectance data bi(i):

(cid:7)

bnew
i

(i) = (bi

G)(i)

(9)

where the radius of Gaussian kernel G is controlled by edit
parameter e. We have applied this operator to the synthetic
dot BTF and the pulli BTF. As shown in Figure 8(b) and
Figure 9(b), the edited area looks less specular.

Tone Adjustment: At each spatial-view point, we adjust
the tonal value of the light reﬂectance data bi(i) by linearly
interpolating the original tonal value with a user given target
tonal value using editing parameter ei as a weight. We have
applied this operator to the measured peanut BTF data. As
shown in Figure 10, after editing, some peanuts look from
fresh to cooked. Notice the difference between Figure 10(b)
and Figure 10(c). In Figure 10(b), with the help of additional
position feature, we only edited 2 selected peanuts. In Fig-
ure 10(c), without position feature, we achieved global edit-
ing effects by changing all the peanuts.

Comparison With Kautz et al. [KBD07]: We make a
comparison between our method and the method of Kautz et
al. [KBD07] on editing the lawn BTF data in Figure 7. The
ﬁrst row shows our result, and the second row is Kautz’s
result. From left to right are slices of BTF from views of
increasing polar angles. Our method keeps consistency in
all views, as shown in Figures 7(a)-(d). However, Kautz’s
method fails to preserve consistency when view polar an-
gle is large, as shown in Figures 7(g)-(h). This is not sur-
prising because the lawn BTF contains a highly complex
non-height-ﬁeld geometry structure, which makes Kautz’s
method fail to determine correct view correspondences by a
reconstructed top-view height ﬁeld.

Failure Case: Figure 11 gives an example of failure case.
The left image gives the top view of the BTF and input
strokes. In this example, user gives a white stroke in a ver-
tical strip and a black strop in a horizontal strip, whose in-
tention is to select the vertical strips and then apply edits to
them. However, since this BTF is almost ﬂat and the diffuse
color is similar among the whole data, the propagation pro-

Figure 3: Column sampling scheme comparison. It plots the
relative RMS error of propagated edit parameters on the
brick BTF using different number of columns and different
parameters.

In our implementation, we set the threshold as 0.9 and ﬁnd
it works well. Since the time complexity of adaptive column
sampling method is O(mn), the additional cost to utilize this
sampling scheme could be ignored. We have compared the
proposed adaptive column sampling with stratiﬁed random
sampling using different number of columns and different
parameters σn and σd in Figure 3. We ﬁnd that the adaptive
column sampling only needs about one-fourth number of
columns compared to stratiﬁed random sampling with simi-
lar approximation error. Since the calculation complexity is
O(m2n), this would lead to a speed-up of 16 times.

4. Results and Implementation

4.1. Edit Operators

After solving the propagated edit parameters, we allow users
to adjust the desired edit operator being applied to the BTF
data, whose editing strength is controlled by the propa-
gated edit parameters. We have tested various edit operators
proposed by Kautz et al. [KBD07], including hue change,
shadow removal, local frame change, angular blur and tone
adjustment. The BTFs used in our experiment come from
Bonn BTF database [SSK03] and Wang et al. [WTS

05].

∗

Hue Change: At each spatial-view point, we ﬁrst trans-
form the light reﬂectance ﬁeld bi(i) from RGB to HSV color
space, then we linearly blend the original hue channel with a
target hue channel using edit parameter ei as a weight while
keeping other two channels unchanged. We have applied this
operator to the synthetic lawn BTF, the synthetic brick BTF
and the measured pulli BTF. As shown in Figure 4(b), the ap-
pearance of the grass is changed from green to withered like.
As shown in Figure 5(b), one side of the brick is changed to
yellow-green. As shown in Figure 9(c), a strip of the pulli
BTF is changed to green color. For the pulli BTF, we recon-
struct a height-ﬁeld geometry and use position as an addi-
tional feature.

Shadow Removal: In this operator, ﬁrst, points are de-
termined as shadow areas if the color value is less than a
given threshold and determined shadow areas are ﬁlled us-
ing the color value computed by reconstructed reﬂectance
and normal feature at that point. Secondly, we adjust the av-
erage brightness and saturation of the shadow area to make
it match nearby regions. Edit parameter e is used to control

c(cid:2) 2009 The Author(s)
Journal compilation c(cid:2) 2009 The Eurographics Association and Blackwell Publishing Ltd.

Kun Xu, et. al. / Edit Propagation on Bidirectional Texture Functions

Figure 4: Editing of the synthetic lawn BTF. (a) The original
BTF; (b) Hue component changed.

Figure 8: Editing of the synthetic dot BTF. (a) The origi-
nal BTF; (b) Less specular by angular blur; (c) Local frame
changed.

Figure 5: Editing of the the synthetic brick BTF. (a) The
original BTF; (b) Hue component changed.

Figure 9: Editing of the measured cloth-pulli BTF. (a) The
original BTF; (b) Less specular by angular blur (with addi-
tional position feature); (c) Hue component changed (with
additional position feature).

Figure 6: Editing of the synthetic block BTF. (a) The original
BTF; (b) Shadow removed.

Figure 7: Comparison between our method and Kautz’s
method.

cess fails to separate the horizontal and vertical strips (As
shown in the middle and the right images).

4.2. Implementation

Interactive Editing Preview: To support a fast iterative
workﬂow, it is important to provide the user with fast vi-
sual feedback in editing. In order to provide fast visual feed-
back, we map the BTF to a sphere and render it under direc-
tional lighting. Instead of propagating edit parameters to the
whole 4D spatial-view domain, we only propagate edits to
those spatial-view points which are used in rendering. Thus,
the size of the propagation domain and the cost to evaluate
Equation 8 are largely reduced. This increased efﬁciency al-
lows user an interactive rendering preview of edited BTF.

Performance: Our system is implemented on a consumer
level PC with Intel Core2Duo 2.33G CPU and 4G RAM. The
performance is reported in Table 1. The memory cost mainly
lies on storing the n × m matrix U. In our experiment, the
largest data is the pulli data with resolution n = 81 ∗ 2562,

Figure 10: Editing of the measured peanut-box BTF. (a) The
original BTF; (b) Tone adjustment (with additional position
feature); (c) Tone adjustment.

Figure 11: An example of failure.
peanut

lawn

dot

60 × 60 × 642

60 × 60 × 1282

96 × 96 × 1282

81 × 81 × 2562

BTF
Res.
Col.
Operator
Memory
Prev.T.
Prop.T.
Edit.T.

50

Ang.Blur

47M
0.1s
0.4s
30s

100
Color
375M
0.5s
9.2s
6.1s

100
Tone
600M
0.8s
22s
16s

pulli

100
Color

2G
2.0s
60s
42s

Table 1: Performance. Each row lists: the resolution of the
BTF data; the number of columns selected; the edit operator
applied; memory cost for edit propagation; execution time
for different stages of the algorithm including edit preview,
edit propagation and the actual applied edit operation.

and we choose m = 100 columns. Storing matrix U (32-
bit ﬂoating-point precision) would cost about 2G space and
could be ﬁt in main memory. For even larger BTF data,
since the calculations involving U in Equation 7 and 8 only
contain matrix-matrix and matrix-vector multiplications, it
is straight-forward to implement an out-of-core scheme by
splitting U into sub-matrices.

c(cid:2) 2009 The Author(s)
Journal compilation c(cid:2) 2009 The Eurographics Association and Blackwell Publishing Ltd.

Kun Xu, et. al. / Edit Propagation on Bidirectional Texture Functions

5. Conclusion

We proposed an efﬁcient method for editing material prop-
erties of bidirectional texture functions (BTFs) based on edit
propagation scheme. In our method, users specify sparse ed-
its on a certain slice of BTF, and those edits would be auto-
matically propagated to the whole BTF data, while keeping
appearance consistency between different views. Since our
method does not rely on explicit geometry, users are allowed
to edit BTF with complex geometric structures, e.g. non-
height-ﬁeld geometry. Besides, our method is extensible and
allows users to easily add new edit operators and new view
independent features to guide propagation. We also proposed
an adaptive column sampling method which is demonstrated
to produce a better approximation for dense afﬁnity matrix
than randomly sampling and is expected to be useful in im-
age edit propagation or other high-dimensional data editing.

Currently, Our method limits to edit uncompressed BTFs.
Since most compressed representations are linear approxi-
mation of the original high dimensional BTF datas, it is un-
clear how to map propagation process and the nonlinear edit
operators to the compressed representation. In the future, we
would like to explore techniques to directly edit on a com-
pressed representation. Besides, how to extend our method
to edit geometric meso-structures of BTFs is another inter-
esting topic.

Acknowledgement: We would like to thank the anony-
mous reviewers for their valuable comments and insight-
ful suggestions. This work was supported by the Na-
tional Basic Research Project of China (Project Number
2006CB303106), the Natural Science Foundation of China
(Project Number U0735001) and the National High Technol-
ogy Research and Development Program of China (Project
Number 2009AA01Z329). Kun Xu is also supported by the
Microsoft Fellowship.

References

[AP08] AN X., PELLACINI F.: Appprop: all-pairs appearance-

space edit propagation. ACM Trans. Graph. 27, 3 (2008), 1–9.

[DD98] DAUM M., DUDEK G.: On 3-d surface reconstruction
In Proceedings of CVPR (1998),

using shape from shadows.
pp. 461–468.

[DvGNK99] DANA K. J., VAN GINNEKEN B., NAYAR S. K.,
KOENDERINK J. J.: Reﬂectance and texture of real-world sur-
faces. ACM Trans. Graph. 18, 1 (1999), 1–34.

[GTR∗06] GU J., TU C.-I., RAMAMOORTHI R., BELHUMEUR
P., MATUSIK W., NAYAR S.: Time-varying surface appearance:
acquisition, modeling and rendering. ACM Trans. Graph. 25, 3
(2006), 762–771.

[HH05] HAINDL M., HATKA M.: Btf roller. In Proceedings of
the 4th International Workshop on Texture Analysis and Synthesis
(2005), pp. 89–94.

[HP03] HAN J. Y., PERLIN K.: Measuring bidirectional tex-
ture reﬂectance with a kaleidoscope. ACM Trans. Graph. 22,
3 (2003), 741–748.

c(cid:2) 2009 The Author(s)
Journal compilation c(cid:2) 2009 The Eurographics Association and Blackwell Publishing Ltd.

[KBD07] KAUTZ J., BOULOS S., DURAND F.: Interactive edit-
ing and modeling of bidirectional texture functions. ACM Trans.
Graph. 26, 3 (2007), 53.

[KMBK03] KOUDELKA M., MAGDA S., BELHUMEUR P.,
KRIEGMAN D.: Acquisition, compression and synthesis of bidi-
rectional texture functions.
In Proceedings of the 3rd Inter-
national Workshop on Texture Analysis and Synthesis (2003),
pp. 59–64.

[LBAD∗06] LAWRENCE J., BEN-ARTZI A., DECORO C., MA-
TUSIK W., PFISTER H., RAMAMOORTHI R., RUSINKIEWICZ
S.: Inverse shade trees for non-parametric material representa-
tion and editing.
In SIGGRAPH ’06: ACM SIGGRAPH 2006
Papers (New York, NY, USA, 2006), ACM, pp. 735–745.

[LFUS06] LISCHINSKI D., FARBMAN Z., UYTTENDAELE M.,
SZELISKI R.: Interactive local adjustment of tonal values. ACM
Trans. Graph. 25, 3 (2006), 646–653.

[LLW04] LEVIN A., LISCHINSKI D., WEISS Y.: Colorization
using optimization. ACM Trans. Graph. 23, 3 (2004), 689–694.

[MMK04] MESETH J., MÜLLER G., KLEIN R.: Reﬂectance ﬁeld
based real-time, high-quality rendering of bidirectional texture
functions. In Computers & Graphics 28, 1 (2004), pp. 105–112.
[MMS∗05] MULLER G., MESETH J., SATTLER M., SARLETTE
R., KLEIN R.: Acquisition, synthesis, and rendering of bidirec-
tional texture functions. Computer Graphics Forum 24, 1 (2005),
83–109.

[MSK07] MÜLLER G., SARLETTE R., KLEIN R.: Procedural
editing of bidirectional texture functions. In Proceedings of Eu-
rographics Symposium on Rendering (2007), The Eurographics
Association, pp. 219–230.

[ND06] NGAN A., DURAND F.: Statistical acquisition of texture
appearance. In Proceedings of the 17th Eurographics Symposium
on Rendering (2006), pp. 31–40.

[PL07] PELLACINI F., LAWRENCE J.: Appwand: editing mea-
sured materials using appearance-driven optimization. ACM
Trans. Graph. 26, 3 (2007), 54.

[RTG97] RUSHMEIER H. E., TAUBIN G., GUÉZIEC A.: Appying
shape from lighting variation to bump map capture. In Proceed-
ings of the Eurographics Workshop on Rendering Techniques ’97
(1997), Springer-Verlag, pp. 35–44.

[SSK03] SATTLER M., SARLETTE R., KLEIN R.: Efﬁcient and
realistic visualization of cloth. In Proceedings of Eurographics
Symposium on Rendering (2003), pp. 167–177.

[SVLD03] SUYKENS F., VOM K. B., LAGAE A., DUTRÉ P.: In-
teractive rendering with bidirectional texture functions. In Com-
puter Graphics Forum 22, 3 (Sept. 2003), pp. 463–472.

[TZL∗02] TONG X., ZHANG J., LIU L., WANG X., GUO B.,
SHUM H.-Y.: Synthesis of bidirectional texture functions on ar-
bitrary surfaces. In SIGGRAPH (2002), pp. 665–672.

[VT04] VASILESCU M. A. O., TERZOPOULOS D.: Tensortex-
tures: multilinear image-based rendering. ACM Trans. Graph.
23, 3 (2004), 336–342.

[WTL∗06] WANG J., TONG X., LIN S., PAN M., WANG C.,
BAO H., GUO B., SHUM H.-Y.: Appearance manifolds for mod-
eling time-variant appearance of materials. ACM Trans. Graph.
25, 3 (2006), 754–761.

[WTS∗05] WANG J., TONG X., SNYDER J., CHEN Y., GUO B.,
SHUM H.-Y.: Capturing and rendering geometry details for btf-
mapped surfaces. The Visual Computer 21, 8-10 (2005), 559–
568.

[WWS∗05] WANG H., WU Q., SHI L., YU Y., AHUJA N.: Out-
of-core tensor approximation of multi-dimensional matrices of
visual data. ACM Trans. Graph. 24, 3 (2005), 527–535.

