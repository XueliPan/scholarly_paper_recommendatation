558 

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE 

PROCEEDINGS 2000 

On the Use of Skeletons when Learning in Bayesian Networks 

Harald Steck* 

Siemens 

AG, Corporate 

Technology 

Neural Computation 

Dept.  Information and 
Germany 

Otto-Hahn-Ring 6, 

81730 Munich, 

Communications 

Abstract 

during the 

operator 

network 
This is done by alternating 

we present 

structure 

the space of directed 

optimizing 
of all the edges in an interme­

a heuristic 
In this paper, 
which aims at simultaneously 
the 
orientations 
diate Bayesian 
search process. 
graphs 
between 
(DAGs) and the space of skeletons. 
found orientations 
scoring 
ditional 
used as an extension 
search strategies. 
ments with artificial 

function 
independences. 

con­
than on induced 
This operator 
can be 

acyclic 
The 

employed 
in experi­

of the edges are based on a 

It is evaluated 

to commonly 

rather 

and real-world data. 

distribution, 

scoring 

local search 

functions 

are commonly 

this can prevent 

from a probability 

score­
When  learning 
used, i.e. 
equivalent 
to Markov equivalent 
DAGs. 
the same score is assigned 
proce­
As a consequence, 
dures from finding optimum DAGs. For instance 
( cf. 
Fig. 1), assume that a local search procedure 
rived at the intermediate 
the same 
assume that the optimum DAG comprising 
in (4). The DAGs (1), (2) 
edges is the one as depicted 
and (3) are equivalent. 
from DAG (3) to (4), there is a difference 
the scores 
assigned 
is infeasible, 
(3) via  DAG (2) might not be found by a simple lo­
cal search 
are not 
regarding the 
related 

to the DAGs. If exhaustive 

to an improvement 

the transitions 

from DAG (1) to DAG 

procedure, 

score. 

has ar­

search 

DAG (1). Let us further 

regarding 

Hence, only in the transition 

because these transitions 

1  INTRODUCTION 

has been made regarding 

structural 

In the course of 
The one 

have evolved. 
on inducing 

conditional 

from the (empirical) 

probability 
distri­

and relies 

networks. 

over the variables 

Much  progress 
learning 
in Bayesian 
which, two main approaches 
is constraint-based 
independences 
bution 
other tries to optimize 
a (heuristic) 
search 
functions 
approach, 
scoring 
bility 
likelihoods, 
mation Criterion  (BIC), 
optimum Bayesian 
an NP-complete 
to heuristic 

a scoring 
strategy, 

strategies. 

or penalized 

in a domain, 

network 

problem 

search 

e.g. [7, 9]. In the latter 
proba­
like the posterior 
e.g. the Bayesian 
Infor­
might be used. Finding 
the 
was shown 

structure 

to be 

[5], so that one has to resort 

e.g. [16], 

and the 
by means of 

function 

(1)� 
(2)� 

� 
� 

(3)

(4)

Figure 1: The DAG (1) is assumed 
one in a local search 
to be the optimum one. 

to be the current 
and DAG (4) is assumed 

process, 

structures, 

which are dis­
graphs (DAGs), can be 

acyclic 

network 

Bayesian 

Several 
played by directed 
Markov equivalent. 
all those DAGs which represent 
ity distributions 
dependences 

determined 

and dependences, 

An equivalence 

class contains 
the same probabil­
by the conditional 

in­

see for instance 

[14]. 

also: Dept. of Computer Science, Technical University 

of Munich, 80290 Munich, Germany; steck@in.tum.de 

due to Markov 

and examined 

DAGs, it has been proposed 

than in the space of DAGs, which has 

rather 
advantages, 
is, however, 

In order to get around this problem 
equivalent 
to carry out local search in the space of equivalence 
classes 
several 
vantage 
graphs in the space of equivalence 
larger 
neighboring 
one can be computationally 

see for instance  [12,  6]. 
that the number of neighboring 

than in the space of DAGs. Also, determining 

given an intermediate 

equivalence 

quite costly. 

classes 

classes 

can be much 

Since this can 

A disad­

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 

559 

render such an approach 
posed to alternate 
between 
and the space of equivalence 
putation 
(6]. 

it was pro­
the search space of DAGs 
for efficient 
com­

rather involved, 

classes 

edge, which is included, 

quent DAGs differ from each other by exactly 
one di­
rected 
eliminated 
The presented 
non-local. 

operator 

might thus be considered 

as 

or reversed. 

scoring 
of the presented  operator. 
we approximate 

the poste­

Informa­

we are con­
functions, 
which are 

First, 

the Bayesian 

as follows. 

(BIC). The next section 

This paper is organized 
cerned with decomposable 
an essential  requirement 
For ease of computation, 
rior probability 
by employing 
tion Criterion 
details 
of the operator: 
tion used for orienting 
on the procedures 
maining 
we used the operator 
strategy 
to the presented 
like the K2 algorithm 
applied 

in order to compare 
operator 

It focuses 
the colliders 

in our computer 

to learning 

in Bayesian 

networks 
[9]. 

Finally, 

the beneficial 

edges. 

provides 

the 
on the scoring 
func­

effects 
due 

as part of a very simple search 

[7] or the local search strategy 

to other search strategies 

experiments, 

for subsequently orienting 

the re­

( v-structures) and 

a single 

several 

is the graph 

(optimum) 

function. 

A skeleton 

with respect 

a heuristic 

search operator 

to the used scoring 

by substituting 

to its corresponding 

the edges in such a way that the result­

edge in a DAG 
comprises 

skele­
operator 
aims 

Given an intermediate 

two 
DAG in the search 
pro­

each directed 
one. The operator 

In this paper, we present 
which makes use of skeletons. 
obtained 
by an undirected 
steps: 
cess, it is first transformed 
ton. In the second step, the proposed 
at orienting 
ing DAG is optimum (among the DAGs with the same 
skeleton) 
focus on finding 
In this paper, we 
graph as opposed 
to inducing 
done for model averaging, 
e.g. (17]. Since an equiv­
alence class of DAGs is defined by its skeleton 
the colliders 
(18], the heuristic 
operator 
the remaining 
based procedures 
(16]. The main difference 
erator 
orients 
function rather 
pendences 
ical) probability 
sented operator 
ments, whereas 
in the SGS or PC algorithms 
unstable 

than according 
and dependences 

inde­
from the ( empir­

procedures 
are known to be rather 

is that the presented 

to the used scoring 
to the conditional 

before orienting 
to the constraint­
used in the SGS or PC algorithms 

( v-structures) 

the edges according 

first identifies 

the colliders 

This rendered 

This is similar 

when applied 

distribution. 
quite robust in our computer experi­
the edge-orientation 
used 

DAGs like it is 

to finite 

induced 

samples 

search 

edges. 

(16]. 

the pre­

and 

op­

from the DAG 

from the 

all edges again: 

the collider 

it alternates 

the equivalence 
class. 

by the equivalent 

and then orients 
at the node c is identified, 
which 

( cf. Fig. 1 ), the pre­
the transition 
DAG (1) to the optimum DAG (4) at 
being affected 
DAGs 

In the above simplistic 
example 
sented search operator 
yields 
intermediate 
once without 
(2) and (3). This is because 
(1) to its skeleton, 
First, 
determines 
tween c and d, between 
f are oriented 
cur. Alternating 
and the space of skeletons 
pensive 
alence classes. 
The operator 
can help search strategies 
out of local maxima, and might thus be used as an 
extension 
ap­
proaches 
like local search. 

Then the edges be­
d and e, and between 
e and 
oc­

the search space of DAGs 
might thus serve as an inex­

space of equiv­
in this paper 

such that no additional 

in the space of DAGs get 

to using the search 

alternative 

presented 

between 

colliders 

to commonly 
as well as to many greedy search strategies 

constraint-based 

employed 

this operator 

Having applied 
during the search process, 
edges might be changed compared 
DAG. This is in contrast 

to an intermediate 
of several 

the orientations 

DAG 

to local search, 

where subse-

to the previous 

2  DECOMPOSABLE SCORE 

In this paper, we assume the scoring  function 
decomposable, 
probability 
{ x1, ... , Xn} described 

like the joint 
over the set of variables 

i.e. to factorize recursively 

distribution 

by a Bayesian 

network, 

to be 

V = 

p(x1, ... ,xn) =  ITp(x; I pam(x;)), (1) 

n 

i=l 

the parents 
used scoring functions 

factor­

of the node1 x; in 

where pam(x;) denotes 
the DAG m. Commonly 
ize in this fashion 
the absence 
for the scoring 
ity 

of latent 

in the case of complete 

data and in 

variables. 

As a particular 

choice 

function, 

we use the posterior 

probabil­

p(m) 

p(miD) =  p(D)

p(Dim) 

(2) 

of the DAG m when given data D. Simplifying 
computation 
we approximate 
marginal 
tion Criterion 

p(Dim) by the Bayesian 

for practical 

likelihood 

reasons, 

the 
the 

Informa­

(BIC), 

logp(Dim) � BIC(m) =  logL(O)- 2log(N)IOI. 

(3) 

' 1 

' 

the trade-off  between 
the maximum 
The BIC states 
B) of the model and its dimension 
likelihood L( 
the Akaike criterion, 
way.  Unlike 
the term 
an intuitive 
on the number 
penalizing 
model complexity 
N of cases in the given sample. 
The BIC is a rough 
the introduction 
approximation, 
over the parameters. 
referred 

For more details, 

and it avoids 

the reader is 

depends 

to [11]. 

IBI in 

of priors 

1 For simplicity, 

we use node and variable 

synonymously. 

�

) B-1 C_;_( m-z

at an 
stage some edge are oriented, 

l L(Ol) 11  (N)d 
+ og--,---og  r 

, all its edges are undirected, 

560 

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 

two nested DAGs which 

two DAGs be denoted 

First of all, let us consider 
differ from each other by exactly 
corresponding 
where the DAG m1 contains 
and let this additional 
node a E  V to node b E  V \ {a}, indicated 
For brevity, 
sparser 

one edge. Let the 
by m1 and mz, 
one edge more than mz, 
edge in m1 be oriented 
from 

by a -t b. 
of node b in the 

we denote the parents 

graph mz by 1r  =  pam2(b). 

the two DAGs m1 and mz it is a 

In order to compare 
natural 
approximation, 
tion G which gives the score of m1 relative 
to mz: 

to use posterior 
one arrives 

at the relative 

choice 

odds. With the above 

scoring 

func­

G(a, b, 1r )  l 

og =--

p(ml) BIC(ml) 
"-
'-·m-
p(
) 
z
p(ml) 
p(mz) 

og--
=  l 

L(fh) 2 

This score was used for model selection 
the maximum a posteriori 
likelihood 
Assuming 
bution, 

configuration 
discrete 

of the parameters 

was chosen. 
with a multinomial 
distri­

in [3], where 
than the maximum 

variables 

ratio is given by 

rather 

the maximum likelihood 
l L(Ol) _  """'
og - -,- - � 

1 NabrrN++rr 
, 
L((}z) a,b,rr Na+rrN+brr 

a b rr og 

N 

(4) 

of a, b and 
where the sum is over all configurations 
1r. The counts from the data are denoted 
by Na,b,rr, 
where the "+" indicates 
that it is summed over the 
corresponding 
differ in exactly 
only on the involved 
independent 
for the degrees 

a, b, and 1r, and it is 
ones. This holds also 

variable(s). Since the DAGs m1 and m2 

one edge, the likelihood 

of the remaining 

variables 

of freedom 

dr, 

ratio depends 

dr =  IB1I-IBzl 

= (Sa-1)(Sb-1)Srr,  (5) 

where the number of (joint) 
able(s) 
use adjusted 

states 
by S. Instead 

of Eq. 5, one might 
of freedom 
[2]. 

is denoted 

degrees 

of a (set of)  van­

(m) ex:  exp(al£(m)l) 

a prior p(m) which decomposes 
one may resort, 

In order to attain 
for instance, 
to the 
the same fashion, 
with the number of 
assignment p
edges l£(m)l in the DAG m and with a constant 
a for 
a priori 
of edges. Then the function 
variables 

a, b and 1r. It is score-equivalent 

G depends  only 

DAGs dependent 

penalizing 

on the number 

on the 

in 

[4). 

If one is interested 
compared 
in G(a, b, 1r ) 2: /,see for instance 

in a notably 

to mz one may use a threshold 

higher score of m1 
value 1 > 0 

[11]. 

3 DETAILS OF THE OPERATOR 

This section 
operator 

which alternates 

the details 
between 

of the proposed 
space of 

the search 

search 

provides 

of the edges. 

In the first step, the 
to its skeleton, 
which is 

the directions 

DAG is transformed 

DAGs and the space of skeletons. 
current 
simply done by dropping 
Given a skeleton 
the orientations 
sponds to the maximum of the scoring 
course, 
way.  For 
intermediate 
orient 

in the second step, 
such that the resulting 

the edge which increases 

we use a greedy scheme, 

can only be tackled 

simplicity, 

the score most. 

this problem 

i.e. at an 
we 

the aim is to find 

DAG corre­
function. 
in a limited 

step of the edge-orientation 

process 

Of 

3.1 ORIENTING COLLIDERS 

directed 

graph 
the same edges as the given 

acyclic 

and even­
all the edges of the PDAG are directed. 
In the 
for being a 

which is a candidate 

three nodes a, band c such that there 

In this step, we use a partially 
(PDAG) which contains 
skeleton. 
Initially
intermediate 
tually 
PDAG, a structure 
collider 
involves 
is an edge between 
there is no edge between 
rected 
oriented 
orientation 

by x  -t  y ( cf. Algo. 1). 

edge between 

a and b as well as between 

b and c; 

a and c. We denote an undi­

two variables 

x and y by x "'  y, an 

one by x =? y, and an edge with a proposed 

the score 

section. 

oriented 

candidate 

to be already 

described 
candidate 

like a {::: b or 
candidates 

mint, it has to hold for a 
that a '--+ b t---' c with +--'E { "' , {:::} 

In an intermediate  PDAG 
collider 
and '-+E { "' , =} }. This means that the edges  are 
not allowed 
b * c. For each of the collider 
Gcol is calculated as 
Next, the collider 
Gcot(a,b,c 
After a collider 
sure that there cannot occur a (directed) 
PDAG when orienting 
thus oriented 
sidering 
collider 
Tnint, some of the scores 
updated. 
collider  candidates 

in the following 
with the largest 
like a =} b {::: c. 
one has to make 
cycle in the 
one more edge. Such edges are 
without 
con­
After a 

the scoring 
or an edge have been oriented 

I Tn int) 2: 1 is oriented 
has been oriented, 

in the PDAG 
Gcol (a, b, c  I Tnint) have to be 

are repeated 
while there are 
than I· 

direction 
for simplicity. 

in the opposite 
function 

The above steps 

with a score larger 

score 

3.2 SCORING COLLIDER CANDIDATES 

a'--+ b t---' c 

candidate 

by comparing 

DAGs with each other 

PDAG Tnint, the score 

Given an intermediate 
Gcot(a,b,c I Tn int) of a collider 
is calculated 
which are identical 
edges a  '--+ b 
b and c have  the  same 
parents 
i.e. pam;no (a), pamint (b), and pamint (c), respectively. 
Let us denote the DAG with the collider 
by me and the three alternative 
collider) 

except of the orientations 
the variables 

a, 
as in the PDAG mint, 

and b t---' c. In particular, 

DAGs (without 
that 

a =} b {::: c 

of the 

by 

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 

561 

• m._ with the two edges oriented 

as a -¢= b -¢= c, 

• m--+ with the two edges oriented 

as a =} b =} c, 

• m+-+ with the two edges oriented 

as a{= b =}c. 

of the 
odds in order to score the DAG me rela­

Like in Sec. 2, we use the approximation 
posterior 
tive to the alternative 
G._(a,b,c 

I mint), 

DAG m._ by the relative score 

G._(a, 

b, c  I mint) 

p(mc) BIG( me) 
p(m._) BIC(m._) 
(b) U {c}) 
G(a,b,pamint
-G( a, b, pamint (a)), 

where we have used the scoring 
in Eq. 4. The second line indicates 
plification 
function 
Analogously, 
m--+, i.e. G--t(a,b,c 
Gt-t(a,b,c 

since the scoring 

at the scores 

one arrives 

I mint), 

function 

G as denoted 

a considerable 

sim­

is decomposable. 
of me relative 

to 

I mint), and relative 

tom+-+, i.e. 

G-+(a,b,c mint) 
b,c m;nt) 
Gt-t(a, 

I mint) 

G._(c,b,a 
a, b, clmint) 
G..._( 
+G(b, c, pamint 
-G(b, c, pamint 
G--+ (a, b, clm;nt) 
+G( a, b, pamint 
(b)) 
-G( a, b, pamint (a)) 

(b)) 
(c)) 

For reasons 
use the following 

the stability 
score of a collider 

regarding 

candidate: 

of the algorithm 

we 

Gcol (a, b, c  I mint) 

= 

min{G._(a,b,c  I 

m;nt), 

G--t(a, b, c m;0t), 

G+-+(a, b, c  I mint)} (6) 

introducing 

In this case, the procedure 

without 

collider. 

this is possible 

variables 
exists, 
an additional 
ProposeOrientations 
entations 
-+or t-, and no arrows of the type t-t are found; 
whose orientations 
can differ in equivalent 
indicated 

( cf. Algo. 1) indicates 
the ori­
edges by arrows of the type 

by "', and they are oriented 

of the remaining 

DAGs are 

in the final step. 

edges 

"' , =}, -¢=, -+, f-, 

procedure ProposeOrientations 
input: PDAG mint with edges "', =}, -¢=. 
output: PDAG mint with edges 
t-t. 
(1) Va,b,c E V: if a'---+ b t-'  c with <---+E {=},-+ 
, t-t} and f-'E { "', f-} and no edge between 
a and 
c then substitute either 
b rv  c by b -+ or b f- c by 
b t-t c. 
(2) Va, b E V: while 3 a f-b and 3 a = 
x1, ... ,xq =  b (xi=/=- Xj,q > 2) such that Xi-1 '---+Xi 
(i =  2, ... ,q) with <---+E {=},-+,t-t} then substitute 
a f-b by a t-t b. 
(3) while  edges 

can be oriented 

go to (1). 

1: The procedure 

ProposeOrzentations 

Algorithm 
tries to orient 
tional 
are oriented 

the edges in such a way that no addi­

colliders 

occur. 

in "both" directions, 

If this is not possible, 
edges 
by t-t. 

indicated 

D 

uniquely 

procedure FixOrientations 
input: PDAG mint, data 
output: PDAG mint 
(1) call ProposeOrientations. 
edges a -+ b by 
(2) substitute 
a=} b. 
( 3) among all structures 
without 
with the highest 
b {=c. 
( 4) if there are edges of the type t-t then substitute 
all those by "' and go to ( 1). 

of the form a t-t b t-t c 
a and c, orient 

score Gcot (a, b, clm;nt) like a =} 

an edge between 

oriented 

the one 

I mint)> 0, this score is a lower bound 
score of the graph which 

a collider 

of the overall 
compared 

If Gcot(a,b,c 
for the increase 
comprises 
which is identical  with 
Gcot(a,b,c 
imum decrease 
ented as a collider 
orientations. 

compared 

I mint)< 0, this is a bound for the max­

of the score when these edges are ori­

to an alternative  graph 

me except of this collider.  If 

to one of the alternative 

(*) Va,b E V: while 3 a f-' b with t->E {rv,f-,-+ 
, t-t} and 3 a =  x 1 , ... , x q =  b (xi  =/=-x j , q > 2) such 
that Xi-1 =} x; (i =  2, ... , 
Algorithm 
cies regarding 
by t-t) by using the scoring 

q) then orient 
resolves 
inconsisten­
of edges (indicated 

2: This procedure 

the orientations 

function. 

a=} b. 

Fix Orientations 

( cf. 

of those edges 

are fixed (by a 

3.3 ORIENTING REMAINING EDGES 

the remaining edges 

as follows. 

have been oriented, 

the algorithm 
Guided by the 

After the colliders 
orients 
heuristic 
of the remaining 
of additional 
a perfect 

of parsimony 

edges such that a minimum number 
In the ideal case, where 

occurs. 

colliders 

map of the probability 

distribution 

over the 

the aim is to find an orientation 

=} or{=) whose orientations 
(de­

Pr-oposeOrientations 

are uniquely 

by the procedure 

In step (2) of the procedure 
Algo. 2), the orientations 
double-arrow
indicated 
noted by f- or -+). However, 
t-t is proposed 
the edges cannot be oriented 
colliders 
Fix Orientations, 
biguously 

by the procedure 

oriented 

occur. Hence, in step (3) of the procedure 

we propose 
edges like a collider 

the pair of am­
which gets the 

to orient 

if an edge of the type 
ProposeOrientations, 

such that no additional 

562 

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 

score. 

Note that the score Gcol might be neg­
the highest 

highest 
ative in this case. Hence, orienting 
edges like a collider 
all score as little 

corresponds 

as possible 

to decreasing 
the over­

in this greedy scheme. 

scoring 

is maximized. 

Of course,  since 

a 

is taken, this procedure 

might only 

In contrast, 

the SGS and PC 

the edges based solely 

on the con­

orient 

function 

a scoring 
greedy approach 
find a local optimum. 
algorithms 
ditional 
is correct, 
known and if there exists 
finite data, however, 
to be unstable 
sented procedure 
experiments. 

(cf. [16]), 

independences  induced 
from the data. This 
distribution 

if the probability 

is perfectly 

a perfect 

map [16).  Given 

the latter 

procedures 

are known 

whereas 

we found the pre­

to be quite stable 

in our computer 

also 

ProposeOrientations, 
has to make sure that a (directed) 

Like the procedure 
FixOrientations 
cycle cannot occur when fixing the orientations 
edges. Hence, the step ( *) has to be carried 
time an edge has been oriented 
procedure 
Fix Orientations. 
possible 
ready fixed. 

of the 
out each 
like {=: or ::::} by the 

due to edges whose orientations 

Note that ( *) focuses 

on 
are al­

cycles 

3.5 LIMITATIONS OF THE HEURISTIC 

is finished, 

mentioned. 

in the PDAG, as already 

in a final step of the edge­
This is done by randomly 
pick­

After FixOrientations 
undirected edges 
Those edges are oriented 
orientation 
procedure. 
ing one of those edges and by assigning 
orientation. 
called 
in order to orient 
by this assignment. 
oriented. 

there might still be 
Finding 
is straight-forward. 
the presented 
find the global 
the edges, 
complete 
efficient 

since learning 
problem 

Then the procedure 

FixOrientations 

the edges  which 

This is repeated 

[5]. However, 

the skeleton 

until all edges are 

to it a random 

are affected 

corresponding 
In contrast, 
is, of course, 

to a given DAG 
the second step of 
to 
of 

optimum regarding 

it might serve as an 

operator 

Bayesian 

is 

not guaranteed 
the orientations 
networks 
is an NP­

way of finding close to optimum orientations. 

in Fig. 2: 

Propose 

Orientations 

the example 
with a score Gcol 2: 1 
orienting 
indicates 
cycle, 

For illustration, 
let us consider 
Assume that the only collider 
is b ::::} c {=: d, cf. (2).  When 
ing edges, 
directed 
cies ( f+) due to a possible 
collider 
Hence, an additional 
is introduced, 
namely the 
score Gcol < /, e.g. at the node 
one with the highest 
J, see (4). In subsequent 
cur, so that the 
g- and hence the equivalence 
cf. (5) and (6). 

oc­
of the edge between 

orientation 

steps, 

cf. (3). 

c and 

the remain­
inconsisten­

class-can be identified, 

no inconsistencies 

(l)t-t:> (2)�D 
(3)lt:)  (4)�D 
(5)u-J  (6) �D 

Figure 2: A simplistic 
ProposeOrien

tations 

example 

and FixOrientations 
work. 

how the procedures 

might considerably 

to the "correct" 

operator 

of this proce­

the optimum 

calculates 

the score of the col­

at the beginning 

In particular, 

scores 
scores, 

because 
might not be known at this stage. 

based on the edges which have already 
in the intermediate graph. Of course, 

The heuristic 
lider candidates 
been oriented 
this can only be an approximation 
scores. 
dure, when only a small number of edges has already 
been oriented, 
the computed 
differ from the "correct" 
parents 
greedy procedure 
When orienting 
cur inconsistencies  regarding 
dicate 
in order to attain 
tic way by orienting  those 
entail 
tionally, 
arise during this procedure. 
heuristic 
evaluation 
to resort 

edges like a collider 
which 
Addi­
it has to be made sure that no directed 
cycles 
This is done in a simple 

considering 
heuristic 

the score. For an 
operator 

might get stuck at a local optimum. 

a DAG. This is done in a heuris­

there might oc­
which in­

the directions, 

that additional 

experiments. 

of the presented 

the remaining 

to computer 

way without 

colliders 

Hence, this 

edges, 

one has 

have to be introduced 

the smallest decrease regarding 

the score. 

4  PRELIMINARY EXPERIMENTS 

we used the pre­

with two additional 
opera­

together 

in order to arrive 

experiments, 

In our computer 
sented operator 
tors, namely one for forward 
backward 
elimination, 
search 
strategy, 
in the remainder 
search 
[7] and the greedy  algorithm 
as described 
ple search 

which we call skeleton 

strategy  works 

of this section. 

for instance 

strategy 

inclusion 

and one for 
at a simple 

search strategy 

We compared 

this 

with other ones like the K2 algorithm 

based on local search, 

in [9). In detail, 
as follows:  Starting 

this sim­
out with 

3.4 STABILITY 

The presented 
a skeleton 
PC algorithms 
procedure 

is similar 

procedure 

for orienting 

the edges given 

to the ones used in the SGS and 
is that our 

[16]. The main difference 

aims at finding the orientations 
such that 

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 

563 

ourselves 
to a uni­

on the network 

We committed 

structures. 

and imposed 
The thresh­

of the strategies. 
form prior over the DAGs, p(m) =  const, 
no constraints 
old value 1/2 =  3 was used in order to include 
only 
those edges into the graph which lead to some notable 
increase 
induced 
namely as the expectation 
tributions 
priors 
see for instance 
[9]. 

in the score [11]. The parameters 
Om of an 
DAG m were calculated 

way, 
in a Bayesian 
dis­

and a small equivalent 

sample size of N' =  5, 

values of the posterior 

parameters. 

over  the 

We used conjugate 

Table 1: Experiments 
The results 
are assessed 
(XV), cf. Sec. 4 for details. 

with four real-world 

data sets. 

by 5-fold 

cross validation 

data nodes skeleton search 
The third step allows the 
set 
SEW  5 0.0504 
ENV  11 0.877 ± 0.018 0.921 ± 0.040 
BOS  14  1.94 ± 0.11 2.15 ± 0.12 
CAR  46 25.53 ± 0.26 25.54 ± 0.26 

XV 
± 0.0035 0.0514 

XV 
± 0.0034 

local search 

the empty DAG, the graph is optimized 
cycling 
three steps: 

the following 

through 

by repeatedly 

1. apply the presented 

operator 

2. carry out one step of forward 

inclusion 
m  the 

space of DAGs 

3. perform 

backward  elimination 

m  the space of 

DAGs 

graph. 

cycles 

before 

of the edges have been optimized 

the orientations 

the edge which improves 
into the DAG. Of course, 

may occur in the resulting 

operator, 

can optimize 

at most one edge is included 

into the 
so that the pre­

After the orientations 
by the presented 
the score most is included 
no (directed) 
In this scheme, 
graph in each cycle of the algorithm 
sented operator 
the next edge is included. 
algorithm 
previously 
have been different. 
search 
the end of the search process, 
to a unique DAG. This is because 
ator is non-local 
the orientations 
ping criterion 
we propose 
eventually 

in the sense that it possibly 
of more than one edge. For the stop­
and 
to keep track of the last few DAGs and to 
choose the one with the highest 
score. 

among two or more DAGs at 

oscillates 

strategy 

this has to be taken into account, 

the presented 

to remove edges which have been included 
when the orientations 

of some edges might 

It might occur that this simple 

i.e. it may not converge 

changes 

oper­

we ignored  the 

It com­

were ob­

states 

influences 
11 variables 

in our computer 

of this data set, since the 

is not documented 

by Sewell and Shah [15], 

with environmental 
on 

of trees [10]. It contains 

which encode the position 

and the former has several 

experiments 
data sets: The 

with high-school 
students. 
and over 10,000 cases. The data 

The results 
tained from the following  real-world 
data (SEW) was gathered 
and it is concerned 
prises 
5 variables 
(ENV) is concerned 
the condition 
and 6168 cases. In our experiments, 
first and the ninth variable 
latter 
hundred 
with respect 
cretized 
that each variable 
data (BOS) by discretizing 
Housing 
nary. This sample contains 
The data (CAR) contains 
were available 
tomers of a car company. 
and URTEI have 16 and 10 states, 
practical 
we summarized 
such that these two variables 
spectively. 

14 variables 
46 variables, 
from [13]. It was gathered 
VORMD 

in this data set such 
We obtained 
the 
of the Boston 

the variables 
Data [8] such that each variable 

and 506 cases. 
and 794 cases 

to a grid on a map. Moreover, 
we dis­

the last two variables 

The two variables 

had 3 and 4 states, 

some of their states 

had four states. 

reasons, 

of the trees 

respectively. 

became bi­

from cus­

For 

re­

When comparing 
plied the same scoring 

the different 

search 

strategies 

we ap­

function 

G ( cf. Eq. 4) to each 

Bayesian 
strategies 

of the 

out 5-fold 

divergence 

distributions 

sets  with  the 

utility 
the expected 
search 
by the different 
we 
(XV), cf. [9]. The 
cross validation 
was used in order to com­
underlying 
the val­

In order to assess 
networks induced 
carried 
Kullback-Leibler 
pare the probability 
idation 
Bayesian 
and the standard deviation 
vergence 
validation. 
sults. 
greedy  algorithm 
for instance, 
in [9]. 

Small values indicate 
search 

In Table 1, we depicted 

over the 5 samples 

ones described 

when averaged 

the skeleton 

We compared 

networks. 

in cross 
good learning 

strategy 

the mean 

re­

di­

by the induced 

to the 

based on local search as described, 

of the Kullback-Leibler 

DAGs than does local search 

strat­

search 

better 

notable 

slightly 

regarding 

the skeleton 

cross validation, 

the difference 
than the standard 
This does not hold for the data sets (SEW) and 
BIG in 
function 

Considering 
egy yields 
for each of the data sets in Table 1. The differences 
might be considered 
(ENV) and (BOS), because 
is not considerably 
smaller 
tions. 
the used scoring 
(CAR). Considering 
the experiment with 
the data 
ton search 
local search 
ues concerning 
that this data set cannot be well described 
of a Bayesian 

the data sets 
in the means 
devia­

the tiny data set (CAR) might indicate 

found only a local optimum. 

(SEW), the simple skele­

found the global 

strategy 

optimum, 

The large val­

network 

whereas 

by means 

model. 

[1] experiments, 

In our Alarm network 
posterior 
ing function, 
rithms employed 

priors 

we used the 
as the scor­
algo­
[9]. The learning 
score G 

probability 

with conjugate 

see for instance 

the corresponding  relative 

564 

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 

2 3 4 5 6 7 8 910 

sample size (x 1 ,000) 

Figure 3: In our Alarm network  experiments, 
terior 
ton search strategy 
which results 
metric mean over 5 samples 

with the DAG mK2 
from the K2 search strategy. 
The geo­

for each size is depicted. 

of the DAG m induced 

is compared 

by the skele­

the pos­

sizes 

graph, we 

priors 

between 

The 
of 

specified 

was chosen, 

described 

the stability 

is known, we could 

probability 

sample size 

analogously 

distribution 

for the posterior 

to Eq. 4. A small value of 

In Fig. 3, we compare 

when the equivalent 

sample size which we varied 

probability with 

to be quite stable regarding 

did not very  sensitively  depend 

variables 
of the variables 

i.e. N' =  1. We 
in 
on the 

[1]. The Alarm network 
con­
and 46 edges. Since the 

the number of edges in the induced 
their number to increase 

oper­
of the presented 
sample sizes, 
we applied 
the 
to data sets of various 

different 
search strategy 
from the probability 

[7] for comparison. 
ordering 
and one can, hence, expect this search 

which can be derived 
conjugate 
the equivalent 
note that the Kullback-Leibler divergence computed 
cross validation 
chosen equivalent 
N' = 1 and N' = 100. This was also noted in [9]. Re­
garding 
observed 
sample size N' rises. 
In order to examine 
ator concerning 
skeleton 
sampled 
by the Alarm network 
tains 37 discrete 
(partial) 
ordering 
use the K2 search strategy 
K2 procedure requires 
an initially 
the variables, 
strategy 
ple sizes. 
strategy 
samples 
egy gets stuck at a local optimum which is far from 
the global 
rect ordering 
the K2 procedure 
ever, from samples 
skeleton 
higher score than the ones found by the K2 search 
strategy. 
ticed that the K2 algorithm 
more edges into the graph than the skeleton 
strategy 
Alarm net­
work structure, 
removed 
func­
by the scoring 
periments, 
since this was favored 
search 
strategy 
either 
tion. Except of this, the skeleton 
induced 
when 
or -in particular 
given small data sets -got stuck at a local optimum 
which erroneously 
orientation 
was related 
to a "wrong" 
The K2 algorithm 
included 
two, edges "erroneously". 

about the cor­
for 
in the case of small samples. 
containing 
at least 2, 000 cases, 
the 
induces 

Figure 4: Time evolution 
ing the learning process 
the Alarm network 
The solid line indicates 
ing present 
cycle; 
present 
and the ones which are not (dotted 

we no­
slightly 
tends to include 
search 

to the K2 search strategy. 
(with less than 2,000 cases), 

one edge was usually 
by both search strategies 

50,-------------------� 
40 

it is subdivided 
in the original 

an additional 
of another 

different 
the skeleton 

Given only small 
strat­
our search 

edge which 
edge. 

one. The prior knowledge 

"erroneously" 

one, and sometimes 

seems to be crucial 

of the variables 

with the original 

� 30 
"0 
<I.)  20 
=1:1: 

search strategy 

differences, 

Considering 

does. Compared 

structural 

DAGs which get a 

contained 

structure 

one induced 

the correct 

the overall 

in most of our ex­

usually 

search 

#cycles 

How­

sam­

10 

0 

"' 

.,. 

.... �-�-·-·-··-----------

.. -·-·"·····"

·" .... ---···-............. 

.. 

0  10 20 30 40 50 60 

of the number of edges dur­

in one of our experiments 
using a sample with 2, 000 cases. 

with 

number of edges be­

in the intermediate 

graph mint after each 
into the edges which are also 
Alarm network 

(dashed 

line) 

line). 

included 

edges are "erroneously" 

reached 
( cf. Fig. 4). Since information 
about the 
ordering 
of the variables 
is not given as input to our 
into the 
algorithm, 
graph. Towards the end of the search process, 
after 
the  orientations 
most of the "erroneously" 
in this example. 
a higher score  than  the 
strategy, 
although 
of the variables 

We found that the resulting 
DAG got 

as prior knowledge. 

included 

the latter 

by the K2 search 

edges are removed 

of the edges have been established, 

used the correct ordering 

it takes to identify 

an approximately 

search 

the skeleton 
process, 
i.e. how 
op­

the end of the search 

how quickly 

we examined 
reaches 

Finally, 
strategy 
many cycles 
timum structure. 
is added, the number of cycles 
the number of edges present 
our Alarm network 
number of edges being present 
quite quickly 

Since in each cycle at most one edge 
cannot be smaller 
than 

in the induced 

DAG. In 

experiments, 

we found that the 

before the end of the search process 

is 

in the graph increases 

configurations 

variables 

we observed 

is typically 

In our computer experiments, 
that com­
of 
puting the counts N of the different 
several 
In 
fact, most of the computation 
task. Hence, a lot of computation 
caching 
Since the DAGs in subsequent  cycles 
ilar to each other, this caching 
can lead to a large 

time was spent on this 

time can be saved by 

are usually 

the scores 

very time consuming. 

G once they have been computed. 

sim­

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 

565 

speed-up. 

5  CONCLUSIONS 

(4] D. M. Chickering. 

A Transformational 

Charac­

of Equivalent 

terization 
Proc. of the Conf. on Uncertainty 
telligence, 

pages 87-98, 1995. 

in Artificial In­

Network Structures. 

In 

(5] D. M. Chickering. 

Learning 

Bayesian 

Networks 

is NP-Complete. 
121-30, 

1996. 

In Proc. of AI €3 STAT, pages 

(6] D. M. Chickering. 

Learning 

Equivalence 

Classes 

of Bayesian  Network 
Conf. on Uncertainty 
pages 150-7, 1996. 

Structures. 

In Proc. of the 

in Artificial Intelligence, 

to finite data. Since the presented 

change the orientations 

(7] G. Cooper and E. Herskovits. 

A Bayesian 
of Probabilistic 

Networks 
from 

Method 

for the Induction 
Data. Machine Learning, 9:309-47, 

1992. 

as non-local 

operator first 

(8] Boston Housing 

Data. 
see for instance 
http:/ /lib.stat.cmu.edu/ 

StatLib 

- Datasets 

datasets/boston

Archive, 
. 

by Markov-equivalent  network-structures 

(9] D. Heckerman, 

D. Geiger, 

and D. M. Chickering. 

Bayesian 

Networks: 

Learning 
of Knowledge 
and Statistical 
port, MSR-TR-94-09, Microsoft 

Data. Technical 
Research, 
1994. 
(10] Environmental  Influences 
Univ. 

on the Condition 

re­

of 

The Combination 

Trees. Data  Sets 
of Munich, http:/ 
data-sets/fioss/floss.html. 

at the Dept. 
/www.stat.uni-muenchen.de/ 

of Statistics, 

to more involved 

(11] R. E. Kass and A. E. Raftery. 

Bayes Factors. 

JASA, 90:773-96, 
1995. 

which 

to a 

with respect 
to other proce­

function. 

when applied 

In order to derive 

which are known to be un­

This is in contrast 

the edges are oriented 

might simultaneously 

edges it can be considered 

the optimum orientations 

independences, and 

a search operator 
a DAG when 
with the aim 

In this paper, we presented 
makes use of skeletons. 
given a skeleton, 
of finding 
scoring 
dures of this kind, e.g. the ones used in the PC or 
SGS algorithms 
(16], which are based on induced 
con­
ditional 
stable 
operator 
of several 
the space of DAGs. The presented 
tifies 
class, 
This operator 
entailed 
our computer 
a simple search strategy 
monly used learning 
periments 
that alternating 
the space of skeletons 
computationally 
efficient 
strategies. 
search 

in 
used in 
to com­
ex­

space of DAGs and 
and 

was thus quite robust against 

which we compared 

experiments. 

which determine 

alternative 

algorithms. 

remaining 

the search 

between 

might serve as a powerful 

iden­

in 

The operator was 

problems 

Our preliminary 

the colliders, 
the equivalence 
and subsequently  orients  the 

edges. 

with artificial  and  real-world 

data suggest 

Acknowledgments 

for his hospi­

and for numerous 

I would like to thank Steffen Lauritzen 
tality 
enlightening 
to Paul Theo Pilgram,  Volker 
also grateful 
reviewers 
anonymous 
helped  improve 
by an Ernst-von-Siemens 

this paper. This work was supported 

discussions. 

for valuable 

comments 

grant. 

Tresp, and 

which 

I am 

(12] D. Madigan, 

S. A. Andersson, 

M. D. Perlman, 
Model Averag­

. Bayesian 

and C. T. Volinsky
ing and Model  Selection 
Classes 
in Statistics -Theory and Methods, 25(11):2493-
519, 1996. 

for Markov Equivalence 

Digraphs. 

of Acyclic 

Communications 

(13] Consumer-Satisfaction 

of Car Owners. 

Data Sets 

at the Dept. of Statistics, 
http:/ 
bmw /bmw.html. 

/www.stat.uni-muenchen

Univ. of Munich, 

.de/data-sets/ 

Reasoning in Intelligent 

(14] J. Pearl. Probabilistic 
Morgan Kaufmann, 
and V. Shah. Social 

Systems. 
(15] W. Sewell 

1988. 
Class, 
and Educational 

couragement, 
J. of Sociology, 73:559-572, 

1968. 

Parental 

En­
Am. 

Aspirations. 

C. Glymour, 

and R. Scheines. 

Cau­

sation, Prediction, and Search. Springer 
Notes in Statistics 

81, 1993. 

Lecture 

References 

H. J. Suermondt, 

The Alarm Monitoring 

(1] I. A. Beinlich, 
and G. F. Cooper. 
A Case Study with Two Probabilistic Inference 
Techniques 
Second European Conf. on Artificial Intelligence 
in Medicine, pages 247-56. 

R. M. Chavez, 
System: 

(16] P. Spirtes, 

Networks. 

In Proc. of the 

for Belief 

UK, 1989. 

London, 

(2] Y. Bishop, 

S. Fienberg, 

and P. Holland. 

Discrete 

(17] B. Thiesson, 

C. Meek, D.M. Chickering, 

and 

Multivariate 

Analysis. 

MIT Press, 

1975. 

(3] P. Cheeseman 

and J. Stutz. Bayesian 
Clas­

D. Heckerman. 
els. In Proc. of the Conf. on Uncertainty 
in Arti­
ficial Intelligence, 

Learning 

Mixtures 

pages 504-13, 

of DAG Mod­

1998. 

(AUTOCLASS): 

Theory and Results. 

sification 
In U. Fayyad, 
and R. Uthurusamy, 
edge Discovery 
AAAI Press, 

G. Piatesky-Shapiro, 

P. Smyth, 
in Knowl­

editors, 

Advances 

and Data Mining, pages 153-180. 

Menlo Park, CA, 1995. 

(18] T. Verma and J. Pearl. 

Equivalence 
In Proc. of the Conf. on Un­
in Artificial Intelligence, 

of Causal Models. 
certainty 
1990. 

pages 220-227, 

and Synthesis 

