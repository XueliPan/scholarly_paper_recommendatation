towards holistic approach integrate parallelism detection base map wang institute compute architecture school university unite kingdom mob abstract much study area yet still find widespread application largely due poor exploitation application parallelism subsequently result performance level far skilled expert programmer could achieve identify two traditional parallelize propose novel integrate approach result performance generate parallel code use parallelism detection overcome static analysis enable us identify application parallelism rely user final approval addition replace traditional inflexible map base prediction mechanism result better map provide scope adaptation different target evaluate parallelization strategy spec two different multicore dual cell blade demonstrate approach yield compare parallelize come close sometimes exceed performance manually parallelize cod average methodology achieve performance spec parallel platform gain cell platform demonstrate potential base parallelization complex multicore subject program program concurrent program general term experimentation measurement performance parallelism detection base parallelism map permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee nine copyright c june introduction one multicore compute widely see viable mean deliver performance increase transistor one however potential realize unless application well parallelize unfortunately efficient parallelization sequential program challenge task generally agree manual code parallelization expert result streamline parallel implementation time costly approach parallelize compiler technology hand potential greatly reduce cost ensure formal correctness result parallel code automatic parallelism extraction certainly new research area two progress achieve restrict loop three four five fact research result whole range parallelize research six seven recently eight complementary ongoing work many parallel program nine ten eleven twelve program model fourteen fifteen sixteen propose interactive parallelization tool seventeen eighteen nineteen twenty provide way actively involve programmer detection map application parallelism still demand great effort user approach make parallelism expression easier past effort involve discover map parallelism still far greater write equivalent sequential program paper argue lack success occur two reason first traditional static parallelism detection effective find parallelism due lack information static source code second exist integrate approach successfully bring together automatic parallelism discovery portable map give number type parallel system likely change one generation next find right map application may repeat many time throughout application lifetime hence make automatic approach attractive approach approach integrate parallelism detection base map single framework use profile data extract actual control data enhance correspond static analyse dynamic information subsequently apply previously train f zero n e l one zero zero v zero zero one v one zero two v two w h l e l c l zero zero v c l zero zero one v c l one zero two v c l two w c l zero zero zero v zero one zero v one two zero v two w c l one w zero w one figure one static analysis challenge sparse array reduction inner loop spec base prediction mechanism parallel loop candidate decide parallel map perform finally generate parallel code use standard approach ie expect user finally approve loop parallelization likely beneficial correctness prove conclusively result evaluate parallelization strategy spec two different multicore dual cell blade demonstrate approach yield compare parallelize come close sometimes exceed performance manually parallelize cod show analyse detect parallel loop static surprise result loop parallel technique correctly identify despite fact single small data input consider parallelism detection furthermore show parallelism detection isolation sufficient achieve high performance neither conventional map base map approach provide across require genuinely portable parallelization strategy average methodology achieve performance spec parallel platform cell platform demonstrate potential base complex multicore overview remainder paper structure follow motivate work base simple section two follow presentation parallelization framework section three experimental methodology result discuss section four five respectively establish context f r r e c n sum p r v e f j one j l c l f r c l one j x j j sum sum figure two despite simplicity map parallel loop take scheme vary across relate work section six summarize conclude section seven two motivation parallelism detection figure one show short excerpt function spec seismic wave propagation function implement sparse product take sixty total execution time application conservative static analysis fail parallelize loop due sparse matrix indirect array indices inner loop dependence analysis provide us additional information actual data dependence inhibit parallelization give sample input still prove absence data every possible input loop parallelization reduction profitably present user approval example user would provide additional knowledge guarantee every col index inner loop unique hence access w col zero w col one respectively result example demonstrate static analysis overly conservative profile base analysis hand provide accurate dependence information specific input combine select parallelization base empirical evidence hence eventually extract application parallelism purely static approach map figure two parallel reduction loop originate parallel show despite simplicity code map example parallel execution loop profitable cell platform due high communication cost process fact parallel execution result massive slowdown sequential version cell number thread platform however parallelization profitable depend strongly specific schedule policy best scheme static result sequential code perform time better worst scheme dynamic slow program two original sequential performance example illustrate select correct map scheme impact performance however map scheme vary program program also architecture architecture therefore need automatic portable solution parallelism map three parallelization framework section provide overview technical detail parallelization framework show figure three sequential c program initially extend plain parallel loop result dependence analysis sequential profile base code analysis code parallel base map code extend figure three parallelization approach combine parallelism detection base map generate annotate parallel program addition data share private data also take place stage second step add work allocation code loop predict benefit parallelization otherwise remove parallel also happen loop correctness prove conclusively base static analysis user disapprove suggest parallelization decision finally parallel code compile native compiler target platform complete overview show figure four parallelism detection propose approach parallelism detection traditional static compiler analyse replace enhance dynamic information achieve devise novel instrumentation scheme operate intermediate representation level compiler unlike need deal particular instruction set obtain dynamic control data flow information relate nod immediately allow us original profile information resume three stag involve parallelism detection one instrumentation c code generation profile two construction dependence analysis three parallel code generation instrumentation profile generation primary objective enhance static analysis traditional parallelize compiler use precise dynamic information main obstacle correlate information gather program execution specific memory access branch data control flow information information embed executable usually detail enough enable reconstruction bridge information gap perform instrumentation level compiler cosy variable access additional code insert emit associate symbol table reference well actual memory address data item data include array structure cover instrumentation information later use memory access static analysis fail analyze similarly instrument every control flow instruction node identifier code record actual dynamic control flow eventually plain c representation close original program additional instrumentation code insert recover use translation pass compile native compiler program result process still sequential functionally equivalent original code emit additional trace data access control flow construction dependence analysis subsequent analysis stage consume one trace item time construct global control data flow graph parallelism detection perform hence necessary store entire trace tool chain appropriately trace item process algorithm one distinguish control data flow maintain various data structure support dependence analysis control flow section construct global control flow graph application include call stack loop nest tree normalize loop iteration section responsible map memory address specific data flow information keep hash table data trace granularity data record data edge edge annotate specific data section array indices dependence data additional bite vector relate dependence surround loop nest maintain data g v graph control edge bite e address set e iteration vector address v hash table mem v current normalize iteration vector v current node procedure instruction handler next instruction memory instruction address access instruction update last writer else use find match defuse edge e add e bite true else control instruction v node reference instruction edge v add v g v algorithm one algorithm construction soon complete trace process construct associate import back cosy compiler add internal statically derive data control flow structure possible dynamic profile contain reference nod addition actual memory address basis detection parallelism however possibility conflict dependence information example may data dependence sequential c code small sample data set user approval instrumentation profile parallelization map static dynamic dependence analysis code generation parallel c code real data set result native execution native compilation figure four parallelization framework comprise instrumentation profile stag follow static dynamic dependence analyse drive parallelization base map stage user may ask final approval parallel code generate code generation perform native enable c compiler materialize profile run case treat loop potentially present user final approval parallelization predict profitable parallel code generation use parallel code generation due low complexity generate require code widespread availability native currently target parallel loop translate correspond maintain complete list true anti require parallelization rather record memory location keep map normalize iteration index memory location level allow us efficiently track memory anti scalar x within loop every path begin loop body use x pass definition x use hence determine inspect incoming outgo edge loop analogous approach apply array reduction reduction recognition scalar base algorithm present unlike original publication use simplify code generation stage sufficient emit reduction annotation recognize reduction loop validate statically detect reduction use profile information use additional reduction template library enable array show figure one synchronization default behavior parallel loop synchronize thread end work share construct mean barrier due high cost form synchronization important good performance redundant synchronization avoid synchronization also increase idle time due load imbalance section program base compute interloop apply compile time barrier synchronization minimization algorithm result minimal number loop default synchronization eliminate extend clause present approach code generation relatively simple essentially rely code alongside minor code yet perform code might help expose exploit parallelism improve data locality target code generation impose number example yet exploit parallelism parallelism even though also extract form parallelism machine learn base parallelism map parallelism map stage decide parallel loop candidate profitable parallelize select schedule policy four offer cyclic dynamic guide static example figure two demonstrate task optimal solution depend particular loop consideration target platform provide portable map approach use machine learn technique construct predictor initial train replace highly often inflexible map traditional parallelization predictive model separate profitably loop challenge task incorrect classification result miss profitable parallel execution even slowdown due excessive synchronization overhead traditional parallelize employ simple base iteration count number loop body decide whether particular parallel loop candidate execute parallel data show figure five suggest scheme likely fail misclassification occur frequently parallelize parallelize n c r n f r e b n ten one ten number figure five diagram show optimal classification execution parallel loop consider experiment machine linear model static feature iteration count size loop body term suitable separate profitably loop simple work base scheme would attempt separate profitably loop diagonal line indicate diagram figure five independent exactly line draw always loop hence potential performance benefit waste need scheme take account set possibly dynamic loop feature b capable nonlinear classification c easily adapt new platform paper propose predictive model approach base classification particular use support vector machine decide whether parallelize loop candidate b schedule classifier use construct multidimensional space program feature discuss follow paragraph identify profitably loop classifier implement model radial basis function kernel capable handle linear nonlinear classification detail classifier provide figure six program feature extract characteristic program feature sufficiently describe relevant program present classifier overview feature give table one static feature derive cosy internal code representation essentially feature characterize amount work carry parallel loop similar dynamic feature capture dynamic data access control flow pattern static feature dynamic feature instruction count count branch count loop iteration count data access count instruction count branch count table one feature characterize loop one classification train data xi xi one one n b hyperplane formulation w xi b one one n c determine minimization w w b subject one b two nonlinear classification nonlinear classification replace dot product one b kernel function follow radial basis function k x x x x two zero b reduce single problem multiple binary classifier distinguish one label rest figure six support vector machine nonlinear classification sequential program obtain profile execution use parallelism detection train summary use supervise learn scheme whereby present machine learn component pair program feature desire map generate library know loop repeat time execution sequential parallel code different available schedule record actual performance target platform prediction model build use available train data learn take place deployment new previously unseen application parallel follow step need carry one feature extraction involve collect feature show table one sequential version program accomplish profile stage already use parallelism detection two prediction parallel loop candidate correspond feature set present predictor return classification indicate parallel execution profitable schedule policy choose loop nest start outermost loop ensure settle piece work three user interaction parallelization appear possible accord initial profile profitable accord previous prediction step correctness prove static analysis ask user final approval four code generation step extend exist annotation appropriate schedule clause delete annotation parallelization promise performance improvement reject user safety issue safety unlike static analysis parallelization conclusively guarantee absence control data every possible input one simple approach regard selection representative input base coverage analysis drive empirical observation vast majority case approach might false positive tool suggest contrary due path data input set cover also give fast way select representative term execute natively record result code coverage course many input dependent appear difference latter verify user current work choose scenario use data set associate profile evaluate available data set surprisingly find naive scheme detect almost loop spec loop furthermore help tool able identify three incorrectly share original fact illustrate manual parallelization prone process contribute program correctness process data dependence information granularity effectively build whole program may need maintain data structure grow potentially large entire address space target platform practice however observe case heap memory need maintain dynamic data dependence structure even encounter experimental evaluation comparison static perform whole program analyse need maintain similar data structure size dynamic trace potentially become large every single data access control flow path record process thus eliminate need large trace store approach operate level compiler need consider detail architecture state hence profile accomplish speed close native sequential speed dependence analysis need keep track memory control flow make incremental update hash table graph structure fact dependence analysis dynamically construct complexity static analysis use static four experimental methodology section summarize experimental methodology provide detail multicore use throughout evaluation target share memory dual distribute memory multicore system cell blade brief overview give table three evaluation select spec sequential manually parallelize available enable us directly compare parallelization strategy parallel independent expert specifically use sequential cod alongside spec correspond spec program suite data data art spec spec spec w b w b w b w b w b w b w b w b na w w w w w w w test train ref test train ref test train ref test train ref test train ref test train ref table two data set hardware cell blade server dual socket cell blade two cell cache per chip os compiler core seven kernel single source compiler cell cell thirty server hardware dual socket os compiler two eight core total core scientific kernel build table three hardware configuration detail two evaluation however note sequential parallel spec cod immediately comparable due amount official parallel cod result performance advantage spec cod sequential even single processor system program execute use multiple different input data set show table two however parallelism detection map use available data result parallel program evaluate input investigate impact input safety parallelization scheme methodology evaluate three different parallelization approach manual use compiler platform approach native code generation program sequential parallel compile use cell respectively furthermore use evaluate base map technique mean k program remove one train model remain k one program predict k th program previously train model repeat procedure program turn cell platform report parallel sequential code run rather single case sequential performance exceed one data set could evaluate cell due memory single ensure report available parallel cod result average across five experimental evaluation section present discuss result overall result figure seven seven b summarize performance result cell strike result compiler fail exploit usable level parallelism across whole range data set size fact result slowdown data set size respectively gain modest data set reason disappoint performance compiler typically parallelize innermost loop level overhead negate potential benefit parallelization manually parallelize program achieve average across data size case eight achieve large data size surprise since embarrassingly parallel program surprisingly able achieve nine due improve cache program exhibit lower data set b comparison w machine document issue specific parallelization achieve performance level close manually parallelize sometimes outperform surprise performance gain attribute three important factor firstly approach parallelize outer loop whereas manually parallelize cod parallel inner loop secondly approach exploit reduction array finally machine learn base map accurate eliminate loop parallelization select best schedule policy situation slightly different spec parallelization still outperform static reach performance level manually parallelize cod cause behavior reveal spec cod equivalent sequential spec program manually example data structure alter list vector standard memory allocation excessive use replace efficient scheme obviously change beyond capable perform fact able confirm sequential performance spec cod average two time art original spec verify approach parallelize critical loop art spec art achieve four whereas spec version six time faster sequential spec version fifty due sequential code also measure performance parallelize version use code achieve comparable overall result demonstrate parallelization scheme significantly improve compiler fact approach deliver performance level close exceed manually parallelize cod average achieve performance cell figure seven b show performance result manual parallelization platform unlike platform cell platform deliver high performance manually parallelize program average cod result overall slowdown program small performance gain could observe however program performance degradation disappoint give program perhaps surprise essentially two reason cell performance potential could exploit firstly clear cod develop specifically cell programmer consider communication cost distribute memory machine secondly absence specific schedule library resort default behavior lead poor overall performance give manually parallelize program deliver high performance level platform result cell demonstrate parallelism detection isolation sufficient map must regard equally important contrast default manual parallelization scheme integrate parallelization strategy able successfully exploit level parallelism result average twenty sequential code individual program success largely attribute improve map parallelism result base approach parallelism detection safety approach rely dynamic profile information discover parallelism obvious drawback may loop potentially parallel exist another data set would highlight dependence prevent correct parallelization fundamental limit dynamic analysis reason request user confirm uncertain parallelization therefore examine extent approach suffer false loop incorrectly clearly approach suffer high number false limit use column two table show number loop approach detect potentially parallel column label false positive show many fact sequential surprise result none loop consider potentially parallel turn genuinely sequential certainly result prove dynamic analysis always correct still indicate dependence analysis may accurate generally consider even profile generate small data set clearly encourage result need validation complex program draw final column three table list number loop compiler able detect considerable number parallel loop addition examine coverage show parentheses see many case cover considerable part program therefore conclude less matter parallelism detection cause perform poorly rather exploit map detect parallelism see section final column table eventually show number loop parallelize percentage sequential coverage show parentheses far loop theoretically possible actually parallelize programmer obviously decide parallelize loop consider hot profitable loop manual parallelization parallelization b w b b b b g c w g c g c b g c p e w p e p e b p e f w f f b f w b l w l l b l g w g g b g p w p p b p e p n r p f e r p e r n r r f e r r e e k q e n r e k q e f e r e k q e e g r e v sequential cod achieve manual parallelization parallelization platform manual parallelization parallelization p e e p nine eight seven six five four three two one zero p e e p six five four three two one zero g c w g c g c p e w p e p e f w f f w l w l l g w g g p w p p e r n r r f e r r e p n r p f e r p e e k q e n r e k q e f e r e k q e e g r e v b sequential code achieve manual parallelization parallelization dual cell platform figure seven due different parallelization scheme application art profile drive loop threshold manual loop loop eight nine fourteen zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero three zero zero zero one sixteen eleven six one three one eight nine sixteen one one six one twelve seventy eleven five seven table four number parallelize loop respective coverage sequential execution time cover part sequential time effective parallelization lead good performance see platform total four false negative column table ie loop identify parallel although safely three false negative contain two due loop zero iteration count data set therefore never profile third one reduction contain inside loop classifier decide parallelize parallelism map section examine effectiveness three map scheme manual heuristic static feature use profile information across two figure eight compare performance approach program case show performance two different map approach default employ profitability check second approach perform check use dynamic profitability threshold case check provide marginal improvement static map scheme static scheme better overall scheme equally poor deliver less half level disappoint performance appear largely due map ie parallelize inner loop rather outer figure compare base map approach scheme use profile information employ fix heuristic similar one implement parallelize compiler see also figure five heuristic consider product iteration count number contain loop body decide static threshold approach deliver nearly performance cod case able outperform static heuristic perform poorly unable obtain performance code translate average rather main reason performance loss default scheme use static code feature linear work model unable accurately determine whether loop parallelize figure nine compare performance result different map approach spec cod base approach outperform fix heuristic average eighty sixty forty twenty zero b p n e v l e r e c n r f r e p zero e v l e r e c n r f r e p p c e p n b p n p c e p e v l e r e c n r f r e p eighty sixty forty twenty zero default profile heuristic profile machine learn b w b b b b g c w g c g c b g c p e w p e p e b p e f w f f b f w b l w l l b l g w g g b g p w p p b p e g r e v profile heuristic profile machine learn platform g c w g c g c p e w p e p e f w f f w l w l l g w g g p w p p e r n r r f e r r e p n r p f e r p e e k q e n r e k q e f e r e k q e e g r e v b spec cell platform figure eight impact different map approach manually parallelize code default profile heuristic profile machine learn v e r g e figure nine impact different map approach spec manually parallelize code approach deliver performance code fix heuristic approach achieve performance level respectively lower performance gain spec mainly due better start point spec see section cell diagram figure eight b show base map approach code cell platform compare approach scheme use profile information employ fix map heuristic manually parallelize program specifically tune cell platform perform poorly consequence map approach show high performance gain particular small input data set still combination profile outperform fix heuristic counterpart far average result program across data set summary combine profile approach map come within reach performance code platform case outperform fix strong enough separate profitably loop perform poorly typically static map result performance level less sixty machine learn approach default scheme unable accurately determine whether loop parallelize situation exacerbate cell platform accurate map key high performance exist generic manually parallelize cod fail deliver reasonable performance even base profile data unable match performance base scheme platform scale well number see figure ten fact due cache memory total observe application limit often saturation effect occur four issue line research figure eleven show performance drop step one two cell platform due fact use generally powerful measure single processor performance use multiple parallel performance diagram reveal best case take three achieve original performance scalable follow linear trend two three four five six seven eight number figure ten platform data set art p e e p p e e p ten nine eight seven six five four three two one zero one sixty fifty forty thirty twenty fifteen ten five zero one two three four five six seven eight ten eleven twelve thirteen fourteen fifteen sixteen seventeen number nine figure eleven cell platform data set number increase however remain saturate low level six relate work parallel program many approach propose change extend exist program enable easier exploit parallelism nine ten twelve unfortunately approach alleviate port legacy sequential program automatic parallelization static automatic parallelism extraction achieve restrict loop three four five unfortunately many parallelization could still discover static analysis approach due lack information source code level speculative parallelization exist automatic parallelization exploit parallelism speculatively execution manner thirty approach typically require hardware support al manually parallelize thread level speculation approach rely upon programmer discover parallelism well support parallel execution dynamic parallelization al apply sensitivity analysis automatic parallelize program whose may sensitive input data set contrast static analysis check approach approach discover parallel well select parallel schedule dynamic dependence analysis hybrid data dependence analysis make use dynamic dependence information delay much parallelization work program contrast employ separate profile stage incorporate dynamic information usual compiler base parallelization without cause overhead interactive parallelization interactive parallelization tool thirteen seventeen eighteen nineteen twenty provide way actively involve programmer detection map application parallelism example explorer thirteen help programmer identify loop likely assist user check correctness similarly parallelization system allow programmer specify intend parallelism mark potential parallel program tool use dynamic profile information find good map parallel unlike approach require programmer mark parallel instead discover parallelism automatically moreover problem map parallelism across well address approach parallelism map prior research parallelism map mainly focus build analytical model forty adaptation approach map migrate task specific platform instead propose new schedule map technique particular platform aim develop automatic portable approach learn take advantage exist system efficiently map parallelism forty use solve task map distribute memory machine model require detail hardware platform communication cost underlie architecture change static analytical model propose predict program example compiler use cost model evaluate cost parallelize program also model predict parallel performance however model require help portable al measure performance allocate adaptive loop select number thread schedule policy parallel region contrast approach paper present static processor allocation scheme perform compilation time adaptive compilation machine learn statistical already use single core program transformation example cooper al develop technique find good compiler optimization sequence code size reduction contrast prior research build model learn effectively map parallelism multicore exist model automatically construct train parallelism make compilation time seven conclusion future work paper develop parallelism detection method enhance static data dependence analyse dynamic information result amount parallelism uncover sequential also show parallelism detection isolation sufficient achieve high performance require close interaction adaptive map scheme unfold full potential parallel execution across program result obtain two complex multicore cell two set spec confirm method aggressive parallelization portable exist static achieve performance level close manually parallelize cod future work focus data dependence analysis ultimate goal eliminate need user approval parallelization prove conclusively furthermore integrate support framework target parallelism beyond loop level reference one h p future sop interconnect advance package two may two l parallel execution loop seventeen two three burke r dependence analysis parallelization four r k optimize modern approach morgan five w lim lam maximize parallelism minimize synchronization affine transform parallel compute six r al parallelize compiler technical report seven w hall j al maximize compiler computer twelve eight nine f c k implementation multithreaded language eleven p husband parry c k performance analysis compiler twelve v v sarkar c concurrent pro modern thirteen l al explorer interactive parallelizer eight fourteen kulkarni k b walter al optimistic parallelism require seven fifteen l f k standard adaptive parallel library inter workshop seventeen f p r triolet semantical parallelization overview pip project eighteen k k c w interactive parallel program use editor two three nineteen brand c al set integrate tool parallelization use high performance part environment parallel twelve twenty h honda development implementation interactive parallelization assistance tool two h k de dynamic analysis tool find parallelism industrial workshop w induction variable substitution reduction recognition parallelize compiler technical report e compile time barrier synchronization minimization thirteen six e b g n v train algorithm optimal margin workshop computational learn theory h hall evaluate automatically map ping h bailey e al parallel international journal five three r e grant comprehensive analysis parallel c version v al new suite measure parallel computer performance thirty b thread multiple path execution j compiler estimation load imbalance overhead speculative parallelization pact r f toward speculation parallelism regular access pattern bridge n al revisit sequential program ming model multicore micro l sensitivity analysis automatic parallelization p dynamic dependence analysis novel method data dependence evaluation k system dynamically program l hybrid dependence analysis automatic parallelization technical report c ding x al behavior orient parallelization w v practical approach exploit pipeline parallelism c program micro forty j p methodology parallelize program complex memory c b chapman cost model j x j allocation sixteen seven empirical selection loop l g valiant bridge model parallel computation eight k cooper p optimize reduce code space use genetic f r machine learn approach automatic production compiler artificial intelligence methodology c j iterative optimization polyhedral model part multidimensional time sixteen al hierarchically tile array parallelism locality ten w al stream compiler