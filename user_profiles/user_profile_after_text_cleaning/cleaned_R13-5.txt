learn thin junction tree via graph cut mellon university abstract structure learn usually focus compactness learn model however general compact model exact approximate inference still therefore focus compactness lead learn model require approximate inference thus reduce prediction quality paper propose method learn attractive class model junction tree permit compact representation probability efficient exact inference use approximation likelihood transform problem find good junction tree separator minimum cut problem weight graph use graph cut intuition present efficient algorithm theoretical guarantee find good apply obtain thin junction tree extensive empirical evaluation demonstrate benefit apply exact inference use model answer query also extend technique learn low conditional random field demonstrate state art regularization one introduction structure learn traditionally focus learn compact probabilistic graphical model however compactness sufficient guarantee learn model useful practice appear proceed international conference artificial intelligence statistics beach volume five w five copyright author mainly use inference even approximate inference compact graphical model therefore one usually resort approximate inference yield unreliable result luckily exist exact inference complexity exponential model model low exact inference perform efficiently paper instead take common approach learn complicate model fit data well run approximate inference learn simpler model specifically junction tree bach jordan use exact inference substantial amount work structure learn chow bach jordan n desire k exist learn structure either complexity least take local myopic step bach jordan unlike previous algorithm take step base global criterion complexity polynomial n also k addition attractive asymptotic complexity approach also fast practice paper show approximation likelihood al find optimal separator equivalent find cut weight graph certain metric base graph cut formulation present efficient algorithm find good apply obtain thin junction tree evaluate method different synthetic show faster exist case learn model similar better quality algorithm require weight graph input easily adapt criteria approximation example paper extend approach learn conditional random field al demonstrate empirical state art learn thin junction tree via graph cut main paper three maximize approximation formulate problem structure learn via likelihood graph cut fast randomize algorithm theoretical guarantee find good combine extend algorithm conditional random field two learn junction tree paper learn certain class namely low exact inference perform efficiently section briefly review model detail see al figure one leave junction tree right correspond network let c collection v c call let set edge connect pair c tree definition junction tree c junction tree satisfy run intersection property c unique simple path x x set call separator correspond edge j size clique junction tree minus one call junction tree k example tree leave figure one right correspond network edge every two vertices appear together clique distribution p v representable use c separator render different side independent case projection p c define p c p equal p otherwise projection p c provide best possible approximation sense structure c p paper address follow problem give data integer k treat complete random v seek find good tractable approximation p v specifically try find junction tree k maximize log likelihood data c log p c want efficiently find model high maximize model likelihood equivalent minimize empirical entropy h v give compute entropy efficiently since entropy decompose sum clique minus separator need compute entropy set structure learn typically need explore large number candidate thus force compute many potentially possible set avoid costly dependence k propose score method base free energy approximation entropy al definition entropy let g v e undirected network entropy h sum edge minus h v e h v deg v one h v v e represent net create edge every pair appear clique figure one thus focus find low h claim minimize h equivalent maximize j e proof sketch h j e h v e h constant need edge strong possible term mutual information one natural way get model start dense model remove weak edge weight mutual information low enough motivate reformulate problem section two definition edge removal learn problem input g v e undirected graph weight function w e r integer k goal remove set edge result graph k edge weight come weight could also come regularization estimate pairwise dependence later learn conditional random field edge weight conditional mutual information fa e four find separator give dense weight graph goal remove weak edge low enough achieve goal identify potentially good one one later recover tree list correspond connect notice separator cut graph two connect definition cut cut partition v disjoint v b b call separator want find cut remove separator render b almost independent approximation want weight cut aa w b low cut edge edge remove thus weight correspond loss entropy figure two figure two leave graph dash edge lower weight right partition separator size two st edge b weak possible note actual b illustrate idea weight graph g leave dash edge low weight separator size two cut g independent however separator partition graph weak edge b note pay edge b directly edge go might strong pay approach use separator recurse side b follow formalize intuition definition min edge cut problem input undirected graph g v e weight function w e r integer k goal find separator set k nod partition b remain weight cut aa wab minimize solution via integer program section encode problem integer program later relax linear program round solution want cut k suppose know advance b b formulate problem integer program figure three round demonstrate leave solution number denote internal distance note zero one two cut show different radii six one node separator min st ce k sa zero zero da zero one c v ce zero one one two three four five ce one indicate drop edge e ie e objective ce cost drop edge one indicate node v separator one two assert k b addition define indicator variable one v b every node v constraint three assert b b b finally constraint four transition constraint every edge v b v b either v remove edge v word every path b either node remove integer solution problem specify cut k two recall assume know advance b b however case naive way pair vertices would consider run n instead note arbitrarily pick one vertex test n one vertices b order guarantee separator may need pick k one potential source total check see one practice often stop smaller number check especially sparse graph use follow property objective find cut optimal solution improve relaxation round integer program instead solve linear program relaxation obtain replace integrality five range zero ce one solve may return fractional solution next step derive separator optimal solution two describe randomize round procedure follow return ce zero one interpret distance ce length edge length node nod bigger length fa e learn thin junction tree via graph cut algorithm one g w k input g v e undirected graph w weight function k separator size output cut v v k one return v find small clique need separate one k one pick v b v b minimize ce subject zero ce one da zero one sa zero zero c v k pick b minimal value return g w k b c e v v algorithm two g w k b ce input g v e graph w weight k separator size solution b output cut v distance distance distance distance section find best cut v v b v b pick b highest b v weight g w check add current section five b b w b k call need valid exist return cost cut else return valid one two three four five six seven one two three four five six seven eight nine ten eleven twelve path v sum edge internal node interpretation distance vertex vertex v ie minimal path length three four thus assert distance zero distance b one pick radius zero one uniformly random partition graph set b st b b let v nod completely inside ball center v nod cut b rest see figure three proposition expect cost optimal objective value relaxation e weight cut ce optimal value integer program expect size k e k typically cut cost would higher would big need find choice fortunately scheme provide entire approximate frontier note although take value zero one critical point transition happen around probabilistic method try one radii find solution either k value frontier allow us regularize want pick smaller even however paper interest k therefore explore part frontier pick cut variant scheme two guarantee k give radius compute definition subset b either belong b b b pay edge b b need pay edge compute weight edge b pick k nod greedily result optimal separator k claim greedy scheme guarantee find cut weight cut generate sample st k procedure find partition graph side b connect weak edge use separator recurse side partition stop set size k one however simple approach work generate might guarantee existence add edge every two appear clique network generate might higher k definition mean combine form see figure four example try find ten one four f suppose first six step algorithm choose unique pair one two three four split one f rest six step clique one two three four big split split two previously appear separator lose run intersection property recall problem definition give weight clique goal remove subset edge result graph low weight remove edge minimal begin edge undecided choose cut commit remove edge b keep internal edge w b b b five recursive procedure proof sketch p e cut ce ce p proposition provide guarantee expectation however value word set edge e partition three edge e e e edge commit keep edge remove edge undecided figure four sequence six separation step leave right result make two algorithm successively pick cut graph start e e algorithm proceed move edge e e e eventually e general two might prevent us get end e e ie separator form part recursion cut latter step violate run intersection property alternatively graph induce e may k order ensure run intersection property choose separator make weight clique see way vertices might use split would cost therefore cut split previous word e e claim choose separator split latter order solve problem need look graph induce e construction exactly edge claim let g v e e e e edge e g k throughout recursive computation return note allow g exceed k stage algorithm e grow increase point claim g k e exist cut keep g k reduce size e g k triangulation size k choose triangulation maintain g k example refer figure four pick last separator three four g four look triangulation previous step see one two three one two four separator one two indeed pick one two split three four lead valid order find cut guarantee claim use bind g algorithm three g w k input g v e undirected graph w weight function k separator size output list form g several connect recurse return v k one record v return b g w k weight finite separator find maintain claim find valid use heuristic run find correspond b one two three four five six seven eight nine ten change g w clique weight edge g w k get valid recurse g b w k algorithm four g w input g v e undirected graph w weight function k separator size potential separator output still exist add change g w clique weight edge g g v e e e e k use heuristic bind g k k return true else return false one two three four see four use triangulation method separator violate drop look another one constrain use one vertices one solve would guarantee find finite weight separator every step thus however round may find finite cut even one exist return infinite cut discard instead use separator get previous iteration b obtain use algorithm situation seem arise rarely practice proposition guarantee find objective function objective function section four minimize total weight remove edge since handle find single cut edge remove assume stay graph however longer case undecided edge e may remove later reason suggest change objective function take account edge remove edge keep min ce first component sum previous objective edge weight ce indicate new indicate edge e internal edge b c e b c e c e learn thin junction tree via graph cut achieve add constraint six edge one end ie exactly edge add e need careful double count edge already e sum edge second component new objective therefore total weight newly add edge alternatively also add parameter sum objective still minimize weight e sum weight e e fix take account edge separator new objective also advantage pick run time complexity claim polynomial n k size graph g pass decrease every iteration therefore number recursive call linear n call solver k one time involve thus solve round cut polynomial n consider radii thus total run time polynomial n k note round fast process practice often solver return integer solution one distinct value number radii need consider much less also note solution always solver return solution need continue look pair vertices b separate note regularization regularization often use estimation sparse model order avoid consider several type regularization describe learn maximal size k however smaller lead simpler model instead stop clique size reach k one let split clique cost small define threshold drawback assumption node amount regularization experiment constant set cross validation secondly define threshold compute best pick separator cost one time cost finally restrict model reduce number instead learn full clique result assume structure learn regularization reduce especially small six experimental result use learn various graphical model synthetic utility approximation approximate inference common solution large contrast approach paper perform exact inference approximate model section compare approach generate several artificial network ten thirty pick exact inference model take long time still feasible compare structure learner compact search first compute log likelihood correspond test set figure algorithm slightly worse comparable surprise since directly try minimize likelihood addition try learn thin junction tree thus less indeed model learn high real goal structure learn ability run inference efficiently accurately therefore another way evaluate quality learn model compare quality inference fix random subset evidence condition model run one exact inference thin model learn two fast approximate inference algorithm residual loopy model compare grind truth exact inference real model variable v compute v grind truth conditional correspond conditional approximate model pa average one n v pa let loopy run time algorithm take usually note implementation loopy c refer figure result point represent single network x axis score algorithm approximate inference point x line close mean algorithm closer grind truth classification accuracy addition synthetic test several comparison use denote koller denote reproduce necessary cache advance learn figure five b likelihood algorithm synthetic data experiment c comparison separator size domain size lighten sign indicate sudden death algorithm usually run memory e g experiment real data run local search structure find cut algorithm algorithm combine objective section h classification experiment learn compact bay net k parent every variable use traffic measure every five month four bin learn model three temperature deployment al four bin learn model two sensor like shape learn thin hard alarm discrete data sample know network al four due computational learn three model method run significantly faster take complexity could handle three full take complete refer figure another interest point illustrate figure five algorithm sometimes take less time k number call increase per iteration number smaller start compare result show figure well traffic well temperature alarm reasonable learn compact allow compute likelihood exactly structure may look good inference like test classification accuracy real structure traffic temperature unknown grind truth compare instead fix random subset evidence e test point compute give evidence e accuracy ratio correctly compare classification model model result show figure despite likelihood result accuracy comparable better case seven structure learn conditional random field al undirected graphical model compactly represent conditional p x x label learn harder learn bay net even score structure require inference therefore work assume know structure often linear chain lattice little work learn topology data lot previous work focus special case different notable exception al propose method jointly learn sparse structure formulate task convex optimization problem consider regularization find global minimum result objective ex figure six leave error rate learn right learn model blue edge appear learn model due triangulation red real model tend algorithm assume form f x initial conditional mutual information evaluate logistic regression set log set log set log grind synthetic time sec separator time separator time sec time domain accuracy b c e f g h set log set log set log grind synthetic time sec separator time separator time sec time domain accuracy b c e f g h learn thin junction tree via graph cut every pair order compare structure learn al generate several synthetic whose size range eight obtain fair comparison use code structure well run algorithm twice learn structure one fix structure learn figure six compare error rate approach algorithm win structure although try minimize error measure one structure learn also show figure six edge false positive due triangulation false negative true eight relation prior work except learn likely one solve time chow problem learn optimal junction tree k thus surprise exist learn provide theoretical guarantee quality solution complexity least usually practical k three popular approach base local search bach jordan guarantee result structure local optimum approach fast local provide important advantage algorithm select entire separator every time step exist local approach take myopic view add single edge recently propose learn arithmetic circuit class model necessarily low still admit tractable exact inference approach local guarantee quality result nine future work learn model require use approximate inference yield unreliable result paper propose learn instead simpler model allow exact inference propose new method learn junction tree attractive class probabilistic graphical model permit compact representation efficient exact inference employ approximation model entropy formulate problem select optimal junction tree separator term find cut graph present fast randomize algorithm theoretical guarantee find good show combine obtain model also extend approach learn structure discriminative model addition theoretical guarantee demonstrate approach produce result faster state art interest future work direction extend model arithmetic circuit relational network al also plan explore general al algorithm currently limit test pairwise mutual information take higher account might improve likelihood future plan enhance algorithm handle case perhaps consider believe general approach naturally extend along lead applicability algorithm better quality model work support part career wish thank helpful reference f r bach jordan thin junction tree nip c chow c approximate discrete probability dependence tree information theory n learn network maximum bound graph soda c efficient learn thin junction tree nip j w freeman free energy belief propagation j f pereira conditional random field probabilistic model segment label sequence data r p network expert koller search simple effective algorithm learn network c value information graphical model c madden j w hong data acquisition sensor network j g cooper alarm monitor system case study two inference belief network conference artificial intelligence medicine k murphy g r structure learn random field heart motion abnormality detection p learn arithmetic circuit l n koller b learn prob model relational structure