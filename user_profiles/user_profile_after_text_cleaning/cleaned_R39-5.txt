annual international symposium erase core robust performance amin advance computer architecture laboratory university michigan ann arbor mi shoe abstract performance reliability power efficiency critical design challenge future multicore although point propose address issue fundamental change fabric multicore necessary seamlessly combat challenge towards end paper propose dynamically adaptive fabric blur individual core encourage resource share across core performance fault tolerance process manifestation vision paper provide detail unify solution assemble network potentially break pipeline design rely interconnection flexibility compiler direct instruction steer merge pipeline high performance flexibility enable route around break achieve level defect isolation together result fabric consist pool pipeline fluidly allocate accelerate performance throughput compute tolerate performance reliability one introduction result grow power thermal complexity concern monolithic major hardware lead migration multicore compose relatively simple core today size vary core core compute sun one two three despite attenuate several challenge pertain performance power reliability still remain multicore paradigm first multiple core effective throughput compute provide little gain sequential even major transition towards parallel program occur future law dictate sequential component application present performance bottleneck second power limit number core keep active chip motivate need process four finally increase vulnerability pass generation five six jeopardize objective throughput lifetime multicore chip landscape multicore challenge prior research focus address issue isolation example tackle performance recent article hill seven introduce concept dynamic figure one allow multiple core chip work unison execute sequential cod notion performance allow chip efficiently address require throughput compute high sequential performance anything core fusion eight lightweight nine federation ten representative work objective however scope present day dynamic multicore limit provide process four eleven better throughput achieve twelve thirteen process four figure one b typically accommodate introduce heterogeneity type number functional execution model different core whereas better throughput provide reliability like twelve thirteen disable break pipeline stag instead entire core figure one c within multicore unfortunately virtue independent combine exist performance power reliability neither straightforward quickly become prohibitive change require solution introduce little amortize across multiple performance require dedicate centralize structure add access global wire require variety static core design reliability require either large amount area cold spar flexibility share across core apart excessive direct attempt combine solution also face engineer hurdle instance combine eight performance solution thirteen reliability solution two prominent issue arise one require centralize structure fetch steer commit across fuse structure become single point failure limit reliability benefit two require defect tolerance compatible within single core tightly couple together instead target one challenge time goal paper devise design philosophy naturally extend handle multitude multicore challenge seamlessly overlap cost maintain efficiency avoid centralize structure towards end paper propose architecture see figure one adaptive compute substrate inherently flexible best align immediate system need eliminate traditional core organize chip dynamically network build block sea build block symmetric heterogeneous nature vary target adaptive substrate c core centralize distribute b block pipeline stage group pipeline stag c c c c c c c c c c c c c c c c b static design heterogeneous multicore c c c c c c c c c c c c c c c c n e g r e e h e w two e r c e r c e w one e r c dynamic multicore centralize c core disable fault tolerance sea build block b configure throughput compute performance fault tolerance process figure one contemporary multicore challenge b c vision work centralize use assist fuse neighbor core b different denote heterogeneity c dark shade mark break granularity individual pipeline stag group stag pipeline block provide full flexibility construct logical complete set build block another key feature proposal use distribute instruction execution across block without change execution model major advancement prior performance work address centralize sea block fluidly allocate number performance power reliability throughput compute optimize form many whereas sequential performance accelerate form power performance improve introduce heterogeneous build block fabric appropriately configure dynamically statically active program phase entire enable dynamic approach process finally fault tolerance administer block granularity disable break time guide architectural vision paper present instance target performance reliability fabric pipeline model use single pipeline stag build block first step define pipeline stag one another inspire architecture thirteen enable salvage work stag different row fabric form logical thereby tackle throughput challenge address performance generalize notion logical form vary issue engineer distribute support assembly pipeline stag processor especially hard due heavy communication solution adopt best effort strategy speculate control data across pipeline ways fall back lightweight replay case violation register hardware scheme formulate distribute control register memory data flow management frequency data flow execute two different pipeline ways find lead performance loss address incorporate compiler hint instruction steer program binary circumvent hurdle fuse core present fourteen also achieve pipeline way assignment overall manifestation present paper rely interconnection flexibility compiler direct instruction steer provide unify solution two relate work within framework multicore chip efficient deliver performance throughput desirable section give overview prior work target issue table one summarize key comparison relevant prior stand simultaneously offer performance reliability eliminate centralize structure section also present study motivate need unify sake efficiency single thread performance dynamic dynamic multicore consist collection homogeneous core work independently provide throughput compute subset fuse together provide better performance core fusion eight dynamic multicore design enable fusion adjacent core form federation ten hand combine neighbor core form approach employ centralize structure fetch management register rename instruction steer assist aggregation pipeline contrast lightweight nine leverage edge compiler support eliminate centralize structure enable scale also eliminate centralize structure compiler support limit generate hint instruction steer modify include hint carry instruction fifteen seminal work compose large logical processor many smaller process use instruction sequencer distribute task among process rely hardware satisfy however prior scheme within individual core tightly couple together dismiss opportunity reliability another distinction fuse form fusion provide hide latency large instruction window size fusion make harder due negligible room inefficiency fact al fourteen argue pipeline table one comparison prior work performance centralize support support reliability structure model heterogeneity paper nine core fusion eight federation ten fifteen thirteen twelve heterogeneous four x x x x x x x x x x x x x x x fusion impractical associate hardware interleave active data flow chain instruction steer circumvent challenge use compiler hint guide instruction steer employ simple detect recover data flow heterogeneous heterogeneous design exhibit good power performance target class however static design effectiveness limit outside set flexibility desire instance scenario prefer throughput compute heterogeneous operate addition static schedule job heterogeneous core also dynamic schedule approach match program phase core core contest eleven one example run program redundantly different core allow faster transfer state inclusion heterogeneous block allow static dynamic well dynamic exploitation program phase architecture map possible due inherent flexibility swap cluster early research cluster enable issue without add sophisticate hardware support sixteen architecture good example use static instruction schedule compile time hand use compiler cluster algorithm seventeen generate hint use dynamic instruction steer also contrast past work solely use hardware support eighteen implement distribute among cluster multicore reliability server like tandem nonstop nineteen typically rely coarse grain spatial redundancy provide high degree reliability however dual triple modular redundant incur significant term area power tolerate high failure rate recently elastic twenty isolation propose disable break core chip architectural core salvage interest approach continue use break core subset computation result impact although good limit failure rate scenario need massive number redundant core without face prospect rapidly decline process throughput fault lead core disable category use isolate break stag core reliability thirteen group together small set stag simple crossbar interconnect enable granularity pipeline stage tolerate considerable number al twelve also propose similar multicore architecture core cannibalization ar performance p performance p throughput r throughput r p r p r e e r r c c f f e e h h r r e e v v e e r r seven seven six six five five four four three three two two one one zero two four eight sixteen number core figure two area overhead measure number core support performance p throughput r different size curve show cumulative overhead plot throughput define ability maintain fifty original chip throughput three usage field exploit stage level however allow subset lend stag break thereby avoid full interconnection support way combine performance reliability prior work target multicore challenge separately either performance throughput reliability central problem require new hardware incorporate exist turn expensive proposition hardware cost additive conduct small study assess cost figure two show result study use core fusion nine performance solution standard core disable throughput line plot show cumulative overhead performance core fusion reliability core disable result overhead almost forty additional area two factor play one cost additive two share nothing common two reliability administer core level instead top design test verification validation need duplicate performance reliability separately next section present unify solution overcome issue large extent three architecture overview manifestation architecture present unify solution allow fusion core accelerate performance well isolation defective pipeline stag sustainable throughput fabric consist large group pipeline stag connect use nonblocking crossbar switch yield highly fabric switch replace direct wire link exist pipeline stag include bypass network branch signal stall signal pipeline within completely pipeline stag symmetric crossbar interconnection allow set unique stag assemble logical pipeline basis design core consist five stag namely fetch f decode issue em figure three show arrangement pipeline stag across four interconnect core conceptual chip note introduce within limit core leave memory hierarchy private unify untouched cache assume protection mechanism like tolerate fault within core figure three despite one stage failure shade per core able salvage three work give set active thread judiciously allocate pipeline resource proportion instruction level parallelism instance figure thread one low allocate one pipeline thread two high allocate remain two performance help deal diversity present modern day keep separate throughput compute dynamically configure two processor sequential morph individual conjoint processor require centralize structure maintain reliability benefit transparent programmer figure three processor two example conjoint processor assimilate use two pipeline stag type part conjoint processor two execute single thread instruction stream fetch alternately two odd number one pipeline even number tag age maintain program order execution instruction commit regardless instruction fetch execute either two depend upon source refer instruction steer instruction execute pipeline fetch say execute pipeline say dynamic instruction steer perform objective minimize data dependency critical achieve true performance employ compiler level analysis statically identify data dependency chain section issue stage apply knowledge steer suitable pipeline natural support allow achieve second objective throughput instance figure three able efficiently salvage work stag pool defective form functional virtue lose smaller granularity isolation break pipeline stag reap far better reward traditional core disable realize reliability benefit system rely fault detection mechanism identify break stag configuration manager consolidate work fault detection achieve use combination infield periodic test detail beyond scope paper challenge complex segment logical elementary pipeline stag f e w benefit approach table two challenge challenge basis single conjoint pipeline check mark x use straightforward extension prior work whereas question mark open solve paper control flow register data flow memory data flow instruction steer single pipeline conjoint x x na na although performance reliability benefit configuration flexibility substantial number hurdle face architecture four principal challenge span correctness performance issue single pipeline well conjoint control flow management nature pipeline make global signal pipeline flush stall infeasible context single control flow management cripple absence global flush signal problem even severe case conjoint pipeline fetch stag need read complementary single program stream make consistent control flow ie whether take branch register data flow management register data typically handle operand bypass network rely timely interstage communication unfortunately design make bypass network impractical case conjoint problem aggravate presence cross pipeline register decentralize instruction execution need mechanism track detect replay guarantee correctness memory data flow management memory naturally serialize case single pipeline processor reach memory stage however similar register data flow memory data flow also occur conjoint processor lead corruption global state instruction steer conjoint processor issue stag option straight steer pipeline cross steer pipeline decision dynamically make every instruction number cross pipeline data minimize recent study al fourteen establish steer central challenge pipeline fusion conclude steer solution impractical table two summarize challenge context single multiple work logical processor subset challenge solve mark x prior work thirteen pipeline fault tolerance interconnection solution generic apply processor one single pipeline di ie thread one low thread two high f f f f processor two conjoint em em em em chip core core core core core core core core figure three chip detail look four tightly couple core stag permanent fault shade red core within architecture connect high speed interconnection network allow set stag come together form logical processor addition show exist two feedback em register em f control update processor two conjoint prior reach em stage switch midway result dynamic steer control register data flow memory data flow instruction steer conjoint paper mark new make harder fact unlike true machine even core fusion eight centralize structure need get performance combine loosely couple sake completeness follow also provide quick overview single pipeline case thirteen detail section describe change need architecture majority clever trick detect control data flow distribute fashion relatively complex task instruction steer compiler section control flow single pipeline single pipeline processor absence global pipeline flush signal complicate control flow management event branch pipeline need mechanism squash fetch along incorrect path introduction stream identification target problem thirteen basic idea use distinguish correct path incorrect path fetch execute stag maintain single bite register initialize value discussion simplify actual scheme add register every stage fetch use tag incoming execute stage match instruction tag execute let run point time branch instruction resolve execute stage execute toggle update send fetch stage tag stale recognize execute incorrect path systematically squash time parallel squash receive branch update execute fetch toggle start fetch correct path note single bite suffice pipeline execution model one resolve branch outstanding give time since follow become invalid table three control case case represent pair consecutive program conjoint processor first second row table show fetch leader follower respectively case one case two case three case four branch take branch take branch take branch take conjoint conjoint processor one pipeline designate leader follower balance usage fetch alternate program stream ie leader fetch p c follower fetch p c four logical program order maintain tag every instruction unique monotonically increase age tag fetch stag augment age counter offset one step two whenever instruction fetch tag thus leader pipeline tag age zero two four follower tag age one three five virtue interleave program counter value together fetch complete program stream record program order age tag tag later use execute issue crossbar commit program order description distribute fetch work fine branch instruction encounter proper operation need decentralize control handle mechanism keep sync make control decision control flow encounter four distinct case show table three case one two straightforward branch take continue normal branch impact control flow case three need take branch simultaneously achieve branch completely mirror address perform maintain mirror send branch prediction update stage fetch stag consistent leader pipeline address branch predictor use leader p follower address use f p c design f p c leader p c four synchronize return prediction target address finally case four need take branch addition mechanism case three follower pipeline must also outstanding instruction buffer f em age pending replay issue ie buffer head buffer tail cam figure four pipeline structure detect register data flow initiate replay outstanding instruction buffer current flow tag register two conjoint also show bypass cache data forward within single pipeline date wrong path simple logic add decode stage carry decode stage invalidate operation one follower pipeline two predict take branch fetch three real branch instruction case branch squash conjoint direct extension scheme present single conjoint maintain single logical value branch resolution update send back concurrently fetch stag register data flow single pipeline data forward within single pipeline emulate use small bypass cache execute stage key idea use bypass cache store result recently execute supply later experiment thirteen show bypass cache hold last six result sufficient conjoint conjoint data flow management get involve due distribute nature execution issue execute different register data occur frequently instruction fetch pipeline x need register produce pipeline ideal scenario would like issue stag always steer dependent execute recently produce source value discussion instruction steer follow later section nevertheless practical design steer mechanism bind make mistake pipeline issue stage incomplete information solution nutshell pipeline maintain local version outstanding data monitor detect data flow might occur upon detect violation replay initiate first requirement data flow management proper maintenance register file register file constitute conjoint processor keep coherent achieve send register issue stag simultaneously similar way alpha keep two cluster consistent two serialize network interface execute issue stag crossbar switch base age tag maintain correct program commit order way data sufficiently far away program go register file however issue write back register file remain vulnerable undetected data flow catch undetected data flow pipeline track locally issue monitor detect data dependency accomplish use new structure issue stage name outstanding instruction buffer see figure four similar concept reorder buffer processor however much smaller size need store five pipeline depth issue worst case instruction entry store one code two source three destination four age tag five execute stage allocation execute stage instruction steer six pending replay one per source operand behave cam second field instruction source pending replay bite source operand denote whether data flow violation insert time issue time instruction follow action take place destination value instruction update register file correspond entry also free use cam return instruction f light use source f light send execute stage execute bypass cache would successfully forward register value pending replay bite reset zero source operand f light f light send execute stage data flow violation possible pending replay bite source set one time replay bite source operand get multiple time final write make producer operation every consumer issue stage receive instruction pending replay bite set source imply producer value instruction execute execute stage different instruction steer therefore data flow violation occur replay initiate point replay mechanism discuss later section memory data flow provide correct memory order behavior conjoint processor use local store queue issue stag monitor load perform forward speculative store buffer stage allow delay release memory store save accidental memory corruption note cache hierarchy leave unmodified cache private single port naturally keep coherent standard cache coherence figure five show pipeline emphasis structure need proper memory handle store buffer age address ex state f em issue value ie fifo cam figure five pipeline emphasis structure add handle memory data flow add stage hold onto store value release memory hierarchy although common structure many store buffer also serve purpose keep speculative store corrupt memory state store queue add issue stag tabulate outstanding store present state every store instruction two possible state issue store enter local store queue get store send state store instruction confirm incorrect execution path point pseudo commit signal send crossbar switch stage execute store store instruction state become pseudo commit send upon receive signal execute release store value head store buffer memory way store correct path execution update memory three possible case involve memory need closer scrutiny see table four table four memory flow case case represent pair flow together conjoint processor leader pipeline follower pipeline st case one case two case three st case one branch occur right store program order since store already execute stage value enter store buffer fortunately accordance commit order branch operation write back store thus store never get release pseudo commit eventually store remove store buffer branch flush pipeline case two release thus memory hierarchy correct order present case three load commit issue stag check outstanding store conflict load use store queue indeed store precede load program order base age send different execution stage replay initiate start load replay mechanism replay mechanism add single bite state issue stage call current flow tag leverage issue stage see figure four single bite similar branch identify old wrong new replay issue tag bite head tail mark window replay event register memory data flow violation violation first identify one issue stage consequently send flush instruction execute stag flip bite reset bypass cache clear store buffer follow issue stag send replay signal start reissue respective start head pointer tag update bite old tag stale uniformly discard issue interconnection interconnection network simple connection employ nonblocking connect adjacent level pipeline stag allow pair stag share crossbar communicate simultaneously interface interconnection network pipeline stag maintain latch input output make interconnection network separate stage thus interfere critical main processor stag order make basic crossbar design suitable architecture three feature require depend capability interconnection send one value multiple instance send issue stage register file simultaneously instruction steer require capability steer issue appropriate execute stage single header bite instruction add specify output execute stage instruction want reach age case older give priority require addition router let case basis age crossbar switch support set crosspoint gate logic high multiple output recently propose base crossbar architecture name demonstrate ability low power area overhead instruction steer age add wrap logic around however paper suggest feature also implement use circuit crossbar reliability power time order protect interconnection network fault tolerant version use similar approach interconnection power break crossbar power interconnection link power account per methodology absence buffer network significantly cut overhead finally model interconnection link latency use intermediate pitch wire model technology make sure exceed critical pipeline stag instruction steer depend upon intelligent steer conjoint order minimize performance degradation data dependency replay instruction steer need make time instruction issue broadly speak instruction steer twofold one balance two two minimize number replay experiment show use purely hardware base solution dynamic steer neither cheap effective pipeline fusion concur conclusion fourteen thus adopt hybrid approach instruction steer nutshell compiler pass use assign instruction stream hint encode steer make part compile application binary hardware recognize special steer use effectively conduct dynamic steer steer different conjoint processor analogous graph partition cluster goal obtain balance take advantage hardware parallelism multiple cluster reduce need move transfer value cluster leverage generic cluster form instruction stream fairly straightforward avoid equivalent move replay mechanism describe previous section ensure dependent separate n intervene incur replay even steer different execute stag n latency therefore two main cluster minimize move overlap move computation naturally result instruction stream amenable architecture use well know greedy bug seventeen cluster algorithm generate hint steer steer instruction introduce order encode steer information two insert leader follower begin every instruction block basic block super block steer simply bite encode pipeline assignment every instruction within block multiple insert large code block figure six show example complete hybrid steer setup action first step consist perform bug cluster algorithm compiler second step encode cluster algorithm suggest pipeline embed steer top two final code l stand leader pipeline assignment f follower pipeline assignment leader pipeline fetch steer learn steer instruction one l three f five l seven l nine f follower pipeline behave analogously four evaluation methodology comprehensive set tool use evaluation evaluation setup span program compilation level simulation area power model table five architectural architecture pipeline frequency area power core branch predictor memory interconnection outstanding instruction buffer store queue store buffer size bypass cache size process thirty global history predictor size sixteen one cycle hit latency per core five cycle forty cycle hit latency full nonblocking wide five three three six specific compilation instruction steer compilation system use perform bug cluster algorithm seventeen instruction steer move latency five cycle use input algorithm simulation simulator model group similar core thirty interconnect form network stag simulator develop use liberty simulation environment architectural attribute detail table five cache unify size number core original pipeline also use performance architectural conduct choose three source area overhead design block wire industry standard cad tool library characterize process use estimate area design block description obtain thirty bypass cache essentially small memory structure estimate use similar size cam structure structure replay logic stream identification control implement obtain accurate area number area interconnection wire stag estimate use methodology eight intermediate take road map power thermal model power dissipation various design simulate use execution trace run media crossbar power dissipation simulate separately use representative activity trace crossbar place rout use cadence encounter run stage crossbar interconnection power calculate use standard power capacitance predictive technology model intermediate node thermal model conduct use thirty model model calculate various system use empirical model find entire core qualify ten calculate use mean generate time module system sake consistency model make similar thirteen ten choose rough estimate cross pipeline dependency fetch pipeline execution pipeline one two three four five six seven eight nine ten mem mem mem two two mem four original code six nine one ten leader l pipeline assignment follower f pipeline assignment step one pipeline assignment hint compiler step two steer insert compiler hint four five seven two three eight one two three four five six seven eight mem nine ten mem mem two two mem four final code steer figure six instruction steer white nod indicate assign leader pipeline shade nod correspond follower pipeline instruction fetch perfectly balance two pipeline execution guide steer future beyond note technology node use get power area quantitative comparison scheme experiment involve compare two one conventional chip core consider faulty fail two chip thirteen share way tackle reliability single thread performance performance rely ability accelerate performance conjoin figure seven show plot compare performance four normalize plot also include sake express number conjoint issue width pipeline stag follow examine single pipeline conjoint single pipeline conjoint conjoin stag compare pipeline conjoint pipeline two pipeline stag inherently set pipeline perform roughly ten worse primarily due interstage transfer similar result work thirteen hand conjoint pipeline find deliver consistent performance advantage lag behind gain prominent kernel fact kernel almost performance gain see use conjoint processor availability long independent data dependence chain make result possible contrast show negligible negative performance use conjoint processor namely due lack independent stream typically form long dependence chain compiler pass steer end allocate pipeline minimize replay cost result nearly complete serialization program render half execution useless case steer different lead data flow worsen overall performance initiate replay bar three rest result strongly favor conjoint processor design average gain see single pipeline processor conjoin stag compare single pipeline processor conjoint pipeline processor processor pipeline stag inherently set single logical pipeline system would behave processor use conjoin two combine form processor conjoint pipeline show improvement performance pipeline improvement note make pipeline stag fault isolation granularity system reduce half discussion continue later section along reliability experiment conjoin two single time show favorable result two main reason one limit availability independent chain two place issue architecture replay cost performance advantage conjoint processor largely determine efficiency instruction steer balance load two minimize replay analyze cost replay conjoint processor figure eight show three total execution time memory flow violation replay cycle register flow violation replay cycle normal operation cycle majority devote small fraction execution time replay cycle average fifteen total replay cycle memory replay contribute negligible fraction expect result memory replay happen store load forward miss system rare event perspective power efficiency result encourage small percentage work perform system go waste note low number replay cycle necessarily imply good performance instance conjoint processor steer pipeline result zero replay dependency however compare would observe energy efficiency comparison one issue one issue two issue two issue single pipeline one issue single pipeline one issue single pipeline two issue single pipeline two issue conjoint two issue conjoint two issue conjoint four issue conjoint four issue c c p p e e z z l l r r n n three three two two one one zero zero fifteen fifteen five five ninety ninety eighty eighty seventy seventy sixty sixty fifty fifty forty forty thirty thirty twenty twenty ten ten zero figure seven single thread performance result normalize processor express number conjoint x issue width pipeline stag replay cycle replay cycle replay cycle replay cycle normal operation cycle normal operation cycle figure eight contribution memory replay cycle register flow replay cycle normal operation cycle total computational time individual run conjoint processor average replay contribute fifteen execution time design compare use metric metric sensitive performance change optimize yield result optimize energy time delay square figure nine show average four normalize processor go pipeline twenty energy efficiency sacrifice however superior performance significantly improve energy efficiency throughput performance system measure either latency thread execution performance prior experiment rate job complete system throughput throughput comparison three compare one another thirteen core refer single issue pipeline resource thus would eight interconnect system utilization vary occupancy ten occupancy refer number thread assign system versus capacity measure number core experiment conduct vary set thread allocate system utilization level figure ten show final throughput result experiment peak utilization level ten deliver best throughput due performance advantage processor single pipeline see per fifteen three two one zero five c c p p p p h h g g r r h h eight eight seven seven six six five five four four three three two two one zero pipeline one issue pipeline two issue conjoint two issue conjoint four issue figure nine compare energy efficiency core eight core eight core eight core eight core eight core eight core five one system utilization number thread number core figure ten throughput comparison different level system utilization utilization five imply four work thread assign system utilization multicore deliver throughput advantage result throughput identical default use one pipeline per thread peak utilization scenario system utilization lower able leverage idle pipeline form conjoint thus system consistently deliver best throughput utilization level one realistic expectation fault tolerance experiment far target performance aspect architecture order evaluate reliability face eight core eight core eight core eight core eight core eight core zero five one five two five three five four five five five six five seven five eight five nine zero one two three four five six seven eight time throughput lifetime fix utilization initial throughput settle system ten core ten core four core four core eight core eight core c c p p p p h h g g r r h h c c p p p p h h g g r r h h seven seven six six five five four four three three two two one one zero eight eight seven seven six six five five four four three three two two one one zero zero five one five two five three five four five five five six five seven five eight five nine zero one two three four five six seven eight time b throughput lifetime fix utilization system show convince result among three consider figure eleven lifetime reliability experiment various consider experiment conduct experiment track throughput system course lifetime experiment fail reach respective system get lifetime whenever failure introduce break stag isolate use interconnection flexibility fault tolerant naturally handle crosspoint configuration manager every time failure occur set change assume simple policy one assign single pipeline two remain allocate thread basis available throughput system compute new configuration base number work logical assign run chip get statistically result average system utilization experiment keep since throughput deliver system improve system utilization lower see figure ten result report conservative figure eleven show throughput lifetime three clearly outperform entire lifetime early achieve throughput advantage utilize idle six thread active leave two free form conjoint regular benefit later lifetime sustain throughput advantage effectively salvage work stag maintain higher number work instance system throughput drop two c around year mark whereas system throughput breach level around six year mark gain add lifetime cumulative work do integral throughput lifetime advantage also note throughput converge later part lifetime happen number thread assign system exceed number work leave option default back single pipeline figure eleven b compare two system choose comparison system area overhead twenty discuss later translate roughly two core system result show early lifetime dominate two expect start maximum amount however accumulate quickly lose advantage beyond two year mark consistently dominate system throughput system perform worst among three two reason one run thread concurrently instead two stag result bigger resource loss stage area area various structure part architecture show table six overhead relative processor core total five interconnection present architecture since share overhead attributable one pipeline case eight connect together form bear crossbar overhead assumption total area overhead architecture traditional contain core table six area different design block design block outstanding buffer five store buffer three store queue three bypass cache six extra stage latch input output miscellaneous logic fault tolerant crossbar interconnection wire five share eight total area overhead area percent overhead fifteen two eighteen nine power power overhead come three source interconnection miscellaneous logic extra latch new table seven show breakdown total power overhead actual power number table one pipeline part conjoint processor note part overhead even traditional relative two independent table seven power overhead report power consumption component interconnection link design block total power overhead power overhead pipeline forty percent overhead percent overhead five conclusion multicore era one hand abundant throughput incorporate die performance power efficiency challenge still confront increase process variation thermal stress limit scale efficiently address longer rely evolutionary design process simply combine exist research performance reliability neither easy paper present highly adaptive fabric design performance reliability target grind interconnection flexibility within ensure impressive couple addition decentralize instruction flow management also merge pipeline accommodate dynamically change application experiment demonstrate merge two within deliver average gain respect pipeline half core occupy merge enhance throughput performance finally lifetime reliability experiment show chip increase cumulative work do traditional six thank anonymous referee valuable comment author acknowledge support research center one five research center fund focus center research program semiconductor research corporation program research also support national science foundation grant arm limit seven reference one p k k multithreaded processor micro vol two march two l al architecture visual compute graphics vol three three processor product brief four r k n p p heterogeneous multicore potential processor power reduction annual international symposium five design reliable unreliable challenge transistor variability degradation micro vol six six j v p bose j impact technology scale lifetime reliability international conference dependable network june seven hill r law multicore era computer vol one accommodate diversity chip annual international symposium computer architecture nine c kim n w lightweight annual international symposium ten boyer k federation scalar core instruction issue design conference june eleven h h e architectural contest international symposium computer architecture twelve b f j core cannibalization architecture improve lifetime chip performance multicore processor presence hard fault international conference parallel compilation thirteen j fabric construct resilient multicore annual international symposium fourteen p c fundamental performance horizontal fusion core international symposium computer architecture fifteen g e breach n annual international symposium computer architecture june sixteen k p chow n z architecture reduce cycle time partition annual international symposium seventeen j bulldog compiler press eighteen instruction distribution annual international symposium nineteen w l commercial fault tolerance tale two dependable secure compute vol one one twenty sylvester e elastic adaptive architecture unpredictable silicon journal design test vol six n p n p j e smith isolation build high availability commodity multicore annual international symposium computer architecture architectural core salvage multicore processor tolerance annual international symposium computer architecture june arm armor cache high defect density annual international symposium alpha family z foo b sylvester swizzle network technology interweave pipeline stag variation tolerant fabric international conference dependable network june ye l g analysis power consumption switch network design conference infrastructure research thirty n j malik august liberty simulation environment deliberate approach system model computer vol three r n chip annual international symposium predictive technology model w r k k compact thermal model method large scale integration vol fourteen five may j v p bose j case lifetime annual international symposium computer architecture june brook v framework power analysis annual international symposium computer architecture june eight e n j core fusion international technology