automatic search engine performance evaluation data analysis fu state key lab intelligent technology university china min state key lab intelligent technology research technology university china development center incorporation china abstract performance evaluation important issue web search engine research traditional evaluation rely much human therefore quite data analysis propose automatic search engine performance evaluation method method generate navigational type query answer automatically base search query click behavior experimental result base commercial search engine user log show automatically method get similar evaluation result traditional subject performance evaluation general term experimentation measurement performance evaluation data analysis one introduction evaluation one key question information retrieval research currently evaluation research base study two approach base set query correspond answer usually call evaluation metrics query process system result compare use evaluation metrics annotation correct answer usually difficult part kind time cost commercial search engine evaluation would unacceptable use manual annotation therefore automatically evaluation process objective reliable useful search monitor improve search several recent attempt make towards automatically evaluation order tackle relate manual assessment however involve use user behavior information valuable three design unbiased automatic assessment use data work base small scale data collection several hundred user click collect reliability effectiveness use data evaluation remain study main work propose fully automatic approach measure search engine performance copyright hold may canada base data instead build query set annotate relevant document manually select annotate answer automatically analyze query log data correctness automatically annotate answer evaluation result compare manual verify reliability approach two evaluation base data analysis search engine performance evaluate effectiveness meet different information need base query log analysis vista one group search request three navigational informational transactional major difference navigational type query two type query whether user fix search target page focus navigational type query evaluation approach firstly always need find particular web site certain web page navigational query frequently propose many commercial search design specific feel lucky function secondly usually one correct answer navigational query avoid problem miss correct answer problem may lead failure evaluation cause lower estimation search provide answer annotate feature extraction navigational type query selection navigational query adopt approach need pick kind query use data analysis previous work five user query accord information algorithm eighty query correctly classification result obtain use train set query separate test set human annotate query algorithm use evaluation approach separate navigational type query automatically answer annotation click distribution feature propose lee al four use lee query type identification find variation also use automatically annotation process click distribution query q define support national key foundation research development plan natural science foundation national high technology project query q session session involve click one navigational type query q define click web search query q propose navigational type query click certain result consider result search target hence likely correct answer q long search return answer relative front position find click give possibility annotate search target page navigational query use information different give definition two click follow equation hold accord one two three session session result query q r r query query q annotation process describe process locate query q step approach include search engine result crawl evaluation metrics calculation similar traditional approach three experiment experiment base data collect june one commercial search web environment part data free log record fifteen million query click per day contrast labor intensive manually assessment approach efficient process automatically merely one hour efficiency result obtain use one four cost us lan network environment answer annotation experiment annotate three group query use log different time five annotate query pick randomly manually check correctness table one size annotate query set accuracy june six august six six six seven check sample set annotate answer annotate query accord result show table one see time period ten thousand query successfully annotate sample annotate answer correct size annotate query set bite smaller time period shorter two check answer sample set find answer usually correct answer instead correct answer cause fact correct answer always query example welcome one popular service china therefore propose query usually click service portal automatically approach annotate answer instead performance evaluation experiment query set answer set annotate section two search engine performance evaluate use traditional metrics one use experiment evaluate navigational type query result show figure one figure one comparison five search evaluation result method automatically method set navigational query select randomly query log answer annotate use pool method result pool build aggregate search result collect search evaluate accord figure one find automatically evaluation result performance rank manual one correlation value two indicate two evaluation result quite similar four future work paper propose automatically performance evaluation approach web search approach base behavior record search user log method one construct navigational type query set annotate answer automatically accord experimental result answer correctly besides automatically evaluation result highly correlate annotate future study focus follow much data need evaluation reliable efficiency whether evaluation result reliable help data collect two search engine combine information besides question also want find whether automatically method extend evaluation type query besides navigational type five reference one taxonomy web search forum volume number two two evaluation evaluation information retrieval proceed three evaluate retrieval performance use click data workshop four lee z j cho automatic identification user web search conference five l automatic query type identification base information one mean reciprocal rank metric navigational type evaluation equal reciprocal correct answer rank result list mean