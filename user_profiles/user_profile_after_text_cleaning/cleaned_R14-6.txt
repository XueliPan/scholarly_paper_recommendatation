r cad knowledge medical abstract propose undirected graphical model generally learn make explicit previously unstated large class type also clarify fail build upon new model propose improve method novel kernel process result approach convex avoid unlike previous learn furthermore automatically estimate much view trust thus accommodate noisy unreliable view experiment toy data real world data set illustrate benefit approach one introduction data sample may sometimes characterize multiple ways describe term textual content page structure one show error rate unseen test sample upper bound disagreement obtain independent ie view data thus example misclassification rate indirectly minimize reduce rate disagreement provide independent conditional class many application class label expensive obtain hence scarce whereas unlabeled data often cheap abundantly available moreover disagreement class label suggest different view compute even use unlabeled data therefore natural strategy use unlabeled data minimize misclassification rate enforce consistency classification base several independent unlabeled sample brevity unless otherwise specify shall use term describe entire genre rely upon intuition although strictly refer original algorithm two jointly optimize objective function include misclassification loss term view regularization term penalize lack agreement classification different view recent time approach become dominant strategy exploit intuition behind consensus learn render obsolete survey section two major approach theoretical guarantee spur interest topic previously publish concern applicability certain analyze precise make optimization criteria better understand approach succeed fail certain section three propose undirected graphical model call show provide one way learn probabilistic model explicitly highlight previously unstated one provide understand framework also able discuss certain fundamental consensus learn section four show even simple visually illustrate sometimes amenable solution matter specific use include empirical study two real world data set also illustrate summarize algorithmic exactly equivalent use novel kernel support vector machine process thus allow one leverage large body available literature kernel intrinsically nonstationary ie level similarity pair sample depend available sample whether label unlabeled thus promote learn therefore approach significantly simpler efficient use previous furthermore automatically estimate much view trust thus accommodate noisy unreliable view two relate work theoretical guarantee iterative alternate method originally introduce two work bootstrap mode repeatedly add unlabeled sample pool label sample retrain view additional unlabeled sample least one view confident decision paper provide guarantee exist weakly useful view data b sample conditionally independent give class label algorithm utilize unlabeled data learn arbitrarily strong one prove guarantee sample size large b different view conditionally independent give class label c classification base multiple view largely agree high probability misclassification rate upper bound rate disagreement base view three try reduce strong theoretical show would useful exist low error rate view b never make mistake classification confident c two view highly correlate sense would least case one view make confident classification classifier view much confidence decision theoretical guarantee intrigue theoretically interest also rather unrealistic many application assumption make mistake confident class conditional independence rarely satisfy practice nevertheless empirical success report relate algorithm four extend original bootstrap approach algorithm operate simultaneously unlabeled sample iterative batch mode five use idea base subsequently unsupervised learn six however also suffer local maxima iteration optimization step clear really expectation maximization algorithm ie lack clearly define overall monotonically improve across seven propose approach consensus learn base simultaneously learn multiple maximize objective function penalize individual classifier include regularization term penalize high level disagreement different view framework improve upon maximize convex objective function however algorithm still depend alternate optimization optimize one view time approach later adapt spectral cluster eight relationship current work present work provide probabilistic graphical model consensus learn alternate optimization base show one algorithm accomplish learn model efficient alternative strategy propose fully classification model practice strategy offer several advantage easily extend multiple view accommodate noisy view less predictive class label reduce memory two figure one factor graph b model three learn process process define prior function statistics nine random function f r follow denote h every finite number data point f f xi n follow n h k mean h h xi n normally fix mean function h zero take parametric usually stationary form kernel function kernel x x two zero free parameter covariance k xi n supervise learn scenario output target give observation xi regression r classification one one model assume latent function f underlie output p p xi p f prior p f h give latent function f p xi p xi take noise model n f xi two regression sigmoid function xi classification dependency structure model show undirected graph fig one maximal graphical model fully connect nod f f pair f xi one n therefore joint probability random f f xi define p f one f xi potential z f f one two f f xi one xi f xi two regression classification one normalization factor z hereafter z define joint probability sum one undirected graphical model learn learn suppose different view set n data sample let x j feature sample obtain use view dimensionality input space view j vector xi x one complete representation data sample x j x j n represent sample view learn let yn single output assign data point one x j x one clearly concatenate multiple view single view apply model basic idea learn introduce one function per view use feature view jointly optimize function come consensus look problem perspective let denote latent function view ie use feature view j let zero j prior view j since one data sample one single label even though multiple feature multiple definition paper overload simplify notation mean clear function three b one one one two two two f f f view ie latent function value x j latent function value data sample view j label depend challenge make dependency explicit graphical model tackle problem introduce new latent function consensus function ensure conditional independence output latent function view see fig one b undirected graphical model functional level output depend latent function depend via consensus function joint probability p one z potential function grind network n data sample let f c xi n f j x j graphical model lead follow factorization n p f c f one f xi f j f j f c two one z potential f j specify dependency structure within view j consensus potential f j f c describe latent function view relate consensus function prior view define follow f j f j j f j f j f c three one two f j f c two j covariance matrix view j ie x j x j j zero scalar quantify far away latent function f j f c output potential xi define one regression classification k x j insight may gain take careful look one rely intrinsic structure view ie covariance set two consensus potential actually define difference f j f c ie f j f c n zero two j also interpret assume conditional f j consensus f c mean alternatively focus f c joint consensus effectively define conditional prior f c f one f n c two c c two c j f j two j two c one one two j j four easily verify product indicate prior mean consensus function f c weight combination latent function view weight give inverse variance consensus potential higher variance smaller contribution consensus function undirected graphical model see discuss detail follow one advantage representation allow us see many exist learn model actually special case propose framework addition interpretation also help us understand benefit marginal one learn take integral two f c ignore output potential moment obtain joint marginal distribution latent function p f one f f j f j five one two one z one two j k f j f k two two j two k clearly see negation logarithm marginal exactly recover regularization term learn first part regularize functional space four view second part constrain function need agree output inversely weight sum correspond perspective five actually define joint prior latent function f one f n zero one matrix definition j j j k j one two j two k one two j two j j j j one j j six jointly target variable marginal instance regression p f one f one z one two j f j two j two two one two f j f j one two j k f j f k two two j two k seven recover least square loss form marginal two kernel joint kernel define six interest large dimension difficult work interest kernel obtain instead integrate latent function two lead prior p f c n zero consensus function two j one one j eight follow call kernel learn important reveal previously unclear insight different view combine together learn framework allow us transform learn problem problem simply use kernel solve classification regression since equivalent five end largely similar algorithm however key difference contrast previous additional benefit kernel include follow one kernel avoid repeat alternate different view f j directly work single consensus view f c reduce time complexity space complexity maintain memory learn two alternate optimization might converge local minima optimize integrate single consensus view guarantee global optimal solution learn three even individual stationary general nonstationary add invert set data partially label kernel label data also dependent unlabeled data hence propose kernel use learn ten benefit propose undirected graphical model provide better understand learn kernel eight indicate equivalent learn special nonstationary kernel also preferable way work learn since avoid alternate benefit mention trustworthiness view graphical model allow view j level uncertainty trustworthiness two j imply less confidence observation evidence provide view thus view data better predict output weight form consensus j particular value two five figure two toy big denote one one label point remain point unlabeled top leave result data mean two two two two center right canonical data mean two zero two zero bottom leave data four center right pure supervise learn result kernel much worse supervise learn case two feature learn unit variance kernel use width one supervise learn one easily optimize framework maximize marginal output omit paper due space limit unsupervised learn propose graphical model also motivate new unsupervised learn spectral cluster similarity matrix view j encode kernel encode similarity two data sample multiple view thus use directly spectral cluster extension learn also straightforward since definition depend unlabeled data well alternative interaction potential function previous learn rely potential three call also possible lead different model actually definition three fundamental lead learn see next subsection mention three interpret define prior four f c mean weight average individual view average indicate value f c never higher lower single view intuitive useful many limit real world evidence different view additive enhance rather average instance radiologist make diagnostic decision lung cancer patient might look image image either two image give strong evidence cancer make decision base single view image give evidence six one scale final evidence cancer higher say eight either clear learn scenario previously propose thus far base enforce consensus view principle graphical model allow form six one x two one x two one x two one x two one x two one x two table one result different number train data bold face indicate best performance significantly better one rank sum test except train model text inbound link outbound link train train eleven seventeen view particular three great interest future research four experimental study toy show toy classification visualize result fig two first example case either feature x one x two fully solve problem top leave ideal case since one single view sufficient train classifier two view conditionally independent give class label second toy data bite harder since two align x one axis case feature x two totally irrelevant classification problem canonical fail top center since add label use x two feature noisy label introduce expand future train propose model handle situation since adapt weight view penalize feature x two top right third toy data follow shape four form binary classification problem linearly separable bottom leave case mention violate fail completely bottom center supervise learn model however easily recover nonlinear underlie structure bottom right indicate kernel suitable problem web data use two set link document experiment data set contain belong six class three natural view text view consist title abstract paper two link view inbound outbound reference pick class contain document test classification performance data set collection academic web page manually group six class student faculty staff department course project two view contain text page anchor text inbound link respectively consider binary classification problem student faculty document respectively compare learn text inbound link method process logistic regression linear use compete canonical method repeat fifty time iteration add predictable one positive sample r negative sample train set r depend number ratio data set performance evaluate use score measure vary number train document ratio proportional true ratio use unlabeled data train process experiment repeat twenty time prediction mean standard show table one two see better supervise case better canonical achieve best performance however canonical good supervise thus also worse supervise though little better maybe text link feature independent give class label especially two class faculty staff might share feature canonical higher due possibility add noisy label also try number fifty seem give overall best performance seven table two result different number train data bold face indicate best performance result significantly better one rank sum test model text inbound link train train seventeen thirteen nineteen fifteen note learn text almost achieve performance method number text feature much link feature text feature link feature multiple view unbalance take account different weight provide natural way five paper two principal propose graphical model combine data show previously derive base train maximize likelihood model process show make intrinsic assumption form p even though explicitly realize also study assumption prove unreasonable thus first contribution clarify implicit consensus learn general particular motivate graphical model second contribution development alternative particular development nonstationary kernel development use classification unlike previously publish approach handle naturally two view b automatically learn view data trust predict class label c show leverage previously develop efficiently train clearly explain optimize overall e suffer local maxima f less demand term speed memory reference nip one pac generalization bound nip two combine label unlabeled data colt three n k yang expansion towards bridge theory practice four k r analyze effectiveness applicability workshop information knowledge management five support vector learn six estimation mixture model use seven b l eight de sa spectral cluster two view workshop learn multiple view classification nip nine c e c k process machine learn press ten learn field process technical report eight