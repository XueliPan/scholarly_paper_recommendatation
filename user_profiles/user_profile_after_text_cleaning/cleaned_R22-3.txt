semiautomatic extraction exploitation hierarchical pipeline parallelism use profile information institute compute architecture school university unite kingdom abstract recent multicore computer leave realm compute virtually today embed compute equip several process core still single parallel program model find widespread support parallel program remain art majority application addition exist plethora sequential legacy automatic parallelization hope benefit increase process power modern multicore past automatic parallelization largely focus data parallelism paper present novel approach extract exploit pipeline parallelism sequential use profile overcome static data control flow analysis enable aggressive parallelization approach orthogonal exist automatic parallelization approach additional data parallelism may exploit individual pipeline stag key contribution paper representation support profile parallelism extraction exploitation demonstrate enhance conventional pipeline parallelization incorporate support loop pipeline stage replication uniform automatic way evaluate methodology set stream process demonstrate machine subject program program concurrent program general term experimentation measurement performance permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee pact ten copyright parallelization pipeline parallelism stream program dependence graph one introduction multicore compute become ubiquitous technological provide unprecedented compute power virtually game console even mobile phone however realize full potential well parallelize unfortunately efficient parallelization sequential program challenge task generally agree manual code parallelization expert result streamline parallel implementation time costly approach parallelize compiler technology hand potential greatly reduce cost ensure formal correctness result parallel code automatic parallelism extraction certainly new research area eighteen progress achieve restrict loop three one nineteen fact research result whole range parallelize research ten recently five complementary ongoing work many parallel program seven eight eleven program model seventeen nine propose interactive parallelization tool twelve sixteen two thirteen provide way actively involve programmer detection map application parallelism still demand great effort user approach make parallelism expression easier past effort involve discover map parallelism still far greater write equivalent sequential program past parallelize compiler technology target mainly scientific abundance data level parallelism hence scope largely restrict detection map particular kind parallelism hand popular many stream typically comprise multiple level parallelism chip well proliferation consumer electronics underline importance tool take approach target one two three four five seven eight nine ten eleven twelve fourteen sixteen seventeen eighteen twenty thirty end input decode input g e e n f e n f level one zero zero stereo g e c l e f c r decode sample e r e zero stereo r e r e n l zero hybrid level three level two zero zero level three f two two else clip f f sample level one approach input decode replication header seven potential output two potential stereo reorder subband synthesis output two bottleneck stage stage figure one typically consist computation operate stream data different granularity figure show code excerpt twenty decode application parallelism paper concern extraction exploitation pipeline parallelism central stream process exclude exploitation level parallelism fact approach present paper orthogonal data level parallelization within individual pipeline stag parallelism detection tightly couple static data control flow analyze provide essential information dependence parallelization scheme must obey order guarantee correctness unfortunately dependence analysis often statically undecidable conservative need make turn limit success automatic parallelization recent advance profile base parallelization however demonstrate many potential materialize aggressive approach deliver performance match manual parallelization require minimal user interaction build top work present optimistic pipeline extraction methodology base intermediate representation profile profile hierarchical whole program representation profile data control inherently unsafe approach guarantee correctness therefore provide user graphical representation extract pipeline highlight critical manual verification traditionally information gap profile compilation due binary profile tool track machine instruction level unable back annotate information make usable within parallelize compiler bridge gap show instrumentation compiler intermediate representation enable us make profile data readily available compiler furthermore develop uniform program representation base program dependence graph use throughout entire parallelization process include profile transformation partition map eventually code generation introduce hierarchical pipeline extraction methodology split function loop demand example achieve better balance pipeline stag individual pipeline stag form performance bottleneck show operate independent data replicate effectively create opportunity execution approach complete comprise stage automatically form process build top lightweight system current code generator target readily available commodity require special hardware operate system os support evaluate approach number popular stream process take spec suit contain complex idiosyncratic program construct typical many demonstrate perform pipeline extraction presence loop dynamic memory management interleave io achieve machine eleven motivate example typically comprise operate stream data different granularity illustrate code excerpt figure one represent simplify outermost loop operate audio frame stereo operate stereo channel decode operate single audio channel invocation clear approach form partition code single level four thirty always bind execution time pipeline stage example partition level one form pipeline two highly stag input decode total execution time output two theory lead minuscule two improvement sequential execution practice lead slowdown due communication similar partition neither partition level two three alone result balance pipeline twenty example operate representation low level perform necessary data require parallel implementation approach hand restrict single loop level extract pipeline stag construct balance pipeline right side figure one use hierarchical representation identify pipeline stage represent bottleneck input decode selectively expose inner level create pipeline span multiple level loop nest equally balance pipeline potential general loop partition expose parallelization thus provide flexibility partitioner instance decode function inherently sequential sample latter fact parallel stage operate two audio channel independently replicate show rightmost pipeline figure one summary approach extract pipeline structure programmer would construct use data flow stream program paradigm instance replication pipeline stag correspond stateless filter program language similarly loop partition correspond multirate signal process filter operate different input output rat twelve among paper demonstrate powerful tool extraction exploitation parallelization bridge loop level gap execution profile maintain within parallelize compiler simple yet effective back annotation capability distinguish approach profile develop approach extraction process sequential exploit power avoid expose overly detail possibly redundant dependence information pipeline extraction pass use iterative selective unfold strategy specifically target level intensive code yield exploitable parallelism extend conventional parallelization two borrow stream namely stage replication additionally demonstrate combination uncover parallelization domain exist approach fail explore remainder paper structure follow introduce methodology pipeline extraction section two follow presentation empirical result sequential source c cosy compiler g partition source c p p e p c fi e li c n e n trace file input profitability estimation explorer whole program dependence analyzer loop analysis partitioner trace analyzer figure two overview parallelization section three section four discuss relate work summarize conclude section five two methodology parallelization illustrate figure two sequential source program write c process cosy c compiler instrument result executable execute one representative input file result set trace file present trace analyzer dependence analysis generate program dependence graph pass partitioner produce pipeline specification relate original cosy compiler process source program second time generate parallel c code call system accord pipeline specification eventually code compile target platform use compiler link pipeline library subsection explain generate program dependence graph trace file hierarchical partition algorithm present paragraph code generation cover paragraph pipeline stage replication generation part algorithm introduce paragraph respectively program dependence graph use perform hierarchical parallelism extraction base program dependence graph six explicit representation control case derive representation augment edge compute dependence analyzer figure two base profile information choice operate representation motivate follow factor provide rich information loop construct data organization facilitate aggressive code data code partition remain simple since still rely full set compiler optimization pass exist structure avoid overly aggressive coarsen partition without compromise node represent seme list addition use pass isolate function call place separate nod define compound nod encapsulate hierarchical structure program follow basic block contain function call site algorithm one parallelism selection input l f loop function set respectively ree tree compound nod virtual topmost loop profitable loop wi ree profile weight available core result select loop select loop data q work queue addition maximum define set specific function call include set belong loop information base variant standard control analysis perform instrumentation pass strongly connect component contain compound nod single initially compute whole program compound nod form fashion base whole program tree point compound nod consist either function loop current implementation base simple iterative algorithm one edge annotate follow field bite mask designate carry relevant dependency bite mask mean size data communicate adjacent nod per iteration level hierarchical pipeline stage partition exhibit pipeline parallelism small number dominate stag target high number core factor eventually limit maximum attainable lead poor utilization available necessitate exploitation multiple parallelism specifically consider parallelism well span multiple replication stateless stag parallelism selection algorithm operate partially fold initially whole program fold single function node algorithm one describe iterative traversal unfold compound nod result selection loop contain profitable parallelism stage algorithm unfold compound nod loop function call likely expose additional exploitable parallelism current use simple heuristic base fix data pipeline parallelism line five seven respectively consider advance possibly machine learn base strategy later stage sequential loop use line eight effectiveness pipeline parallelization mainly determine execution time stage therefore primary objective pipeline partition algorithm one procedure two q three q c c add c else c l p c p add c else c ree else c f threshold c ree four five six seven eight nine ten eleven twelve thirteen fourteen fifteen end balance load across available core key observation balance effective granularity node small relatively stage size theoretically balance pipeline use maximum available core ie p unfortunately size nod inherently limit code structure function loop nest among address problem partition loop stage replication therefore perform two step first perform applicable algorithm two next partition result dag use heuristic effectively try balance load give number available partition loop step iterate loop attempt break compound node block computation begin iteration collapse result direct acyclic graph dag ensure algorithm process chunk computation actually execute parallel case dominate stage eagerly augment nod preserve replication property ie without introduce new cycle augment stage replicate multiple time till longer dominate stage pipeline line alternatively loop function dominate loop body opt unfold line order generate use loop partition finally case strongly connect seek uncover computation part dependence cycle select unfold compound node highest w line iterative process repeat dominate stage threshold p line seventeen since partition increase attainable addition bind unfold loop depth since partition deeply nest loop limit overhead consider loop depth five adequate uncover available exploitable parallelism second step partition loop partition give loop successive stag effectively follow topological order graph consequently interstage data point p p wi algorithm two partition input l target loop available core result p set partition data q sort descend list sort v p pi set nod p assign partition p wi aggregate weight partition pi v v one v p one v replicate one procedure l two dag three k zero four root dag five q k v first q p v k k one p zero continue else v one add v v k k v one else add v p p ready v dag seventeen eighteen end nineteen add remain nod p twenty return zero pi p w six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen one procedure l two set one l three repeat four dag p peek v dag update v dag v two v v one v one v l p v else v else v f peek w p w else break five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen return dag stage preserve sequential semantics loop execution nod process use descend sort list sort profile weight node case tie node minimize communication next stage select node include current stage addition result exceed threshold give input line six threshold p give parameter equal maximum node weight derive thus avoid eager partition subsequently lead higher communication context switch cost decrease execution time dominate stage addition use slack factor avoid excessive number partition loop partition along set partition p also return weight dominate stage assist hierarchical parallelization determine whether add current loop set loop effectively parallelize line nine algorithm one proceed component hierarchy line twelve parallel code generation system code generation process figure three take input partition pipeline specification original parallel code generation perform procedure cosy compiler description code generation process follow base example code figure four leave hand refer contribution statement relation whole program execution pipeline specification code generation sequential figure three overview code generation process time initial use profile stage well parallelization show figure five basic block one unconditional control statement place loop control structure present test increment block respectively insert initial control flow analysis pass first algorithm three compute set v original include control determine execution current pipeline stage line three set compute union vertices vertex use edge figure five stage two v r two create new function original include block assign stage partitioner v block v r line ten also replicate header block respectively later use inject pipeline communication code initially replicate contain control instruction original algorithm three pipeline parallel code generation input g original graph p dom tree g p program dependence graph n number pipeline stag n pipeline stage v result g n es graph stage data v r predicate assign stage stage set assign stage one procedure two n three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen v r p v r es e v v e v v v ancestor v p dom v v es es e v end g g es g g g g g g sixteen seventeen end eighteen g every edge connect vertices also edge substitute dangle outgo edge redirect edge ancestor miss vertex tree original graph line instance stage three previous example edge replace edge show figure five c final step add capture outcome v subsequent stag line eleven statement define new local variable predicate assign value conditional expression transformation enable us communicate predicate value equivalent utilize single uniform pipeline mechanism line fourteen five twenty fifty twenty five one two three four five six seven eight n r e f l e data n decode data raw data transform data enhance filter raw data w r e f l e data figure four source code example figure five one line twelve reference local original function patch refer thread private data every variable reference body outline stage function new local variable define structure array might change facilitate share copy object reference global line thirteen complicate handle since thread share global scope aggregate type substitute avoid allocate initialize memory one terminology regard variable object visibility storage base compiler one iso c nevertheless usage term clear context thread access specific variable first follow common parallel program technique use index thread id second option use thread local storage implement optimize mechanism access thread private data evaluation two lead conclusion adopt latter although purely portable thread id index result performance degradation particularly like see table access global repetitively intensive code function name total compression thread reduction memory access load store table one comparison use global static compare index array scheme table show reduction dynamic memory access dominate function compression algorithm investigation reveal due combination poor common elimination increase register pressure addition effect overemphasize like limit amount architectural register thread local storage hand suffer utilize free segment register fix offset perform access private data nevertheless retain index array order provide mechanism access private data thread finally local static storage duration handle similarly pipeline base communication system pipeline thread connect immediate predecessor n n one zero n call call decode call call one zero one l four five data n seven eight eleven l nine l ten two three l control data control data l vector c control data partition zero one four five six seven eight nine eleven ten two three stage stage stage zero one six four five seven eight nine eleven zero one seven eight five ten eleven zero one two three five replicate normal sequential b fold c partition figure five simplify example base source figure four demonstrate intermediate use partition parallel code generation cessor use two queue one outgo produce data one incoming recycle consume buffer queue push pop communicate effectively point structure contain communicate data interstage satisfy loop iteration stag predecessor pop context buffer restore content begin outer loop fig five similarly stag successor store outgo data buffer structure push outgo queue end scalar context buffer dynamically allocate object copy value producer side unless analysis show dependence variable latter case however case data push subsequent stag establish additional pointer recycle queue nonadjacent stag copy copy data addition data communicate interstage code generation handle copy copy respectively line qualify copy similar semantics clause variable need initialize value sequential thread copy identify incoming edge source outside loop body variable copy multiple stag also part interstage dependency initialize stage pipeline ensure later stag receive initialize copy hand qualify copy definition loop body reach external use handle copy complicate though loop multiple stag point last modify stage identify initialize copy sequential thread reference implicitly copy action necessary private one stage finalize sequential thread use array private copy thread store reference buffer use terminate order determine terminate loop use special variable private thread mechanism analogous one describe block pointer allocation pointer handle like variable scalar aggregate might point data though complicate thread consume multiple incoming buffer turn contain copy data data must initialize appropriately principle presence recursive link list require either scan update whole structure patch dynamic pointer access propose al thirty follow latter approach introduce overhead every pointer fortunately practice majority stream cess use simpler structure therefore opt simple yet effective mechanism dynamically two two c program array ie translate result access nest function pointer rather array access three translate object register original address context sequential thread translation table case statically allocate object emit explicit call pipeline section object intercept memory allocation call interstage buffer allocation buffer assign sequence number object copy associate id dynamic static size figure six effectively establish notion alternative object access initial context sequential thread associate special zero pipeline buffer pop queue every copy pointer update base current value sequence number incoming buffer translation table base size address base size context id zero id base size allocation creation x e n c k zero one k id translate figure six pointer stage replication pipeline stag stateless thus replicate order reduce critical path pipeline kind parallelism highly beneficial contain single dominate stage form performance bottleneck order process multiple incoming buffer parallel framework spawn multiple thread data carry false within replicate stage facilitate independent execution incoming buffer process thread pool since output either completely independent explicitly push next stage outgo buffer important stress point property create regard work distribution policy ie policy specify thread consume incoming buffer thus case different stage show great variation process time flexibility employ load balance scheme study employ simple policy benefit queue detect parallel stag use relatively straightforward use dependence bite set profile stage mask data manifest across inner loop level compute additional data eliminate false defer translation table practically implement use search tree rather table generation phase stage replication invalid case stag loop exit modify global data later use outside loop copy data otherwise replication permit without option process due induce control dependency although stage replication essentially diverge purely linear pipeline paradigm straightforward extend code generation strategy handle extend system follow push buffer one multiple outgo queue selection base current implementation support static balance pop buffer multiple queue previous stage notify thread insert special token tail incoming queue latter necessary replicate stage contain loop exit block otherwise responsibility precede thread replicate across extend analyze handle partition span one effectively coalesce multiple operate different rat single linear pipeline important note choice base representation oppose base one enable us handle rather complicate control nest loop uniform transparent way therefore partition algorithm proceed distribute internal nod unfold inner loop level different partition largely disregard code generation part p g transformation unfold loop process bottom order replication additional control determine execution nod nest loop result replication multiple thread similar partition data handle fashion instance figure five process inner loop first case control dependent header nest loop include result partition stage one two figure five c finally code relevant inject stage one outgo stage two incoming safety use profile dependence analysis guarantee safety unlike static analysis reason possible program execution analysis limit small number hence might miss data control therefore expect user perform final verification suggest partition scheme work take follow step verify correctness generate parallel code first run number test sequential parallel program different input data set compare output second manually inspect generate code second step guide tool generate additional graphical pipeline diagram highlight data code static dynamic dependence information differ server hardware dual socket two eight core total core scientific kernel gnu os compiler table two hardware configuration detail evaluation platform fully aware manual verification process scalable envisage scenario whereby dynamic check dependence insert generate code check may implement underlie pipeline library base additional hardware however beyond scope paper three empirical evaluation evaluate methodology share memory system comprise dual configuration target platform give table two performance evaluation choose four spec suit line code detail show table three program two suit amendable pipeline extraction mainly interest stream process hence restrict program representative application domain follow paragraph provide discussion four parallelization result performance decode twenty implement standard digital music compression audio layer three show motivate example figure one decode pipeline comprise multiple process encode data stream various level granularity range whole audio frame frequency key challenge parallelize application expose sufficient work spread multiple loop level partitioner order facilitate extraction pipeline make use idiosyncratic program typically evade static analysis return function value buffer pass function deeply nest function call extensive use dynamically allocate buffer exist approach either address issue four rely manual code function full loop unroll loop distribution latter process importantly guide selective lead suboptimal result use loop distribution stage replication achieve six core sequential extract pipeline structure show figure seven fact structure resemble one contain explicitly parallel stream result encourage suggest pipeline extraction methodology capable imitate algorithmic parallelization previously alone hand expert programmer compression implement lossless data compressor exhibit typical pipeline structure operate constant size data block consist roughly follow stag input encode inherently sequential independently compression block perform transform transform output figure seven b depict extract pipeline structure process constant size block accomplish parallel replicate stage extensive use dynamic memory allocation pointer arithmetic pointer different type use access allocate buffer particular challenge tool still overall eight could achieve example purely static approach would fail completely detect parallelization profile accurately correctly identify independence individual block even ultimately verify user selective unfold split function nod successfully interleave io computation nest function alternative approach would use special hook bypass output call save output intermediate buffer later flush thirty however would assume use specific io use memory map io instead standard library therefore approach presume manually modify source video decode twenty implement widely use international standard video compression algorithmic level decode feature multiple process stag coefficient decode saturation control motion compensation successively operate encode input stream frame different level granularity frame slice use tool extract pipeline structure show figure seven c result utilize three give solely rely pipeline parallelism attempt exploit short vector data parallelism result impressive application eventually restrict unbalance distribution work stag see figure seven c next extract pipeline relative time spend pipeline stage show integrate approach target short vector inside pipeline stag may eventually contribute outside scope paper compression twenty implement image compression algorithm dominant standard digital image source twenty twenty twenty st program frame six eight three two replication split core parallelization table three use evaluation header seven fifteen input decode input color convert forty transform generate generate table motion quantize encode output output four output stereo reorder subband synthesis output two bottleneck stage stage decode b compression c decode compression figure seven flow graph extract pipeline application table three use tool manage extract pipeline see figure seven result moderate mainly due fact although feature abundant parallelism even algorithmic level interdependent order parallelize application annotate memory allocation analyze application necessary reference implementation use custom memory manager utilize os feature like memory map io nevertheless relatively straightforward process compare like claim transparent case would probably require major modification source memory manager safety test use extensive set additional previously unseen input data set input result identical output sequential parallelize cod addition manual code inspection reveal violation result initially surprise however static target work complex regular dependence pattern expect four relate work early work extract pipeline parallelism embed fourteen introduce number theoretical fail provide practical scalable methodology detect exploit kind parallelism author demonstrate fifteen use dynamic loop analysis help overcome static data dependency analyse lead detection loop typical embed domain work present al relevant work author propose parallelization methodology exhibit stream computation pattern use library manually annotate pipeline stag subsequent step correctness partition verify use tool manage multiple process communicate pip simplify code generation pipeline stage process private address space explicit necessary however unnecessary overhead communication process management introduce consider major disadvantage user methodology require good knowledge algorithm parallelize actual sequential implementation manually determine profitable partition generally hinder deeply nest function loop idiosyncratic program style addition use manual match communication especially case object complicate work flow actually demonstrate fact propose partition highly unbalance fail unwind sequential io parallel four map methodology extract task parallelism sequential c map onto present cluster individual cluster approach take result relatively thread efficient execution require architecture special thread communication support map assume clean code operate loop show deliver good result small embed proprietary low communication latency platform unclear map approach scale generic hardware al thirty focus efficient exploitation pipeline parallelism use data speculation system create copy static well dynamically allocate data similar study handle loop use fix pipeline skeleton three stag effectively applicable loop induction variable increment begin end loop dominate middle stage sense scope approach apply complex case show section three predominant addition author discuss give information problem correlate profile information base information binary instrumentation information necessary perform parallelism extraction thread partition automatically result show excellent scale report whole application case presume process multiple input file parallel recent development work statically split program critical path path thread run concurrently unlike prior attempt automatically sequential program try partition program totally independent thread instead program dependent communicate thread al twenty first propose parallelize sequential use analysis code generation main difference approach present paper operate instruction base representation despite detail memory dependence information apply aggressive impact scope result parallelism require communication communicate register value rely cache hierarchy communicate memory contrast target parallelism communication give rise broadly resemble algorithmic stag application enable us use commodity hardware longer communication free unlike rely special support unfold full performance potential al extend support speculation rarely occur control memory thus extend applicability ing better balance partition one thread present subsequently e al variant exploit stag achieve increase nevertheless still rely dedicate hardware support recent work include speculation system use parallelization achieve good performance commodity hardware simple loop partition single dominate stage parallelism extraction necessary code perform manually systematically case al demonstrate significance profile information uncover pipeline parallelism manually parallelize two use approach lack code generation methodology addition follow profile approach incur high overhead lack facility back annotate profile information higher level code order drive code transformation parallelization process al use similar approach base control annotate explicit edge automatically parallelize scientific however approach restrict loop thus applicability coarse loop typically include io inherently sequential phase encode limit addition intermediate representation fail expose control prohibit parallelization general nest loop finally author use supervise train statistical model determine profitability loop base static dynamic program feature approach although promise easily extend handle parallelization scheme profitability depend overall structure rather local feature five summary future work paper develop semiautomatic methodology extraction pipeline parallelism sequential cod improve exist work rely manual code annotation involve user final approval consider work feasibility study limit parallelism detection use profile information approach cover loop hierarchical pipeline stage replication uniform framework thus avoid performance bottleneck result pipeline stag exist pipeline parallelization approach suffer demonstrate methodology successfully exploit pipeline parallelism stream feature idiosyncratic program construct machine promise demonstrate potential semiautomatic parallelization approach target parallelism beyond loop level future work focus extraction exploitation dynamic parallelism target heterogeneous improve safety consider use machine learn construction improve partition six reference one j k optimize modern approach morgan san ca two brand c j f j c set integrate tool parallelization use high performance part environment parallel compute twelve environment tool parallel scientific compute three g burke r k dependence analysis parallelization four four j j w sheng h r g h h map integrate framework application parallelization proceed annual design conference page five c g r b chapman compiler infrastructure emerge architecture symposium tutorial international symposium parallel distribute process six j k j j warren program dependence graph use optimization program nine three seven c e k h implementation multithreaded language volume page new york eight w j lin lamb c leger j wong h maze stream compiler proceed international conference architectural support program operate page new york nine j g g b c hierarchically tile array parallelism locality volume zero page ca computer society ten w hall j p b r murphy e lam maximize performance compiler computer twelve eleven p husband c k performance analysis compiler three proceed annual international conference page new york twelve f p r triolet semantical parallelization overview pip project proceed international conference page new york thirteen h honda development implementation interactive parallelization assistance tool two fourteen h exploit fine parallelism embed program pact proceed international conference parallel compilation page fifteen h overcome traditional loop parallelization compute network sixteen k k c w interactive parallel program use editor parallel two three seventeen kulkarni k b walter g k l chew optimistic parallelism require seven proceed conference program language design implementation page new york eighteen l parallel execution loop seventeen two nineteen w lim lam maximize parallelism minimize synchronization affine transform proceed symposium program page new york twenty g r august automatic thread extraction proceed international symposium r j p p tu k parallelize compiler technical report h park park polymorphic pipeline array flexible multicore accelerator execution mobile proceed annual international symposium page new york h kim r mason b august speculative parallelization use multithreaded proceed fifteenth international conference architectural support program operate march e g j bridge august eight proceed annual international symposium code generation optimization page new york l f k standard adaptive parallel library international workshop scalable page h k extract parallelism program eight proceed symposium practice parallel program v v sarkar c concurrent program modern seven proceed symposium practice parallel program page new york w v practical approach exploit pipeline parallelism c program micro forty proceed annual international symposium page computer society w language stream lecture note computer science thirty c v r copy discard execution model speculative parallelization micro proceed annual international symposium page computer society g z wang b f towards holistic approach integrate parallelism detection base map nine proceed conference program language design implementation page n intelligent speculation thesis university n r e j bridge g august speculative pact seven proceed international conference parallel architecture compilation page computer society