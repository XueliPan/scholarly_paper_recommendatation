dynamic architecture enable continuous optimization janapa dan fay l settle dirk department electrical computer computer science engineer university colorado boulder f g university colorado boulder f g abstract future computer integrate multithreaded processor core single chip die result concurrent program thread share system design cornerstone improve throughput compute server however date appropriate operate system system compiler emerge machine adequately explore future require sophisticate hardware monitor continuously fee back resource utilization information allow operate system make optimal thread also continuously optimize program nevertheless order continually automatically adapt program application need c information must collect adequately enable dynamic code optimization operate system schedule generally optimization limit time require collect pro time require perform optimization inherent bene optimization initial e utilize information dynamic optimization inform thread schedule future multithreaded present subject hardware performance analysis design aid operate process management permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute pro commercial advantage copy bear notice full citation page copy otherwise republish post redistribute list require prior c permission fee five may four six copyright general term performance design performance counter pro ling schedule one introduction leverage advance semiconductor system explore new soc chip multithreaded evolution dictate future integrate multithreaded processor core single chip die result concurrent program thread share system design cornerstone compute server also emerge embed manage thread require continuous optimization system resource thread execution critical advance current operate system system compiler emerge machine although important understand complete view multiple core necessary build e model multithreaded core execution likely basis multicore design multithreaded address grow gap support multiple hardware thread capable hide memory individual thread coarse grain multithreaded issue single thread cycle switch thread long latency cache miss de nable time alternative hardware thread perform useful work increase throughput single thread would stall processor release six commercial implementation course grain processor simultaneous multithreaded seventeen thirty share branch target bu ers one physical processor multiple virtual simultaneously execute cycle design intend low design overhead allow add exist processor design without cost estimate add support alpha processor require additional five die area find similar cost implementation call nineteen commonly available processor processor thirteen technically similar design describe research literature although unique particular certain physical partition virtual share support enable con table application con power interface run conventional operate system system enable virtual processor appear operate system two distinct base operate system need detail knowledge certain fact logical despite e enable transparency multithreaded exist potential operate system aware multithreaded model importantly multithreaded multicore emerge increasingly important operate continuously monitor application behavior assess job schedule explore space work evaluate intent provide initial result rationale enable operate figure one show matrix value di spec run reference input set use processor design greater one pair time completion application less enable less one e run sequentially use application pair either achieve little achieve thirty case however pair achieve much thirty slowdown order improve schedule require like operate optimization future deploy guide pro information improve performance however order maximize performance gain e pro ling require accurately describe program behavior pro ling provide valuable information whole class formation twelve code position improve function eleven ideal pro collection system three distinct first provide accurate pro information dynamic utilize second system ideally would gather pro information one stage finally importantly collection information occur little overhead unfortunately approach pro ling meet one two three provide rate pro cost overhead well convenience compilation novel emerge collect use hardware performance counter although structure e capture information begin study bene amount type information need drive optimization seven eighteen modern provide rich set performance counter hardware performance monitor commonly place onto provide engineer low overhead mean performance tune usually fairly simplistic allow sample counter certain provide set eighteen event counter collect fifty di apple computer hardware understand development tool two use sample performance counter paper illustrate potential use hardware performance monitor information schedule optimization multithreaded processor core demonstrate use information require substantial analysis construct e aid rest paper organize follow section two discuss relate work multithreaded schedule pro ling section three present potential use modern pro ling optimization section four give overview construct accurate model use hardware counter information turn implementation model multithreaded job section five describe e present section six two relate work multithreaded schedule operate direct role performance multithreaded machine number thread exceed number hardware thread since contention share multithreaded system throughput machine bene job schedule process select among set application thread simultaneously execute share processor work thread symbiosis al idea present paper al propose operate system mechanism discourage thread poor performance pair execute one another sample optimize perform name suggest set process sample collect information performance counter follow optimize schedule calculate base performance counter attribute record sample phase number pair function propose yield period symbiosis schedule job deem bene execute concurrently propose set base intuition knowledge system show certain use data cache miss rate instruction per cycle indicate art crafty eon gap mesa parser swim vortex ninety ninety art crafty eon gap mesa parser swim vortex ninety ninety ninety figure one comparison run application pair sequentially concurrently processor express percentage concurrent execution sequential execution shade add base range likely pair yield poor result comparison paper present methodical statistical model use derive schedule pair function importantly use onetime machine characterization avoid need distinct sample optimization phase instead sample characterization continuous essential give limit possible sample likewise paper illustrate actual operate system implementation discuss build effective schedule model real large history schedule try exploit program improve throughput good survey exploit higher level process communication io access work focus schedule task base observe execution behavior enter queue simply adjust selection job equal priority limit work system actually implement operate system evaluate commercial processor similar study compare technique tera yield ten parallel program use manual rise balance parallel serial section di multithreaded program well machine way statistical model use system relate work fifteen derive linear combination performance counter validate activity base power model however consider percentage variation c counter consider counter pro ling hardware counter specialize hardware pro ling propose collect pro information conte eight examine use branch handle hardware couple branch predictor obtain branch information twenty work explore use branch behavior bu er collect branch pro data incur low overhead e gather data one program run er accuracy design collect edge pro adore dynamic optimization system seven eighteen one excellent example directly use hardware information dynamic trace generation adore use performance monitor unit collect pro information aim improve data cache performance primary goal adore use detect small amount hot trace optimization adore interest trace optimize future need gather exploit much information possible correlate sample characterize nature information respect program behavior sample originate continuous pro ling optimization one sixteen sample performance monitor pro information drive optimization application paper demonstrate important addition area continuous program optimization illustrate future make use exist hardware monitor optimization three path profile use hardware monitor collect program information critical direct next generation number optimization use pro adapt program behavior well allocation however hardware pro ling several issue bring e use question accuracy critical enable e optimization importantly order pro ling feasible system must do minimal collection overhead finally unlike pro ling hardware pro ling deterministic use sample might occur di time point program execution performance monitor work section paper use fourteen include set counter con count among also allow sample event address register capture recent data instruction cache miss follow section illustrate sample processor branch execution obtain accurate partial study use hardware generate path pro path pro ling three show important form pro ling four path pro correlate branch keep track path execution count instead simple branch count however path pro ling usually come increase overhead path pro ling versus sixteen edge pro ling three use hardware help generate path pro substantial promise however since execution information limit size type collect information must assemble transform put usable form figure two illustrate problem adapt hardware monitor information problem path pro ling illustrate hot path program execution partial information trace collect performance monitor essentially sample code indicate complete path pro code region figure two path detection use information center problem hardware monitor hardware limit capacity maintain program information instance contain eight register collect branch execution register treat circular bu er execute branch instruction usually require two register one branch instruction address another branch target address register effectively act four branch circular bu er user able conduct sample set follow set experiment use spec compile base con research compiler collection tool base kernel interface library ten construct collect sample register sample analyze within module expand effect sample period figure three show e sample rate overhead well number unique discover sample period vary clock cycle naturally lower sample rate decrease overhead provide lower number unique high sample rate increase overhead provide unique sample overhead remain relatively low less ten way around sample rate increase percentage overhead increase quickly fifty sample period number unique discover rise steadily increase sample rate pro determinism pro ling infrastructure enable aggregate pro information collect multiple run program compare gather information separate run analysis lose due statistical sample measure figure four show e aggregate branch sample multiple run input illustrate additional run increase number unique increase occur combine ten run slight level possible collect multiple run important partial miss run however important understand collect accurately important program execution accuracy result measure accuracy full path pro generate pin tool pin design provide functionality similar popular atom nine alpha unlike atom pin instrument executable statically rewrite execution rather add code dynamically executable run make possible attach pin already run process collect pro information however experience average order collect detail information meet pro ling constraint low overhead path pro compare full path pro gather sample period sample period fifty forty thirty twenty ten zero sixty fifty forty thirty twenty ten zero e h r e v n e c r e p e h r e v n e c r e p sample period sample period figure three overhead number unique various sample zero two four six eight ten twelve fourteen sixteen eighteen twenty zero two four six eight ten twelve fourteen sixteen eighteen twenty number aggregate run number aggregate run forty thirty twenty ten zero fifty forty thirty twenty ten zero e h r e v n e c r e p e h r e v n e c r e p h p e q n h p e q n zero zero h p e q n h p e q n zero zero h p e q n h p e q n zero h p e q n h p e q n zero zero zero two four six eight ten twelve fourteen sixteen eighteen twenty zero two four six eight ten twelve fourteen sixteen eighteen twenty number aggregate run number aggregate run figure four number unique find aggregate data run input set pin tool use method similar wall weight match scheme accuracy describe fraction estimate hot path compare hot path full path pro accuracy p two p p two f p f p p equation f p ow path de count divide count add together represent percentage count path p account set full path pro set threshold threshold use similar previous path pro ling study four five create select path pro equal number figure five show accuracy result use method describe general accuracy range average eighty accuracy particularly bad sixty evidence show although method perform well could use improvement noise path match scheme likely distort path count nevertheless data clearly motivate use hardware collect pro information optimization require even c information follow section closely examine use hardware information impact multithreaded schedule c r c c n e c r e p ninety eighty seventy sixty fifty forty thirty twenty ten zero mesa art parser figure five accuracy path pro ling four schedule simultaneous multithreaded performance impact operate system schedule multithreaded depend directly integrate model processor application example important view logical multithreaded physical processor core dynamically vary pair asymmetric case one logical processor run memory intensive application logical processor treat like machine memory since behavior depend dynamically vary application demand current allocation thread logical determine logical processor pro ling application demand adjust internal schedule model base pro build e model schedule topic follow section consider case two logical single multithreaded physical processor r two r two r two g two b b b one one total r process ready execute furthermore assume possible process either good cause minimal slowdown schedule process bad cause slowdown schedule process g b process available run schedule interval probability schedule two bad job physical processor concurrently use random would b probability schedule exactly one bad job would one since operate system must eventually run process best outcome run two bad job time assume always use logical run full combination spec figure one concurrently g b three consider bad process would result two schedule two bad job schedule concurrently schedule one bad program run good program figure one indicate run combination bad job impact performance twenty detect bad job overall performance may improve four overall potential performance improvement increase smaller number process involve g two b two overall performance might improve twenty six potential small simple analysis mainly indicate improve schedule mechanism e worth implement improve performance single architectural improvement simple schedule mechanism achieve comparable gain value implement long hurt performance bad process frequently random would due need extreme e paper explore use hardware performance counter predict process might interact support performance counter processor design gather data performance tune organization similar many processor limit number performance register number performance counter give performance counter associate c performance register additional performance information synthesize performance counter example per cycle deliver processor calculate number retire processor cycle count important characterize interaction logical system use empirical approach determine thread rather adopt ad approach use prior study five performance metrics indicate either particular program activity number branch point implementation c processor branch trace cache miss occur due incorrect processor speculation show table one metrics choose b retire branch trace cache miss cache miss f retire float point per cycle table one performance metrics record application characterization report metrics normalize number issue count thus value five f would indicate five cycle spend point good representation thread behavior give performance counter allocation since two thread counter must allocate twice one per logical processor certain count certain counter task structure include counter shadow hardware counter counter value record execution use schedule pairwise spec suite run reference input set measure performance counter application time invoke use r statistical compute package linear model data predict base sum performance counter extract application application pair motivation use sum performance counter intuition architectural form capacity limit example processor memory two process tend approach limit probably interfere one another goal determine simple set performance register use predict slowdown schedule could use c set performance counter derive model predict application pair likely yield separately observe application behavior reasonably consistent across schedule quanta word immediate past reasonable predictor immediate future program exhibit degree phase behavior use feature one reason technique al undergo periodic determine process coe counter compute respectively single lag period indicate use prior sample provide reasonable accuracy rather use sample period periodically decrease indicate less predictive accuracy example sum six schedule quanta drop imply mechanism al would higher overhead simpler mechanism predictor linear model normalize performance counter include multiplicative term capture feature model form residual full linear model include total term weight individual term wi de ne contribution particular factor b combination factor factor include c factor example represent contribution interaction trace cache miss cache miss linear model multiple correlation coe indicate model high predictive accuracy residual term represent error term need linear model data correlation coe occasionally provide mislead measure model accuracy analysis perform verify statistical model accurate linear model serve three purpose first indicate possible accurately predict use sample performance counter sample independent second use linear model possible determine performance counter predict do use two involve add remove term linear model see reduce set performance counter yield model accuracy de metric complete analysis perform drop individual performance counter find omit one counter reduce value seventy thirty thus appear important include full set performance counter basic performance counter explain ninety variation pair base linear model possible select compare scale sum performance counter correspond model b simple heuristic capture relative contribution lead performance variation involve simple calculation yield e solution fifteen f one schedule inform schedule make two ways migrate task decision likely positive second select task predict behave well task currently run logical processor previous work illustrate inform operate system information processor performance counter potential improve multithreaded section examine complete implementation construct inform multithreaded performance counter con prior execution record separate metrics logical processor branch miss miss point due performance counter allocation possible directly record performance counter use linear model concurrently thus use reduce set counter time invoke counter along time stamp read set zero read value multiply large integer factor divide elapse time scale metric per cycle create without use point unit within kernel adjust value store task structure previous task provide estimate per cycle previous schedule quantum new process null value counter mean schedule ahead process correct process execute record performance counter value cation do add hook default call function register module module handle instrumentation task selection decision migrate task migration part statically load kernel schedule data structure export use queue base scheme task queue highest priority schedule next order maintain schedule fairness responsiveness still exploit di process look four task never skip task three time linear model useful predict give us information critical schedule however less useful predict actual pair task residual e actual counter example model predict high point count yield high obvious pair two high point wrong decision instead directly apply model factor use heuristic guide schedule instead sum counter process absolute di take weigh indicate di scheme pair job di usage pattern follow prediction function derive assume process run logical zero decision process require process set counter represent event count per cycle example process value calculate v four process among three priority queue highest v select notice scale term fourfold b f term do model indicate contribute twenty variance compare counter similar argument hold counter counter subtract since higher better counter two situation job randomly distribute run queue possible encounter situation little choice schedule two process solution add two bite saturate counter task structure counter interval interval task two bite counter maximize migrate designate already locate mask set process migrate away task fall weak state two bite counter mask restore mi grate another processor necessary load balance fortunately adequate job load balance additional compensation perform account task algorithm remove practice method e group task average two three task set eight give time prevent imbalance number task cap half total runnable task count five result experiment report run single use kernel execute use reference data set experiment involve execute eight randomly select time interval four base measure use throughput measure per cycle rather processor time limit choose several reason first seem best way evaluate system would allow job run completion make di cult understand result set job suppose job except one improve system obviously increase throughput time completion remain sum individual execution time also awe improvement shorter cascade e result longer also since kernel make least experiment figure six show random set eight di draw suite report average three four minute program execution run combination use default version period time compare throughput processor eight process achieve average sixty value range figure seven give closer look worst median best set three along every experiment severe disparity amount individual receive even though overall positive important note although process choose run di order always receive amount time processor since fair respect time imply issue unfairness must address six conclusion work demonstrate good potential improve throughput use schedule multithreaded across random set eight di average six positive value range eleven result show clear potential operate future p e e p n e c r e p sixty fifty forty thirty twenty ten zero ten figure six realize set base fifty set eight spec individual worst set median set best set p e e p fifty r p z b p g p g p z g c l r e r p k c r x l l r e v p f r c n e e k q e l e g g l f l w f l w x e r v l l r e v c c g l e g g l p z g w w w f l w f l w l l r e v set figure seven individual set worst median best performance counter integrate operate however improve throughput distribute unevenly thread sometimes penalize due architectural unfairness receive dramatic improvement default receive percent mechanism evaluate well job novel problem operate system schedule traditionally operate schedule time increase share may require schedule combination simultaneously believe methodology use select pair function apply future many core di paper also present system extend information relate optimization construction pro ling system demonstrate path pro ling information accurately estimate require little performance overhead overall schedule pro ling result illustrate promise potential perform continuous optimization future processor resource thread execution seven reference one j l j dean r r l sit c w e continuous pro ling cycle go symposium operate page one fourteen two apple computer three ball j r e path pro ling proceed annual l symposium page four ball p edge pro ling versus path pro ling showdown proceed symposium program page five bond k practical path pro ling dynamic proceed international symposium code generation optimization march six j r j r n eleven w w p p chang function l carter symbiotic research compiler k r c pro guide code position proceed conference program language design implementation page sixteen june r development core team r language environment statistical compute r foundation statistical compute settle j performance analysis simultaneous processor proceed international conference parallel compiler sair b phase track prediction proceed annual international symposium computer architecture page press tera workshop multithreaded execution architecture symbiotic simultaneous multithreaded processor proceed ninth international conference architectural support program operate page press g symbiotic simultaneous processor proceed international conference measurement model computer page press b sprunt four feature micro four page simultaneous maximize parallelism annual international symposium computer architecture june j l lo j h levy support synchronization simultaneous processor international symposium architectural support program operate page w wall predict program behavior use real estimate pro proceed conference program language design implementation page seventy june r multithreaded processor commercial journal research development six seven h j yew dynamic trace selection use performance monitor hardware sample proceed international symposium code generation optimization march eight conte b patel k n j cox pro ling e technique pro optimization international journal parallel program two nine atom interface build high performance program analysis tool proceed winter conference ten development company project expansion compile realistic c program proceed conference program language design implementation page june twelve w w w p p chang n j r r g r e hank g e haab j g holm e technique compilation journal seven one thirteen corporation special issue technology journal one one fourteen corporation two processor reference manual development optimization may methodology empirical data proceed annual international symposium page computer society sixteen continuous program optimization vol fifty n six june seventeen v j architecture speculative nine eighteen j h yew design implementation lightweight dynamic optimization system journal parallelism six page one nineteen f l hill g j miller technology architecture technology journal six one four fifteen twenty c r trick e r j c w w hardware mechanism dynamic extraction program hot spot l computer architecture page june fifteen c power monitor thirty j h levy