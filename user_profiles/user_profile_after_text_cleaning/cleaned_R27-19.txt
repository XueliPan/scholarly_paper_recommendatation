locate relevant text within document one archive information study university two institute university three department computer science queen mary university abstract traditional document retrieval show competitive approach element retrieval since element retrieval task request relevant document part retrieve paper conduct comparative analysis document element retrieval highlight relative approach explain relative effectiveness document retrieval approach element retrieval task subject information storage retrieval information search retrieval digital general term measurement experimentation performance retrieval focus retrieval passage retrieval one introduction focus retrieval practice initiative evaluation retrieval two require locate relevant document also locate exactly relevant information inside document hence focus retrieval combine two different retrieval relevant document similar traditional document retrieval retrieval relevant text within document focus retrieval approach need distinguish two step may target relevant information directly irrespective document context typical example pure approach every possible element treat document extreme ignore second aspect always return entire article effectively document retrieval latter strategy repeatedly prove focus retrieval approach three six relevant context task introduce combine explicitly require retrieve topic group per document paper look detail document element retrieval approach fare focus retrieval term locate relevant document term locate relevant text within document throughout paper use two base run document run base index article level element run element treat document index use language model default set smooth length prior group belong document remove overlap rank document best score element run base ad collection one consist document million copyright hold eight figure one document element run two locate relevant document investigate first aspect focus retrieval good document element retrieval approach find rank relevant document derive document rank element run select document context basis map ad style document contain least relevant text consider relevant document figure one plot curve document element run clear document rank document run superior sixty recall precision document run substantially higher expect element index small element word one two get high score difference map document run element run highly p one although element run could improve tune use mixture model five come surprise approach effective first aspect focus retrieval three locate relevant text within document investigate second aspect focus retrieval good document element retrieval approach find relevant text inside document look ratio relevant text relevant retrieve relevant document result per topic figure two distribution relevant document ratio relevant text give relevant document distribute ratio relevant text first bin contain document five relevant text second bin far relevant document bin less five relevant text small increase last bin almost text relevant interest see document run relevant document bin zero two four six eight one zero one two three four five six seven eight nine map element map figure three precision leave recall middle right ratio relevant text document element run element run obviously run rank initially document large fraction relevant text document retrieval make sense also overall mean average generalize precision document run higher significantly standard run element index regard run rank list nonoverlapping element evaluate use measure focus task result higher interpolate precision one recall element run p five show initial result element run well target element run may improve tune bring document context main point document retrieval competitive five discussion analysis show document retrieval competitive approach focus retrieval document rank superior element retrieval approach possibly pick average bigger document enough document large fraction relevant text also get high per document several propose make document retrieval less attractive option focus retrieval evaluation combine recall precision great impact reflect underlie task well score equally weight precision recall hence document run predominantly determine recall focus retrieval task may intuitively require emphasis precision use score weight precision twice much recall give relative figure three radical weight may need make document retrieval less attractive also suggest use measure reward document proportional length ratio relevant text example use measure grade article score however promote document retrieval approach reduce impact document little relevant text research support organization scientific research project reference one l p corpus forum two initiative evaluation retrieval three j de b importance morphological normalization retrieval proceed first workshop page four j j evaluate relevant context document retrieval twist proceed page press new york five b j de workshop proceed approach retrieval page six j j well best context reflect ad retrieval page figure two distribution ratio relevant text table one element level evaluation index document index run element index run relevant context five focus one element run document first bin look precision much retrieve text relevant recall much relevant text document retrieve ratio relevant text figure three show per document precision recall ratio relevant text document score combine precision recall f one expect element run much higher precision lower high run high precision score term recall document run get perfect score one since whole document retrieve recall curve element run show recall much affect ratio relevant text bin recall around four curve clear document text relevant document retrieval approach superior whereas many document low fraction relevant text element retrieval approach effective document retrieval approach obviously sensible approach large fraction document relevant much less attractive small part relevant four relevant context relevant context task combine two focus retrieval measure use task calculate score per document base well retrieve match relevant text combine per document score use average generalize precision four since document also evaluate run element level table one show five score document element run see five document generalize precision document run higher significantly zero two four six eight one zero one two three four five six seven eight nine zero two four six eight one zero one two three four five six seven eight nine zero two four six eight one zero one two three four five six seven eight nine zero