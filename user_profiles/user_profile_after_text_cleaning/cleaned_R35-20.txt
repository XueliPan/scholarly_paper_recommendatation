use historical data enhance rank aggregation pablo de superior de abstract rank aggregation pervade operation technology hypothesize performance aggregation may affect artificial usually meaningless consistently occur input score distort combine result individual bias differ propose rank aggregation model source score normalize common distribution combine early experiment available data several show support proposal subject information storage retrieval information search retrieval retrieval model general term measurement performance rank aggregation score normalization score distribution one introduction rank aggregation pervade operation technology five name rank aggregation take place combination multiple relevance criteria search merge output different combination relevance personalize search one even combination multiple collaborative retrieval aggregation explore prior research six hypothesize performance aggregation may affect artificial usually meaningless consistently occur input score affect performance rank technique separately distort combine result individual bias differ therefore possible improve result undo order combine score produce different source value first make comparable across input two usually involve normalization step five prior work normalization typically consist linear three relatively straightforward yet effective normalize sum score input system one shift mean value zero scale variance one five none take account detail distribution score thus sensitive noise score bias furthermore normalize single search result isolation even take account result good bad copyright hold six august comparison result engine whereby best result bad run may assign similar normalize score best result good one propose effective aggregation model source score normalize common score distribution use perspective historic behavior particular trend rank source early experiment available data several show support proposal two score normalization model approach first generalize notion rank source follow rather consider rank source input list information object define identify score function retrieval space define instance search engine document collection would define set query engine q would similarity measure q personalize content recommender could take set may consider personalize search engine real score function thus model use call specific define application require b use value x induce total order relation rank instance search engine would typically take q q query enter user rank result set q search engine use notation state rank fusion problem follow let set rank source merge let r arbitrary combination input value rank source give input rank want compute aggregate score base individual score value r way select arbitrary make assumption technique application build senseful way case involve fair amount redundancy instance usual even application system merge output different single query collection however redundancy irrelevant analysis discussion definition theoretical model hand allow maximum generality representation arbitrary rank aggregation model advantage make easier model historical score data core approach specifically consider score function call run different way possible collect output value return thus build statistic series historical data either advance collect data certain period regular use rank source dynamically assume history source score normalization technique work follow give compute aggregate score follow step r normalization normalize variant method three instead use single run rank set retrieval object get score set several run use namely follow x h x h see approximation cumulative distribution p x distribution may bias accidental individual score system compensate step b method b standardization assume define common ideal distribution f one one free bias noise potential bias individual score function compensate choice f critical compute f one method one possible strategy would use exponential follow study al four alternatively current experiment consist approximate f cumulative statistical distribution obtain normalize score one standard linear normalization three b join normalize historical data source define h x h x joint compute numerically f h c combination finally normalize score merge linear combination technique three evaluation result test four test namely web track comparative evaluation try technique two reference combination function normalization step refer fuse score compute ie score normalization step follow r method five b h r r h r zero r r technique name prior work five compare function combination step use different normalization method apply comparison take result publish six label standard score normalization normalization standard score normalization table one show average result four see globally better although show average result behavior consistent four table one average precision ten combination two twelve rank list average four two four six eight ten twelve base data figure one give idea size historical data need method reach good performance see far expensive n c e r p e g r e v thirty zero ten twenty thirty forty number query fifty figure one number run need reach performance four research support mesh ministry science education five reference one p al information retrieval framework workshop web semantics vol napa two croft w b combine approach information retrieval advance information retrieval recent research center intelligent information retrieval academic three lee j h analysis multiple evidence combination research development information retrieval new york four r rath r f model score combine output search conference research development information retrieval new la five relevance score normalization information knowledge management ga six e web rank score base rank aggregation symposium apply compute