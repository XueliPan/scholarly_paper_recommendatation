trace io stack jo woo one department computer science engineer state university university park pa two mathematics computer science division national laboratory three department electrical engineer computer science northwestern university abstract efficient execution parallel scientific require storage design meet io io intensive access multiple layer storage stack disk typical io request may include access io execute cluster parallel file like turn support native file like order design implement parallel exercise io stack important understand flow io call entire storage system understand help identify potential performance power bottleneck different layer storage hierarchy trace execution io call understand complex multiple file propose automatic code instrumentation technique enable us collect detail statistics io stack propose io trace tool trace flow io call across different layer io stack configure work different file also analyze collect information generate output term different metrics interest code instrumentation parallel io one introduction emerge make demand storage system performance therefore face term io wall io behavior primary factor determine application performance clearly unless io wall properly address engineer able exploit full potential emerge parallel machine work support part grant doe grant r al c berlin trace io stack g application segment instrument metrics interest library source trace trace record instrument code output statistics fig one io trace tool take input application program io stack information configuration file capture metrics interest target source description region interest code automatically generate run instrument code finally collect analyze statistics fly run parallel climate prediction computational chemistry brain image first step address io wall understand unfortunately trivial io behavior today result complex take place among multiple refer collectively io stack example io stack may contain application program library eight parallel file system three native file system io call application program flow layer io stack flow fragment multiple smaller call originate different call set io storage cache io network disk space therefore understand io wall mean understand flow io call io stack understand behavior io stack one option let application instrument io stack manually unfortunately approach manual instrumentation difficult practice extremely error prone fact track even single io call may necessitate numerous file pass information motivate observation work explore instrumentation io stack approach show figure one instead instrument source code io stack manually application programmer specify portion application code instrument statistics collect propose tool take information input along description target io stack source cod application program io stack generate output instrument version application code well instrument kim al layer io stack necessary instrumentation io stack application file carry automatically unique aspect approach work different io stack different metrics interest io latency io throughput io power experience tool encourage far specifically use tool automatically instrument application collect detail performance power statistics io stack section two discuss relate work code instrumentation profile section three explain detail propose io trace tool experimental evaluation tool present section four finally section five conclude paper summary plan future work two relate work past decade many code instrumentation tool target different machine develop test atom insert probe code program compile time dynamic code instrumentation hand intercept execution executable insert cod different point interest dynamo one monitor executable behavior interpretation dynamically select hot instruction trace run program several propose literature reduce instrumentation use fast reduce incur instrumentation design dynamic instrumentation twelve comparison fit five static system aim rather instrumentation optimization fifteen also dynamic instrumentation tool apply reduce instrumentation code al propose io trace approach combine aggressive trace compression however strategy provide flexibility term target metric specification tool charisma twenty pablo tau tune analysis nineteen design collect analyze file system trace eighteen parallel several tool parallel environment four exist lightweight profile tool identify communication scale well reduce amount profile data collect statistical information function typically trace data generate profile tool visualize use tool fourteen upshot eleven thirteen work different prior use source code analysis instrument io stack automatically also unlike exist profile instrumentation tool approach specific predetermine metric instead target entire io stack work different performance power relate metrics trace io stack three approach view instrumentation goal provide io trace functionality parallel exercise multiple layer io stack minimal impact performance end implement work io trace tool illustrate figure one comprise three major code execution engine data process engine show figure one code consist parser probe selector probe inserter context probe piece code insert application code io stack source cod help us collect require statistics code take input application program high level io metrics interest write specification language target io stack consist library current parser parse io metrics interest configuration file extract necessary information instrument io stack fashion top bottom store use later tool probe selector choose appropriate probe metrics specify user finally probe inserter automatically insert necessary probe proper place io stack note depend target io metrics interest tool may insert multiple probe code table one list representative set metrics trace use tool table one sample metrics trace collect use tool io latency experience io call layer client server disk stack throughput achieve give io read write call average io access latency give segment program number io nod participate collective io amount time spend communication execute collective io call disk power consumption incur io call number disk access make io call execution engine compile run instrument io stack generate require trace finally data process engine analyze trace log file return statistics base user query collect statistics view different example user look io breakdown server client amount time spend io call layer target io stack also visualize technical detail instrumentation section discuss detail code component tool let us assume purpose illustration user interest kim al application l log w c four three three three three three four three three three three q latency inclusive list list list list p common l main p common l p common l p common l p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n p latency n fig two example configuration file collect statistics execution latency io call layer io stack amount time spend io call client server disk layer sample configuration file capture request give figure two file write specification language table two describe detail parameter configuration file let us explain content sample configuration file parameter description l c q p table two flag use configuration file example user want collect execution latency write indicate use w occur line application program call also user specify three io stack layer client server application program finally user describe trace log file name data process engine base target metric interest latency appropriate latency probe automatically insert designate place probe specification application file name path path io operation interest code segment interest trace io stack specification trace file location generate tool metric interest probe name insert location trace io stack fig three illustration insert probe application program io stack code application client server represent instrument io stack figure three illustrate code work take input user configuration file along parser parse configuration file extract information require probe inserter data process engine base specify target metric ie execution latency write layer include client server disk probe selector employ latency probe help minimize associate instrumentation follow call sequence write function library though client server figure three probe inserter selectively insert necessary probe start point end point layer describe configuration file instrumentation probe inserter compile instrument code compilation also patch small array structure call function match trace contain information layer layer id io type pass layer client layer insert probe extract information generate log file latency statistics boundary layer note call fragment multiple small example io six consist io phase communication phase trace io call across layer io stack trivial implementation call unique id current layer pass layer help us connect call hierarchical fashion also help data process engine see figure one combine statistics come different layer kim al zero zero one zero twelve six fifteen six eight one three one two one four zero one fig four computation io latency io throughput metrics systematic way example hold latency information different layer associate use server layer tool use unique structure call flow perform request io client probe insert start point server layer extract information pass client pack flow since flow pass entire server probe server extract necessary information collect latency relate statistics without much complexity execution engine run instrument code generate trace log file layer finally data process engine analyze trace log file collect execution io latency induce operation layer io latency value compute layer equal maximum value io obtain different layer however computation io throughput value additive ie io throughput compute layer sum io different figure four illustrate computation metrics compute io power use power model describe nine four evaluation demonstrate operation trace tool run program use three three cluster consist six processor nod connect node system run copy measure disk power consumption per io call use disk energy model nine base data sheet disk table three give important metrics use calculate power consumption parameter default value disk drive module storage capacity table three important disk power calculation evaluation use flash io seven simulate io pattern flash create primary data structure flash code generate three file file plot file center data plot file corner data use two io sixteen ten data structure size simulation eighty block hold processor write active power consumption watt idle power consumption watt maximum disk speed trace io stack client zero server zero c e c n e l eighty sixty forty twenty zero five twelve ten c e b h w n b eight six four two zero five c e c n e l fifty two eighteen sixteen fourteen twelve one eight six four two l e j n p n c r e w p zero five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen zero five client zero server two call id server zero server one zero client zero server zero client one server zero two client two server zero call id fig five latency client zero use fig six latency server zero use client zero client one client two server zero client zero client one client two server zero six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen call id call id fig seven disk throughput server zero use fig eight power consumption server zero use three file fifty file write function call use size figure five nine represent size first figure five show io experience call client zero perspective use call show take client zero server zero server one server two leave right see call sixteen take server one experiment call five call direct server write header information file call record io server log file figure six hand plot observe server zero perspective three bar every call id represent cumulative client client zero client one client two leave right bar also give breakdown io latency take request process client server layer respectively result one see example client one client two spend less time client zero server zero far call id sixteen concern two plot figure five figure six clearly demonstrate tool use study io latency breakdown server eighty kim al client zero c e figure seven illustrate io throughput server zero one observe plot detail io throughput pattern different regard server compare figure six figure seven one also see bottleneck io call application code depend whether io latency io throughput target figure eight hand present power consumption result server zero see power consume io call fifteen except call power client zero client one similar server fig nine latency client zero use zero one two three four five six seven eight nine ten eleven twelve thirteen call id c n e l server one server zero server two client zero twenty sixty forty eighty zero final set result give figure nine depict io latency value observe client zero viewpoint use instead overall sample set result clearly show tool use collect analyze detail latency throughput power statistics regard io stack five conclude remark future work perform code instrumentation manually often difficult could hence propose automatic instrumentation technique use trace analyze scientific use high level io like io file like trace utility use exist io function call therefore add minimum overhead application execution take target high level metrics like io latency io throughput io power well description target io stack input analyze collect information generate output term different metrics future work plan extend analysis available io characterize io behavior also plan investigate dynamic code instrumentation make use information available code data optimization reference one v al dynamo transparent dynamic optimization system two efficient transparent comprehensive code lation thesis three al parallel file system cluster proceed annual showcase conference four al user guide program trace io stack five de bus b al design implementation fit flexible proceed paste six rosario al improve parallel io via access strategy news five seven fisher al turbulence computation use application framework blue system j dev twelve eight w al complete reference vol two nine al dynamic speed control power management press server class disk ten hierarchical data format eleven v lusk e study parallel program behavior upshot technical report national laboratory twelve al language compiler dynamic program instrumentation pact vol springer thirteen huck ka ad performance data mine framework parallel compute fourteen e lusk e performance analysis program workshop tool parallel scientific compute fifteen n al low overhead program monitor profile proceed sixteen li j al parallel scientific io interface paste seventeen al pin build program analysis tool dynamic instrumentation eighteen al review performance analysis tool parallel program j vol p springer nineteen al scalable approach application performance analysis di b j vol springer twenty n al parallel scientific parallel distribute seven da design test global model r al direct numerical turbulent lean journal physics conference series one h pablo instrumentation user guide university tech atom system build program analysis tool data sheet j c lightweight scalable profile k al scalable io trace analysis al toward scalable performance visualization j high perform thirteen three b al flash adaptive mesh hydrodynamics code journal supplement series report