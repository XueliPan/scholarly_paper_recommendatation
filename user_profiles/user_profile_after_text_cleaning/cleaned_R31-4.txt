jaw schedule exploration turbulence wang eric burn malik tamas computer science university eric center university physics astronomy university mechanical engineer university institute data intensive engineer science university present jaw batch improve query throughput scientific cluster reach scan vast amount data extract feature gain importance however acute performance bottleneck result multiple query execute simultaneously compete io solution jaw divide query schedule identify overlap data within execute batch maximize data share reduce redundant io jaw extend previous work one support query exhibit data exploit knowledge cache combat starvation adaptive incremental query throughput response time instrument jaw turbulence cluster two yield nearly threefold improvement query throughput contention high introduction improve physical instrument data lead exponential growth data gray term data avalanche three instance direct numerical simulation isotropic turbulence large eddy turnover interval yield data two similarly panoramic survey telescope rapid response system astronomy produce daily four scientific various discipline turn cluster manage data cluster achieve high degree parallelism aggregate throughput partition data either spatially temporally across multiple nod include turbulence cluster two cluster four astronomy survey five cluster six within scientific cluster new class scientific emerge correlate mine extract feature vast amount data query long run last even days strain io scan large portion data throughput degrade precipitously multiple query execute concurrently furthermore multiple query may belong sequence experiment complicate schedule goal alleviate performance bottleneck simultaneously execute enable explore data scale batch process motivate turbulence two typify cluster distribute across multiple nod individual experiment last several days track example position velocity distribute entire span space time eight million query access billion point seven date turbulence cluster must serve multiple query simultaneously many query access data time span spatial turbulence benefit data share query schedule need ensure high performance scientific cluster specifically must ensure high query throughput presence concurrent access scientific first step implement one batch astronomy sloan digital sky survey eight promise result however leave several open limit applicability scientific include failure account execution order query lack cache exploit schedule knowledge put forth jaw inherit benefit address goal provide batch submit batch include long run query exploit data share reorder query one eliminate redundant access disk two amortize cost data access multiple query schedule execution order base amount contention access data rather arrival order query achieve io amortization query access data jaw provide schedule extend data share benefit batch schedule execute large number query order sequence scientific group job c personal use material permit however permission material advertise promotional purpose create new collective work resale redistribution list reuse copyright component work work must obtain new consist sequence query relate experiment query within job exhibit data must execute order one capture order exist turbulence cluster scientific example track movement time turbulence position next time step depend result query previous time step jaw provide greedy algorithm identify data share job synchronize execution job realize data share explore cache replacement exploit knowledge improve performance jaw performance depend crucially cache request turbulence service cache furthermore jaw employ two level framework access group relate data order exploit locality reference computation turn cache schedule ensure group data use together cache together nine present two cache replacement one utility rank cache incorporate full knowledge pending request two segment least recently use approximate base little knowledge minimal overhead instrument evaluate benefit server page replacement algorithm variant ten also describe method combat starvation make adaptive incremental query throughput response time performance base saturation many query turbulence second focus small spatial region highly selective maximize query throughput inevitably starve query await completion long run query moreover since query may belong large job delay individual query indefinitely negatively impact entire job realize starvation resistance jaw make dynamic incremental maximize query throughput lower response time saturation change instrument jaw batch turbulence cluster improve query throughput performance nearly threefold owe higher throughput schedule also reduce response time treatment include study response time versus throughput demonstrate system ability adapt automatically change saturation addition find incorporate knowledge cache replacement improve throughput sixteen relate work query batch paradigm study large tertiary storage order minimize io cost thirteen explore paradise system reorder query data store magnetic tape reorder achieve sequential io collect ing data phase without physically perform io reorder tape request finally execute query concurrently one batch al twelve provide process partition data fragment physically contiguous tertiary device schedule concurrent query per fragment basis al fourteen describe general framework cache reuse intermediate result among analysis query reduce io although jaw leverage also explore query exhibit data fifteen attractive paradigm parallel computation query yang al sixteen extend paradigm efficiently support relational join add merge phase process heterogeneous simultaneously moreover al seventeen combine procedural style declarative construct al eighteen incorporate batch process identify share file among map task job group batch sequential scan large file share among many simultaneous job possible also include age policy avoid job starvation work complement result however direct application query schedule difficult several reason model sensitive job arrival rat correspond stationary process suit steady state problematic serve continuous stream query representative available share file also large fit memory environment single file scan time contrast cache replacement crucial performance jaw jaw also realize data share query exhibit data within astronomy nineteen system sloan digital sky survey twenty avoid starvation short query scan query use job submission system query class assign different throughput long run query improve partition data evaluate query parallel across multiple however distinction long short query arbitrary short query interfere short queue long query experience starvation jaw rely ad distinguish long short run query rely multiple process query data query size support single system also jaw automatically adapt maximize throughput reduce response time short query saturation change schedule turbulence section adapt prior batch process framework query schedule turbulence evaluate query base amount turbulence turbulence decomposition decomposition time zero fourteen fourteen time zero function function data access query data schedule schedule query query result result reorder fig one batch process contention data provide system tunable starvation resistance turbulence cluster turbulence cluster two store complete direct numerical simulation perform spatial temporal exploration turbulent flow use public web service cluster store result time series structure grid data turbulence consist query perform large amount data include one evaluate statistical array turbulence entire part volume two track forward backwards time three identify turbulent structure track formation evolution current version consist turbulence simulation history time step two second simulation time time step consist velocity pressure field grid stationary hydrodynamic turbulent flow data partition fix size storage block roughly size practice atom length four replication side performance reason total per time step serve fundamental unit io hierarchical spatial index base order use partition index space particular logically partition space cub side k zero log n n index also act space fill curve provide linear order disk preserve spatial locality close together order also near space cluster b tree access path key combination index time step use retrieve atom hence range containment query efficient respect io improve cache reuse point query sort evaluate order atom read b batch process describe query focus data query instead access secondary storage achieve large query throughput suit scientific index partition spatially temporally data query know prior execution query subdivide framework adapt turbulence prove effective astronomy one evaluate query two stag figure one first query identify data access turbulence query provide list position perform computation identify data atom correspond position hence list access query also output list query satisfy follow set position fall within atom execute order result original query obtain combine result second step group access atom together single pass data thus evaluate whose position fall within atom read correspond atom region form fundamental unit io potentially nearby spatial interpolation query moreover position sort order prior execution close space execute close succession improve cache reuse reference set evaluate together sort position also amortize disk seek time dense query multiple position reside within atom li al two discuss query evaluation turbulence extensively c schedule throughput favor contention term pending request number query position order maximize query throughput longer queue mean cost read atom disk amortize query thus evaluate data contention order let denote list denote list query w j represent set position contain within queue atom consist union w one j amount contention atom ai define use throughput metric j w j w two ut w j w j one w j denote size ai queue estimate time cost read atom disk computation cost single position respectively cost derive assume uniform io cost access atom since finally function one n zero one zero ai memory one otherwise numerator denote total size queue pending atom ai denominator sum io cost amortize across multiple query computation cost evaluate greedily order decrease throughput capture rate queue consume also provide system tunable technique combat starvation greedy policy describe lead high query throughput may starve individual query starvation occur small queue access infrequently query response time increase due last mile query finish every atom access process employ age throughput metric balance need process query arrival order resist starvation maximize throughput metric account age queue time e atom ai age throughput metric ai define ut one e two ut denote throughput atom ai real value zero one specify strongly bias towards process query arrival order parameter influence completion order concurrent query bias zero select contentious first whereas one process query arrival order even set one data share occur among query f f r r e e f f l l w w j j execution time execution time fig two compare job execution jaw result time step calculate new position outside submit new query next time step new position moreover multiple long run job may execute simultaneously identify sequence query belong job use combination user spatial temporal operation perform time step query time consecutive query heuristic highly accurate practice section schedule b gate execution often conduct experiment sequence relate query define job collection query belong experiment may involve query execute several days prior knowledge access pattern long run job immensely valuable schedule categorize job two type batch order batch job query execute independently order ie gather aggregate statistics data typical batch job number query position remain constant query may look change space time little movement jaw treat job similar manner query however focus optimize order job order job require query execute one sequence query exhibit data reuse result predecessor particle track experiment one example track movement time position next time step depend state compute previous time step section address query order job realize data share job identification turbulence job transform input new query base result previous query make difficult predict data access pattern sample job instance may scatter point randomly space track diffusion point time may run several days user collect jaw employ algorithm schedule order job query exhibit data algorithm first identify data share pair job use dynamic program data share position mark jaw synchronize execution different job realize data share finally pairwise dynamic program merge greedily maximize data share job mean jaw identify data share miss figure two compare jaw schedule three job node denote individual query indicate execution order within job value inside node denote data region access thus query share value access set time step ease illustration distinguish query overlap partially data access figure two jaw complete faster access data namely delay execution order align execution job query access time first introduce terminology describe framework let j list order job job consist sequence query query k evaluate query complete execution define j set access query j two query j l exhibit data share j l also define j wait ready queue do state eleven eleven eleven precedence edge precedence edge gate edge gate edge zero zero zero zero zero zero zero zero zero one one one one one one one one one two two two one two three three fig three identify data share dynamic program query j namely j wait state schedule due precedence ready state gate describe unsatisfied queue state satisfy do state complete execution furthermore maintain precedence graph job vertex individual query edge denote precedence constraint initially job precedence edge direct every query j one j jaw schedule immediately query queue state j do first query job initialize queue state introduce additional gate edge undirected query identify data share across job gate edge connect two vertices correspond query access data encounter query gate edge precedence graph ensure adjacent query time formally gate k edge exist query j l j l gate edge feasible ie conflict precedence exist another gate edge query x x j l x j l moreover pair job query one job one gate edge job thus jaw schedule query j do every adjacent via gate edge query l ready state first phase jaw take input precedence relation job identify gate edge maximal possible data share every pair job accomplish use dynamic program base algorithm find best global alignment two sequence let job consist n query respectively algorithm align query exhibit data share two job use follow score system query j l let l one exhibit data share zero otherwise penalty skip query either job zero goal find alignment query maximize score alignment translate gate edge indicate jaw pair query realize data share figure three illustrate dynamic program solution two job correspond gate edge identify note dynamic program compute bottom entry mi k matrix compute n zero zero one n n new job n merge job one two three four five six seven eight nine ten eleven twelve thirteen fourteen g admit b n b n b l b l b admit n n admit g one admit fig four admission new gate edge maximum k mi k give n job average query per job dynamic program phase incur time complexity n two describe final merge phase introduce concept gate number intuitively gate number g j minimum number gate edge must evaluate j schedule compute gate number every vertex precedence graph perform single sequential pass job execution order number use accept deny admission new gate edge merge phase figure three note gate number query upper right hand corner correspond vertex example gate number last query access data region three due existence two prior gate edge ensure query access final phase greedily merge gate edge pair job dynamic program phase accomplish first sort job pair base number gate edge find dynamic program solution merge begin first pick job pair number gate edge next jaw pick job pair dynamic program phase gate edge involve previously merge job ie new job let job pair merge exist precedence graph admit gate edge deadlock schedule gate edge add similar fashion new job merge gate number update vertices pick subsequent job merge iteratively job include precedence graph provide n job average query time complexity merge phase overhead low practice give graph sparse complete query continually prune several condition must satisfy new gate edge admit precedence graph consider admission gate edge new job previously merge job jaw first evaluate query precedence order determine gate mean ut mean mean mean ut sixteen do do do queue queue queue ut three sixty ut four twenty ut three ut four ut three twenty ut four two eleven eleven do do do do do do ready ready ready do do do ready ready ready precedence edge precedence edge gate edge gate edge fig five schedule three job gate edge jaw edge encounter whether admit omit edge figure four detail admit gate edge n new job merge job line two show n inherit gate edge incident due transitivity line determine maximum gate number prior query line determine feasibility new set gate edge consider line nine check schedule use gate number line ensure new edge violate precedence ie query one gate edge combine figure five illustrate precedence graph merge three job query show various state mark do complete execution prune graph include query access jaw share gate edge query mark queue await execution query ready state mean schedule adjacent via gate edge query ready namely query must query realize data share finally query mark wait await completion prior query within job note new job arrive add exist graph compute new pairwise dynamic program merge v extend jaw beyond schedule jaw extend one employ two level schedule framework exploit locality reference two cache replacement schedule three provide solution starvation resistance two level schedule framework inspire disk schedule cello cello disk first allocate application coarse level request different interleave level meet quality service guarantee similarly jaw ut one eighty ut two sixty ut one fifty ut two ut one ut two eighteen three three fig six two level batch schedule k three first select coarse level single time step evaluate selection base mean throughput metric equation one compute time step jaw pick time step highest mean throughput tend yield higher density allow amortization io query next k throughput metric greater mean schedule execution k refer batch size maximum number per time step k sort order correspond atom evaluate order figure six illustrate two level schedule select time k three selection batch size parameter k significantly impact performance sufficiently large k ensure data access pattern conform locality reference particular interpolation may require position access data multiple nearby space access atom part kernel computation schedule together within atom avoid redundant data access later schedule batch k single pass accomplish moreover large batch size improve query response time provide balance service query within time step rather focus contentious data across time step however large k negatively impact throughput execution order conform less throughput metric may also flush cache process portion time step roughly size explore batch size k section adaptive starvation resistance jaw provide solution starvation resistance make adaptive incremental query throughput response time saturation ie query arrival rate change specifically jaw prefer maximize data share throughput saturate order avoid explode queue time contrast low saturation jaw prefer lower response time rely system additional capacity preserve throughput jaw extend age throughput metric equation two adapt automatically saturation jaw automatically tune age bias base change performance saturation vary jaw divide run r consecutive query measure query performance run adjust base observe performance compare past run let average response time throughput query run consist q end run age bias run decrease bias towards contention increase bias towards age follow manner one one min two one min one one indicate saturation rise average response time increase run one query throughput bias towards contention two increase saturation decline query throughput decrease commensurate improvement response time increase commensurate rate practice need ensure adjust get stick bad initial value avoid rapid run incorporate performance past run accomplish calculate response time throughput performance respectively run zero zero zero zero also difficult recover poor initial choice saturation exhibit little change extend period vary age bias explore performance curve change two consecutive run note feasible explore curve multiple value real time b cache replacement schedule jaw benefit cache reuse avoid unnecessary io fact experience astronomy one demonstrate forty request service cache develop two cache replacement schedule improve cache hit rat quantify benefit provide amount computational overhead compare server base ten page replacement algorithm section first segment least recently use base algorithm inspire prior work approach exploit notion data access frequently tend reuse instance inertia may cluster turbulent structure highly strain interest data repeatedly query multiple thus correspond interest flush cache mediator mediator one node n schedule work jaw jaw jaw jaw schedule work batch batch manager queue query processor processor manager queue query processor processor cluster cluster fig seven jaw architecture query scan entire time step divide cache two segment probationary segment small five ten cache protect segment cache segment order recency access end run promote frequently access protect segment evict segment insert recently use end probationary segment implement policy incur almost additional overhead second utility rank cache algorithm incorporate full knowledge access pattern achieve best cache hit ratio evict likely access farthest future specifically rank cache priority queue base respective order two level schedule framework batch k time step evaluate together jaw cache need group use together thus within time step evict order increase throughput two time step ti mean throughput greater ti evict prior implement incur maintenance overhead new query every time step process must update rank correspond time step c architecture turbulence cluster figure seven data partition spatially indicate shade store across different nod run separate jaw instance incoming query first evaluate query take input set position evaluate position assign queue correspond queue sort age throughput metric organize two level hierarchy manager addition manager maintain state information include age b j f r e b n ten one c n e q e r f r e q zero execution time five one fifteen two simulation time second fig eight distribution job execution time fig nine distribution query time step access query queue map query position queue finally jaw batch k queue time step highest mean age throughput metric submit list position evaluate result return jaw jaw combine buffer result deliver final result user experiment implement jaw turbulence instrument performance use derive log turbulence cluster evaluation study performance benefit two level schedule adaptive starvation resistance addition show sensitivity cache replacement parameter selection compare jaw respect query throughput response time evaluate query independently io share arrival order adapt turbulence section age bias adaptive define manually also employ two level schedule framework single atom schedule time query execute sample full turbulence include time step second simulation time experimental system capacity fraction production run experiment production system quite io intensive would disrupt scientist experiment conduct server server memory data table strip across set four disk raid five configuration log file assign separate disk ensure sequential io next characterize present main result analysis turbulence collect past two reveal vast majority query belong job total unique job consist eight million query access nearly thirty billion position figure eight show job vary greatly execution time majority persist one thirty job access data single time step three job iterate one hundred time step two second simulation time cover ten evaluation employ query trace roughly job week exhibit representative access pattern figure nine hint potential benefit batch process turbulence illustrate query access frequency time step seventy query reuse data dozen time step mostly cluster start end simulation time observe similar reuse along spatial dimension although skew less pronounce reuse also occur along point spike four second addition downward trend access frequency indicate many job iterate time terminate midway experiment lower access frequency mean query access time step toward end simulation time susceptible starvation moreover find query overlap time step access occur close temporally ie concurrent experiment user benefit cache b result compare query throughput performance jaw implement two instance explore two extreme value age bias denote bias one schedule arrival order base age request differ query reference data current query arrival order denote bias zero throughput maximize moreover better quantify benefit various jaw implement two jaw lack include two level schedule adaptive starvation resistance jaw six five four three two one n c e e r e q p h g r h eight seven six five four three two one sixty fifty forty thirty twenty ten zero n c e e r e q p h g r h n c e e e n p e r g v n c e e r e q p h g r h six five four one schedule algorithm fig ten query throughput schedule algorithm five ten saturation query arrival fifteen twenty throughput include initialize five set batch size k section v fifteen jaw figure ten illustrate time improvement query throughput jaw remove able exploit data share job jaw suffer thirty performance drop transition jaw incur less pronounce performance drop much benefit derive batch process two level schedule provide modest twelve boost performance finally performance gap attribute improve cache reuse schedule figure eleven explore sensitivity various saturation measure saturation use value modify arrival rate consecutive job original one thus submit job two follow original two indicate submit one minute throughput performance figure eleven show jaw scale saturation increase benefit higher concurrency data share contrast arrival order schedule plateau much around three query per second even though converge lower less data share jaw still perform significantly better due improve throughput performance jaw come ignore query arrival order lead higher response time figure eleven b show response time performance gap remain fairly insensitive change saturation perform worst even compare io overhead evaluate query independently lead higher queue time overall perform poorly even lower delay query indefinitely result also highlight benefit adaptive starvation resistance jaw become saturate jaw decrease age bias aggressively maximize five ten saturation query arrival fifteen twenty b response time fig eleven sensitivity performance vary saturation two five ten fifteen twenty fifty batch size k fig twelve performance impact vary batch size k jaw put lead large query throughput evident figure eleven response time approach however lower increase become attractive accept small drop query throughput lead large query response time fact jaw outperform saturation owe better overall query throughput performance thus jaw make effective throughput response time base saturation figure twelve study sensitivity jaw batch k use two level size parameter k recall cache hit table performance overhead cache framework schedule batch k per time step result indicate optimal batch size lie ten fifteen vary base amount memory k small fail exploit locality reference computation may access atom multiple time different pass however even k one jaw outperform due end increase batch size beyond twenty lead rapid performance degradation negatively impact cache reuse evict content moreover large k mean schedule conform less contention measure throughput impact beyond fifty marginal throughput greater mean value consider schedule evaluate cache replacement server base ten page replacement lack knowledge modify page replacement within server rather manage cache externally first retrieve set flush server buffer computation cost simulate static set pin memory table illustrate cache hit ratio query performance overhead algorithm note overhead server page replacement measure exploit knowledge improve cache hit seven modest two five cache allocate protect segment translate sixteen four improvement query performance respectively benefit modest low overhead incur make attractive overhead low assign protect segment per run since time step mean throughput change frequently overhead low practice moreover small total size roughly even four million fit cache discussion evaluate several jaw generalize batch process framework use turbulence motivate application include schedule query data improve query throughput thirty two level schedule framework exploit locality reference computation achieve twelve improvement technique starvation resistance achieve best two lower response time less saturate maximize throughput high saturation lastly cache replacement provide sixteen performance benefit exploit knowledge relative benefit improve increase saturation overall jaw achieve threefold sixty improvement query throughput respectively future plan provide quality service guarantee scientific batch schedule environment age throughput metric enable starvation resistance loose completion order guarantee may insufficient interactive query currently explore provide predictable fair completion time guarantee proportional query size short query delay less long query observe even bind completion time query still elasticity permit reorder query exploit data share another direction design declarative style better support schedule submit interface provide hint data experiment help perform accomplish instance allow explicitly link relate query predeclare time space interest use information data job avoid page fault instance extrapolate trajectory job time space ie velocity bound box time step delta consecutive query predict data access subsequent query also help mask cost random read large amount data alternatively could encapsulate job within facilitate schedule presently write series loop iterate time step gather data turbulence cluster benefit arbitrarily experiment experiment different mass however compute paradigm make difficult identify data share much computation occur outside one solution turbulence would implement functionality iterate space time inside provide knowledge query job expense generality flexibility plan apply schedule jaw scientific like turbulence involve complex author wish thank turbulence research group university assistance data work support part grant reference one x wang r burn malik batch process exploration scientific two li e wan yang c r burn g public turbulence cluster study evolution velocity turbulence journal turbulence vol nine three j gray rubber meet sky bridge gap science data engineer bulletin vol four four g bell j wonder r burn fay j scalable cluster architecture data intensive compute five j n object data manager bulletin astronomical society vol one p six v n high performance system view query retrieval data distribute across cluster seven turbulence cluster available eight j gray p malik j c j public access sloan digital sky server data nine e optimal cache ten e j p e g page replace algorithm disk buffer eleven j relational join data tertiary twelve query process tertiary memory storage thirteen j query batch paradise approach efficient process query raster image fourteen h j efficient execution multiple query data analysis fifteen j dean simplify data process large cluster sixteen h c yang parker simplify relational data process large cluster seventeen c b reed r pig language data process eighteen p c schedule share scan large data file nineteen w n li batch back serve data web twenty sloan digital sky survey available b needleman c general method applicable search amino acid sequence two journal molecular biology vol three p h cello disk schedule framework next generation operate low overhead high performance buffer management replacement algorithm r j love b g wherry cache improve disk system performance computer vol three k squire j k preferential concentration turbulence physics vol three five