efficient reliable scientific system department computer science federal de brazil state university abstract paper present fault tolerance framework process data use distribute network fashion framework save intermediate result message exchange among application distribute data management system facilitate quick recovery experimental result show framework scale well approach introduce little overhead application execution one introduction scientific six five nine provide suite tool infrastructure build data analysis execute challenge implement support scientific many analysis oftentimes require process large data series simple complex domain advance image possible acquire high resolution microscopic image tissue quickly image size even image obtain single sample image analyze series data process include signal extraction registration segmentation feature research support part national science foundation grant bisti board support type data process efficiently system leverage distribute compute power storage space disk memory space implement large data retrieval schedule io computation another challenge issue enable fault tolerance fabric analysis complex large data take long time execute probability failure execution consider eight efficient need support recovery failure without redo much computation already do paper propose evaluate fault tolerance framework process data use network distribute environment work eleven describe system create compile program share manage execution extend framework provide support fault tolerance scientific data analysis provide functionality efficiently manage input intermediate output data associate recover certain type fault may occur system approach intermediate result message exchange among application maintain distribute data management infrastructure along additional infrastructure consist persistent storage manager store information distribute distribute cache reduce overhead develop protocol among various system manage seventh international symposium cluster compute grid relate information efficiently experimental result show approach provide asynchronous data storage mechanism minimize overhead execution two relate work bower al two describe framework support structure embed generic within process network framework use develop via author present mechanism data retransmission examine impact mechanism fault tolerance kola six propose strategy data placement computation order reduce need recomputation kola work also differentiate persistent transient automatically recover transient fault however work differ approach require input user provide transparent phoenix seven system provide tool allow fault tolerance data intensive tool detect transient persistent tool also handle transient failure appropriately however recovery carry accord provide user execution present architecture grid ten file system aim manage distribute grid system employ replication fault tolerance work hand aim support fault tolerance maintain state system distribute environment three framework section provide overview core framework employ work framework design support development execution data analysis network interact distribute environment anthill framework implement use anthill three four anthill support execution distribute heterogeneous program model base model implement system call one paradigm application implement network data process refer filter filter exchange data chunk stream main process function carry computation receive data chunk many process data do fashion filter different stag execute concurrently process filter system receive data multiple upstream filter send data multiple downstream filter chunk associate paradigm also allow combine use via multiple filter execute concurrently different host multiple copy application filter anthill extend original model implement one several particular allow model general another specific feature incorporate mechanism call label stream allow selection base upon data transfer one specific filter copy several application feature important filter refer state partition across several host data associate specific portion state framework four distribute data management provide data source enable creation distribute environment grid global model exchange service provide support create manage reference name space document store index manage environment require conform schema register mako service enable creation management across multiple host machine document conform store query remotely retrieve mako instance data store mako index query efficiently use implementation anthill provide support distribute execution support fault tolerance implement anthill filter use mid seventh international symposium cluster compute grid persistent data storage manager architecture framework framework compose three main see figure one share library repository program maker environment first two part allow store share provide generate base share repository environment divide execution support system distribute management subsystem data storage also refer persistent storage manager execution component implement top anthill share library repository program maker describe detail another paper eleven work introduce management system persistent storage system support fault tolerance management system component divide manager subsystem data storage subsystem responsible manage entire execution work intermediary application filter persistent storage manager extend component provide transparent communication management system facilitate exchange information application filter information include filter available data process chunk process manager work data manager entire execution component develop anthill filter information read write application fly also responsible decide demand portion input data process filter monitor manage data chunk message receive create fly store relate message example create unique identifier message also know store filter instance process create message provide share library repository get program maker filter maker program application filter environment execution support memory data storage manager management system data storage figure one framework protocol access create update necessary execution start run decide data chunk send filter instance data partition do demand time filter read input request receive goal always assign local data chunk filter data storage work cache user application component also develop anthill filter implement distribute cache read write information data chunk base provide read necessary data store output component multiple instance environment instance memory space cache data chunk execution user application instance application filter assign instance assignment do one instance responsible subset application filter instance example instance responsible application filter instance run node instance execution filter request data chunk process request send instance filter assign decide data chunk retrieve consult may multiple instance instance responsible manage subset seventh international symposium cluster compute grid base provide instance read necessary data chunk store memory send application filter component also store intermediate data send stream application filter output data memory space store data chunk instance instance send subset data chunk cache persistent storage manager persistent storage manager build top four use data store distribute environment use store input output application define store application output document output first send store data facilitate fault tolerance system also use store data chunk exchange application filter stream data chunk first send interact efficiently store four support fault tolerance section present approach employ framework fault tolerance describe protocol use different system enable fault detection fault recovery chunk information utilize recovery protocol describe section framework identify four type output data chunk input data chunk one one one type filter receive one message ie one data chunk process data message send two n result next filter pipeline one filter receive n message one stream generate one output n may vary accord application also may assume different value application execution three n similar n one type output message may generate one filter one stream four one n characterize one filter receive one message generate many output use one output stream data chunk assume one follow three different process state one process every data chunk initial consider process begin execution mean available process two process data chunk process status filter get process data chunk create fly filter consider process create send filter process three process one data chunk consider process data chunk dependent create store message log create message log number develop add fault tolerance achieve high availability centralize distribute twelve common base message log rollback recovery employ message log approach message log message receive process save message log application restore recent replay message use order receive system data chunk consume produce application filter system message log order system correctly create message log recover application fault system need keep track among data chunk produce consume process status creation management message log current implementation fault tolerance support recover filter present data dependency type one one n one log creation do follow responsible store data chunk exchange stream keep state data chunk improve fault tolerance state data chunk also replicate either fail use reconstruct global state receive data chunk store memory cache available local memory space exceed subset data chunk stag set number data chunk stag determine base amount memory need store new data chunk memory application filter instance want seventh international symposium cluster compute grid application filter b six process document one five data storage transparent copy one two three two four doc one two manger transparent copy persistent storage application filter one two b three data storage transparent copy manager transparent copy persistent storage one two five eight seven nine four one six two necessary write document figure two communication filter read data b filter write data filter b b read data chunk storage read doc message show figure two generate send ask via message ret free doc one data chunk relate filter input stream search data chunk process return data chunk via message free doc id use chunk able retrieve data chunk either local memory send available doc message filter instance generate output data chunk send output data chunk upstream filter b output data chunk also transfer dependency information need later update status data chunk process thus message get ret doc figure two b use retrieve data chunk dependency information filter b figure two b receive data chunk input stream data chunk also store chunk update process status chunk dependency information transfer write doc message check available memory cache chunk space available cache data chunk stag notify one data chunk create one application filter chunk need identifier store identifier creation two phase commit protocol use protocol carry follow message begin new doc cache notify new data chunk identifier create chunk return message new doc id complete protocol message six seven also transmit end message exchange know data chunk store correct identifier next step update data chunk process do via message doc process message doc process send back seventh international symposium cluster compute grid make sure receive update status detect fault fault detection mechanism build include anthill infrastructure base parallel virtual machine fault detection package fault model define two type process fault node fault fault map one two type process fault occur process exit unexpectedly kill machine become unreachable due network failure machine crash treat node fault node fault detect thus unlike process fault node fault may detect immediately framework assume follow fail application filter instance failure application filter instance detect via anthill fault detection mechanism since implement anthill filter consider fail state try communicate n time without recover fault section describe system recover three different point failure user application filter assume fail time execution one fault occur another fault happen system recover current implementation system recover use save state fail execution stop initialize begin application filter fault fault occur application filter application filter instance kill system restart approach choose state reconstruct dependency information data chunk state information save recovery phase system determine chunk reprocess regenerate reprocess reprocess data chunk generate filter implementation default value n ten yet process next filter carry use one three different approach first reprocess data chunk end execution another approach reprocess chunk first start process remain data chunk last approach start process new chunk reprocess data chunk save time current implementation employ last strategy data storage fault instance fail anthill fault mechanism restart fail instance send notification message instance restart instance instance execute recovery step execution guarantee associate data chunk ie process status data chunk process process process store instance data chunk store instance ask ie instance data chunk state store do guarantee begin execution failure data chunk store instance correct status fail filter also need notify data chunk locally maintain lose rollback base data chunk data chunk lose status must update process data chunk retrieve reprocess manager fault instance fail anthill fault mechanism restart fail instance notify instance fault state instance lose restart instance initialize data chunk state information base initial client query define input set data chunk ask instance assign instance data chunk state afterward ask instance data chunk process last step internal state instance reconstruct next step execute instance first step verify pending need execute seventh international symposium cluster compute grid figure three application four different analysis apply digitize microscopy image wait notification say execute second step update data chunk state process process data chunk data chunk process fault occur read process state update process five experimental result evaluate performance framework cost recover fault use image analysis application application give figure three use consist image acquire tissue mouse placenta use digital microscopy scanner size experiment store persistent storage manager across multiple nod system experiment run cluster twenty connect use fast switch node processor two main memory run operate system experiment one instance cluster node one one nod first set experiment evaluate system performance partial result stage save system number nod scale figure four execution time last stage show partial result form color classification filter save save system see form figure execution time decrease nod add application execution addition overhead small total execution time average evaluate cost fault detection recovery follow type fault execution time color classification tissue segmentation image save intermediate state save intermediate state c e e n c e x e zero two four eight twelve sixteen twenty number nod figure four performance result color classification tissue seg mentation stag execution one one instance application filter kill around halfway experiment two one filter instance kill first part experiment second part experiment three experiment divide three part one filter instance kill four kill around halfway experiment figure five show increase execution time fault type introduce application chart test time denote total execution time process function time spend process data process function filter process function time increase system recover fault data reprocess another factor contribute increase total execution time fact application filter restart data lose fault read however chart show cost recovery low compare overall execution time six paper propose framework incorporate fault detection recovery process large data fash seventh international symposium cluster compute grid fault type execution time eight nod test time deviation process function deviation one two three fault type fault type execution time eight nod hist norm test time deviation process function deviation n c e e n c e x e n c e e n c e x e zero zero one two three fault type b figure five performance system fault introduce ion framework employ message log approach implement distribute mechanism maintain intermediate result message exchange among application experimental evaluation show introduce little overhead scale well enable fast recovery certain type fault make framework viable platform long run reference one c chang j distribute process large parallel eleven two bower b h enable scientific reuse structure composition proceed workshop data flow scientific ga three r w l b g r g anthill scalable environment data mine international symposium computer architecture high compute rio de four hastings j distribute data management integration project global grid forum semantic grid june five hastings pan k r j support distribute execution scientific record six g kola j r j disc system distribute data intensive scientific compute proceed first workshop real large distribute four san ca seven g kola phoenix make grid proceed international workshop grid compute eight g kola fault large distribute proceed parallel process august nine b c e e lee j tao scientific management system concurrency computation practice experience special issue scientific eighteen ten ten n soda grid architecture data intensive compute international symposium cluster compute grid eleven g r w pan j support efficient execution scientific distribute symposium computer architecture high performance compute brazil twelve survey parallel seventh international symposium cluster compute grid