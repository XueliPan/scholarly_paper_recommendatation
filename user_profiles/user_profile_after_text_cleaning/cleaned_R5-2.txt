stateful hardware decompression network environment hao terry one yuh bass two corporation abstract compression decompression significantly lower network common traffic drive demand enterprise network intrusion system paper define examine popular decompression network process scenario particular stateful decompression require arise packet orient nature current network decompression data packet depend decompress content compose data stream propose effective hardware decompression acceleration engine fetch history data accelerator fast memory hide associate latency explore parallelism decompression process specify evaluate various design implementation mechanism ie frequently use history history buffer management reuse fetch history data performance study show effectiveness propose mechanism hide overhead stateful decompression show effect design impact overall performance network service stack intrusion system one introduction increasingly complex rely dynamic content dynamic script increase user experience accordingly requirement grow significantly utilization data compression start gain popularity recent survey eleven indicate among top utilization compression web increase three typically network traffic include code cascade style sheet permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee eight san ca copyright table one site yahoo number original size size file ratio format gif already compress text benefit significantly compression fact show table one collect set approximately web page eighteen popular web page contain content exclude refer set average file size utilize one common utility average size compress file reduce yield average compression ratio give save obtain compression incorporate message body compression mean content encode commonly utilize deflate define three implement algorithm compression algorithm identify input stream occur past replace length compress stream consist sequence literal copy request common practice compress encode deflate common algorithm underlie popular tool zip decompression achieve reverse order decode follow decompression figure one illustrate decompression perform top part picture show string decompress analyze packet flow arrive minimum delay ie wait compress data stream compress content consequence high concurrency network traffic extend decompression module require stateful e maintain state data stream process packet stream state stream dictionary aka history paper utilize stack seven stateful decompression implement depend number cycle spend decompression hence line speed increase crucial accelerate decompression growth utilization compress content expect large number concurrent compress data stream flight contain series hold section compress file consequence plurality stream cause decompression request associate belong different data stream force decompression accelerator maintain state compress flow data stream need push load memory process every compress packet frequent stream context switch introduce delay addition environment generally occupy limit area processor chip option keep state active stream accelerator size state stream default deflate standard paper make follow reduce delay associate frequent switch state associate stateful decompression propose novel hardware acceleration mechanism effectively hide overhead bring dictionary data reside memory accelerator without block accelerator continue depend data mechanism feasible explore parallelism decompression decompression write output buffer independent carry parallel extensive simulation study establish give reasonable memory otherwise overhead associate stateful decompression successfully hide execution describe characterization compress apply real intrusion system clearly indicate necessity efficient hardware accelerate stateful decompression fold implementation assume eleven improvement throughput improvement average response time complete contain compress data exploit evaluate design space decompression acceleration mechanism facilitate efficient implementation accelerator term design resource rest paper organize follow section two describe dynamics subject system section three present figure one decompression example compress list literal copy request decompression step input pointer process record compress stream literal record literal copy input buffer buffer copy request string copy exist history end buffer history also refer dictionary typical step transmit file utilize include sender side file compress application layer break multiple lower network layer send b receiver side reassemble recover compress file first decompress application layer retrieve original content usage scenario decompression request work one complete block data compress file different request share state kind decompression refer stateless stateless decompression address sixteen eight two decompression request accelerator create reference temporary history buffer private request stream decompress data motivation paper originate inspection piecemeal compress content context enterprise level intrusion detection intrusion prevention fifteen seven network attempt example attack port scan detect via analysis packet header information attack buffer overflow insert particular across exploit network application stack communication effectiveness pin ability identify via content scan analysis stop attack possibility ie stop forward first packet flow could result vulnerability exploitation system incoming network traffic highly parallel contain potentially millions concurrent flow need track inspect tight latency high concurrency commonly address multithreaded ie network hardware ten packet process system particular compression sender do flow data stream level inspection do packet level multitude flow arrive interspersedly specifically flow compress content stack need original order b b figure two figure three packet distribution mechanism design decompression accelerator section four present comprehensive evaluation architecture design benefit performance section five discuss relate work section six conclude paper two characterization section describe two identify specific compression associate relate content ie cascade style sheet file first comprise ten representative subset ie file previously describe refer second packet trace obtain client account consist embody distinct flow flow flow carry compress content amount three embody compress content flow reconstruct content individual file first analysis utilize popular tool establish compression profile two file set profile show figure two consist sort original file size sort compress file size compression associate original file well sort order see file set similar profile consider boundary case compression typically range factor four seven file size range generally speak compressibility slightly improve increase file size next turn flow concurrency instrument stack track number active flow currently decompression state flow enter state first content identifier encounter one leave state number sequence express content header receive average number flow active decompression state packet eight maximum eighteen exhibit average follow would average flow decompression state respectively utilization compress content expect grow potentially thirty estimate order flow active decompression state give time use resource simulator track temporal locality subsequent decompression request flow learn high number flow active decompression state cause every subsequent decompression request attribute different flow currently available decompression pro stateful decompression support rapid state switch force current stack utilize decompression module determine overhead implementation overall application performance collect instruction count packet use hardware simulator twelve result packet window show figure three packet plot instruction count without overhead decompression clarity contain compress content plot wo result show high count perform decompression figure three b show cumulative distribution function instruction count include exclude decompression overhead chart show spend decompression despite fact three perform decompression therefore crucial utilize hardware acceleration provide performance improvement next section discuss design hardware decompression acceleration satisfy high number active decompression stream frequent context switch associate network figure four parallelism stateful decompression figure five history buffer enable three hardware stateful sion basic approach decompression algorithm essentially explain execute list either copy literal copy operation copy string character within history buffer literal operation copy single character input buffer history buffer copy write distinct history buffer parallelism exist decompression process figure four show decompression segment compress data contain list copy literal whose address overlap however exist across copy among copy implicitly form dag direct acyclic graph parallelize individual copy stateless decompression accelerator history data data stream reside fast memory usually acceleration unit copy efficiently execute parallel via stateful decompression almost every time accelerator receive request potentially need load history data correspond stream main memory gate memory latency one possibility make history data available accelerator fast memory fetch complete history buffer execution copy however reference pattern history data associate decompression request highly irregular first large portion history data reference decompression request addition recent history higher reference frequency older history suggest fetch complete history data process decompression request fetch unneeded data accelerator introduce unnecessary delay traffic propose load history data execution decompression copy specifically copy fast copy slow copy fast copy source data available figure six dag slow copy accelerator fast memory slow copy either source data available fast memory part source data depend another slow copy ie address region copy source overlap address region destination another slow copy classification encounter slow copy set aside leave hole history buffer continue execute fast copy meanwhile load module fetch source data slow copy main memory accelerator fast memory whenever source data pending slow copy fetch fast memory pending slow copy ready execute original token buffer outstanding ready slow copy ahead fast copy wait execute execute ready slow copy soon possible avoid unnecessarily subsequent copy slow therefore result copy execute order call scheme asynchronous scheme try aggressively execute decompression request parallel per se instead similar exploit execution fast copy overlap load history data main memory slow copy figure five show one way partition history buffer fast memory facilitate mechanism decompression maintain write pointer update copy literal operation maintain one additional pointer commit pointer separate commit region outstanding region commit region contain output complete copy literal ready write main memory outstanding region contain output fast copy hole correspond slow copy whenever slow copy finish hole outstanding region fill commit pointer may get advance benefit fill hole region order intuitively keep size outstanding region minimum commit pointer advance promptly data commit region copy addition commit reduce number slow copy indirectly depend main memory figure six show dependence dag slow copy square figure depend represent load main memory indirectly depend memory load load engine execute dag separate curve slow copy execute parallel maximal parallelism explore load engine limit maximal outstanding load request support data transmit path memory bus system buffer write main memory avoid additional latency decode decompression operate thus decode overlap decompression involve load table less negligible compare dictionary average additional work item specify deflate algorithm creation dynamic table creation average invoke six take total spend decompression context paper expect overhead creation amortize across decompression request data stream history buffer recent history compression algorithm try find recent recurrence decompression algorithm reference history data immediately output generate currently active decompression request ie recent history frequently history data back refer history data data recent history figure eight confirm effect figure show load memory stateful decompression request data set reference distance plot correspond distance copy plot show give copy operation closer data write pointer higher chance reference specifically eighty generate copy whose reference distance smaller fix plot correspond copy reference history data generate previous decompression request data stream currently active request copy potentially need source data load main memory plot show depend previously generate history fetch reference back previously generate history chart suggest availability recent history fast memory performance impact rest history beneficial short recent history accelerator process decompression request addition reuse data recent history data distant history overall recent history reduce total number load main memory addition recent history data overlap decode module receive input stream load dynamic table figure nine show history buffer history partition recent history previous history use circular buffer implement slide history buffer accelerator fast memory buffer separate four show figure nine b region end circular history buffer figure nine c show initial state history buffer small history buffer accord reference distance distribution decompression may necessary keep large figure seven decompression engine organization figure seven show diagram decompression accelerator support scheme architecture support popular deflate decompression algorithm compose decode decompression assume decompression accelerator attach system memory bus general packet inspection perform system core hardware thread communicate synchronize accelerator establish mechanism paper concentrate step deflate decompression correspond unit diagram show diagram decode unit compose step one two three receive input stream compress data decode operation keep token buffer token buffer fifo method first fetch operation token buffer rout copy engine load engine specifically operation literal operation fast copy operation rout copy engine otherwise operation rout load engine long load engine resource queue operation copy engine responsible perform string copy inside fast memory among token buffer outstanding buffer history buffer load engine responsible load history data need list copy outstanding buffer multiple outstanding slow copy keep load engine data become available fast memory mark ready rout copy engine via fifo among copy rout copy engine load engine higher priority decompression accelerator attach system load engine manage bus allocate accelerator addition load engine manage copy indirectly depend data main memory request get rout load engine source address region overlap hole outstanding region figure five therefore whenever new data load outstanding buffer copy directly depend data rout copy engine first load engine check outstanding copy indirectly depend data finally data commit region history figure eight reference distance distribution b recent history circular buffer circular buffer support deflate resource efficient less area smaller circular buffer may provide reasonable figure nine show circular buffer reduce size note write region quickly advance overlap recent history region eventually overlap commit region possible side effect reduce size circular buffer data region commit region may overwrite need reload main memory reference later performance impact small circular history buffer accelerator evaluate section four history reuse initial analysis reuse history data deflate decompression suggest similar distribution back history less data reuse assume block size reuse rate average across history data sixteen suggest complex expensive cache mechanism may beneficial chip study simple mechanism keep tag block history buffer tag bite indicate whether block hold valid history data cache history data whose address fall previous history region figure nine e history buffer cache data whose address fall recent history offset previous history offset addition size region reduce write pointer advance eventually previous history region may become empty simulator investigate whether well approach decompression help hide latency stateful decompression develop simulator implement step figure seven input simulator compose list request contain list copy literal assume general purpose computer system submit decompression request accelerator together record contain address input buffer buffer data buffer initially reside main memory model data traffic main memory accelerator follow model memory bus six nineteen thirteen table two show use simulation memory bus straightforward typical bus architecture thirteen recent history c initial state overwrite scenario e reuse history data figure nine scenario history buffer copy engine assume sequential implementation contiguous copy copy multiple sixteen copy pipeline latency twelve cycle stage delay one cycle contiguous literal thus take two cycle observe little impact change literal operation latency one cycle expect average number literal decompression request relatively small copy use process time stall time performance assessment process time cycle spend decompression whose source data available accelerator include three literal operation b copy inside history buffer c copy outstanding buffer history buffer stall time count copy engine wait return memory load b free bus slot become available c load engine become available receive slow copy end overhead decompression request define ratio stall time process time four evaluation section use three evaluate effectiveness stateful decompression mechanism also evaluate performance various design finally show table two simulation memory bus load engine architecture outstanding load load unit latency per load unit peak bus separate bus sixteen cycle five copy engine feature copy unit latency per copy unit pipeline stage latency literal latency one copy request twelve cycle one cycle two cycle impact hardware acceleration solution stateful decompression stack besides also use two file set one standard evaluate compression exist two file set large standard contain variety different file type relatively low compression although less representative compare serve verification data point performance demonstrate advantage propose method stateful decompression compare several alternative bring need history data onto decompression accelerator execute copy result show figure ten three set straightforward approach load valid data history buffer onto accelerator start write call method history estimate execution time history sum process time time need fetch complete history data depend available memory bus another alternative method fetch need history data stall accelerator ie process decompression beyond copy correspond outstanding load refer form method cache accelerator maintain synchronously load history data history data always load unit give average length copy copy standard respectively smaller common size simulator introduce bigger overhead history base effect reference rate history data reduce exponentially along distance write location natural small part recent history onto accelerator leave rest history data load refer method recent history since maximal copy length deflate standard worst case six worth data bus model reach use eight sixteen bus slot ie support eight outstanding load available bus mostly idle figure ten performance total reduce history asynchronous performance improve bus available show execution time two eight outstanding request five sixteen outstanding request result show five bus almost completely hide latency reference history data reside main memory total execution almost best possible performance time history data accelerator achieve seventy aka forty time reduction compare best alternative method particularly large overhead time nevertheless performance improvement best alternative still hold summary experimental result show compare alternative asynchronous mechanism effectively reduce response time stateful decompression data explore design space asynchronous section show detail performance various design describe section three specifically design one reduce buffer save area two recent history onto accelerator begin decompression three cache fetch history data cut bus traffic figure eleven show performance overhead change size history buffer size recent history available outstanding request bus available cache top chart bottom chart legend specify size history buffer whether cache en figure eleven design space exploration group bar correspond specify cap bus group contain eight bar four cache disable four cache enable four bar correspond specify size history buffer size case recent history exclude latency expect hide decode unit gain follow chart first permit sixteen outstanding bus request provide large history buffer almost completely hide latency fetch history data limit history buffer size accelerator increase overhead increase thirty forty persistent without cache history data second cache give thirty forty overhead reduction except case cap outstanding request sixteen therefore cache option bus tight need attach multiple bus third recent history recent history give twenty overhead reduction show five overhead reduction finally effect three factor size cache size recent history orthogonal therefore independently apply implementation figure twelve show bus traffic term per cycle respectively maximum quota first chart five second chart packet load bus sam plead every cycle data show average across format refer size recent history size history buffer result show begin process packet load bus significantly higher later due nature decompression reference recent pattern thus copy tend reference back previous history intensively fact bus congest time suggest multiple decompression share bus proper skew schedule result show similar effect begin time line later time line consume slightly bus occur large output count freshly write data overwrite reference later nevertheless result indicate overhead commit history thus recent history offer save consumption bus impact stack implement separate event simulator evaluate impact compression hardware context multicore host allow concurrent request set simulator take instruction count previously identify account replace decompression instruction count identify accelerator model scale input achieve desire load resource contention track accelerator identify two figure thirteen impact hardware acceleration stack acceleration bound weight decompression stack consider decompression work stack spend per decompress obtain via instruction profile decompression take cycle host system count per assume eleven implementation accelerator yield fold throughput compare run single core general purpose system body research improve performance decompression parallelism exploration fourteen eighteen seventeen five twenty nine main difference work address issue stateful decompression create maintain history live data stream main memory history data stream available accelerator fast memory except associate active decompression request among systolic array base study intensively fourteen eighteen essentially implement execution decompression optimize throughput orthogonal technique extend stateful decompression extension similar load engine describe paper memory expansion technology twenty aim extend capacity via achieve latency optimize relative short data stream implement parallel decompression parallel decompression access dictionary five mechanism decompression process depend compression side construct dynamic dictionary parallelization algorithm discuss seventeen nine inefficient explore parallelism level copy decompression implement parallel higher level granularity need additional information stateful decompression address industry propose additional protocol application layer system network stack four sender side receiver side work exchange special exchange use alter history use sender receiver could force smaller figure twelve impact bus traffic five relate work tor sufficient limit stall due accelerator availability less one system wide even contain thirty require decompression rest section apply one accelerator per host previously state total weight decompression stack system perform packet content inspection contain compress data figure thirteen show impact acceleration quality service stack chart blue plot correspond performance solution red plot utilize decompression miss rate give percentage exceed desire maximum response time one millisecond incoming packet rate increase system limit available compute cycle miss rat jump ninety data show decompression base system start get saturate packet income rate reach achieve improvement base solution system hit miss miss rate exceed specification maximal throughput achievable improve significantly response time enforce solution sustain higher throughput decompression base solution saturate system reduce average response time resp maximum response time resp overall result show decompression base solution significantly improve overall performance base system note improvement system obtain replace decompression five p j j parallel compression dictionary construction data compression page six core connect bus architecture white paper chip literature bus architecture seven security network eight network compression card nine wiseman parallel cod discrete apply mathematics two ten p efficient memory utilization network deep packet inspection six symposium architecture network page eleven survey compression top web sit survey twelve h p j j c j l design validation performance power simulator journal research development five thirteen b r n j r j j b system journal research development four fourteen e g smith j storer parallel data compression j two fifteen snort home page sixteen c data network storage seventeen l parallel text compression technical report eighteen j storer j h reif parallel data compression j parallel distribute compute thirteen two nineteen j j j j field b system journal research development one twenty r b p j c b smith e p bland memory expansion technology journal research development two j universal algorithm sequential data compression information theory three history also facilitate send vector compress content allow receiver reconstruct state thereafter decompression independent contrast approach keep decompression side independent compression side comply popular compression achieve keep state ie history data stream local receiver thus keep receiver completely isolate sender network particularly environment highly advantageous post dependency side six conclusion decompression address issue maintain dictionary across multiple decompression sessions correspond common compress file implement network environment particularly increasingly important intrusion detection prevention decompression do packet level multitude data stream arrive interspersedly stateful decompression require need decompression state concurrently active stream repeatedly load engine request mitigate otherwise overhead associate stateful decompression network service system propose novel asynchronous method effectively hide fetch dictionary data thus minimize overhead integrate decompression engine attach general purpose compute system support efficient stateful decompression extensive analysis performance evaluation aside reveal text compression establish propose achieve optimal performance fifty improvement decompression rate compare alternative hardware acceleration explore several independent design impact performance simplicity implementation propose mechanism apply real intrusion system propose hardware stateful decompression method yield least time implementation stateful decompression improvement throughput improvement average response time contain compress data seven reference one r bell corpus evaluation lossless compression data compression two aha corporation compression accelerator quick start guide show three l p deflate compress data format available engineer task force four method apparatus data compression network patent