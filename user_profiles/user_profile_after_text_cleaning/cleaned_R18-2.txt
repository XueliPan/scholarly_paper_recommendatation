adaptive system anomaly prediction host tan north state university wang research china abstract host require automatic system anomaly management achieve continuous system operation paper present novel adaptive anomaly prediction system call alert achieve robust host contrast traditional anomaly detection scheme alert aim raise advance anomaly alert achieve anomaly prevention propose novel anomaly prediction scheme improve prediction accuracy dynamic host implement alert system deploy several production host system stream process cluster experiment show alert achieve high prediction accuracy range system impose low overhead host infrastructure subject performance reliability availability serviceability general term reliability management experimentation anomaly prediction prediction model one introduction host become important many cloud compute one massive data analytics six seven enterprise data center many stream process require continuous system operation unfortunately today host still vulnerable various system performance bottleneck resource service level objective system often overwhelm task correct anomaly time pressure thus imperative achieve automatic system permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee ten copyright anomaly management provide robust host previous distribute system anomaly management work twelve sixteen broadly two one reactive approach take corrective action anomaly happen two approach take preventive action backup system beforehand reactive approach prolong service often unacceptable continuously run stream process contrast approach offer better system reliability incur prohibitive overhead motivate us explore new predictive anomaly management approach raise advance anomaly alert trigger proper anomaly correction fashion achieve efficient predictive anomaly management one big challenge provide high quality anomaly prediction although previous work address anomaly detection problem anomaly prediction need capture raise advance anomaly alert anomaly happen second run host infrastructure often opaque infrastructure provider demand approach anomaly prediction third host infrastructure often consist host many make anomaly prediction practical host must employ lightweight learn importantly many real world data stream process eighteen run host long live operate change execution input fluctuate resource availability may exhibit behavior example give input per second forty normal stream operator achieve throughput fifty per second faulty stream operator memory leak bug input allocation achieve throughput five per second however allocation reduce five normal stream operator achieve throughput five per second thus without consider execution context distinguish faulty component memory leak normal component low allocation thus anomaly prediction system must adaptive order achieve high quality prediction dynamic host paper present design evaluation novel adaptive anomaly prediction system call alert focus anomaly prediction raise advance alert anomaly happen alert depend anomaly detection system eleven seventeen provide normal anomaly state label different measurement sample however achieve prediction alert employ multivariant stream r b e h n g n f r r c r e n l e e c r base n l p r e c e n r p p p p p r n n g r b e n r n g n l p r e c r n l alert n l p r e v e n n n l diagnosis anomaly alert diagnosis report e n r r figure one predictive anomaly management distribute host scheme capture special alert state addition normal anomaly state alert state correspond set precede anomaly state allow prediction model capture thus prediction model raise advance alert monitor component enter alert state rather wait component already anomaly state adjust scope alert state tune sensitivity prediction model ie accurate false alarm adapt dynamic execution one simple approach continuously update prediction model new train data however simple incremental approach two fundamental first anomaly prediction system may incur large overhead monitor infrastructure due frequent model retrain second accuracy prediction model may low execution context fluctuate lot alternate high low input high low resource availability third execution unknown exhibit evolve behavior address challenge alert employ prediction model specific execution context prediction model give consistent state label measurement scheme first employ cluster scheme automatically discover different execution train set prediction model capture anomaly behavior specific execution context alert dynamically switch different prediction model base context evolve pattern achieve high quality anomaly prediction dynamic approach differ previous model ensemble approach several first unlike previous ensemble approach wherein classifier learn fix window data might span multiple execution approach cluster data belong one context learn model data second establish explicit map prediction model different execution improve prediction accuracy well avoid repetitive learn implement prototype alert system make alert system practical host employ fully decentralize monitor learn prediction illustrate figure one test alert system system stream process cluster network system five experimental result use real system performance host show one range system indeed exhibit predictability two alert achieve much higher prediction accuracy exit alternative fifty higher true positive rate eighty lower false alarm rate three alert impose low overhead host prediction performance train time prediction time rest paper organize follow section two present design detail approach section three present prototype implementation experimental result section four compare work relate work finally paper conclude section five two system design section present design detail alert system first present basic anomaly prediction model describe context discovery scheme third describe adaptive anomaly prediction algorithm anomaly prediction model perform system anomaly prediction deploy monitor host host infrastructure continuously monitor set metrics consumption memory usage data rate buffer queue length run host application component example collect twenty metrics host system metrics host four monitor sensor periodically sample metric value certain rate one sample every ten second form measurement stream achieve anomaly prediction employ stream classifier continuously measurement sample normal alert anomaly state prediction model raise alert component enter alert state precede anomaly state train prediction model first employ anomaly detection module label either normal anomaly state simple anomaly detection module use anomaly predicate twenty base user service level objective example use anomaly predicate process time check whether system anomaly state term performance previous work also provide advance anomaly detection scheme accurately distinguish anomaly application change fifteen infer anomaly label use similarity cluster seventeen note focus work anomaly prediction rather anomaly detection prediction introduce special alert state capture feature space alert state correspond region precede anomaly point illustrate figure two different normal abnormal label anomaly detector whether set label alert control alert interval denote time interval thirty second anomaly incident suppose anomaly incident happen time measurement point sample label alert example use alert interval three figure two one figure two b thus alert figure two include measurement point figure two b result prediction model figure two measurement sample alert state anomaly alert anomaly alert anomaly b anomaly b normal alert anomaly normal alert anomaly alert interval three b alert interval one figure two tunable anomaly predictor different alert x normal region anomaly region p yes p normal yes alert anomaly alert region normal alert x anomaly figure three anomaly prediction use decision tree classifier figure two b intuitively alert interval measurement point alert likely predictor raise anomaly alert thus use alert interval tune knob control predictor true positive rate rate anomaly predictor say raise correct alert predict anomaly indeed happen shortly anomaly alert issue let denote true positive number false negative number false positive number true negative number respectively true positive rate false alarm rate anomaly predictor define standard way one one extreme set zero alert become conventional reactive approach alert state always empty alarm generate anomaly happen ie zero zero extreme set alert become traditional approach perform preventive action unconditionally ie one one optimal solution often lie two practice motivate us develop tunable prediction model show later experimental result alert interval indeed use tune true positive rate false alarm rate choose decision tree component state paper since decision tree classifier produce rule direct intuitive interpretation thus predictor raise anomaly alert also provide cue possible anomaly figure three illustrate simple case classification use two metrics state classification decision tree essentially apply sequence threshold test metrics predicate adaptive anomaly prediction scheme restrict decision tree classification method apply classification well correspond alert region p determine follow path tree lead leaf label alert decision tree classifier train use label measurement data three state order effectively automatically discover appropriate feature monitor metrics prediction classifier incorporate multiple feature train phase additional benefit decision tree inherently select metrics appropriate state classification seek possible tree explain data case feature selection occur whenever induce new decision tree classifier specific execution context train every new classifier incorporate monitor metrics rely decision tree classifier train algorithm perform feature selection anomaly prediction model present anomaly prediction model train algorithm illustrate figure four first employ cluster algorithm discover different execution dynamic train set prediction model describe section responsible predict specific context contrast common learn algorithm frequently update model new train data scheme induce set prediction model long period train data avoid unnecessary repetitive learn importantly induce model data produce high quality prediction model approach base two first system component enter certain execution context stay context period time certain event occur lead system another context second system may operate similar execution repetitively long period time example web server often receive higher morning lower even execution context switch repeat figure four show stream measurement sample divide stream data block stream data block di contain small fix length measurement sample data block color belong execution context system component operate three different execution context instance three noncontiguous time zero eight eighteen system component switch context time six achieve anomaly prediction one big challenge system hard identify characterize evolve time goal group measurement sample correspond context learn model group group essential individual instance occurrence context often contain little information fully characterize context classifier train insufficient data large error generalize badly future test data hand group together measurement sample belong context able learn high quality prediction model minimize error employ cluster algorithm discover different goal algorithm partition measurement stream set stream segment denote di stream segment contain several continuous data block p minimize global error e p err di one dip one di two three err di denote error predictor learn prediction model prediction model prediction model context context context stream time zero two four six eight ten twelve fourteen sixteen eighteen twenty thirty figure four anomaly prediction stream segment di discuss compute err di let us first understand rationale behind optimization problem easy see e p minimize quality accuracy predictor learn maximize reason follow two segment belong context merge reduce error predictor learn merge segment hand two segment belong different merge create train data set conflict reduce accuracy predictor train data thus minimize e p maximize quality discover compute err di error predictor train segment di perform cross validation instance twofold cross validation randomly split data segment two set use data first set train predictor use set compute err di percentage wrong make train predictor easy see order work segment must satisfy constraint di one avoid trivial partition wherein di contain one data sample core task minimize e p equation three give us optimal partition stream data hence want discover e p minimize dynamic program since cluster problem define optimal let n denote stream length n di j denote data block j let pi j denote optimal partition di j train segment define pi j minimum validation error e pi j task find n optimal partition entire stream assume know k n optimal partition k n k one k n consider n two case either contain subpartition ie entire stream n form single segment union two partition k n certain k first case learn predictor n estimate validation error predictor second case validation error derive k n n k e k e n e n min k n four clearly find best partition k n subproblem structure solve reach simple case j partition di j one j n note need solve subproblem memorize reuse already solve intermediate result e pi j store reuse succeed computation store backtrack correspond optimal partition n recover easily assume decide optimal partition n stream segment ie n segment n represent context occurrence boundary two segment mean context change one context usually multiple throughout entire measurement stream therefore need perform cluster algorithm group segment maybe noncontiguous number unique example figure four stream segment merge specific context form similar way still minimize objective function equation three decide optimal partition difference two stream segment ie contiguous noncontinuous eligible merge term complexity decide optimal partition continuous data block subproblem e pi j require us train test classifier get prediction error value subpartition note use decision tree classifier adopt implementation two assume measurement sample di j k feature sample previous work prove total cost build decision tree classifier log log two totally decide optimal partition n stream segment naive solution require us examine n two possible segment pair di examination incur cost build decision tree classifier get prediction error temporarily combine segment di clearly overall complexity high however heuristic optimization greedy develop find good suboptimal implementation use hierarchical cluster algorithm fourteen belong greedy technique show later lightweight heuristic algorithm fast train time still achieve good performance extract different execution train set prediction model one k q unique model context contrast monolithic prediction model train use measurement sample adaptive anomaly prediction approach employ ensemble prediction model different previous model ensemble approach train model different consecutive train data approach induce prediction model group example context train prediction model use train data figure four contrast previous model ensemble approach different model induce consecutive train data model induce intelligently group data necessarily consecutive adaptive anomaly prediction decide alert raise base system time must first find context time apply model correspond context system make decision however find current context problem since instead exhibit simple pattern periodicity context change may occur time example figure four system component operate change execution approach find current context base analysis collect statistics context change measurement stream analyze different interact result context switch model enable us switch context give cue monitor stream let denote probability context time let denote new measurement arrive time let p denote probability generate context let p denote prior probability context time probability context time see accord rule p p five goal find context suffice compute p p approximate p use prediction error specifically let cross validation error thus predict correctly p one otherwise p probability p compute p six probability change context context previously active furthermore assume probability context time one finally need find derive historical data number transition context total number context transition seven example figure four altogether six context transition two context follow context block block thus probability context follow context dynamic host infrastructure new context emerge system actively log data prediction unsatisfactory data alert raise never lead anomaly even without intervention also data alert raise lead data may indicate emergence new execution context capture current prediction model thus induce new model data also update context switch model incorporate new model dynamic context switch pattern time may also remove inactive ie zero switch probability keep set active small current prediction model consistently provide unsatisfactory prediction result trigger prediction model train algorithm describe section induce new set prediction model base new train data three implementation evaluation section evaluate alert system first describe system implementation show collect several shorthand p p shorthand p system metrics stime rout metrics description percentage free cycle available memory virtual page rate free disk space receive data object transmit data object drop data object receive transmit input queue length process time spend user mode process time spend kernel mode system data handle time address space use component lock component resident set size usage heap usage stack executable description load last one minute load last five percentage free cycle free disk space slice percentage utilize disk space free disk space available memory register slice host table one subset monitor metrics trace set contain anomaly data third present experimental result validate prediction accuracy approach system implementation implement prototype alert system deploy two production host one system stream process cluster consist blade dual two four memory two network system achieve generality alert system implement base standard allow us port alert system different host infrastructure easily collect twenty metrics host system metrics host four table one list subset key metrics collect alert system collect metrics use train decision tree classifier make alert practical host employ fully decentralize monitor learn architecture illustrate figure one first deploy distribute monitor collect various monitor metrics deploy set associate different system avoid affect normal application strive use idle host infrastructure perform prediction model train also install set anomaly continuously provide class label new measurement sample later use train data note set measurement sample precede anomaly sample later label alert predictor base alert interval describe join component flow diffuser component video stream ping throughput throughput x e n c f r e b n twenty fifteen ten five zero x e n c f r e b n twelve ten eight six four two zero x e n c f r e b n twenty fifteen ten five zero memory leak loop err buffer err rate mismatch memory leak loop err buffer err rate mismatch fault fault b node id c figure five number execution dynamic implementation detail anomaly different section experiment setup first collect measurement trace system cluster deploy monitor host continuously collect various metrics sample rate one sample every two second install set anomaly system catch anomaly one anomaly average process time component exceed certain threshold four second join component diffuser component two throughput anomaly output rate component lower threshold thirty ratio output rate input rate lower threshold four experiment run system reference application consist fifty distribute stream process perform complicate multimodal stream analysis reference application share start different infrastructure trace include measurement sample trigger inject various fault different different time test adaptability anomaly prediction model employ set common fault stream one component execute buggy code segment keep forget free memory two component spawn thread include infinite loop error ie update mistake three component forget remove process data input buffer fault last fifty second activate fault time remove fault time fifty report result two commonly use stream process one join component continuously correlate different traffic flow base join condition two diffuser component dispatch data news video stream different host load balance start collect measurement trace since four experiment deploy monitor nod collect various metrics sample rate one sample every ten second deploy set anomaly catch host ping monitor host respond five successive ping failure detect way record information system also collect monitor metrics three fail host capture behavior host ping failure trace consist sample trace alert detect multiple execution indicate real exhibit evolve behavior execution context change cause input data rat zero ten thirty twenty seventy alert interval sec forty sixty fifty eighty anomaly f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble one nine eight seven six five four three two one c r c c n c e r p one nine eight seven six five four three two one c r c c n c e r p zero ten thirty twenty seventy alert interval sec sixty forty fifty eighty b throughput anomaly figure six true positive rate false alarm rate system join component memory leak fault fluctuate resource availability share host infrastructure figure five show number discover different system trace data observe join component process traffic stream experience dynamic execution context diffuser component process video stream show next anomaly predictor generally lower prediction accuracy dynamic reason number different predictor less certain find right context evolve context train data also observe eight twelve execution discover different trace data comparison implement three commonly use exist learn one monolithic scheme train single anomaly prediction model use entire train data two incremental scheme build update prediction model use slide window recent three ensemble scheme maintain ensemble prediction model train use different train data moment best perform model base balance accuracy past window data use predict anomaly current window data window include sample approach use decision tree classifier build anomaly prediction model zero ten thirty twenty seventy alert interval sec forty fifty sixty eighty anomaly zero ten thirty twenty seventy alert interval sec forty sixty fifty eighty anomaly one eight six four two c r c c n c e r p one nine eight seven six five four three two one c r c c n c e r p f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble one nine eight seven six five four three two one c r c c n c e r p one nine eight seven six five four three two one c r c c n c e r p f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble zero ten twenty thirty forty seventy alert interval sec fifty sixty eighty b throughput anomaly zero ten twenty thirty forty seventy alert interval sec fifty sixty eighty b throughput anomaly figure seven true positive rate false alarm rate system join component fault figure eight true positive rate false alarm rate system join component fault adapt decision tree package two make work learn classification use positive rate false alarm evaluation metrics rate denote equation one compare performance different learn say prediction model make true positive prediction raise anomaly alert time anomaly indeed happen time otherwise say prediction model fail make correct prediction predictor raise alert predict anomaly happen period time alert interval say prediction model raise false alarm result analysis first set experiment evaluate prediction accuracy performance join component diffuser component system six replicate join run different host application perform operation similar exhibit similar identical behavior experiment run choose one component train set predict five replicate configuration swap six time component use train exactly follow experimental result join component aggregate way diffuser component replica application use half collect data train data predict half data algorithm show true positive rate false alarm rate calculate whole trace data ideal predictor true positive rate zero false alarm rate show impact alert interval sensitivity prediction model repeat experiment use range alert ten twenty thirty forty fifty sixty seventy eighty second figure six figure six b show true positive rate false alarm rate faulty join contain memory leak fault show different alert use prediction model show true positive rate false alarm rate achieve different prediction result show alert consistently achieve much higher true positive rate lower false alarm rate alternative confirm anomaly necessary real application also observe alert interval indeed use control knob tune sensitivity prediction model generally speak increase alert interval derive prediction model sensitive increase true positive rate false alarm rate provide opportunity us achieve tunable anomaly management base benefit achieve accurate cost handle false alarm configure prediction model proper alert interval achieve optimal prediction reward also observe different anomaly type exhibit vary predictability case anomaly easier predict throughput anomaly high false alarm rate note premise anomaly prediction anomaly exhibit gradual symptom thus anomaly prominent predictor high ensemble approach sometimes observe up down true positive rate false alarm rate alert interval increase reason fluctuation use balance accuracy criteria select best prediction model ensemble define ba two therefore look true positive rate individually possible true positive rate lower smaller essence know ensemble approach perform worse alert even term balance accuracy figure seven figure seven b show prediction accuracy result join contain fault observe algorithm achieve much better performance scheme however cause fault difficult predict cause fault term high false alarm rate reason effect fault sudden fault make less predictable particularly high false alarm rat throughput anomaly make unpredictable figure eight figure eight b show prediction result join contain fault result f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble zero ten thirty twenty seventy alert interval sec fifty forty sixty eighty anomaly zero alert interval sec average prediction accuracy zero ten thirty twenty seventy alert interval sec forty sixty fifty eighty b throughput anomaly zero alert interval sec b prediction accuracy host one nine eight seven six five four three two one c r c c n c e r p one nine eight seven six five four three two one c r c c n c e r p one nine eight seven six five four three two one c r c c n c e r p one nine eight seven six five four three two one c r c c n c e r p one nine eight seven six five four three two one c r c c n c e r p f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble f f f f alert alert monolithic monolithic incremental incremental ensemble ensemble figure nine true positive rate false alarm rate system diffuser component memory leak fault show approach consistently achieve higher true positive rate lower false alarm rate learn although different anomaly type exhibit vary predictability prediction model consistently achieve much better prediction accuracy similar previous case throughput anomaly difficult predict anomaly evaluate anomaly prediction approach use different type application component call diffuser compare join component diffuser component simpler application semantics dispatch input data different host base load condition second diffuser process different input stream news video stream exhibit less rate variation network traffic stream due space limitation show subset result figure nine figure nine b show prediction accuracy result faulty diffuser memory leak fault observe scheme still consistently outperform scheme monolithic approach achieve similar true positive rate scheme incur higher false alarm rate incremental ensemble approach low true positive rate cause learn conflict train data contain present anomaly prediction result ping failure host host use half trace data train data predict ping half trace data figure ten show average prediction accuracy ten fail host experiment figure ten b figure ten c show prediction accuracy two specific fail host result show ping failure show good predictability approach achieve high prediction accuracy ninety true positive rate near zero false alarm rate contrast achieve much lower true positive rate higher false alarm rate examine trace data find set metrics exhibit difference normal sample anomaly metrics change gradually also measure prediction lead time ie early ahead prediction model raise alert achieve alert system due space limitation show subset zero alert interval sec c prediction accuracy host b figure ten true positive rate false alarm rate ping failure prediction accuracy result figure eleven show minimum average maximum prediction lead time achieve alert system observe alert interval affect lead time increase alert interval prediction model tend raise alert since measurement sample include alert state alert interval may also incur higher false alarm rate show previous figure similarly figure eleven b show prediction lead time host failure result indicate alert achieve second several lead time allow anomaly diagnosis correction one design objective alert system achieve scalable anomaly learn prediction achieve goal employ reservoir bias sample reduce measurement sample overhead one question whether bias sample greatly affect prediction accuracy repeat experiment use bias sample retain fifty thirty total due space limitation show subset result show figure twelve figure twelve b observe bias sample maintain similar prediction accuracy reduce sample overhead evaluate overhead alert system figure thirteen show cumulative distribution function model train time collect different experiment run train time include time discover build decision tree correspond different compare model train time use full measurement sample one eight n c r f six four two zero zero one eight n c r f six four two fifty thirty five one fifteen two time zero zero two fifty thirty four six time us eight train time b prediction time figure thirteen alert system computation cost min c e e e l n c e r p fifty zero min c e e e l n c e r p eighty seventy sixty fifty forty thirty twenty ten zero one nine eight seven six five four three two one c r c c n c e r p zero ten ten twenty thirty forty fifty sixty seventy eighty fifty alert interval sec system alert interval sec b figure eleven alert prediction lead time unsampled bias fifty bias thirty unsampled f bias fifty f bias thirty f one nine eight seven six five four three two one c r c c n c e r p unsampled bias fifty bias thirty unsampled f bias fifty f bias thirty f twenty thirty forty fifty sixty seventy eighty alert interval sec zero fifty alert interval sec join component anomaly b ping failure host figure twelve true positive rate false alarm rate different rat train time use subset sample obtain reservoir sample full model train time range twelve retain thirty sample reduce train time three six millisecond figure thirteen b show mean prediction time collect different experiment run result show prediction algorithm fast require less four model train use thirty bias sample result indicate alert indeed support anomaly prediction evaluation speed also measure cost system monitor generally impose less one load monitor host four relate work system anomaly detection extensively study example noble al propose anomaly detection algorithm data system predictability quantify graph regularity al propose change profile base approach detect system anomaly check performance deviation reference target execution condition wang al propose automatically system check status machine run application al propose sketch data monitor structure correlate anomaly rule ten propose approach detect metric behavior web service al build transaction model application performance detect anomalous application fifteen al explore method correlate monitor data failure detection complex magpie nine request extraction model tool record system correlate use application specific event schema capture control flow resource consumption request pinpoint thirteen also take approach tag call request id diagnose fault apply statistical identify highly correlate fail request al propose black box performance technique analyze trace system infer causal eight triage leverage lightweight support deal system bug without require intervention programmer different previous work research focus apply machine learn achieve advance anomaly prediction instead detection work closely relate system also address failure prediction problem distribute different system rely anomaly detection individual performance metrics achieve system state prediction work provide use whole system classification easily achieve tunable true positive false alarm rat furthermore consider execution context change dynamic distribute present initial design anomaly prediction system estimation however address dynamic compute context change common many stream process recently machine learn show ing autonomic failure management much previous work focus system log analysis thirty forty al develop console log analysis detect system contrast research focus characterization system use performance resource metrics power al investigate performance prediction power different statistical learn approach al apply network tan perform performance diagnosis sixteen propose signature concept capture essential characteristic system state seventeen al extend tan model propose use model diagnose performance fa system employ various machine learn achieve automatic failure diagnosis query process nineteen al propose construct io throughput model reference expect performance use statistical cluster characterization performance guide different previous work research focus apply learn achieve adaptive anomaly prediction host five conclusion paper present alert system provide adaptive anomaly prediction system host ing alert provide tunable anomaly prediction model employ learn algorithm adapt dynamic host best knowledge work make first attempt achieve anomaly prediction dynamic distribute implement alert system deploy several production host learn follow prototype implementation one range system exhibit predictability two alert achieve much better prediction accuracy exist learn dynamic three alert provide prediction performance impose low overhead host infrastructure six work sponsor part army research office grant manage secure open initiative exploratory stream analytics award faculty award express paper author necessarily reflect view government seven reference one elastic compute cloud two release eight three four distribute monitor system five six stream group stream stream data manager data engineer bulletin one march seven j al design borealis stream process engine eight k j j wiener p performance distribute black box nine p r r mortier use magpie request extraction model ten e l l lightweight monitor production page eleven j r blake computer bottleneck detection belief net page san ca morgan twelve g g fox technique cheap recovery thirteen e fox e brewer failure evolution management fourteen h wang p stop chase trend discover high order model evolve data fifteen l k n mi j e anomaly application change change towards detection application performance anomaly change page sixteen kelly j j chase correlate instrumentation data system state build block diagnosis control twenty g w king p detect past present predicate w al challenge experience multimodal stream analytic monitor application system b h p spade system declarative stream process engine x p p chang toward predictive failure management distribute stream process x h wang anomaly prediction robust cluster x p h wang adaptive load diffusion stream join z g h k track probabilistic correlation monitor data fault detection complex page r g efficient decision tree construction stream data e fox detect service neural network liang h r failure prediction event log thirty j f g f k comparison machine learn predict hard drive journal machine learn research c c noble j cook anomaly detection page august k approach detect performance change distribute web service page l culler blueprint introduce disruptive technology new jersey r power short term performance forecast enterprise page r k al critical event prediction management computer cluster b g disk real world mean fast k c c li x li performance anomaly identification page k c li io system performance use anomaly characterization fast j c triage diagnose production run user site forty r c v j l predictive management computer journal h j wang j c r ym wang automatic misconfiguration page w p failure prediction distribute h e frank data mine practical machine learn tool morgan seventeen j kelly fox w l fox jordan capture index cluster retrieve system history system detection mine console log eighteen j dean simplify data process j fox large cluster nineteen babu k fa system failure diagnosis ensemble model diagnosis system performance