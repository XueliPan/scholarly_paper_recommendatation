iteratively construct via conjugate gradient method research wa abstract consider problem solve symmetric positive definite system linear method solve precondition conjugate gradient method performance method depend crucially know good matrix show conjugate gradient method produce good allow us derive new asymptotic bound time solve multiple relate linear subject numerical analysis numerical linear algebra linear direct iterative general term theory conjugate gradient method precondition one introduction one basic useful computational solve system linear ax b give matrix b give vector x vector paper consider linear matrix symmetric positive definite wide range include computer graphics nine machine learn sixteen scientific compute many branch engineer comprehensive introduction linear system find standard reference seven eleven seventeen nineteen roughly speak divide two permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee seven june san copyright direct typically work factor matrix canonical form system become easy solve prototypical elimination factorization factorization iterative produce sequence guess solution iteration algorithm adjust previous guess obtain new guess closer final solution process similar function minimization gradient descent prototypical iterative descent conjugate gradient iterative interest number reason first numerous preferable rapidly compute approximate solution problem instead wait amount time need compute exact solution next iterative often superior performance system solve sparse example size n n nonzero solver require produce exact solution contrast best theoretical run time know direct solver one iterative widely use specialize develop various problem method twenty algorithm eighteen discussion iterative refer interest reader literature two eleven twelve nineteen qualitative difference direct iterative efficiency iterative usually depend crucially spectrum matrix usually formalize define condition number quantity derive capture attractiveness spectrum analyze convergence iterative solver function condition number motivate study precondition instead solve system ax b one introduce matrix p call solve relate system p ax p b intention p smaller condition number problem become easier solve introduce p impose additional cost importance precondition emphasize nineteen nothing central computational science next century art transform problem appear intractable another whose solution approximate rapidly iterative precondition provably good derive many special structure twelve thirteen eighteen general symmetric positive definite case typically base two three paper investigate new approach construct present algorithm iteratively construct good observe execution conjugate gradient method descent system ax b eleven overview begin informal overview algorithm problem ax b amount find minimum ax b two convex quadratic function level set descent work roughly follow give guess x compute improve guess compute gradient x move x direction gradient work since gradient x point roughly towards minimum level set nearly spherical procedure work well point closely towards minimum however level set far spherical require nearly spherical level set amount small condition number algorithm repeatedly perform step measure much progress make towards minimum iteration fail make much progress yield certificate level set spherical direction x certificate use find new improve condition number new act squash level set make spherical state generally algorithm approach monitor progress basic algorithm basic algorithm slow indicate give instance undesirable bad condition number one take alternative action improve instance update thereby accelerate performance basic algorithm similar approach underlie linear program eight fourteen eight basic algorithm method run time exponential input size improvement process accelerate polynomial fourteen analogous improvement obtain basic algorithm certain variant simplex method case linear iterative update provably accelerate performance already diabolically fast nineteen algorithm instead iterative construction make follow three first give entirely different approach derive asymptotic bound comparable know asymptotic bound conjugate gradient second bound hold even basic algorithm use construct descent see section six mark contrast descent inferior performance relative absence iterative update finally regardless whether descent use serve additional purpose use accelerate solution relate linear matrix new vector b analogous direct reuse matrix factorization compute twelve design mention ideal improve condition number give system without introduce much additional computational cost naturally two well understand achieve general linear algorithm give framework make precise outline algorithm produce sequence suggest p improvement amount update p increase cost compute p see section hand algorithm precisely determine extent suggest update decrease condition number system thus algorithm allow design arbitrary policy decide suggest accept reject best knowledge framework previously know exist similar sense iteratively compute new step algorithm variable metric nonlinear optimization four five six ten fifteen compute successively improve quality sense converge towards however unlike framework show condition number improve step two overview algorithm denote system linear try solve x symmetric positive definite size n n x actual solution system x guess solution error define e x x residual define x e solve system incorporate p consider new system p p p new system equivalent original one give solution new system obtain solution x original one set x p furthermore matrix p p symmetric positive definite assume p nonsingular algorithm show algorithm one generate sequence guess solution sequence iteration algorithm define ai p pi p step error residual precondition respectively define xi x potential function algorithm analysis base two potential function define let x positive definite matrix define new condition number x call eccentricity follow e x x twelve x twelve two two potential function two change potential potential function change perform iteration clearly unchanged decrease give algorithm one overview main algorithm let arbitrary initial guess let repeat initialize conjugate gradient algorithm repeat increment step one perform one iteration conjugate gradient algorithm compute better guess sufficiently small halt much smaller certificate ai eccentric step two compute better pi less eccentric ai modify match new forever algorithm two single iteration variant conjugate gradient algorithm step one algorithm one initially xi current guess solution current residual di current search direction new respectively equivalently xi di equivalently e ai function measure magnitude residual transform space ie precondition purpose measure quality current ie extent level set spherical follow lemma make precise illustrate lemma consider show figure one lemma e x uniquely minimize x identity matrix e one lemma show measure extent level set unit sphere eccentricity function property invariant scale ie e x e c one discuss issue section seven three conjugate gradient algorithm step one algorithm one base variant conjugate gradient algorithm algorithm two present single iteration variant familiar notice several variant usual formulation discuss section unfamiliar may learn highly readable exposition seventeen iteration convenience let us drop ai also define inner product x algorithm maintain three x r iteration update x accord formula xi di di di value choose minimize see although fact use proof call search define formula zero di di di imply similar error residual standard proof variant appendix b lemma algorithm two di di di di one two lemma obtain one two one two di di two di di two di di lemma two one one two summarize computation fact two appendix imply indeed decrease unless zero zero one one residual decrease substantially make progress towards solve linear system decrease substantially follow inequality hold two one figure one two illustrate definition eccentricity positive definite matrix correspond ellipsoid principal ax point ellipsoid ax length axis ellipsoid equal correspond eigenvalue whereas length axis ellipsoid equal reciprocal eigenvalue ellipsoid two obtain average axis result ellipsoid always contain unit sphere converge unit sphere approach one determinant two proportional volume correspond ellipsoid minimize equal one two unit sphere two unit sphere two case vector certificate eccentricity one thus certificate eccentricity prove least two distinct hence level set f eccentric elliptical rather spherical step one algorithm fail decrease norm residual prescribe amount guarantee certificate eccentricity use step two informally certificate eccentricity proof ellipsoid correspond ai mean also certificate property capture comparison usual three main formula usual one one use nonstandard potential function analyze progress namely two two use different definition conjugacy namely relative rather three initial search direction residual vector gradient potential function gradient usual potential function choice variant accident find usual formulation produce guarantee need later analysis namely certificate eccentricity interestingly aware previous work find need depart usual formulation mention point three search base gradient potential function instead gradient usual potential function rather remarkably obtain certificate eccentricity use mismatch gradient potential function four improve step two algorithm update ply update specifically set pi one scalar v nonzero vector specify later algorithm also set appropriately new discuss change detail choose v note become ai analyze change affect lemma change give one e e ai one one one ai two two change could less greater one depend choice v ideally change would less one mean eccentricity decrease ensure happen suppose previous step one produce certificate eccentricity fact two two two one certificate allow us choose v decrease significantly first define one one one obviously one since positive definite obtain zero one indeed choice v definition actually value minimize choose v consider two case result last inequality case case case pick v ai case pick v ai use together formula give lemma one may design variety decide algorithm one perform step two one possible policy perform step two whenever sufficiently small say policy assume remainder paper show follow lemma policy ensure step two provably decrease factor least though policy sufficient asymptotic analysis actual implementation might benefit use different policy lemma step two perform follow claim hold case fourteen zero one one case one fourteen one zero case two one choose modify change linear system solve might case xi good guess bad guess therefore specify new value follow one xi one xi lemma choice affect follow one one one case case five analysis section analyze number need reduce residual fraction original magnitude recall two consideration p transform residual untransformed residual p clear relationship transform untransformed p algorithm seek minimize r perform step decrease course uniquely minimize zero nonzero step decrease norm might actually increase r analyze progress decrease r one must consider p matrix relate r specifically r p recall matrix pi modify step two step two modify impede effort reduce untransformed residual two ways issue one norm transform residual might crease lemma suggest issue two norm p might increase word step two might increase upper bind gap transform residual untransformed residual analyze issue let denote respective value iteration step two first deal issue one lemma imply total amount norm transform residual increase one quantity bound follow lemma case lemma case one e two second issue deal follow lemma lemma k p k e two analysis issue one two find step one must decrease norm residual additional factor e four discussion yield follow result theorem number require algorithm one decrease untransformed residual factor log one log e proof since step one decrease constant factor obtain follow upper bind number time algorithm need perform step one log one log e step two perform log e time six comparison mention section one best know asymptotic analysis direct solver asymptotic analysis admit simple close form instead require find f f small particular analyse rely one interest aspect present work provide analysis variant without resort base several useful bound give article one example number require decrease error factor bound follow log one one log one b b n log one log n classical bind use bound possible additionally know find exact solution n mention appendix b although bind frequently fail hold practice use inexact arithmetic bind algorithm one state theorem similar bind give special case n one see suppose example matrix n normalize n one log log e remark theorem remain valid even algorithm one simplify use ordinary descent instead step one standard bind seventeen descent require log one decrease error iterative update accelerate descent algorithm performance comparable ie bind n one iteration cost precede discussion consider iteration count neglect computational cost iteration iteration involve one hence require time number nonzero one also use add overhead cost multiply vector cost use produce algorithm depend represent option store p explicitly typically nonzero first update option seem attractive give matrix dense ie case p substantially increase cost alternative option store accumulate update constitute p separate product p w compute consider individual update product w clearly compute n time refinement approach identical better performance practice wy factor form refinement store update two rectangular matrices frequently use householder algorithm eleven p represent separate update wy form cost multiply vector ai become p number update occur iteration p never log e one obtain follow bound algorithm one n log e product cost one log one e one log e one step one step two n log e log one log e multiple relate let us consider performance algorithm one scenario one must solve k instance problem x different direct well suit factorization easily use solve k rapidly contrast able reuse work perform solve prior vector algorithm one able reuse work do step two give advantage direct solver set present concrete example lead asymptotic improvement run time let dense matrix eigenvalue zero fix parameter eccentricity asymptotically two two two n two two n bound number require matrix large least b n one least b n one bind log one n zero log n log n one take two log n two exponent asymptotic analysis direct mention section one obtain log e state bound also let k n one run time direct algorithm one k k log one log e k log one analysis illustrate class algorithm one bind improvement bound direct seven discussion many interest question leave open work definition eccentricity note section undesirable several reason intuitively cause algorithm favor wasteful update push towards one rather useful update push closer together natural way resolve problem would run algorithm matrix rather way estimate geometric mean rapidly compute alternatively alternative approach update affect scale ie pi one n n suppose one need solve two ax b b intuitively good good b e p tap small e p also make rigorous lead interest analyse interior point step one algorithm one modify use standard conjugate gradient algorithm rather variant example inner product x use instead update modify maintain conjugacy word necessary step two iteratively update numerical stability algorithm one extend handle matrices cautiously optimistic progress question could positively impact solve linear practice author thank alan steven g gilbert strang teng helpful second author support natural engineer research council canada scholarship contract grant eight reference one g rate convergence precondition conjugate gradient method two r w berry f j j j v r c h van solution linear build block iterative three precondition large linear survey journal computational physics four c g convergence class minimization j math five r h p j c limit memory algorithm bind constrain optimization journal scientific compute sixteen five six w c variable metric method minimization j optimization seven direct sparse linear eight j simple algorithm solve linear program proceed annual symposium theory compute page june nine r j stam h w visual simulation smoke e editor computer graphics proceed page press ten r fletcher j rapidly convergent descent method minimization computer journal eleven g h c f van loan matrix twelve k combinatorial sparse symmetric diagonally dominant linear thesis mellon university thirteen joshi optimization sparse linear thesis university fourteen j randomize simplex algorithm linear program proceed annual symposium theory compute page may fifteen j wright numerical optimization springer second edition sixteen r g yeo regularize least square classification advance learn theory model science series computer volume page press seventeen j r introduction conjugate gradient method without agonize pain august manuscript eighteen sh teng time graph partition graph solve linear proceed annual symposium theory compute page june nineteen l n numerical linear algebra twenty p introduction sons appendix general fact one let matrix matrix call update matrix several useful symmetric positive definite assume one one inverse determinant one norm one zero one one zero proof first property trivial second property formula eleven let us consider eigenvalue direction v one one subspace orthogonal v imply third fourth fact two let x positive definite let w nonzero vector zero two one furthermore two two proof recall inequality state p q r one p one q one let x let eigenvalue associate may write j j w j w quantity positive j prove lower bind apply inequality xi p q two ie square two establish upper yield bind prove show follow upper bind claim one two three two two apply inequality p three q xi take cube yield two three two four three two desire upper bind b proof variant appendix show inner loop algorithm two indeed variant define criteria algorithm one search conjugate orthogonal relative specify inner product two error step conjugate subspace span previous search note criteria imply convergence algorithm n formalize criteria follow predicate e di zero j zero j trivial zero assume true inductively prove one claim e one true proof j di di di di zero di di di directly j zero e di zero zero claim j ark zero j k one proof zero j k zero e k similarly j zero assumption j k e k imply zero expand j use yield desire result j ark zero zero j j j one proof arbitrary j j follow multiply rearrange claim show term side zero accordance statement claim claim one true proof j di di di di zero di di di directly j zero claim di zero zero c proof proof lemma matrix x let j x denote eigenvalue x x positive semidefinite j x twelve j x clearly two whenever x zero equality hold x one follow zero x one two one thus obtain j x one two n j two j ai one j ai two one n furthermore equality hold j ai one unique matrix equal one identity matrix lemma prove lemma di di di zero proof follow prove lemma di di di di di second equality follow claim di di di di second expression di di di two thus di zero claim b one b proof lemma di lead desire expression di di obtain di di di di di derivation also use di di di two therefore suffice show two zero follow straightforwardly proof lemma convenience drop let denote ai r denote lemma e one e one one one one one one one one one one two fact one two two zero lemma thus proof complete lemma let b proof lemma drop let denote ai r denote lemma wish analyze one one one one two two one two two one one one one one two one proof proof use fact determinant homomorphism first note b one case case set v r imply r twelve b two twelve condition case twelve twelve fourteen claim assume hypothesis case fourteen two eighteen proof lemma e one one two lemma lemma case one two one one two since lemma fourteen two consequently fourteen one one since thus e fourteen case prove lemma proof lemma e one one two case one one two case one fourteen lemma since follow one fourteen thus two one e two one one may write p one k k k k p one k fact one p k case p k one one case case case case one one one one one fact one e two proof first show fourteen equivalent show fourteen fourteen one fourteen one fourteen one two fourteen one two fourteen zero zero twelve zero hold zero therefore obtain two eighteen two recall one twelve therefore zero desire one one since fourteen one one case case set v ar imply r condition case twelve fourteen claim assume hypothesis case one fourteen two eighteen proof use argument similar claim show one fourteen obtain one two eighteen two one since one fourteen twelve since one one follow one zero desire conclude proof lemma claim claim obtain follow bind decrease one two eighteen proof lemma new residual become ai thus fact one one zero one zero lemma proof complete