learn network structure prior versus data diagnosis medical valley stream parkway pa abstract approach structure learn graphical model equivalent sample size ess prior model recently show important effect estimate network structure first contribution theoretically analyze case large complement previous work among result find presence edge network absence even prior data imply independence long conditional empirical distribution notably different uniform second contribution focus realistic provide analytical approximation optimal predictive sense accuracy also validate experimentally approximation provide understand data main effect determine optimal one introduction posterior probability marginal likelihood graph one popular score function learn network structure al require value free parameter specify researcher equivalent sample size ess originate prior model al elaborate experiment twenty data set al show choose decisive effect result map graph estimate experiment show figure one al number edge increase monotonically zero maximum ess value grow two empty complete graph indeed reach several data set almost reach remain data set al effect map graph estimate notice one main theoretical analysis case small paper also provide result case ess bay factor converge zero limit mean neither presence absence edge give inconclusive behavior limit first contribution paper concern case large finite theoretical analysis section three derive give data presence edge graph contribution concern case large finite combine result small imply immediately condition number edge increase grow although necessarily monotonically second contribution concern case realistic intermediate section four goal understand give data set determine optimal predictive sense achieve derive explicit analytical approximation validity also assess experimentally two brief review network learn network model comprise direct acyclic graph g model conditional see al overview describe joint probability distribution p vector random x n number consider discrete random xi multinomial distribution paper accord graph g joint distribution factor p n denote set parent xi graph g joint state denote state xi xi various approach learn graph g give data develop literature see al overview approach appear popular score function employ posterior probability graph g give data p p p g marginal likelihood graph p equivalent prior distribution graph p g choose uniform marginal likelihood graph express close form use prior model al importantly prior distribution ensure likelihood equivalence al desirable property structure learn particular likelihood equivalence hold xi prior express xi xi one n q prior distribution x positive constant independent equivalent sample size ess q choose uniform distribution one arrive score one xi denote number joint state variable hence xi xi ess remain free parameter marginal likelihood graph g p g equal al n ni xi xi xi two gamma function first product extend second one joint state parent variable xi third product state xi xi cell count contingency table serve sufficient statistics sample size n xi approach prior regularize parameter estimate g xi ni three expectation respect parameter posterior distribution fix data ess find map estimate graph g maximum respect marginal likelihood two problem one thus resort heuristic search like local search al find graph number prohibitively large exact solution ie globally optimal graph g find reasonable due recent advance structure learn besides popular score various score function devise structure learn include information criteria schwarz information criteria minimum description length use second contribution section three large effect learn graph section present first contribution namely large finite value ess affect learn optimal network structure complement theoretical result small derive experimental result intermediate obtain al one section define new uniformity measure discuss measure determine result structure learn large ess value see next section definition define conditional distribution p b two random b give set follow p b b p b b p b p b p b p four b denote number joint state obviously definition equivalent p b p two b p b b p b p b p b one b b b b p b two p two p b two p two five rewrite term conditional first line term square second line interest property representation weight sum four term weight number joint state proposition one measure follow three basic symmetry p b p b p b zero p b zero conditional independence p b p p b state p zero least one marginal uniform p p b concern note zero state p zero exist neither one p p b uniform include b conditionally independent skew word necessarily equal zero independent proof symmetry obvious proof two focus representation term conditional five obviously hold state hold also sum remainder proof understand conditional state p zero omit notation necessary condition minimization b p b one account introduce multiplier first partial vanish ie state b b b zero six normalization constraint follow immediately two follow particular choice consider difference pair pertain state b different state p b p b p p b hence p p p uniform hence p b p p b arbitrary p b otherwise ie p six p p b p p b p b uniform p arbitrary conversely verify easily distribution indeed fulfill original condition six moreover zero distribution finally show second derivative positive definite complete proof also like mention four term relate square five suggest relationship index p x one x x use impurity measure learn decision tree also view special case entropy p x one x p x one parameter two entropy use statistical physics understand expansion entropy x one generalization entropy limit one entropy entropy coincide entropy p x log x p approximation bay factor section theoretically analyze behavior approach structure learn review section two case large finite determine likely graph structure note marginal likelihood p g graph g important relative marginal likelihood p g compete graph g particular consider two graph follow identical except edge b present g absent g let denote parent graph g imply g parent b presence edge b give parent absence ie graph g g log bay factor log p g p g positive negative absence b give complete data ie miss data marginal likelihood factorize two term bay factor cancel depend b parent log p g p g b log na b b b na b log b b log log n seven note bay factor symmetric b expect independence test asymmetry edge b cause parent rather b graph present main result large finite aside besides interest fact weight sum number state function weight proposition two give score approximation bay factor two graph g g define read large n log p g p g n two n p b two eight uniformity measure define p b empirical distribution imply data ie p b na b n one b one well know give proof end section let us first discuss interest follow approximation first note replace conditional mutual information term n would identical information criteria analogy suggest eight serve penalty model complexity word n p zero mean p notably significantly zero n p zero refer p notably zero give new measure discuss section hence follow directly corollary one give score exist value r finite presence edge b give parent absence n p b zero ie empirical distribution p imply notable dependence b give notable skewness ie nonuniformity p p b note b conditionally independent choose assume variable condition set parent data imply notable dependence sufficiently skew distribution note however latter condition may necessarily hold large set parent give data set small many count occur joint number state number sample data proof proposition two four term seven take form log bay factor denote joint state set random obtain use z z one first line log log k one log k one log two log one k one log two nine insert approximation log bay factor seven obtain note b b score denote number joint state random log p p na b log b one two b b b na b b na b na b n b b ten conversely absence edge b give parent large value n n p b zero ie empirical distribution imply na b one two two b b notably dependent give state p zero one marginal distribution p p b notably different uniform distribution first term vanish b uniform score second term equal p b two two third term equal n two illustration second give apply edge graph follow immediately complete graph achieve marginal likelihood sufficiently large finite must correct count originate prior score ten note different g section section provide simple example show combination uniform prior distribution skew empirical distribution create dependence individual distribution imply independence key na b n p b four understand optimal theoretically analyze case large first part paper case small remainder paper focus optimal predictive sense tackle question optimal fifty twenty data set elaborate experiment al result also reproduce table one comparison range two thirteen remain data set section aim provide answer question ie goal understand data determine optimal optimization graph ess give crucial effect learn network structure al depart orthodox approach choose without see data remainder paper treat ess latent random variable learn light data various objective function determine optimal ess value discuss al find yield essentially result paper choose jointly optimize graph structure g g p g eleven joint optimization eleven expensive simple ascent tractable approach comprise two alternate round k zero one convergence one optimize graph fix g k p g two optimize fix graph k p g k first step solve standard algorithm structure learn review section two concern graph iterative algorithm several convenient choice learn graph optimize information criteria schwarz criteria approximation log marginal likelihood two also independent ess ie contain free parameter hence use initial know promise yield initial graph figure one example illustrate dependence increase skewness imply data several ten one solid curve top bottom dash curve top bottom approach prior log bay factor essentially base weight sum two see section two three empirical distribution p weight n uniform distribution q virtual data point due prior weight obviously sum two imply independence necessarily result distribution imply independence n p p b q q b six c p p b distribution p constant c general deviation equality typically statistically uniform q skew p long sufficiently small increase weight sum deviation become statistically illustrate follow example let us consider artificial data set n sample imply independence two binary b ie empirical distribution factorize like p b p p b simplicity assume p p b control skewness marginal parameter z zero five p one z p zero one z z five imply uniform distribution skewness distribution grow z decrease value range five one symmetric case consider figure one illustrate log bay factor seven increase negative value imply independence positive value suggest dependence empirical distribution become increasingly skew ie z decrease figure one show increase degree skewness p diminish prevent log bay factor imply dependence aside note curve quite flat large small either one dominate sum contrast maximum z zero reach n ie weight equally bay factor already close optimum respect marginal likelihood network model main contribution remainder paper derive analytical approximation optimal second optimal ess give graph section provide understand important data affect value optimal ess derive analytical approximation optimal fix graph g step two step two maximize predictive accuracy sense al data point consider arrive individually sequence optimization problem difficult solve replace similar objective depart statistics minimize test error commonly do even though objective original one expect yield sufficiently accurate approximation far aim understand difference fifty versus ten lower various data set al note assumption follow validate experimentally twenty data set section follow combine two test error obtain explicit approximation optimal carry reference point space convenience choose distribution p imply network model graph g parameter estimate test error model respect true distribution p x read p x p x p x log p twelve use log loss p unknown evaluate however test error approximate train error e p x log p penalty term model complexity p x p e p x log p g n one thirteen e p x log p x n p x log p n xi log fourteen ni g n xi ni fifteen indicator function one zero zero otherwise data set sufficiently large cell count positive g equal number give network xi one denote number joint state variable obtain second approximation test error twelve follow assume unknown true distribution p functional form regularize parameter estimate three use optimal value sixteen p xi n n n two seventeen n expansion estimate reference point assume n justify end section insert sixteen seventeen twelve result second approximation p x p e p x log p log p n x n n two next equate approximation one thirteen finally arrive explicit approximation optimal two n g e p x log p x log p eighteen x log p expectation respect prior distribution q analogous fourteen regard robustness zero cell count use n one place practice xi note denominator eighteen indeed positive prior distribution q entropy h empirical distribution p case score obvious e p x log p x log p h q h p q p equal maximum log likelihood divide sample size g effective number give divergence entropy h interpretation eighteen explicit approximation optimal ess provide understand main data determine optimal table one experimental validation analytical section see text detail skewness dependence denominator eighteen tend increase entropy negative likelihood empirical distribution factor accord graph structure p decrease case strong along edge conditional skew data set one hence expect small value conversely data set neither imply skew distribution extremely strong result increase value sample size eighteen explicitly depend sample size n indirect relation however effective number measure model complexity tend increase n complex optimal model also entail increase maximum likelihood hence also affect denominator one may hence expect enumerator denominator grow similar way n increase suggest weak dependence n apart n sufficiently large model complexity tend approach true value increase thus sufficiently large data set become independent n consequently assumption n indeed hold sufficiently large n note behavior also consistent one expect asymptotic limit n namely n zero effect regularization vanish number nod eighteen imply expect unaffected number n nod sparse graph average enumerator denominator additive nod hence average grow proportionally add additional nod network experiment additional confirmation underlie section determine optimal value base iterative algorithm use eighteen twenty data set bay use data use al imputation miss value continuous compare exact result table one summarize result confirm validity approximation obvious approximate agree well exact result al obtain heavy computational cost table one exact data balance iris thyroid liver abalone diabetes post yeast cancer shuttle glass page heart heart heart st wine adult n ninety n five five six seven eight nine nine nine nine ten ten ten eleven eleven eleven fourteen fourteen fourteen fourteen fifteen one one three two two three six seven ten six six three five three five one six six ten one three seven fifteen five six three three thirteen sixteen five six seven ten eight eight two two three two three four eight eight seven six three four three three six six seven eight three three sixty eight five six six three three nine thirteen five five ten ten seven eight fifty k one two five three three three three three two two two two three four two three three four three three solution maximization problem eleven range yield map graph al approximation always agree precisely exact value correctly identify data set optimal ess fifty oppose ten lower remain data set suggest analytical approximation underlie capture main effect influence optimal moreover note sample size n vary sixty zero number n range five fifteen table one show n n obvious effect optimal expect analytical approximation see discussion section moreover note result two similar see section three four decisive data imply case skewness imply distribution insight surprise increase optimal ess value relate reduction maximum number edge graph attainable experiment al data set large optimal fifty exactly data set maximum number edge graph achieve increase less eighty complete graph see column range ta one al namely data set imply neither strong skewness p p proceed seven th conference uncertainty artificial intelligence morgan aside note iterative algorithm section use approximation eighteen efficient converge within small number k table one convergence criteria require one k five paper present two would light learn graph affect value equivalent sample size ess first analyze theoretically case large finite ess value complement result small value literature among result surprise find presence edge network absence even prior data imply independence long conditional empirical distribution notably different uniform second contribution provide understand give data determine optimal ess value predictive sense consider free parameter analytical approximation also validate experimentally show optimal approximately independent number sparse graph sample size moreover optimal small data imply skew distribution strong along edge graph interestingly condition concern optimal similar one derive large finite finally show crucial effect ess value graph structure maximize score suggest similar effect expect concern posterior distribution graph hence model average one embark popular approach one hence choose prior great care grateful r encouragement support work provide data anonymous valuable comment reference h information theory extension maximum likelihood principle page b n f second international symposium information theory w theory refinement network page b learn network page proceed international workshop artificial intelligence statistics al r g p l j probabilistic network expert springer p statistical theory approach journal royal statistical society series choice model fit data exponential family annals statistics sixteen one al learn network combination knowledge statistical data machine learn twenty bay bay archive k exact structure discovery network journal machine learn research five j model data description fourteen schwarz schwarz g estimate dimension model annals statistics six two p simple approach find globally optimal network structure page proceed conference uncertainty artificial intelligence al p p sensitivity map network structure equivalent sample size parameter proceed conference uncertainty artificial intelligence h prior neural information process nip fifteen c cond possible measure complexity