optimal outlier removal space abstract study problem find subset set point probability distribution space point x define outlier exist direction w square distance mean along w greater time average square distance mean along w main theorem zero exist one fraction original distribution n improve previous bind asymptotically best possible show match lower bind theorem constructive result one approximation follow optimization problem give distribution ie ability sample parameter zero find minimum exist subset probability least one b log n one introduction term outlier familiar one many several typically quantify far outlier rest data difference outlier mean difference outlier point rest data addition difference might normalize measure scatter set range standard deviation data point outside threshold label identify fundamental ubiquitous problem data set might represent experimental error case would desirable remove could affect performance computer program slow even mislead algorithm machine learn area train data could algorithm find wayward hypothesis even purely theoretical standpoint remove could lead simpler mathematical model might constitute phenomenon interest one find address question first answer another precisely outlier paper assume data consist point distribution space case one could use one allude point outlier distance mean greater factor time standard deviation figure one top data set depict definition data point solid circle mean along department mathematics support part career award one mean plus minus one standard deviation hash mark leftmost point standard away mean follow generalization higher dimension use let p set n point x p call outlier exist vector w point square length x along w time average square length p along w ie r x two ex p x two note x two square distance along w origin figure one bottom two picture show different point may furthest different w graph solid circle data point line direction w hash mark along line data point onto line figure one define r n long history statistics machine learn definition outlier equivalent definition use terminology field machine learn point distance greater statistician might say normalize covariance data point away origin constructive procedure identify section two show equivalence definition two first problem address follow exist small subset point set p whose removal ensure remain set precisely remove subset consist fraction p remain set respect remain set natural approach find set remove do first apply linear transformation describe section two result average square length distribution one along every unit vector isotropic position isotropic position use speed random walk bring distribution isotropic position allow us identify easily point two outlier simply square length main difficulty remain set might still possible point previously become happen repeatedly force us throw set main result answer question surprisingly small value n present general framework let b denote set place point set p discussion n b probability distribution let denote probability distribution probability subset space one n z z theorem one outlier removal integer support one let probability distribution n b every zero exist z one x two x b log n n e x two x w n r proof theorem one section four constructive prove theorem one prove similar theorem arbitrary support theorem two section three although hypothesis support distribution theorem two much need additional assumption proof one two make use principal idea section two describe two algorithm outlier removal prove use either variant although obvious algorithm extremely simple convince reader include implementation algorithm section eleven point set point n algorithm run time section five show algorithm also use unknown distribution allow draw random sample distribution number sample require run time accuracy one one variant algorithm identical algorithm immediate inspiration work bind one two improve previous best bind give use crucial component first algorithm learn linear threshold function presence random noise due high value bind run time learn algorithm although polynomial somewhat prohibitive contrast theorem imply improve bind learn linear arbitrary presence random noise bind asymptotically best possible show section six example one two bind better n log one possible b main theorem give bind natural question complexity achieve best possible particular distribution give distribution early version work one claim slightly different version theorem one insufficiently strong hypothesis three parameter want find subset probability whose removal leave set possible show question even case reduction section seven prove algorithm achieve one one approximation best possible give case may desirable translate data set origin coincide mean rather fix origin prove follow corollary standard mean section eight let probability distribution b fraction distribution along every direction zero exist one standard point away mean direction also give one approximation algorithm correspond optimization one problem b log n p n section nine prove theorem describe connection outlier removal robust statistics section ten conclude prove technical matrices use elsewhere paper two outlier removal first question address detect since definition involve might obvious do finite time order detect use linear transformation let e x sample draw accord probability distribution positive definite exist matrix consider transform space z transformation preserve z outlier direction w transform space untransformed space correspond x outlier direction w vice transform distribution isotropic position refer transformation round previously use design make geometric random walk efficient full rank still positive semidefinite instead round span familiar distance normalize covariance data set transformation show equivalence definition outlier two isotropic distribution point x outlier direction w also outlier direction x follow fact isotropic distribution e x two one every w one projection point x w x direction w w x thus outlier identification easy isotropic first algorithm follow simple form remove stop description give distribution two exact value specify proof one two algorithm one restriction one round exist x let x x x retain point two repeat condition meet four algorithm one identical outlier removal algorithm follow variant algorithm significantly easier analyze whereas previous algorithm remove every direction one step algorithm two remove one direction per step algorithm two restriction x two one round exist unit vector w two let x x two two retain point two repeat condition meet three outlier removal arbitrary support prove follow theorem outlier removal distribution arbitrary support proceed prove theorem one refer condition hypothesis theorem two condition theorem one remove condition replace condition support distribution theorem two outlier removal arbitrary support let probability distribution n satisfy r unit vector w x unit vector w one x r every zero exist r r r n e x two x w n r one x two x prove theorem analyze set return either algorithm set clearly outlier free remain show discard much distribution main idea proof show every step volume associate dual ellipsoid increase bound total growth dual ellipsoid volume course algorithm deduce certain fraction original probability mass throw algorithm terminate towards end need matrix define e w e x one w x ax one refer e w primal inertial ellipsoid dual ellipsoid respectively subset n denote matrix give r x e x x five word obtain restrict zero point outside renormalize distribution denote restrict probability distribution directly throughout chapter denote restriction subset space never new unrelated distribution useful property attain round respect restriction original distribution e x two x x one every unit vector w expectation probability respect x draw actually prove theorem two e x two x place e x two note statement original theorem let x denote x x zero let span denote span x x also need follow elementary volume ellipsoid give product axis time volume ax give unit ball denote f n ellipsoid singular axis w e give singular value one follow v w v e f n two function solely dimension x one lemma one relate dual volume growth loss probability mass lemma two upper bound total dual volume growth lemma one restriction slab let fix let isotropic distribution suppose one w w x two x two let x x two two p x v w w proof let e x two x x start identity e x two ex x two x ex x two x use x two two x get one imply one e construct vector w length belong dual ellipsoid let w wa suffice since w unit vector assumption also show every v w also belong w e x two x x one w w w x six hence v x v x x two v one v w last step assumption v imply v w length point boundary ellipsoid lower bound length axis since least one axis dual ellipsoid length ax length least one v w f n v w f n imply dual volume grow least factor conclude proof lemma one note desire apply lemma analyze result later iteration algorithm two go simply replace start identity ex x two x ex ex x two x x two x analysis conclusion remain lemma two dual volume growth let distribution satisfy unit vector w x unit vector w one x r let restriction assume one v w v w f n f n proof first lower bind initial dual volume v w consider vector v length v e x two x one z v belong dual ellipsoid thus dual ellipsoid initially volume least f n next upper bind v w consider vector v length x one thus v w thus ultimate volume dual ellipsoid volume sphere radius yield claim upper bind r seven proof theorem two final distribution result application either algorithm use one two prove algorithm two terminate satisfy theorem two proof theorem two let four n one suppose algorithm terminate subset throw original probability mass every w r x two x x two x x remind reader normalize probability distribution point rather point outside replace zero increase side inequality factor one increase side thus inequality still true even normalize thus achieve outlier free subset two four one n r r remain show ie throw probability mass claim suppose iteration algorithm step one pi fraction original point throw total amount throw pi two two p pi compare lemma one total amount dual volume increase bind total increase dual volume lemma two yield e p q two two p pi e r r n en r r one two four n r r one two n r r since rely apply lemma two one remain catch show slight overload notation let denote cumulative probability mass remove point algorithm suppose purpose iteration j one establish contradiction iteration j step j apply lemma two analysis however single iteration maximum probability mass algorithm might throw twelve see proof lemma one one zero one twelve p two step j one thus one step increase one still conclude proof theorem two give alternate proof theorem two use construction give algorithm one begin prove analogue lemma one lemma three restriction ellipsoid let fix let isotropic distribution let two x x v w p x w eight proof first establish radially symmetric distribution show radially symmetric distribution worst case want let radially symmetric distribution define p calculate increase v w let e x two x w one center sphere radius projection sphere direction sharply concentrate around n square expectation exactly use identity x w e x two ex x two x ex x two x proof lemma one w deduce one thus proof lemma one observe w include vector length direction w since true every w dual ellipsoid volume increase least factor n show case radially symmetric distribution n one e v w w show radially symmetric distribution worst case want suppose isotropic distribution statement lemma true construct new isotropic radially symmetric distribution statement also false begin note every point throw also throw rotation follow fact isotropic let expectation random rotation radially symmetric distribution probability choose x distance less r origin exactly probability choose x distance less r origin every r let correspond consider axis direction wi e e denote axis length axis also radius e find construction x wi one e x two x e x two x x one n n one n n one way visualize equality take simply consider achieve average ax onto ax since discrete set clear square axis arithmetic average square axis make take continuous set without affect axis consider volume e v e f n f n f n ai v e n n n v n one n nine use arithmetic mean inequality imply v w v w conclude proof lemma three finally prove algorithm one terminate satisfy theorem two proof theorem two proof theorem two use algorithm two let four n one lemma two still hold rate increase dual volume throw probability mass lemma three lemma one thing need address call one remain catch proof use algorithm two bind amount probability mass throw single step longer twelve however four one two conclude analysis algorithm one r follow connection show success either algorithm imply succeed criterion point x outlier direction w instead x two e x two x p x p one two throw exact point must yield bind function see note outlier definition remain outlier point remove remove eventually also point ever remove unless currently outlier thus two throw exactly set point end alternative definition outlier section seven develop observation approximation algorithm problem outlier removal use standard definition outlier alternative definition pause stress gain allow point distribution remove force zero even hypothesis theorem two may unbounded even radially symmetric distribution satisfy hypothesis support denote ball radius r might large allow zero achieve four one n r r four outlier removal discrete support theorem two might suffice many indeed possible outlier removal arbitrary set condition might violate indeed dimensionality remain set might decrease section prove follow theorem show condition entirely unnecessary ten theorem one outlier removal discrete support let probability distribution n b every zero exist z one x two x b log n n e x two x w n r proof theorem present two present proof theorem two first might initially lie entirely subspace might lie subspace removal point secondly even distribution lie subspace lower bind singular value distribution singular value matrix associate insist hypothesis theorem two b discrete singular value must least roughly equivalent two follow example case may singular value actually two make clear example one let b let row matrix represent point space denote first n n denote last row p one row one b one b one b one one set point clearly singular value one one find order b however direction w b since w one singular value actually two zero p two b slightly less two b n two n b example one show even disregard issue distribution use theorem two treat distribution integer support unless will settle extend prove theorem one show although one singular value may small small simultaneously appropriate amortize sense first thing shall define potential function generalize dual ellipsoid volume use proof theorem two potential function account distribution concentrate lower dimensional subspace even possibility simply quite close lower dimensional distribution begin define core distribution subset distribution lie subspace span every large subset distribution help define indicator function e e logical statement core give e one zero e true e false eleven definition one core define core maximum choose w span w zero x zero x x x establish core include core lemma four characterization core w span w zero x zero x w x zero x x zero x b x x x x q core q core q core q core q core suppose core dim span k dim span k k k proof first establish let arbitrary assume b hold ie x write exist x direction w x zero x zero w zero p span x zero x one x zero x since one x span hold p find x suppose b hold w zero hence b imply x x x x show give algorithm construct core p span x zero x hold x span x p one exist x direction w x zero x zero x remove x x p two repeat exist x argue correctness algorithm suffice show x meet criterion step one x r r zero x obvious since w associate x step one x therefore zero yet x x x zero zero x zero x r satisfy x p w x x r p x p twelve six six six six six six six six six six six six six six six six six six six six x identify step one core since algorithm stop arrive satisfy b point remove could core core establish core consider order point identify step one algorithm apply consider point order omit point q algorithm run q would always remove point well simply x q q minus x x x zero zero q x x point algorithm remove prior iteration consideration p prove use p q core q core q core q core q core q core q core q core q core core note core q q core q core core q core q core combine yield core q core q see construct follow greedy manner dim span dim span w span x zero x x x w span could write w span span x zero x zero x x span remove every point x argue zero less hence w fraction total probability mass note cause dim span drop k time hence least one therefore construction iterate k x x x p p p k x x k two x zero define potential distribution subset distribution definition two potential function let core let v w volume dual ellipsoid full dimensional instead lie space dimension k let v w volume within span dual ellipsoid prove upper lower bound analogous lemma two case core although version lemma may possible analysis sufficient show asymptotic result theorem one thirteen six six six six six six six lemma five bound denote core suppose hence n n n proof lower bind show vector v satisfy dual ellipsoid use element x length greater find v n v v two x x x x one x x claim lower bind follow fact w contain ball radius n upper bind use volume ellipse equal product axis time factor depend x dimension f n show decompose set simpler plus extra point x p imi xi x zero p n pick point span pick point mi positive definite matrix see decomposition begin pick point span continue always make choice consider direction w perpendicular span previous point point nonzero inner product w guarantee exist n definition core lie span previous point first set point j one form must restrict yield use slight overload notation definition pick point n always able form new mi core long subtract less fraction probability mass distribution thus far process see terminate finite number step support initially finite number point every step support decrease least one refer extra point simply point remain operation form sufficient number mi come end j p p note matrix mi mi one sum many integer term positive mi positive definite show may ignore term establish lower bind another consequence mi positive definite mi ai since determinant two product eigenvalue equal unit vector two p fact two section ten since geometric mean least min p p one p p p imi min mi xi fourteen last step write imply yield claim upper bind n n note lemma five imply log upper lower bound n b fifteen log n relevant set proof theorem one compare favorably correspond ratio continuous case n r suggest introduce much slack extend amortize singular value address issue dimension drop refer growth title lemma six may drop remove distribution see consider example one initially f n remove point p become roughly two proof theorem two bound drop probability mass bound increase volume dual ellipsoid may decrease greatly course algorithm core drop dimension bind final value longer enough bind drop probability mass happily still bind growth follow sense one f n b n lemma six growth course either algorithm distribution let denote relative increase core span subspace dimension one core never concentrate subspace dimension log n one proof suppose initially core v simplify assumption construct distribution core v result apply removal algorithm core nothing lemma five differ factor log n thus suffice prove bind v remove simplify assumption defer issue core might initially end proof q suppose algorithm go r dimension one dimension run produce still dimension simplify assumption mention dimension core fall one step ease exposition assume distribution equal core without loss generality define term core point outside core irrelevant lemma construct dimension one r r apply construction iteratively dimension yield dimension n satisfy v fifteen let us construct define r let p j j p j p p xi define p j fact two section ten exist j denote particular x let min j one x x x weight x weight thus note p rotate span span show equal first ax x lie span first one ax denote vector form first x x one one st x x one distance x span x one also distance x span f r f one zero zero zero two x one x one x one x one x one x one x one two upper leave matrix block one x one matrix subtract scalar multiple row another row change determinant calculate subtract x l x one time last row matrix row every l yield zero zero zero two x one x one x one two zero one two zero therefore x one f show two f identical calculation yield identical result remove simplify assumption extend construction case step core fall dimension one r differ k dimension construct adjoin k point r weight k show find point since r core span subspace k dimension less use construction lemma five write k xi ai set k point span ai xi x min span r min k xi sixteen let denote ai realize minimum let weight weight k k r construction remain show show previous calculation give fact simplify assumption repeat k time let define first l point weight k l similarly previous calculation yield l denote one one two one two one k k conclude construction least turn case initially k dimensional k n case adjoin n k point weight form may probability distribution total weight one n n n construction lower bind lemma five iterative construction n without simplify assumption yield n n n log n one conclude proof lemma p prove algorithm two apply distribution yield theorem one proof theorem one let let two six n three time drop probability mass due action algorithm lead increase either algorithm cause dimension core drop algorithm remove probability mass lie outside core b four log n first consider fraction distribution part core initially dim span n suppose dim span core k lemma four n k fraction lie outside core point leave core remove algorithm algorithm take one step dim core dim core latter case probability mass lose core remove algorithm give lemma four core core since core core core seventeen lemma four since cumulative drop dimension core n dimension n fraction distribution ever leave core without remove algorithm course algorithm bind amount distribution remove algorithm step core drop dimension proof theorem two show single step algorithm two throw twelve fraction distribution since n step core drop dimension throw fraction way yield n fraction probability mass throw away contribute increase proceed exactly proof theorem two algorithm two every time remove pi probability mass core core drop dimension lemma one must increase throw fraction fraction contribute increase application lemma one lemma six yield e two two three three log n n n b four log three two two e three two two three one three three conclude proof theorem one use algorithm two prove theorem one use algorithm one note previously may obtain corollary success algorithm two direct proof raise additional issue explore resolution issue lead bind smaller lead constant proof theorem one let let two three n three new issue bound amount probability mass remove algorithm step core drop dimension might remove fraction single step asymptotic bind would stand could remove fraction probability mass course algorithm b four log n x x round suppose core fall k dimension one step algorithm rather consider point outside remove imagine instead probability mass every point uniformly decrease increase continuously except k discrete time step dimension core drop apart step core drop increase function probability mass remove exactly imply lemma three every time core drop dimension amount probability mass leave core lemma four therefore k amount probability mass remove without increase think experiment probability mass core remove algorithm without increase thus n amount probability eighteen mass remove without increase apply lemma three lemma six obtain e log n three n three n b four log two two three two two e three two two three one three three conclude proof theorem one use algorithm one five time sample complexity section describe polynomial time computational model allow unit time point set suppose distribution specify explicitly set point weight correspond achieve exactly state value either algorithm run time either algorithm give time compute time round distribution time find outlier need repeat whole process time yield time bind discussion make worst case assumption one data point throw iteration round look case single data point throw round distribution do efficiently distribution initially isotropic v probability p remove give new inertial ellipsoid factor one symbolically one one one one two v choose b symmetric verify calculation note p use matrices commute calculate b one one one one one two one one one two one one two p p nineteen plug complete verification one one one one p one p one old distribution b yield formula p one p x new isotropic distribution one x one one one v x v p computable time n point x another explanation formula correct inertial ellipsoid direction v type update step sometimes refer update use observation compute scratch round distribution scratch find outlier use formula total time yield improve time bind throw away less fraction point set time bind z n b case distribution specialize analysis core throughout algorithm obtain run time different relevant suppose step algorithm one parameter remove equivalently dual ellipsoid volume increase factor one remain data set one outlier free may remove many point use technique develop time bind per iteration however upper lower bound log one two increase one final bind run time obtain one outlier free set arbitrary suppose give explicitly rather ability sample n ease exposition refer case support b restriction specify part contain ellipsoid algorithm z one get set p sample two run outlier removal algorithm discrete point set p parameter two three let p subset p restriction p give restriction give e one zero fourteen accuracy parameter one e p p xi main theorem section follow twenty theorem three sample complexity let n log log two two n n b log n two high probability either outlier removal algorithm run parameter two one return ellipsoid e satisfy one one one one two achieve deterministic omniscient algorithm omniscient know distribution exactly one remainder section assume deterministic omniscient algorithm parameter two find subset statement simply since implicit exactly one w x two x x two x x two x two x rather x x two x able conclude important subtlety since x x two construct algorithm always convex whenever instead x zero ie define x although might subset assign positive probability point lie outside alternative definition would increase length proof know two bind run time prove arbitrary value two always achievable case may better would able conclude w every direction let suppose step estimate e within one two one every point perceive least respect true distribution remove throw away point deterministic algorithm keep similarly perceive distribution true distribution one remove may work estimate within one however whenever wrong one true outlier respect original distribution throw even use flaw estimate line reason make rigorous allow us find one one subset space two achieve deterministic version algorithm lemma seven show particular direction particular iteration lemma eight extend proof theorem three extend every step bound sample complexity lemma seven outlier detection one iteration fix direction w let subset space let number sample two two consider sample distance direction w give variance denote sample variance xi let denote true x two x xi two one constant probability x two x x two x one one two x x two proof property say correctly estimate variance restriction distribution property assure us restriction distribution probability mass past two time sample variance ie always safely throw away probability mass use sample variance claim fix direction w note assume hypotheses lemma eight rely upon part explicitly involve similar argument let xi random variable represent square distance xi along direction w xi xi two zero xi one one appropriate scale first show since two apply bind determine probability good estimate x two x without loss generality assume occur constant probability two two e x two x show let assume loss generality accurate estimate analysis previous paragraph case one imply one two imply x two x one without one two would find two lemma eight outlier detection many fix w assume let two two log n b log n two two constant probability either outlier removal algorithm restrict w parameter two produce subset space x x two value subset space along w one one along w proof either outlier removal algorithm restrict w simply mean version two consider achieve deterministic omniscient version algorithm restrict w since outlier removal algorithm throw away probability mass necessary possible restriction free define lemma seven lemma seven part good approximation ensure good probability identify prove remain show algorithm reason choose substantially set one one define one follow fact suppose x x two one x two x one one two x two x x monotonically increase function e x two x suppose estimate set case algorithm might return answer sample also lead us calculate one one reason precede paragraph every show nearby within factor one value estimate sample variance restriction sufficient accuracy proceed analyze value need consider n uniform two probability distribution assume without loss generality w unit vector easy upper bind x two develop lower bind need use assumption write x two e x two decompose manner lemma five obtain statement x two e term e two lower bound singular value previously show product singular value since individual singular value least n log n two therefore restrict attention least n one k k integer union bind log one two n n possible value k show estimate one good probability actually one respect true distribution reason since within one one one show one good probability sample show least one let xi random variable represent square distance xi along direction w xi xi two zero without loss generality assume one define lemma seven xi place assumption e xi one condition sample show least one one one apply bind xi one one p one e state bind case one let yield event one one probability calculation one one one one one one one four two one two two two one probability correctly identify furthest outlier e one two one two applicable alternate form bind one one two one e one set yield e since n n two allow us union bind possible value show constant probability estimate case algorithm might return one one imply different value consider two log n n extend analysis seven eight fix direction argue correctness entire algorithm prove theorem three proof theorem three let ellipsoid find deterministic algorithm ie subset point lie ellipsoid assume initially rather consider original space consider transform space unit sphere consider many w give grid unit cube form zero two one grid choose every w w lie choice apply lemma eight part n simultaneously x two union bind good probability every w grid x ie direction contain show arbitrary direction w one contain x two x wi consider arbitrary unit vector w round every w integer multiple obtain point grid set possible round form box surround w unique subset n point satisfy w convex cone since w unit wi denote within w n wi wi wi vector wi length define distance boundary along direction since convex one quantity w lower bound minimum distance point wi convex hull projection w convex hull since point convex hull three since within one origin away wi w everywhere one contain conclude proof origin since w within wi wi wi one one one consider since one well every w grid one one along w lemma eight part consider transform space one unit sphere let e actual inertial ellipsoid let w arbitrary unit vector define one r wi therefore one one one reason r w wi three one one one one wi yield wi wi within n remove assumption suppose rather span subspace suffice consider w w projection associate w move distance w upon projection compare three along wi along wi lower bound lemma eight along wi similarly lower bound even though wi change lower bind asymptotically negligible therefore apply lemma eight part wi thus two three thus one contain establish part wi w previously along wi within factor one three w wi wi one one extend proof part case identical manner conclude proof theorem three corollary one run time algorithm run time proof section three two never need sample plug value bound section yield algorithm run time bind refer introduction time achieve one one one approximation optimal value pose relate problem suppose give parameter two rather ask find appropriate two lemma nine show point determine within factor one much probability mass within fix ellipsoid since two value two consider loss factor one value find two therefore simply try estimate one whether two require us throw away one fraction distribution log one one log thus two achievable deterministic algorithm give find subset space satisfy two asymptotic run time still lemma nine probability mass location let e ellipsoid let number constant probability estimate one fraction sample outside e one fraction outside e least fraction outside e one xi proof round e let random variable one e event estimate one fraction sample outside e less fraction truly lie outside e one upper bind probability event use bind one let one p x one e e three one one one one one one one one upper bind probability constant one case alternate form bind applicable find number sample still sufficient one similar calculation event one one use one e e three involve set one yield x one one one one similarly alternate form bind one therefore probability significantly underestimate amount probability mass outside e constant value one consequence section sample size estimate inertial ellipsoid distribution fraction thus bring nearly isotropic position enough b remove six match lower bind show fourteen exist distribution support satisfy exist w one z n b x two x x two x x e x two x two n case throw half distribution base comparison upper lower bound log one b b log n n n b log one describe theorem one asymptotically optimal b c twenty one one weight position figure two lower bind motivate construction worst case distribution construct three simpler prove lower bind strong lower bind follow examine distribution composite three show lower bound twenty prove first weak lower bind let uniform distribution illustration give figure two part claim point one four best achievable ie satisfy b proof simple suppose data point keep ignore factor w since x one dimension b since find b e x twenty one e prove next weak lower bind construct distribution figure two part b let probability distribution point give one one one e x three x four neither point throw away thus yield one one one third weak lower bind let distribution space particular let uniform distribution n point one axis one unit distance origin illustrate figure two part c one two throw away point least ax w unit vector along x two x one ax point throw away one e x two x four n thus n composite construction use prove strong lower bind illustrate figure two part obtain composite distribution take distribution part make two copy weight translate two point compose distribution part b place copy new distribution along axis distribution part c restate construction formally proceed analyze fix n b b let point distance four log one two one let copy follow distribution along axis twenty one one consider distribution place one fraction probability mass uniformly first b point fraction uniformly remain b point distribution maximum bite length along axis log b many ways choose subset distribution quickly restrict set interest treat axis symmetrically purpose establish contradiction suppose help treat different ax differently begin note distribution concentrate ax fix vector w maximize x two x x e x two x always occur axis see note round transformation need scale ax maximize w round direction point ie along axis therefore maximize w round also along axis let one distribution concentrate ax symmetric axis possible throw fraction distribution achieve parameter suppose minimum achievable achieve asymmetric let axis axis maximum outlier occur suppose along axis throw fraction total distribution let subset one throw point along every axis yet achieve along throw along axis axis contradict assumption symmetric subset could throw achieve axis j along axis j throw fraction probability distribution achieve construct take replace choice point throw along axis point throw along axis j yield contradiction thus restrict attention symmetric along axis ie x x j x direction w along axis projection onto w point n ax zero obtain one e x two e one n ignore factor n rest proof restrict attention single axis suppose furthest point keep achieve point exponent k choice distribution throw half point one k b calculate expectation x factor e x x one twenty two one b yield one e lower bind case e b case thus b b one n b log one seven approximation algorithm b log n show paper distribution achieve n question naturally arise well particular distribution compare best possible particular distribution formally give seek minimize subject one w x two x e x two x really approximation problem note case look normalize probability distribution outlier free show problem even data reduction problem exhibit one one approximation algorithm task one case give distribution explicitly sample distribution algorithm yield one one approximation constant zero one high probability problem give pi pi constraint pi one removal problem let p p pi subject one form correspond instance outlier one n find maximize let give zero one p point one probability mass one two p point zero probability mass pi pi let possible solution instance outlier removal problem since p one otherwise problem trivial point one remove hence one one two minimize subject one remove probability mass point zero e x one thus ratio e exactly problem find optimal solution problem two zero one two two one prove lemma enable approximation result lemma ten preservation let distribution outlier least one outlier respect subset satisfy one proof let x outlier original distribution w x two e x two e x two x e x two x satisfy x two one e x two x x approximation algorithm simply either algorithm describe section five error parameter case sample could determine optimal fix binary search suppose value achievable restriction satisfy algorithm see point outlier respect distribution know outlier restriction lemma ten point throw optimal solution thus run algorithm force us throw away point optimal solution also throw away yield achieve one approximation case explicitly provide distribution one run time n one one one outlier removal algorithm fact find approximation every one pass section two use define outlier order point set namely first point outlier second point approximate best possible particular value simply remove initial fraction point outlier order one time look back see value achieve eight standard mean prove variant theorem show find large subset original probability distribution point many standard away mean corollary two standard mean let probability b let subset space denote probability x choose accord let e x x every zero exist w e x two two x z b log n n one x x w w n r proof proof corollary much like proof theorem one appropriately modify outlier removal algorithm construct simply translate data set origin coincide mean removal step easily show translate origin coincide mean never decrease volume dual ellipsoid explain variation potential function upper lower bound imply modify algorithm throw fraction data set analyze volume dual ellipsoid consider fix direction w let one e x two r length dual ellipsoid direction translate origin value z along w one z two single variable calculus show value maximize r z e x mean thus translate w e x thirty origin maximize length dual ellipsoid every direction simultaneously thus drop probability mass growth dual ellipsoid show one three also hold modify algorithm describe modify need define distribution choose maximum subject requirement affine hull lower dimension affine hull minus fraction choice fraction definition appropriately modify version lemma four still true define appropriately modify v w explain derive upper lower bound analogous lemma five case lower bind immediately imply argument translate origin mean decrease dual volume derive upper bind consider set whose affine hull lemma five argue n one point positive integer zero choice thus one let one write ai must lower bind ai ai ai p n one xi ai ai n one n one ai n one p n one ai xi second term positive nonzero determinant integer matrix hence original determinant least origin correspond mean set point maximize dual volume bind hold origin upper bind n n n one n one prove statement analogous lemma six cumulative drop revisit construction r define object proof lemma six r least extent calculate mean f mean give x let origin correspond origin mean r p two x x two x x two x x two two perform analysis use elimination previously compute ratio yield f one two f two one x one two two two one two one two assume never remove fraction probability mass circular reason proof theorem two use algorithm two upper bind assumption imply never remove fraction probability mass since never remove fraction one step assumption always hold use assumption calculate two two one two twelve one four two one two one one multiply factor together n step iterative construction yield additional cumulative factor negligible combine bite additional slack new bind full dimensional case possibility n end proof lemma six finally arrive bind total cumulative drop immediately imply claim value corollary two log n three one one one show approximation algorithm section seven naturally extend approximation algorithm set measure respect mean rather fix origin establish suffice prove follow analogue lemma ten one lemma eleven outlier preservation variant let distribution corollary two measure square distance mean rather fix origin suppose outlier point outlier least one one outlier respect subset satisfy one proof proof lemma ten consider unit vector w two e x two let two difference bind bind lemma ten result mean possibly move closer remove point xi without loss generality let mean origin let e x two one suppose reach remove point total mass e x two x x xi two xi e x two x one xi two xi one calculate new mean e x x x zero xi xi e x x zero xi xi one xi one xi xi xi xi therefore new distance mean calculate zero xi xi one p two variance two pi xi xi pi xi two xi one one one one one one xi xi two xi two xi p p let p xi xi numerator amount decrease derivation average point remove weight change xi two xi denominator xi follow p follow two p x one zero x one two one two b two x one two one two along line fact two follow fact one section ten show two may consider remove single point weight order lower bind may view constrain maximization problem one expression f two two one one two one constraint would zero would imply one tight variance distribution remove tight would two one constraint two one one one two two one one two one one two two two one one neither constraint tight may solve unconstrained optimization problem set zero find local maximum evaluate f maximum one one f two one two v one one one v v two v zero zero two one one one zero one one one one one one one two one two one two one one two f zero zero one one two one one one two one one prove lemma nine robust statistic robust statistics choice median quintessential robust statistic commonly motivate describe robust version mean particular note data set mean data set change arbitrary amount simply move one data point infinity contrast median go absurdity literature commonly put least half data change adversary data set define median point least fraction data lie leave point least fraction right call point median every direction w satisfy definition median projection w one use helly theorem one prove exist data set distribution best possible point call propose robust estimator data show high breakdown point technical criterion robustness shall discuss teng al give first polynomial time algorithm compute approximate center point polynomial n algorithm produce one show n b algorithm section eight produce yield one distribution one one z theorem four let distribution let e x x x two x median x one one one suppose satisfy two x w n r proof suppose initially one without loss generality consider particular direction give unit vector w assume zero e x two one since restrict attention w rest proof may define xi denote distribution let denote index set partition let define via xi zero xi xi xi zero xi xi xi xi xi zero xi one xi xi xi xi obtain xi use xi one xi xi xi one turn lower bind xi xi xi xi xi two xi xi xi xi xi one drop assumption one one ten matrices proof section four rely fact two speculate present proof fact since use otherwise necessary rest section four fact one x positive definite proof statement equivalent clear square twice x two x p x two equivalent x two one x x one x one x x one x one one four one four one four one four one four x x one one one one one let x case show b b least one consider arbitrary eigenvalue pair e reduce one show claim every eigenvalue one end also let b four two e one four one four two one since one one e eigenvalue least one b use zero true since positive definite since form basis whole space b also fact two positive definite xi zero one p xi xi proof straightforward generalization fact one suppose first exactly equal integer pi case may apply fact one iteratively find x j x j equate pi theorem must hold binary approximation fact two follow standard continuity xi recover fact two exactly general x j reference one c becker gather maximum asymptotic bias outlier technical report university p gather identification multiple journal statistical association p frieze r algorithm learn noisy linear threshold function one j optimal outlier removal space proceed symposium theory compute one l r isoperimetric convex body localization lemma discrete computational geometry thirteen l r random walk volume algorithm convex body random structure eleven one r v behaviour robust estimator journal statistical association ninety k l g l miller c teng approximate center point iterate radon point proceed symposium computational geometry san ca appear international journal computational geometry l breakdown location estimate base depth project annals statistics twenty four eleven implementation let x n matrix whose row point distribution let n beta epsilon value n let variable do indicate whether finish remove complete implementation give follow code require x epsilon beta do zero do one x covariance matrix x sparse isotropic version x one remove current beta x zero do zero end end end spring illustrate outlier removal algorithm available