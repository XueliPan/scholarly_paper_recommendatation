packet schedule deep packet inspection multicore terry institute technology institute technology communication stream maintain state information large number concurrent packet arrive layer fully parse inspect state information associate communication stream layer update store work require many cycle fortunately work therefore multicore use increase available cycle raise question schedule multicore platform paper explore question use protocol analysis module pam system highly pam seven seventeen nineteen library use detect respond malicious network traffic proprietary module use security implement intrusion detection prevention pam fully parse protocol layer emulate state transition client server layer pam detect client server vulnerable state watch attempt exploit pam constantly update new protocol add old update detect latest threat continuous addition new code cause average number need inspect packet slowly increase security update addition network speed continue increase today one standard ten become commonplace data center next standard schedule soon ratify forty begin show data center six continuous increase network speed average per packet instruction count result pam require cycle inspect traffic line rate past pam rely law satisfy demand cycle processor manufacturer release new processor higher operate frequency performance pam increase without code change work well many processor able hide latency bottleneck cache unfortunately due heat power longer case processor turn exploit thread level parallelism order continue deliver regular performance multicore hardware thread level parallelism instruction pam currently transform single thread application multithreaded application take advantage pam thread process link transport layer packet parallel packet however application layer process parallel base abstract critical performance component multicore commonly use network highly packet schedule significantly impact well scale deep packet inspection complex network make packet schedule difficult impact performance also packet latency order differ depend whether application deploy therefore different packet schedule make base deployment paper evaluate three packet schedule protocol analysis module pam application use network trace acquire production network intrusion prevention deploy one packet schedule evaluate commonly use production thus useful comparison two design result show packet schedule base cache affinity important try balance specifically three network trace test cache affinity packet outperform two increase throughput much subject multiple data stream computer communication network general security protection general term design measurement performance security packet schedule deep packet inspection protocol analysis module pam multicore one introduction deep packet inspection process examine content packet system reconstruct communication permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee ten la ca copyright state flow must process serially fact segment flow must process sequence order way pam emulate transition client server small number base must process serially well application layer sequence order implement sessions application layer however majority process parallel paper summarize follow design implement two new packet schedule one design maximize work balance cache affinity compare two packet schedule packet schedule algorithm commonly use network demonstrate schedule cache affinity important balance work load fact one network capture use evaluation schedule cache affinity improve throughput rest paper explore packet schedule goal maximize throughput minimize latency paper organize follow next section present relate work section three describe packet schedule design implement evaluate section four explain test methodology present result section five paper conclude future work discuss link two relate work packet distribution load balance schedule term typically use process assign packet resource perform work paper term use interchangeably packet schedule occur many place network schedule physical link rout sixteen physical link link aggregation eleven distribute network five forward parallel forward service fourteen thread one nine ten although work schedule vary widely many apply across packet schedule domain packet schedule overview packet base schedule packet base schedule assign basis example simple packet base schedule algorithm complex assign least load resource resource number outstanding process packet base schedule good job balance load across however packet reorder occur impact network performance addition packet base schedule share memory cache inefficient result reduce performance packet base schedule commonly find schedule option network flow base schedule flow base schedule ten maintain table active flow flow associate resource map flow schedule resource associate flow new flow assign least load resource since flow assign resource per flow packet order maintain also cache efficient reason method first table active flow must maintain table require large amount memory many active flow addition take significant number clock cycle perform finally flow equal number process associate thus difficult assign new flow work balance fix hash schedule direct hash indirect hash four two common fix hash schedule direct hash apply hash function subset use result modulo number determine resource assignment indirect hash use result hash function modulo size indirection table index bin indirection table associate resource map bin process resource indirect hash allow unequal resource weight indirection table tune adaptive hash schedule fix hash schedule hash value assign resource therefore per flow packet order maintain cache efficient addition stateless since table maintain downside method lack control resource assignment result load fix hash schedule commonly find schedule option network adaptive hash schedule adaptive hash schedule nine sixteen twenty attempt combine simplicity fix hash schedule ability change resource assignment load scale twenty adaptive hash schedule algorithm use balance arrive network adapter use indirect hash schedule load imbalance detect host protocol stack try balance traffic calculate new indirection table author sixteen describe evaluate several adaptive indirect hash rout adaptive hash schedule improve fix hash schedule load however add overhead reactive typically occur network impact flow burst schedule flow burst schedule five fifteen try combine balance packet base schedule cache affinity per flow packet order flow base schedule flow base schedule flow table maintain however need contain flow system flow entry contain number currently system last packet map flow packet arrive map flow entry use subset flow entry exist system flow packet assign resource process hand flow entry exist system flow packet assign least load resource since flow system time process resource cache efficient packet order maintain also balance work better flow base schedule flow fix resource reassign packet burst however maintain flow expensive still occur packet schedule packet literature design complex information use make schedule typically apply instance number thread literature essentially fix process time per packet contrast number pam need process packet vary dramatically depend protocol attribute example packet application layer protocol pam recognize may require clock cycle whereas identically size packet compress content may require clock cycle inspect thus difficult determine amount work assign pam thread make schedule decision maintain packet order egress important feature packet design packet reorder within flow negative impact performance network due fact reorder within flow result duplicate data segment reduce data transmission rate two three require per flow packet order egress always mean process restore per flow packet order egress eight also per flow packet order egress important always deploy however pam require segment process sequence order pam receive segment save copy miss segment arrive impact performance large number segment receive maintain per flow packet order packet process improve pam performance reduce number segment must save multithreaded state snort thirty currently available beta support multithreaded execution model allow multiple analysis operate traffic simultaneously therefore unlike pam utilize packet connection level parallelism case eighteen multithreaded version currently available however author describe architecture could use create multithreaded architecture use flow base schedule algorithm distribute analysis thus improve packet schedule algorithm could significantly increase performance deploy packet maximize throughput minimize average latency bind maximum latency one millisecond minimize number reorder within flow system longer important fact become maximize throughput bind maximum latency much higher value one second three packet goal packet schedule maximize amount network traffic inspect without noticeably impact ideal packet follow load balance work evenly distribute across thread low schedule overhead cost schedule term memory cycle small comparison work perform packet order flow egress arrival order cache affinity schedule thread associate data structure already cache minimal packet delay variation minimal variation latency add flow section describe design implementation three packet schedule evaluate pam first algorithm describe direct hash algorithm commonly use schedule use evaluation comparison purpose two design create packet maximize load balance however maximize property make order cache affinity algorithm design last flow bundle goal maximize cache affinity like sacrifice packet delay variation single packet ideal packet goal determine important achieve maximum inspection minimal network impact packet use source destination address instead entire flow identifier fid choose include transport layer port would require parse include fragment except first fragment significant impact distribution network capture hash function choose know provide good load distribution four addition always hash address first connection go thread direct hash direct hash simple fix hash schedule algorithm widely use select comparison packet arrive packet parse data link network layer extract fid fid hash result modulo number thread determine pam thread process packet several appeal stateless state maintain information need distribute network traffic contain packet additional memory overhead table order use subset input hash function flow hash value process thread thus packet order maintain cache affinity flow always process thread memory associate flow assign thread processor cache increase likelihood cache hit process packet furthermore flow often arrive burst refer packet train thirteen result temporal locality use improve cache hit rate assign packet train processor use load imbalance control distribute author prove direct hash balance due thirty probability distribution flow find network traffic header parse packet must understand parse protocol contain fid field addition must able skip lower level reach fid network traffic even lower level complicate contain label switch virtual lan stack extension therefore packet must complex enough extract fid field network high distribution overhead header parse hash distribution data plane execute every packet important efficient however esoteric network traffic complexity good hash function make distribution bottleneck packet packet packet schedule algorithm design goal maximize concurrency available packet base schedule algorithm least load thread get next packet therefore algorithm best distribute work evenly across thread two type queue use distribute packet receive packet place packet receive queue thread busy ie currently process packet go get next packet empty wait ie spin packet arrive thread packet parse link transport layer packet packet transport protocol extract pam thread use fid pam track transport application layer state associate connection connection entry ce store connection table fid use map packet ce table connection table share thread ce map find new entry create thread set owner do set flag ce thread find ce check see another thread own own thread set owner ce own indicate another pam thread currently process packet associate packet flow must process sequence order therefore instead wait own thread complete process release ownership thread packet connection queue associate ce return next packet ce need thread own ce process application layer current packet update synchronization require thread finish process current packet check see place associate ce own remove owner ce set flag ce proceed next packet packet thread retain ce ownership next packet instead pam thread one ce time ce ownership occur packet map ce currently own pam thread release ce ownership finish process current packet associate ce empty pam thread packet associate ce current owner ce receive cache benefit application layer process thread however ce ownership release may system either process map ce high probability process different thread interleave thus receive cache affinity benefit advantage load balance thread pull queue ie busy link transport layer process parallel pam thread own ce process thread map own ce n pam thread process least n receive queue map different thread idle low distribution overhead packet need parse packet compute hash therefore schedule overhead low disadvantage packet order preserve two flow process time different thread race acquire ownership ce thread higher packet number acquire ownership copy packet save addition pam packet transmit cache affinity map part flow thread therefore cache line associate move processor processor look update addition cache coherence overhead amount cache line available store reduce increase memory access queue overhead thread compete implementation contention issue due fact average packet size network capture fourteen thread pull smaller average packet size thread could become bottleneck addition contention limit performance large percentage hand single thread last flow bundle last flow bundle flow burst schedule algorithm design goal maximize cache affinity schedule similar way last processor schedule task idea process system map flow single thread interleave process map flow use two type queue single receive queue table flow bundle queue distribute method packet arrive process packet extract fid use input hash function however instead use result hash function select thread process packet use map packet table select empty packet hand select empty packet system initialize pam thread proceed get first packet packet process pam thread associate ie packet map packet pam thread empty newly process packet pam thread select packet head process next process repeat empty empty pam thread return next packet packet select select empty packet permit non busy pam thread ie pam thread currently associate process however packet select empty packet ensure pam thread currently associate process pam thread process contrarily pam thread fully process leave packet process inform packet pam thread currently associate association occur pam thread packet pam thread associate empty packet place system map therefore give time one packet map give advantage order flow hash one pam thread assign system map process thus flow reorder process serially order arrive system cache affinity map system time process single thread packet train process processor increase likelihood cache hit data structure associate flow addition thread choose process packet map previous packet arrive map increase cache hit rate eliminate interleave flow process could evict cache line associate previous packet flow load balance table two order magnitude number pam thread number flow small compare dynamic pam thread assign map system thread finish process last packet system go get next packet n thread process least n system map unique thread idle disadvantage header parse like direct hash packet must complex enough skip precede protocol layer extract fid increase packet delay variation flow intentionally process together improve efficiency however cause packet train jump ahead older queue result higher latency enough jump ahead increase high distribution overhead addition overhead describe section require packet packet two place map empty four performance evaluation section describe test methodology data use evaluation result goal understand impact schedule algorithm throughput latency therefore packet schedule algorithm measure follow maximum raw throughput measure fast packet schedule algorithm process network capture ignore packet time information maximum scale throughput measure fast packet schedule algorithm process packet capture use time information capture average packet latency average amount time packet spend system maximum packet latency maximum time packet spend system test methodology design test harness take network capture input collect throughput latency describe test harness begin load capture memory eliminate disk io parse packet load extract fid calculate hash pam thread wait barrier phase memory proceed barrier record value cycle counter packet last packet capture sentinel let pam thread know do pam thread sentinel packet calculate difference value current cycle counter record start value cycle count record pam thread use time measure throughput measure maximum raw throughput packet fast pause queue full cycle count record pam thread use calculate throughput measure maximum scale throughput complicate load memory packet scan record packet capture calculate average maximum define maximum maximum bite rate sustain minimum one millisecond packet scale packet new average match program argument provide test harness process program argument provide test harness process begin packet use cycle counter es cycle counter adjust packet new adjust packet specify time periodically packet check specify time periodically packet check number outstanding number exceed number outstanding number exceed specify maximum value consider drop drop actual appliance fix number packet actual appliance fix number packet buffer value exceed incoming buffer value exceed incoming drop consider pam drop drop consider pam drop packet thus unable perform choose h choose maximum number outstanding latency maximum number outstanding latency start increase significantly exceed start increase significantly exceed measure latency packet latency measure record value cycle packet latency measure record value cycle counter packet calculate difference difference cycle counter pam thread process latency cycle counter pam thread process latency associate receive forward would associate receive forward would present appliance measure test harness present appliance measure test harness however goal measure latency relative latency relative schedule algorithm believe measurement schedule algorithm believe measurement sufficient latency take eighty sufficient latency maximum scale throughput evaluation platform evaluate packet schedule system evaluate packet schedule system ing kernel two run kernel two quad twelve four ram hyper four ram enable processor affinity set enable processor affinity set pam thread packet execute different pam thread packet execute different hardware thread addition thread assign thread assign two execute core unless execute core unless thread core furthermore priority thread core furthermore priority thread set base base memory controller per processor memory controller per processor connect via quick path interconnect connect via quick path interconnect create architecture processor four core processor four core core data data cache core processor share inclusive core processor share inclusive eight cache cache line size cache coherency maintain protocol maintain protocol network capture use three network capture evaluate three network capture evaluate performance packet schedule consist real network consist real network traffic network appliance traffic network appliance deploy contain entire every packet deploy contain entire every packet determine number clock cycle pam require determine number clock cycle p process packet clock cycle process packet clock cycle section take record clock cycle counter section take record clock cycle counter enter pam packet process function enter pam packet process function record difference pam return table one show record difference pam return table one show follow information capture conn total number unique unique total number average average network traffic average network traffic capture maximum maximum network maximum network traffic sustain minimum one millisecond traffic sustain minimum one millisecond capture average cycle average number clock cycle pam expend process packet clock cycle pam expend process packet connection density average number unique per one hundred per one hundred table one network capture attribute network capture attribute cap conn dominant many balance thirteen seven fifty e l c c k c c l zero rank ten rank nine rank eight rank seven rank six rank five rank four rank three rank two rank one dominant many balance balance remain figure one top ten clock cycle choose three capture want evaluate want evaluate vary network packet schedule condition use real network traffic figure one show clock figure one show clock cycle distribution top ten capture cycle distribution top ten capture top ten dominant dominant capture responsible fifty total clock cycle traffic capture fifty total clock cycle traffic capture dominate single connection responsible responsible fifty clock cycle top ten fifty clock cycle many capture responsible less capture responsible less four mean much lower variance clock cycle h lower variance clock cycle spend flow capture addition e capture addition different spread throughout capture spread throughout capture per average fifty unique contribute fifteen balance capture top ten total clock cycle dominant connection balance capture responsible eight place balance capture capture responsible eight place two term connection term connection clock cycle distribution throughput result dominant capture raw throughput capture raw throughput figure two show maximum raw throughput result figure two show maximum raw throughput result dominant capture algorithm highest throughput capture algorithm highest throughput every thread count test show near linear every thread count test show near linear one four thread thread add thread add throughput increase cease dominate dominate connection responsible majority connection responsible majority five thread single thread process thread single thread process connection nothing else per connection nothing else second fast single thread process connection fast single thread process connection add additional thread decrease average latency add additional thread decrease average latency fourteen thread fourteen thread performance decrease due fact hardware thread core inspect dominate connection share cycle another hardware thread process every thread count test cache benefit thread get prefer process packet flow bundle process may arrive even one thread perform better fourteen thread around faster b zero b zero b zero one two three six seven fourteen four five thread figure two dominant capture raw throughput algorithm perform well capture two reason first work balance sense single thread process dominate connection work balance improve pam require connection process sequentially second work schedule performance every packet dominate connection process core process core therefore data associate dominate connection stay cache improve cache hit percentage result cycle per instruction algorithm also receive cache benefit process dominate connection core however thread process also process dip peak graph show thread number result less map thread process dominate connection fourteen thread map thread process dominate connection however hardware thread share approximately twelve lower core ultimately throughput limit algorithm worst performance fourteen thread ie hand thread number thread increase number hand packet another thread expensive data link transport layer information copy packet avoid process layer also packet another thread require atomic operation since could multiple ie multiple connection different pam thread reason performance decrease four thread many capture raw throughput many capture contain single connection dominate packet process cycle fact connection responsible less four total figure three show maximum raw throughput result many capture highest throughput one two three six seven fourteen four five thread figure three many capture raw throughput perform better almost perfectly balance like get cache benefit thread process every packet flow nevertheless interleave different flow perform well fifteen even less ten hand thread performance due cache miss packet connection entry cache balance capture raw throughput data associate throughput result balance capture present figure four highest throughput every thread count test cache benefit schedule algorithm fourteen thread faster perform well five thread well four six seven fourteen due load fact outperform four thread show load imbalance outweigh cache benefit large enough however cache miss limit four thread one two three six seven fourteen four five thread figure four balance capture raw throughput scale throughput maximum scale throughput result capture show figure five throughput decrease comparison maximum raw decrease comparison maximum throughput spike fifteen three throughput spike fifteen three time average last several time average last several sit accumulate latency wait accumulate latency wait process happen many capture happen many capture high connection density large number high connection density large number prevent starvation starvation dominant many balance balance figure five scale throughput scale throughput notice throughput decline algorithm notice throughput decline algorithm dominant capture significantly less suspect due fact process suspect due fact process different order rest example assume three different order rest example assume three millisecond burst least millisecond burst least expensive term clock cycle process first first packet buffer available arrive packet buffer available arrive process original order thus increase process original order thus maximum scale another interest observation another interest observation perform better dominant capture balance capture due load due load packet burst result drop therefore reduce packet burst result drop therefore reduce throughput latency result figure six show average latency three packet figure six show average latency three packet schedule capture number represent schedule capture number represent mean time take process packet eighty maximum mean time take process packet eighty maximum scale throughput packet schedule algorithm e packet schedule algorithm latency number highest average latency three capture highest average latency three capture work cause wait queue even work cause wait queue even system lightly load dominant many many balance figure seven maximum latency figure seven maximum latency cache figure eight show average number cache show average number cache miss per packet seven pam thread thread three packet schedule network capture dominant network capture dominant capture see considerable number cache miss capture see considerable number cache miss result large number packet large number packet average number cache miss per packet average number cache miss almost identical dominant capture capture yet higher throughput process around eight process around eight core process dominant connection core process dominant connection additional expensive process due additional expensive process due associate state cache cause average cache cache cause average cache miss per packet higher dominant core see miss per packet higher dominant core see figure nine plus higher average number plus higher average number cache miss per packet cause cache miss per packet cause c e zero e k c p r e p e e h c c e g r e v eighty sixty forty twenty zero dominant many balance balance figure six average latency maximum latency result present figure seven result present figure seven dominant balance highest maximum latency dominant capture due order process order process starve burst traffic process cause index process figure eight average cache miss per pac miss per packet many capture produce highest number cache many capture produce highest number cache miss per packet due large per packet due large number concurrent ie connection density number concurrent ie connection density capture result interleave connection process capture result interleave connection process balance capture cache miss balance capture cache miss capture general network capture general load cache miss load connection interleave b zero c e eighty sixty forty twenty zero system cache size impact order determine system cache performance three schedule measure performance three schedule throughput half cache available available since way disable half cache processor way disable half cache processor create cache clobber thread run one one core processor cache clobber thread allocate cache processor cache clobber thread allocate cache align memory read first cache line align memory read first cache loop thus half invalidate pass pass figure ten eleven twelve show result experiment figure nine dominant core average cache miss cache miss per packet half full zero b figure ten dominant capture twelve cache capture twelve cache half full figure eleven many capture twelve cache twelve cache dominant capture performance decline performance decline around seven cache miss increase small seven cache miss increase small amount core process dominant connection amount core process dominant connection state dominant connection easily fit state dominant connection easily fit smaller cache access enough keep cache access enough keep cache percentage decrease due experience higher percent increase cache miss process core dominant connection process core dominant connection result many capture show similar percentage result many capture show throughput decrease three packet schedule three packet schedule large number ie many large number ie process concurrently result need process concurrently cache state case balance capture decrease throughput decrease throughput result uneven distribution work across core result uneven distribution work across core core assign work increase number core assign work increase number cache miss proportion proportion core thus even though increase cache miss slightly higher even though increase cache miss slightly higher much impact much impact performance half full figure twelve balance capture balance capture twelve cache discussion algorithm best performer term algorithm best performer term throughput temporal locality cache benefit throughput temporal locality gain prefer process map gain prefer process map last packet may arrive last packet may arrive reorder optimal temporal locality reorder optimal temporal locality negative impact latency fact observe effect negative impact latency fact balance capture algorithm also benefit te algorithm also benefit temporal locality flow bundle always locality flow bundle always process thread addition algorithm process thread addition algorithm reorder high maximum latency less issue reorder high maximum latency less issue however interleave different flow load however interleave different flow lo reduce maximum throughput algorithm reduce maximum throughput algorithm produce balance load term process produce balance load term thread yet throughput typically lower throughput typically lower exploit cache benefit exploit temporal locality network traffic temporal locality network traffic five conclusion future work future work paper discuss design implementation paper discuss design implementation schedule invent one two packet schedule invent maximize different attribute ideal design maximize different attribute ideal two packet algorithm compare two packet commonly use schedule network use schedule network result show packet result show packet schedule fact packet maximize schedule fact packet maximize cache affinity outperform two term ache affinity outperform two term throughput network capture balance network throughput network capture balance network capture throughput faster next best throughput faster next best importance cache affinity importance cache affinity e k c p r e p e e h c c e g r e v twenty fifteen ten five zero zero b b zero result show schedule cache affinity important throughput balance evenly future plan modify schedule algorithm thread cache affinity information maintain even exit system one way record thread packet number associate last packet process new packet arrive packet use information decide thread process last least load thread addition finish integrate packet driver plan compare performance twenty live network furthermore plan evaluate packet schedule core hardware thread count become available us suspect schedule cache affinity important important six author would like thank palmer read early paper provide invaluable feedback addition would like thank paper shepherd anonymous comment improve paper finally want thank pam twenty team support feedback seven reference one application layer packet classifier two j c partridge c n packet reorder pathological network behavior seven six three e make robust packet reorder rev one four z wang z e performance scheme load balance nineteenth annual joint conference computer proceed five g network processor load balance link performance evaluation computer telecommunication san ca network world seven h al exploit heterogeneous multicore processor network process journal research development network optimize compute vol one eight h al introduction processor architecture journal research development compute vol one nine al adaptive highly thread hierarchical multicore server proceed symposium network new jersey nineteen twenty nine ten al scalable multithreaded design multicore proceed symposium network san six seven eight new york eleven standard local metropolitan area network link aggregation association twelve processor thirteen r packet train measurement new model computer network traffic journal select six fourteen li c g k performance guarantee service proceed international conference distribute compute may nineteen computer society fifteen al dynamic load balance without packet reorder rev two march sixteen martin r al accuracy dynamics load balance rout network international conference vol fifteen seventeen intrusion prevention system achieve gold award certification eighteen v system detect network computer network fourteen nineteen network intrusion prevention system snort lightweight intrusion detection network proceed conference system administration seven twelve shi w h p load balance parallel forward network six j bridge twenty receive side scale shi w h p r v weaver architecture scalable load balancer forward traffic exploit proceed symposium architecture network five new york shi w l adaptive load proceed symposium architecture network san three five six new york snort thirty exploit multicore parallelize network intrusion prevention ten e use affinity information schedule parallel four two h al stateful hardware decompression network environment proceed symposium network san six seven eight new york thirty g k human behavior principle