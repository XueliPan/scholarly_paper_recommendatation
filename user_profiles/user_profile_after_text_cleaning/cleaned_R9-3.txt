learn decision tree unbalance data v university dame dame abstract learn unbalance present convolute problem traditional learn may perform poorly objective function use learn typically tend favor less important class paper compare performance several popular decision tree split criteria information gain measure identify new skew insensitive measure distance outline distance class imbalance propose application form decision tree perform comprehensive comparative analysis decision tree construction method addition consider performance tree within powerful sample wrapper framework capture interaction split metric sample evaluate wide range determine operate best class imbalance one introduction data set one class particularly rare important term unbalance unbalance continue pervasive problem large variety supervise learn range finance medicine web categorization biology typically sample fifteen use counter class imbalance decision tree particularly six among popular significantly help sample counter high imbalance class three four seven fact vast majority paper three workshop unbalance data include base classifier understand sample generally improve decision tree induction undetermined interaction sample decision tree form six cart eight two popular decision tree induction however correspond split criteria information gain measure consider skew sensitive nine specific sensitivity class imbalance use sample prior decision tree induction become de standard literature sample alter original class distribution drive bias towards minority positive ten suggest improve split criterion top decision tree induction know various author implement one without loss generality assume positive minority class decision tree split criterion show improve performance unbalance nine eleven twelve however also show weakly nine eleven posit class imbalance also characteristic feature space addition skew class thus become important design decision tree split criterion capture divergence without dominate class end consider distance thirteen fourteen decision tree split criterion show also demonstrate distance albeit distance offer skew insensitivity finally consider popular sample study impact decision tree split criteria skew insensitive split criterion mitigate need sample key include follow one characterization distance metric data mine context skew insensitive decision tree split metric two analytical demonstration utility propose formulation distance use graph three theoretical comparison distance four decision tree algorithm call incorporate distance tree split criterion five comparison effect sample decision tree split use total nineteen vary skew study also use statistical measure suggest fifteen robustly compare across multiple note use unpruned decision tree experiment irrespective split criterion use previous work point prune unbalance sixteen seventeen two distance distance measure distributional divergence thirteen fourteen let denote measurable space p q two continuous respect parameter let p q measure p q respect definition distance give p q p q p equivalent p q two one p z p integral note distance depend choice dominate parameter also define r q two countable space p q p q p p one two distance carry follow one p q zero two two symmetric imply p q q p moreover square distance lower bind divergence paper p q one two assume normalize feature value across class allow us capture notion affinity probability measure finite event space p q distance zero maximal affinity p q completely disjoint distance two zero affinity dictate decision tree split criterion separability class want select feature carry minimal affinity class thus distance use capture propensity feature separate class p application decision tree split criterion assume countable space continuous feature p partition bin assume problem class class essentially interest calculate distance normalize aggregate partition two class x x distance x x let x class x class x x v x p x two x three postulate formulation strongly skew insensitive prior influence distance calculation essentially capture divergence feature value give two different class factor class prior show effectiveness enumeration isometric plot compare eighteen propose use isometric line define bias evaluation metric plot contour give metric range possible value present case study information gain produce class skew note highly skew distribution may lead conclusion two metrics yield similar generalization effect fact difference could detect equal class distribution eighteen subsequently nine connect isometric plot roc analysis demonstrate effect true false several common evaluation metrics accuracy precision addition also present three major decision tree split criteria entropy use information gain six index eight ten also establish effect class skew shape nine adopt formulation paper isometric plot show contour line roc space representative performance different decision tree split criteria respect estimate true false positive rat condition skew ratio c decision seven zero six zero five zero four zero three zero two zero one zero three zero two zero one e r e v p e r eighty sixty forty twenty zero zero e r e v p e r eighty sixty forty twenty zero zero one two three four five six seven zero eight e r e v p e r eighty sixty forty twenty zero zero e r e v p e r eighty sixty forty twenty zero zero one two three four five six twenty forty sixty eighty false positive rate twenty forty sixty false positive rate eighty fig one information gain eleven b zero five zero four zero three zero three zero two zero one three zero two b b zero one one zero two zero three one two three twenty forty sixty eighty false positive rate twenty forty sixty false positive rate eighty fig two eleven b tree split binary class problem define confusion matrix follow parent node p os positive n negative assume binary split one child carry true false positive instance child carry true false negative instance different decision tree split criteria consider paper model impurity distribution negative thus isometric plot contour represent true false negative generate particular value give decision tree split criterion example one contour figure one indicate value information gain one f approximately zero twenty twenty sixty eighty twenty zero sixty twenty eighty along contour figure one b information gain observe contour form roc space skew one one one ten respectively skewness increase become flatter information gain operate poorly split criterion eighteen nine observe similar trend additionally nine note weakly affect like information gain therefore therefore cart highly skew dependent b eight seven zero six zero five zero four zero three zero two zero one zero two zero one e r e v p e r eighty sixty forty twenty zero zero one two three four five six seven eight e r e v p e r eighty sixty forty twenty zero zero twenty forty sixty eighty false positive rate twenty forty sixty false positive rate eighty fig three eleven b zero one zero two zero seven zero six zero five zero four zero three zero two zero one e r e v p e r eighty sixty forty twenty zero zero one two three four zero five zero six seven eight twenty forty sixty eighty false positive rate fig four distance isometric degree additionally contour twist contour intersect zero one zero skew far skew sensitive metric group consider two class proportion one one one ten highlight impact even marginal class skew point interest reader paper elaborate analysis class skew use three metrics nine hand important observation may draw isometric distance first use model relative impurity derive follow distance figure four contain distance contour distance deviate contour vary class skew c factor c relative impurity formulation result follow previous section independence distance parameter case respective class isometric contour distance unaffected increase class skew rate f two one one p f two compare distance posit distance similar albeit distance skew insensitivity consider distance within canonical two class binary split problem p l p r designate weight fall leave right branch respectively p p represent probability belong class term may state follow two p p l p l p l r p r p r four p p apply term equation three term distance maybe state follow p p l p l p r p r p p p two two two two p l p l two p r p r r p q five six p p represent form demonstrate clear similarity capture divergence split point albeit place notion branch strength term p l p r correspond leave right branch moreover also take account class also consider upper bind pure split two hand upper bound two take p account notion class skew calculation simply capture deviation p x split node without factor relative class parent node also highlight may less skew insensitive distance p p distance aim pure leave aim partition space capture class node however result smaller coverage may damage balance class rat could prove helpful highly unbalance try form purer leave minority class specific nevertheless depend relative distribution feature respect class hand may greedy stop split sake coverage demonstrate property use value surface figure five display full range possible split value metrics figure five show distance throughout possible class skew figure five b c display value class eleven respectively skew increase surface flatten potentially reduce set usable value get dominate skew factor favor majority class note vast number data set scenario may arise may converge similar nevertheless important consider difference may want grow completely unpruned tree unbalance lower nod tree class skew may get extreme obviously condition property data theoretically possible point b c r e g n l l e h five one zero one k five one zero one k five one zero one k five one zero one five p zero zero five p one five p zero zero five p one five p zero zero five p one five p zero zero five p one fig five full value surface total range distance distance remain unchanged possible class skew range value vary class skew note b eleven c distance may prove amenable thus suggest use give skew insensitivity albeit caveat general case converge similar want prepare worst case distance decision tree follow algorithm outline approach incorporate distance learn decision tree refer distance distance base decision tree rest paper algorithm indicate subset train set class specify subset value j feature k identify subset class value j feature k algorithm one input train set feature f one value v f two v three end q four return v two q case give feature continuous slight variant algorithm one use sort base feature value find meaningful split calculate binary distance split return highest distance identical methodology use practical distance calculator algorithm two outline procedure induce tree c return algorithm two input train set cutoff size c one two three end four feature f five f six end seven b h eight value v b nine c ten end consider prune smooth leaf estimate primarily motivate provost sixteen likewise consider unpruned decision tree cart smooth leaf estimate three sample treatment class imbalance include result improvement true without significantly increase false three nineteen however believe important understand interaction sample different decision tree split metrics different skew study examine combine two sample random smite five seemingly primitive randomly remove majority class show improve performance class imbalance train information lose counterbalance improvement minority class accuracy smite advance method generate synthetic random know positive discuss interaction cost class imbalance twenty propose simple method calculate optimal sample level however evaluation occur without explicit cost case calculation simply indicate sample class balance proportion addition approach leave open much interpretation negative class positive class combination use reach balance point address search sample space include several potential balance point via wrapper determine optimal class proportion nineteen test pair smite result intractable search space wrapper framework first explore amount result improvement performance define decision tree classifier learn original distribution data subsequently majority class point performance deteriorate wrapper search appropriate level smite strategy remove excess negative thereby reduce size train make learn time tractable smite add synthetic positive generalize performance classifier positive class primary metric consider performance unbalance use wrapper objective function final performance metric point reader paper al nineteen detail wrapper framework perform experiment use base decision tree classifier combination sample wrapper note smite contain randomization therefore first construct exhaustive set sample different amount different amount smite let wrapper search prior construct smite determine appropriate level sample different split criteria example split metric consider removal exactly majority class first comparison course split criterion may converge different amount ensure uniformity result potential performance stem bias decision tree metrics rather possible variance due randomness apply sample four experimental evaluation section provide experimental result determine performance compare cart combination sample wrapper use variety unbalance mix feature type wide variety comprehensively outline use skew insensitive metrics versus information gain use appropriate unbalance data set latter result elevate type one error particularly punish unbalance false false negative fifteen also encourage use statistical among across statistically evaluate compare use holm procedure test procedure determine statistical significance performance rank across multiple fifteen table one describe use experiment number briefly describe table one use paper feature letter estate forest cover boundary one two three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen nineteen phoneme segment oil page splice twelve ten two sixteen six ten sixteen five eight nineteen sixty four four five twelve thirty nineteen two four ten ten five ten fourteen use paper estate contain state series compound national cancer institute yeast anticancer drug screen highly unbalance record information calcification oil contain information oil spill relatively small noisy phoneme originate project use distinguish nasal oral sound boundary various biological splice available remain originate repository originally multiple class convert keep class minority rest majority exception letter vowel become member minority class majority class aside state use experimental result first compare decision tree table two report average roc experimental framework relative rank classifier indicate parenthetically use holm procedure test fifteen compare rank across nineteen four determine statistically significantly better cart decision tree confidence interval thus apply decision tree unbalance data select typically yield edge cart general converge towards similar tree therefore final performance reflect tie fifteen nineteen marginal improvement average rank thus empirical table two decision tree roc result relative rank parentheses average rank apply tie achieve best overall rank use test compare rank confidence interval per recommendation fifteen appropriate compare rank use multiple multiple x bottom row indicate statistically significantly improve classifier boundary estate forest cover letter oil page phoneme segment splice cart three one four two twelve four twenty four twelve two fourteen two four two eleven three ten fifteen seventeen four ten fifteen nine three ten one three fourteen one two three two fifteen four four two fifteen eleven three thirteen fifteen four thirteen fifteen sixteen one four four two four two eight three thirteen fifteen seventeen four thirteen fifteen four one four three five fifteen ten four five fifteen five three two fifteen seven four two fifteen ten three five one four nine fifteen seventeen three nine fifteen thirteen three nineteen fifteen nineteen four nineteen fifteen nine three eight fifteen eleven four seven fifteen six three seven fifteen seven four seven fifteen sixteen one fourteen four fourteen five two fifteen two two fifteen six two seven four six four rank five x x agree isometric analyse discussion previous section split criterion become relatively decision tree tend perform strongly unbalance data interaction sample consider effect sample decision tree split criterion use wrapper approach describe section three determine potentially optimal level sample decision tree wrapper optimize note wrapper use separate validation framework determine sample level decision tree algorithm use train set determine optimal sample level optimize determine entire train set amount evaluate correspond test set approach outline paper al nineteen report test set result show table three result show compel trend benefit clearly erode cart still remain worst perform classifier statistically significantly however couple oil cart sample produce best classifier note small oil fourth table three roc value produce decision tree combination sample wrapper relative rank note parenthetically x bottom row indicate improvement cart statistically difference among three decision tree boundary estate forest cover letter oil page phoneme segment splice rank five cart four seven one two one three eight three eight three eight three seven one eleven seven ten two sixteen one thirteen twelve four thirteen two three one fifteen one four one fifteen twelve three nine fifteen ten four nine fifteen fifteen one fifteen three fourteen two two two three two three two eleven one thirteen eight four thirteen four three two four one four six four four three three two fifteen three four two fifteen five three six fifteen six four six fifteen thirteen one fifteen two nineteen fifteen thirteen three fourteen one thirteen two four three five fifteen six four five fifteen six three seven fifteen six four seven fifteen thirteen one ten two nine three two three one fifteen one four one fifteen sixteen four four four fourteen four fifteen four one x x x ratio suggestive curse dimensionality moreover oil also noisy one question stage much different decision tree benefit sample compare respective figure six depict percentage improvement across apply sample different decision tree split criteria figure show compel trend cart biggest sample skew insensitive achieve gain sample fact note often experience reduction performance sample apply fourteen nineteen show reduction fifteen nineteen show reduction also point wrapper sample amount validation set diminish generalization capability thus use natural distribution let one skew insensitive split criteria work way data potentially beneficial use expensive step sample finally point amount sample determine decision tree vary elucidate first generate various level sample let wrapper search space decision tree metric ensure due randomness sample intrinsic base decision tree split metric table five appendix show different sample level r e p p r w r f n e e v r p n e c r e p fifteen ten five zero five cart one two three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen nineteen fig six percent improvement roc sample decision tree type relative rank note cart generally exhibit highest improvement yield wrapper position correspond number table one continue share amount sample level well cart generally require higher sample level amount sample test holm procedure show statistically difference rank level sample decision tree finally consider comparison eight potential decision tree counterpart eight across nineteen note exactly train test set use decision tree albeit train set modify sample use wrapper comparative rank across present table four interest table still achieve best rank wrapper positive effect cart point negative effect thus merit use skew insensitive metrics sample statistical significance test establish better cart wrapper cart five primary focus paper learn decision tree unbalance first propose distance decision tree split criterion thoroughly compare four different decision tree split criteria different skew data distribution also consider evaluation effect sample impact different metrics differently draw follow distance share similar isometric section show distance isometric plot table four comparative roc rank across entire set test x bottom row indicate use test statistically significantly better respective classifier wrapper cart cart boundary cam estate ism letter oil page phoneme segment splice eight seven five six seven five one seven two eight six six six eight six five one seven two three three fifteen fifteen two fifteen two fifteen fifteen fifteen fifteen seven eight eight eight eight seven eight eight eight one eight eight eight seven eight eight eight eight seven rank five x x five one fifteen fifteen fifteen two fifteen one fifteen fifteen fifteen fifteen one three two five fifteen six four three four seven one five five three five six four five four four three fifteen five four four four three five six one seven six eight six seven two seven seven seven seven seven seven seven seven x three three four fifteen four three four six vary class skew ratio however section go demonstrate although divergent carry experimental result note frequent convergence identical performance produce superior decision tree class imbalance without use sample statistically significantly outperform cart sample generally benefit cart hurt believe compel observation study avoid use sample use appropriate decision tree split criteria remain superior even consider sample general recommend use decision tree methodology give skew insensitive best rank statistical significance part future work expand study include balance also want explore effect prune prune may appropriate focus study largely decision tree believe also consider distance separate conquer instance space reference one n class imbalance problem significance conference artificial intelligence two address curse train set selection international conference machine learn three g r study behavior several balance machine learn train data six one four van j experimental learn data five bowyer hall lo smite synthetic minority technique journal artificial intelligence research sixteen six induction decision tree machine learn one seven n ko proceed work shop learn data set eight l j stone r classification regression tree chapman hall nine pa geometry roc space understand machine learn meet roc ten apply weak learn framework understand improve international conference machine learn morgan eleven c r exploit cost sensitivity decision tree split ting criteria twelve b c obtain calibrate probability estimate decision tree naive international machine learn morgan san ca thirteen divergence distance measure signal se lection fifteen one fourteen c review canonical alternative analysis use distance nineteen fifteen j statistical multiple data set journal machine learn research seven sixteen provost f p tree induction rank machine learn three seventeen data set investigate effect sample method probabilistic estimate decision tree structure workshop learn data set eighteen r quantification evaluation metrics classification nineteen da hall lo joshi automatically counter imbalance empirical relationship cost data mine special issue international journal data mine knowledge discovery twenty c learn international joint conference artificial intelligence approximate statistical test compare supervise learn neural computation ten seven machine learn detection oil spill satellite radar image machine learn thirty p dunker ak z classification knowledge discovery protein journal chang c lin c library support vector machine available machine learn repository appendix wrapper select sample level table five optimize e level respective split metric along relative rank sample level among note level reflect percentage negative class remove smite level represent percent synthetic add train data relative original positive class size boundary estate forest cover letter oil page phoneme segment splice cart twelve eleven eleven twelve fourteen eleven eleven eleven eleven eleven eleven thirteen eleven fourteen rank smite rank