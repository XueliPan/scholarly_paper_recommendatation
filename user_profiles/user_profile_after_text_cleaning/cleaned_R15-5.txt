application specific low latency instruction cache flash memory base embed lee department computer science engineer department computer science engineer university san university san embed demand high capacity code storage increase amount code grow flash memory one popular storage purpose however extremely long access latency make flash memory less attractive domain code storage paper introduce highly effective cache system utilize application specific information technique effectively hide long latency flash memory base code storage lessen negative impact extremely long access latency propose page algorithm utilize information provide temporal distance sufficient load page physical flash memory experimental result show propose architecture exhibit reduction average access time juxtaposition cache demonstrate enhancement nearly two order magnitude reduction time basic direct map li cache architecture introduction disproportionate hardware cost embed especially consumer electronic incur memory subsystem variety different memory concurrently adopt embed memory embed far diverse constrain generic computer single universal standard memory device like dram fail satisfy various set embed addition cost factor exist number embed need consider adopt memory device major common memory consider embed summarize table vantage point cost unify memory embed beneficial many ways however ideal memory device satisfy whole slew design typically encounter flash memory long consider adopt practice preferable data storage device since suitable embed generally require special low cost low power high capacity high reliability high robustness however adoption flash memory code storage severely constrain due several intrinsic flash memory inaccessibility long access latency significant amount research undertake overcome utilize flash memory unify memory infrastructure many continue envisage flash memory plausible candidate purpose paper first step towards unify memory system embed concentrate new instruction architecture embed generally compose fix set exhibit great degree application specific nine ten fact encourage many exploit application information various ways two popular approach employ application specific information one consist transform relocate exhibit enhance behavior information gather give set help extract application specific information give set tune hardware base obtain information utilize second approach work offer increase flexibility efficiency design introduce novel instruction cache architecture demonstrate significant enhancement average memory access time flash memory base primarily focus exploration possible ways mitigate impact long access latency flash memory exploit program exhibit strong spatial temporal correlation group instruction segment high predictability among group extensive simulation use simulator suit illustrate significant advantage architecture generic cache architecture equivalent hardware cost rest paper organize follow section briefly present flash memory base code storage section deliver background basic technical comparison among memory table device dram flash flash cost highest high medium low capacity low medium medium high read highest high low low write highest high low yes yes energy consumption high high low low motivation section introduce cache architecture access latency reduction highly accurate page prediction section v present experimental result base simulation provide comparison cache architecture section provide overall summary propose architecture relate work due increase complexity embed memory get diverse complicate significant amount research perform field enhance overcome underlie hardware specific best utilize application specific information instruction memory number novel cache thirteen introduce save power enhance eleven performance research two also widely study low power low cost embed typically focus intelligent incorporation application specific target program research area flash memory discuss section considerable amount research initiate overcome flash memory base park al eight employ flash memory primary memory author add simple direct map unify cache processor flash memory minimize access latency time critical base criticality however system require relatively large cache suffer overall performance degradation six hybrid flash memory introduce mainly target embed flash memory basically relieve problem bitwise inaccessibility describe problem cause long latency remain seven four three author develop still unresolved flash memory base code storage incorporate virtual memory subsystem processor introduce exploit parallelize flash memory access specific buffer application specific information however consume significant dram space still suffer high access latency flash memory background motivation instruction cache important component code execution unit processor core thus time fetch minimize directly affect execution performance processor access code memory two different time factor exist one time change state specify memory chunk retrievable state transfer time bring prepare memory chunk cache line access time performance major memory summarize table compare modern flash memory outperform term access time memory due flash specific cell architecture however discuss section accelerate popularity storage memory access time among memory show relatively fast initial access transfer time access unit whose size normally small however flash memory demonstrate long initial access time access unit page whose access granularity significantly greater consequently initial access time page critical factor access data flash memory moreover advance processor worsen problem latency flash memory modern embed also adhere law clock achieve embed page access time flash memory nonetheless near constant value tend increase even new flash memory emerge architectural point view embed incorporate small size instruction cache low latency memory like dram average cache miss time represent product cache miss rate penalty sufficiently low compare cache hit time due relatively small cache miss penalty consequently need use complex instruction cache may increase average memory access time due increment cache hit time cause complexity add hardware however long latency memory device like flash memory relatively low cache hit rate also result unacceptably high average memory access time due extremely high penalty cache miss consequently research explore lessen hide latency instruction fetch flash memory base embed initiate denote code chunk whose frequency access significantly elevate thirteen analysis reveal program usually compose seventy specific time memory table device access time read write five dram fifty flash seventeen basic block delimit branch program spend execution time highly limit set basic block particular embed program fix set functionality subsequently lend static information extraction moreover information intrinsically static relative frequency access tend time space invariant fix set obtain good application specific thoroughly study statically two different basic type loop exhibit strong spatial temporal locality also show elevate correlation among neighbor code segment demonstrate high spatial locality context dependent access behavior thus proper employment information processor instruction cache greatly enhance code access behavior however information static unable capture dynamic behavior program dynamic memory access behavior exploit cache capture possibly introduce additional generic li cache suffice purpose analysis program behavior hybrid cache architecture lend follow interpretation cache guarantee access cache set conjunction active code segment li cache form set transition set highly predictable set constitute strong set use application specific information hide least lessen access latency flash memory base firstly cache couple li cache greatly enhance cache hit rate cache absorb conflict capacity miss li cache place highly active code segment static cache secondly cache also reduce cache miss penalty exploit direct set represent tightly couple snapshot cod set reside cache extend period time transition set highly predictable embed consequently high chance next possible code segment high accuracy timeliness propose architecture application specific cache give application set behave code segment whose access behavior considerably repetitive code execution space show fig one code space show fig one necessarily physically contiguous form strongly correlate logical set information maintain unique cache enhance cache efficiency capture improve temporal spatial locality discuss section dynamic nature code execution capture regular li cache hybrid cache design li cache attempt dynamically trace track locality absence prior knowledge program cache statically code base know access frequency conceptually define section lessen burden conflict capacity miss li cache useful capacity miss involve information utilize instruction cache system obtain multiphase program profile process target application profile process extract useful information base follow criteria code density higher specific threshold x code higher priority static quality information define hit rate total size ratio quality information may vary base total cache size proportion access frequency logical code execution space fig one behavior specific best practice reduce access latency cache design next possible code segment significant amount research instruction five twelve however flash base fetch page take huge amount time compare block dram base consider amount latency flash memory expose result deliver slight benefit unless commence sufficiently early appropriate flash memory base system highly challenge task necessitate consideration timely technique capable deliver high accuracy observe section represent temporally spatially couple logical group execution code refer set phase profile partition logical program space number set represent examine program control flow set set logical entity whose whole set correspond code segment fully maintain cache system transition subsequent set may increase number cache miss thus record information cache miss direct set information utilize select code page direct set however effectiveness dependent follow criteria accuracy threshold possibility next page access raise cache miss timeliness threshold time spend direct set without cache miss obtain information process satisfy threshold condition information simply express next page address subsequently strong predictability choose achieve require level accuracy timeliness inaccurate untimely negatively impact average memory access time however impact inaccuracy pronounce impact timeliness conservative value apply set threshold control accuracy obtain information combine information also store container incorporate information associate information utilize soon possible timely load next possible code page information ready buffer additionally introduce exist cache fig three whenever new contain associate information access notify initiate correspond operation fig two cache architecture area consume total cache size give profile program explore design space control static quality manipulate threshold code density parameter space exploration require get optimal information extensive experiment show precise relationship static quality effectiveness cache obtain optimal information transfer special cache embed processor cache design hold static information specific whenever system processor automatically retrieve information memory location call container base introduce hybrid cache architecture show fig two two disjoint cache place access parallel process flash memory selector retrieve instruction one cache cache prefer one consequently cache system may experience reduce hit time overhead expense increase number cache reference however hybrid cache enjoy reduction main memory access due elevate high cache hit rate pronounce importance term performance power consideration overall system cache capture access involve loop basic unit loop activity subdivide basic block delimit conditional branch previous research show basic block size relatively small range five eight popular embed one design cache line size keep small hold basic block without excessive waste space li cache organize direct map relatively large cache line size fully utilize spatial locality program minimal access latency b page cache greatly improve cache hit rate relatively small hardware overhead however long access latency flash memory remain still unresolved specific efficiency average memory access time f zero f n ninety z fifteen ten five fig three cache buffer seventy eighty ninety three different state correspond one ready process request immediately initiate two ready process request reject page already load buffer three midst process previous request result request queue pending job finish condition one two negatively impact efficiency however last condition cause inaccurate untimely prediction mechanism deteriorate access latency system design cache system typically three basic design consider lower hit time lower miss penalty higher hit rate propose architecture focus provide higher hit rate utilize application specific information miss penalty flash base extremely high order cycle advance embed furthermore trend show tend increase future consequently try maintain lower hit time introduce small static cache simple li cache hit time propose architecture equivalent slightly longer standard direct map li cache much shorter complex cache architecture include set associative cache v experimental result experimental framework extensively use perform experiment various condition base architecture direct map li cache cache line table summary application mad crafty gap type image game interpreter simulator size suite fig four quality cache effectiveness size obtain optimal information give hardware configuration use parameter space exploration technique differentiate code segment normal code segment describe subsequent subsection line size cache set simulate two different cache cache cache page prediction case total size cache cache li cache buffer applicable always set size base architecture perform simulation million per program simulate set representative program approximate embed table outline use experiment b parameter space exploration effectiveness propose architecture significantly dependent quality information obtain describe section fig four show relationship static quality average memory access time cache static quality characterize size hit rate empirical data show half total cache size whose hit rat exceed ninety consider appropriate cache effectiveness half cache size dramatically deteriorate due inability fully capture dynamic access cause cache miss li cache li cache shrink total size cache remain constant relatively simple buffer accuracy threshold fifty timeliness threshold observe easily see result effectiveness improve target application able partition number work set whose transition easily identify specific table result miss rate mad crafty gap forty nineteen seven two seven two size twelve table v result average memory access time application mad crafty gap dram c average memory access latency reference general overall cache hit rate propose architecture outperform base architecture due reduction overall cache miss hybrid cache system accurate timely page mechanism rule thumb cache hit rate propose architecture equivalent set associative cache result see table v assume cycle access latency dram cycle access latency flash memory two page buffer see average memory access time propose architecture highly reduce comparison base architecture furthermore propose architecture show result comparable base system dram propose architecture work perfectly program behavior concentrate small set set whose transition highly predictable conclusion paper propose new instruction cache system minimize memory access latency flash memory base code storage design static cache embed processor extract application specific access behavior give program cache frequently access code segment avoid cache conflict code segment isolate segment furthermore prediction technique develop hide long memory access latency utilize set information characterize thus successfully predict load next code segment accurate timely manner extensive use simulator suit show propose architecture exhibit reduction average access time combine solution cache demonstrate enhancement nearly two order magnitude reduction time base cache architecture retain average memory access time equivalent dram base three four one k modify cache architecture low energy fast cache embed proceed embed computer architecture model simulation two zero r optimal memory allocation scheme embed embed compute one one j shin h kim demand page scheme flash memory seven proceed conference tool page c park w e n chang demand page six proceed international conference system synthesis page five use proceed annual international symposium computer architecture page six b kim cho high performance low power memory solution code data storage proceed nonvolatile semiconductor workshop seven c park j lim k j lee l min demand page embed flash memory four proceed international conference embed page eight c park j bae h kim kim b kim memory architecture mobile embed three proceed international conference system synthesis page p energy frugal tag embed cod two proceed tenth international symposium page p framework efficient branch resolution embed embed compute four two nine dynamic overlay eleven l p memory energy minimization four proceed international conference system synthesis page twelve c j instruction cod layout optimize reduce cache miss proceed annual international symposium computer architecture page thirteen yang lee cache joint temporal spatial locality exploitation energy reduction four proceed international symposium low power electronics design page ten symposium application specific