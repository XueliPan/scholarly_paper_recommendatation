discriminative category match efficient text classification huge document management university hong management university hong hong china hong china computer science hong science technology hong china abstract rapid growth textual information available good model manage document automatically important document archive new term new frequently appear without doubt update classification model frequently rather use old model long period absolutely essential challenge obtain high accuracy classification model b consume low computational time model train operation c occupy low storage space however none exist classification approach could achieve paper propose novel text classification approach call discriminative category match could achieve state extensive experiment use two large collection conduct encourage result indicate approach feasible one introduction tremendous growth volume information represent textual format news article government report company product new term new likely appear ever concept collection may easily drift one aspect another also boardly speak general model huge document collection assumption possible concept drift follow x x one high classification accuracy two low computational cost storage construct initial model b update model model build c operation three parameter tune highly adaptable x mains exist approach show effectiveness text classification nine ten fourteen none able achieve state particular three difficult achieve one somehow conflict two x x x exist properly best term accuracy however train cost quadratic number train instance fourteen term computational cost naive bay best however accuracy high besides approach require performance tune feature selection threshold decision parameter set seven fifteen lead difficulty handle huge document collection frequently model update paper propose novel text classification approach call discriminative category match could satisfy state use new weight scheme argue none exist weight scheme suitable text classification fact design information retrieval classification motivation feature proceed international conference data mine two appear frequently many document limit discrimination power therefore four seventeen however case text classification addition difficulty update feature domain model build four eight new weight scheme aim discriminate category assign feature higher weight appear extensive experiment conduct use two large collection article namely news collection experimental result show achieve high accuracy low computational cost storage requirement make feasible classification rest paper organize follow section two discuss relate work section three describe detail experimental result report section four finally summary conclusion give section five two relate work section briefly discuss merit two popular approach naive bay support vector machine naive bay algorithm basic idea compute posterior probability incoming document give particular category eleven twelve mathematically calculate bay rule p c p c j j p c j j j p one j tor assumption p p c j j incoming document category compute maximum likelihood compute word independent c j c j two multinomial mixture former one take term occurrence account later one take term frequency consideration general multinomial mixture model perform better thirteen efficient implementation rainbow package merit lie robustness consume little computational major criticism word independence assumption several research try address problem generate symbol mean k f f x l n k n k k c w w k ie total number feature two k x x k document category feature feature length document total number document category total number collection number time feature number document contain feature risk estimator feature set contain feature weight feature weight feature document category category k f f f f f f k appear document category k k table one mean rule three fourteen fifteen although improvement classification accuracy obtain computational cost far expensive support vector machine develop base statistical learn theory solve pattern recognition problem eighteen conceptually try generate decision hyperplane maximize margin positive negative train set mathematically require minimize follow quadratic program one one w q b two two j x x x q multiplier ing example zero otherwise symmetric positive definite kernel matrix b equal one positive train algorithm use solve linearly separable case extend solve linearly case either introduce soft margin map original higher dimensional space efficient implementation include sequential minimal optimization sixteen recent research show achieve high accuracy text classification nine major draw back expensive train cost quadratic number train instance lead problem learn update three discriminative category match section introduce novel text classification approach discriminative category match table one show list correspond available available proceed international conference data mine two mean use follow like exist work use vector space model represent document category f w f w f w n n one one two two h k f w f w f w k k k k k k one one two two h classification achieve find maximum similarity measure coefficient k three four n e c f r e b n zero one k two two two w w k two two two k w w w w k p p p p five figure one distribution earn cat k k ten number feature log scale coefficient express degree overlap document category proportion overlap whole provide intuitive practical fitness model match document category cosine coefficient may suitable take size document category account note equation five feature document category compare feature appear document consider follow present equation five detail weight feature within document weight relative importance f log one two w log one two l six time imply feature assign higher weight appear frequently within document point two appear important thus logarithmic relationship rather linear relationship take actually follow result study two four seventeen especially one harman five n n weight feature within category weight f k w ai k k p two two two w c c c k seven two two w c c c k q two use normalization p main one relative importance compute follow three average importance two relative importance among cate w c c c ai k k k k k w one zero f f f w c k one k one n k c c log f g n w c one k c k two n k one k w c log n p two k w k ai k k p k two w c k eight nine ten eleven explain order demonstrate effectiveness use illustration explanation extensive experiment also conduct present section four relative importance feature cate gory w c k document belong category inherit common theme nature otherwise would reason group together word feature appear frequently within category critical term classification result equation eight formulate equation eight numerator denominator logarithmic base observation frequency feature appear many document rare figure one illustrate use category earn figure one probability feature appear many document low similar find also port six figure two show linear shape obtain show number feature appear many document correspond importance form logarithmic relationship versus w c k k proceed international conference data mine two ten number document log scale figure two relationship category earn k k w c k zero zero ten thirty twenty forty number fifty sixty c j j feature one eight six four two n e c f f e c r g e c n h w zero one feature barrel feature regulation category crude earn fuel gas heat retail five one seven four two two two two one one category doc crude earn gas four gold ten grain interest six nineteen fourteen sugar trade twelve one one three one one three two six four seven doc ten seventy table two distribution two feature barrel regulation relative importance feature across c c k w c design capture information within category gain global information feature distribution whole collection however global view across important appear instance give two feature half appear provide far precious obviously information text classification much higher discriminative power f f f f f f f j j j j specific consider feature appear document obviously value classification word feature valuable occurrence skew however compute importance feature across argue realistic weight importance simply count number feature appear suggest twenty example distribution two feature barrel feature feature one eight six four two one eight six four two n e c f f e c r g e c r c n e c f f e c r g e c r c zero one ten number document log scale b k figure three c c regulation show table two appear eleven weight importance simply count number appear assign weight however note occurrence barrel skew category crude whereas regulation less evenly distribute word barrel much higher discriminative power regulation thus barrel receive much higher weight unfortunately report study weight discriminative power feature classification purpose paper first propose formula weight feature base discriminative power equation nine summation use gather total importance feature across maximum use average summation value feature regard important many feature obviously important classification word higher value numerator smaller discriminative power term use normalization figure three show relationship log c c n one zero one c c figure three b show relationship c c c j j proceed international conference data mine two feature feature r e k r two eighteen sixteen fourteen twelve one one ten number document log scale figure four risk estimator k versus document category earn use illustration k c c c c first tend decrease figure three show number important increase even expect equation nine second appear number necessarily discriminative power reference two feature c c c c k c c f f j j j j j j j c c c c barrel lat c c compute finally although turn depend trend could see figure three b important system may bias large size fact independent depend w c c c c c k k k k average importance feature cate gory ai k k c c w c compute category level however importance feature within individual document yet address thus feature introduce equation ten term within bracket average weight among document use determine suitability average motivation estimator base observation average value document contain better estimator true importance risk estimator k f f f instance give two feature appear appear document thus higher risk document confident declare average likely reflect true status higher document contain correspond feature f f f f f j j j figure four show value versus number doc decrease linearly equation deteriorate result contain remove detail give section four f c c c w h g e w e b n c c c c w h g e w l n f zero one one eight six four two three two one zero one ten number document log scale without ai k feature ten number document log scale b ai k figure five effectiveness ai k standard dev ai ai k k without table three standard deviation without ai k w k k ai figure five show effectiveness overall shape figure similar point feature figure five b much important standard deviation weight among different reduce feature lower word discrepancy among weight increase recall precision model table three show effectiveness without use ai k ai k four experimental study experiment conduct sun physical memory run feature stem convert lower case punctuation mark remove number web page address ignore proceed international conference data mine two six news collection train test cat feature table four summary corpora use compare standard measurement recall precision two use order take harmonic average f two precision recall f one precision recall twelve restrict evaluation single category classification assign document one category full advantage continuous learn whenever new document arrive implement two first version denote update build second version denote continue learn whenever new document corpora use news collection summarize table four detail follow take select document assign one category category least one document train one document test note collection highly skew category contain train document category contain one contain less document select subset mesh selection criteria one least four relevant document train data set two least one relevant document year test set select document assign one disease category news collection set news article archive directly march task assign news article one morgan capital international category note already article correct news article order broadcast time first eighty use train remain twenty use test method one one table five summary evaluation result method one one table six summary evaluation result evaluation use table five table six summarize accuracy respectively four standard macro one micro one f f show table outperform significantly replicate previous find nine fourteen compare outperform perform better macro level whereas better micro level explain performance consistence different size look global picture performance favor performance large size however direct comparison inappropriate focus different since neither dominate measure conclude approach better depend measurement one may concern order evaluate time train operation phrase measure scale corpus factor ten figure six show model train test time second figure six train time increase significantly number train definitely lead problem model update huge document advantage low train operational cost however accuracy worst among although proceed international conference data mine two c e e p c g n n r zero zero c e e p c g n e seventy sixty fifty forty thirty twenty fifteen number train document train time zero number train document b test time figure six train test time operational cost highest still perform well evaluation use large collection study section show effectiveness efficiency use however list table four rather small size number feature conduct another experiment use news collection contain feature number train test document respectively table seven show summary classification accuracy f perform well one value less five among perform best term one measure whereas measure form best term fact corpora use always perform excellent give suggestion may beneficial use f f method one one table seven summary evaluation result news collection use use use w c c c ai k k one two three four five six seven eight p p p p p p p p p p p p p p table eight analysis w k combine effectiveness three use w k k k ai c c w c section examine importance equation five eight experiment conduct use show table eight tick denote correspond component include whereas cross denote ignorance experiment seven set three ignore experiment eight include risk estimator use include risk estimator result experiment show table nine obviously none outperform experiment eight use experiment four five use obtain extremely poor result either however accuracy use experiment one improve dramatically experiment seven ignore risk estimator obtain inferior result explain importance w c w c c c c c k k five conclusion paper propose novel text classification approach call discriminative category match need generate sophisticate model require simple statistical data furthermore update immediately whenever new document arrive computational time depend number number feature collection classification highly feasible respond time short data storage need proceed international conference data mine two one method one two three four five six seven eight one table nine comparison use different combination store whole document require simple statistical data two main advantage one train model efficiently regardless size collection two able deal update newly receive feature higher weight do fact text classification approach focus model generation performance tune none try discover feature distribution within collection detail analysis feature present certainly provide valuable develop weight scheme text classification work describe paper partially support grant research grant council hong special administrative region china reference one g incremental support vector machine learn proceed advance neural information process page two w b r information retrieval data structure prentice hall three n network machine learn four w r theory term weight base proceed ploratory data analysis international conference research development information retrieval page five harman experimental study factor important document rank proceed international conference research development information retrieval page six j holt efficient mine association rule text proceed international conference information knowledge management page seven j lewis text categorization low quality image symposium document analysis information retrieval page eight text categorization support vector machine learn many relevant feature technical report university nine text categorization support vector machine learn many relevant feature proceed conference machine learn page ten w lam c ho use generalize instance set proceed automatic text categorization international conference research development information retrieval page eleven lewis evaluation phrasal cluster proceed text categorization task international conference research development information retrieval page twelve lewis naive bay forty independence assumption information retrieval proceed conference machine learn page thirteen k comparison event model naive bay text classification workshop learn text categorization fourteen h text classification international conference information knowledge management page fifteen h b study proceed large bay classifier conference machine learn page sixteen j sequential minimal optimization fast algorithm train support vector machine technical report research seventeen g c approach automatic text retrieval information process management five eighteen v nature statistical learn theory springer nineteen h e frank data mine practical machine learn tool morgan twenty k automatic text classification method simple approach natural language process pacific rim symposium yang evaluation statistical approach text cate information retrieval twelve one yang study text categorization proceed international conference research development information retrieval page yang x text categorization proceed conference research development information retrieval page proceed international conference data mine two