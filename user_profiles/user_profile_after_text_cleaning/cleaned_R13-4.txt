turn noise mellon university mellon university mellon university mellon university abstract recent experience substantial increase number post publish daily force cope information overload task guide flood information thus become critical address issue present approach pick set post best cover important define simple elegant notion coverage formalize optimization problem efficiently compute solution addition since people vary interest ideal coverage algorithm incorporate user order tailor select post individual taste define problem learn personalize coverage function provide appropriate model formalize learn framework task provide algorithm quickly learn user limit feedback evaluate coverage personalization extensively real data result user study show simple coverage algorithm well popular aggregation sit include search yahoo buzz furthermore demonstrate algorithm successfully adapt user believe technique especially personalization dramatically reduce information overload subject artificial intelligence learn probability statistics general term experimentation one introduction many world need ask time magazine claim already many indeed experience substantial increase number permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee nine june one copyright post publish daily one immediate consequence many suffer information overload vast majority worth read average user even good many keep moreover often overlap content among multiple complicate matter many seem resonate extent largely uncorrelated true importance example spring politico break story haircut post almost instantly seize upon rest next two haircut story spark several major debate avoid story difficult web nearly impossible interest politics particular line debate goal paper turn noise assume limit time read post thus goal show small set post cover important currently discuss furthermore allow personalize process one man noise may another man music paper formally define mean set post cover one desire property notion coverage must efficiently computable function instance due large size data set use cluster require quadratic computation addition coverage function must expressive enough recognize important time identify important feature particular document finally notion soft allow partial probabilistic coverage post rarely offer complete coverage propose simple elegant notion address formalize correspond objective function exhibit natural diminish return property know present efficient algorithm optimize function extend notion coverage personalize coverage post cover average population may optimal particular user give personal example user may like badminton irrespective prevalence learn personalize coverage function allow us show post better suit taste formalize address problem learn personalize coverage function first define interaction model user feedback take account order post read use model define learn set coverage function provide simple algorithm guarantee quickly adapt user evaluate algorithm turn noise real data collect two week period compare popular aggregation sit search four yahoo buzz seven three one measure topicality redundancy result user study show simple coverage algorithm perform well better sit include base user vote human edit perhaps importantly demonstrate ability successfully adapt user personalization improve user satisfaction also able simulate different interest believe algorithm especially personalization dramatically improve information overload situation summary main define notion cover formalize optimization problem provide solution define formalize problem learn personalize coverage function provide algorithm learn user limit feedback evaluate algorithm real data use user study compare popular aggregation sit two coverage figure one show typical day seventeen size word proportional frequency across examine picture spot popular day inauguration conflict many post cover story inauguration moreover may certain degree overlap intuitively goal select small set post capture important day time wish avoid redundancy follow section formally state problem coverage present efficient optimization algorithm document feature characterize post feature feature arbitrary collection object high example word name noun phrase extract corpus even semantic example refer figure one feature common name document one feature formally definition triplet post cover finite set feature post finite set post relation post feature capture cover function r quantify amount post cover feature case cover binary indicator function turn post feature later explore coverage function probabilistic cover feature give model post cover wish determine effectively give small set post cover important formally goal pick set k post post order maximize coverage objective section define desire objective function propose solution address perhaps natural idea first cluster post post cluster cover feature give cluster pick representative post k cluster cluster approach common literature however cluster require us compute distance every pair post amount n post due sizable amount post publish daily require computation practically infeasible first desirable property coverage function ie able evaluate coverage time linear number post another solution require quadratic complexity would formulate coverage maximize function f one x function measure degree post cover feature post correspond collection feature cover binary indicator function one reduce budget maximum coverage problem definition budget maximum coverage give set grind collection budget k zero select size k maximize number cover set coverage formalize maximize f one one x although problem several efficient effective approximation task however naive approach suffer serious feature significance corpus feature corpus treat equally thus emphasize importance certain feature example cover cathedral high school valuable cover objective function characterize relevant post particular feature post speech cover much post barely mention side effect objective reward post post include many feature without incremental coverage coverage notion strong since see one post cover certain feature never gain anything another post cover feature correspond intuitive notion coverage subject law diminish return additional time see feature get additional reward decrease number example suppose show user post inauguration second post consider show effect presidency china figure one b show raw coverage second post feature however take account fact already cover feature extent first post coverage second post change figure one c show incremental coverage second post illustrate significance post towards diminish reward would come cover china address three issue address feature significance corpus simply assign weight wi feature f wi one one x feature word weight correspond frequency data set b c figure one global word frequency across seventeen size word proportional frequency b coverage c incremental coverage post china give already saw post incremental coverage much smaller regular coverage turn attention feature significance post post exhibit different coverage feature contain achieve soften notion coverage one approach use generative model estimate probability feature give post p example feature discover topic model term simply probability document j topic generally generative model particular set feature use define probability give probabilistic model define notion soft coverage formally feature sufficiently topic model post think single feature case p alternatively feature name could assume post feature feature pick example random replacement p coverage become one one p require post cover number feature alleviate problem since post cover large number feature well p p wi probabilistic approach allow us define feature importance individual post well whole corpus however define coverage f problem would persist function possess diminish return property instead extend probabilistic interpretation view sample procedure post try cover feature probability feature cover least one post succeed thus grow add post provide less less additional coverage formally define probabilistic coverage feature set post one one two finally propose follow objective function problem probabilistic coverage task find k post maximize objective function f x ak f three four optimize coverage use notion coverage two goal find set post maximize objective function three unfortunately show reduction objective suggest exact maximization function intractable however objective function satisfy intuitive diminish return property allow us find good efficiently definition set function f sub modular b v v b f f f b f b claim probabilistic coverage function sphere three fifteen intuitively characterize notion read post read small set post provide coverage read already read set b although maximize function twenty discover property problem take advantage several efficient approximation theoretical guarantee example classic result al show simply apply greedy algorithm maximize objective function three obtain one one e approximation optimal value thus simple greedy optimization provide us solution however since set post large naive greedy approach costly therefore use provide approximation guarantee use lazy often lead dramatic three personalization thus far define global notion coverage however user different interest select post cover prevalent may contain many interest instead goal section utilize user feedback order learn personalize notion coverage user recall previous section f assign fix weight wi every feature represent importance practice feature importance vary among different one user might care feature may indifferent address issue augment fix weight wi personalize feature follow assume user coverage function form f wi five x unknown set weight goal learn user coverage function f learn optimal set interaction model order receive personalize result need communicate since f set function natural notion feedback machine learn perspective would provide single label set post present indicate whether like dislike entire set however approach suffer two first point view user natural provide feedback entire set post second since exponentially many set likely need extensive amount user feedback term set post could learn function instead assume go list post order submit feedback like one indifferent zero dislike one post take feedback post mean indifferent personalization minimize regret objective function define term set feedback term individual post provide appropriate credit assignment one possible solution would assume feedback user provide particular post independent post present set case one view user feedback label data train classifier determine post user like however assumption fit interaction model user might like post either content previous post already cover story address issue consider incremental coverage post ie advantage provide previous post incremental coverage receive add post set note define two incremental coverage probability first post cover feature furthermore view set document order set ak one sum incremental telescope sum yield coverage set document x x j shorthand set document use incremental define reward receive present user obtain feedback f f wi x x user like document ie j one reward become exactly coverage function seek maximize f wi five p algorithm maintain estimate user time step give estimate optimize f pick set document show user receive feedback f gain reward f time step average reward therefore f one since time take account feedback receive time one make may suboptimal comparison consider reward would receive make inform choice user consider feedback time step f six one see user feedback would right choice user preference weight difference reward best choice retrospect call regret definition regret average regret time step difference positive regret mean would prefer use weight maximize six instead actual choice weight learn algorithm one describe next section allow us learn go infinity regret go zero rapid rate intuitively guarantee mean learn sequence well fix include true user set post user present learn personalize coverage function particular user manner post provide tailor taste guarantee would show weight well set post learn also post would select use true user preference weight day example consider user interest politics sport also passionate may never show post since likely common thus may never receive feedback would allow us accurately model portion user true intend address issue future work learn user describe algorithm learn repeat user feedback sessions like many twelve approach update estimate use multiplicative update rule particular approach view special case multiplicative weight algorithm eighteen algorithm start choose initial set weight one assume weight normalize sum one since coverage function insensitive scale absence prior knowledge user choose uniform distribution one one prior knowledge user start correspond set weight every round use current distribution pick k post show user receive feedback f would like increase weight feature cover post user like decrease weight feature cover post user dislike update achieve simple multiplicative update rule f one z seven order could define order post present user one pick greedy algorithm z normalization constant zero one learn rate intuitively f measure contribution negative feature reward p wi f j f eight two wi normalization two wi simply use keep term range five five learn rate small make large move base user feedback learn rate tend one update become less thus intuitively start small value slowly increase claim number personalization use learn rate give one q one two nine preference learn procedure regret bound r since regret go zero go infinity approach call algorithm proof follow eighteen formalize learn process repeat matrix game involve algorithm user extend version paper detail fifteen four evaluation evaluate algorithm real data collect two week period post come diverse set include personal news sit commercial many obtain data index crawl twelve million rate approximately post per day five perform simple data clean step duplicate post removal reduce number post per day data set however real web data still invariably noisy even clean thus algorithm must robust content extraction post extract name noun phrase use name entity recognizer seventeen part speech tagger respectively remove infrequent name noun phrase common year leave us total collection size nearly detail find extend version fifteen evaluate algorithm high level topic feature refer define set feature latent allocation nine topic model learn noun phrase name describe take weight feature fraction word corpus assign topic describe section directly define p set topic model probability topic use sample implementation nineteen default parameter extract name noun phrase part run iteration period run sample select sample single processor process take less ram twelve run eight hour corpus post function optimization need generate post take minute also evaluate variant algorithm feature consist name noun phrase directly refer variant use feature set assume post cover multiple feature thus use coverage function cover feature describe section value set average number name per document corpus approximately sixteen set post selection take five evaluate coverage detail section two main objective algorithm select set post best cover important prevalent currently discuss major world take place time correspond data set include conflict inauguration gas dispute russia well global financial crisis example set post algorithm select eight hour period eighteen budget k set five one unilaterally halt fire rocket persist two down jet lift river three doctor lose three niece tank four eu wary russia reach gas deal five first day president war council white house reception select five post cover important particular day conflict appear twice set due extensive presence time important note however two post present different conflict prevalent story right expand budget fifteen post algorithm make additional relate major day w bush legacy also select post religion cook since represent large portion directly relate news current another example top five select post morning day academy award announce one button top nominee two rule open border gain three choose senate four fearless kitchen recipe medieval lamb wrap five avoid misguide policy blunder post describe movie curious case benjamin button supplant conflict top list cook post make fourth position wish quantitatively evaluate well particular post selection technique achieve notion coverage describe real data however standard information retrieval metrics precision recall directly applicable case since label identify prevalent give day assign specific post rather measure topicality individual post well redundancy set post say post topical respect give time period content relate major news event period post r redundant respect previous post p contain little additional information post p ideal set post cover major discuss would high topicality low redundancy conduct study obtain label topicality redundancy data compare figure two topic represent peanut butter recall eighteen size word proportional importance topic four popular aggregation sit front page search yahoo buzz intend evaluate well fee unavailable days evaluation period additionally also examine performance simpler objective function post selection task measure topicality order measure topicality post need idea major news time period express information study provide headline gather major news source five different world news politics business sport entertainment headline category aggregate three different news source provide selection avoid name single source definitive news outlet category instance politics present headline today post collection headline akin condense newspaper refer reference present reference gather particular time eighteen call reference time show participant set ten post choose one six post selection ask mark whether post relate reference post present title along short description make aware technique post come bias rat post select choose eight hour window data end reference time post select popular aggregation sit retrieve sit within fifteen reference time figure three leave show result topicality user rat six average set ten post select search yahoo buzz contain five topical post ten present topicality significantly better select post day seem good heuristic cover important many post technology page help social sit highest rank post eighteen select top post user vote thus top select post consist prevalent many entertain shock post teen stab make job interview top post six outperform feature capture better way feature example one eight hour period data set coherent topic gas crisis therefore cover topic present story crisis however name entity russia may cover multiple select post russia plan go ahead open pediatric medical center despite current financial crisis since contain important name like russia crisis hence cover feature might select post topical yet contain multiple important feature topicality capture major aspect notion coverage important current cover select post one drawback evaluation method post adequately represent difficult define set reference sit summarize day important prevalent tip instance furthermore case want show study five reference overwhelm result post relate important technology story would likely consider topical leave category measure redundancy user study describe previous section allow us measure whether post topical however topicality enough judge goodness set post since may story hence interest instead want post diverse capture important well appeal everyone interest part user study ask look set fifteen post select one six previously describe post selection mark think redundant present either two three set post generate different time period aware source post figure three right show algorithm outperform search redundancy metric word algorithm select diverse set post diversity primarily due diminish return property objective function cover important feature story cover yield small reward search highest number redundant result high variance suggest days many post front page similar fact average post select search nearly six time redundant select however note perform well redundancy metric alone sufficient example may turn post pick algorithm meaningless hence interest user thus algorithm need perform well topicality redundancy metric order useful yahoo buzz two perform well metrics however yahoo buzz use web search trend user vote feature select post achieve topicality redundancy performance select post use simple text feature furthermore adapt result user describe section alternative objective function alternative objective function define three consider two simpler objective function modular function modular function additive set function element associate fix score value set sum score since score post depend set incentive select diverse set post naive way select post use fit modular framework first pick top k base weight corpus one pick post cover e c n e r e f e r l e e r p f r e b n g v six five four three two one zero e l e r r e p p n n e r f r e b n g v fifteen three two one zero five buzz buzz figure three leave result user study measure topicality bar show average number post ten find topical respect reference right result redundancy user study report number redundant post post selection technique present error bar plot indicate standard error addition potential redundancy mention technique suffer fact commit topic irrespective quality post cover furthermore even post cover multiple well might select may post better cover individual topic use strictly objective function alleviate example define feature base model train eight hour data set eighteen topic weight peanut butter recall major news story time figure two thus select fifteen post follow naive approach pick post topic however weight topic nineteen much lower mean topic weight twenty moreover since topic closely correspond prevalent news story many post cover high probability select post unlike naive approach simultaneously consider topic weight post coverage budget maximum coverage another simple objective function consider budget maximum coverage introduce definition feature case noun phrase name weight corpus frequency optimize objective lead post example eight hour data set twenty second post select announce schedule rock band upcoming world tour thus completely cover feature boston new york dozen post select incentive cover feature personalization two evaluate well algorithm personalize post select response user feedback one set conduct user study directly measure many present post study participant would like read second set simulate user target set post observe objective function f change respect case real short summary user instruct read list post one one mark post would like read would like read indifferent user tell make decision respect previous post display set capture notion incremental coverage example user might excite read post inauguration appear top slot particular result set thus would mark like read however four similar post appear time get rat fifth inauguration post row likely label like read set ten post personalization algorithm use user rat update weight select personalize set post next also ask user mark post present order two condition present randomize repeat process total five longitudinal study wish overly tedious accelerate personalization process use learn rate five correspond learn horizon ie nine nine figure four show result study twenty vertical axis plot show average number post like user single epoch one would expect epoch zero post always number like post approximately personalize run however two already show preference towards personalize result user prefer sport post personalization easy user interest narrow study however simply instruct rate post personal people often eclectic vary interest task harder realistic thus notable still able successfully adjust user taste show improvement case instead ask rate post accord personal taste ask pretend want read post specific subject observe interest qualitative behavior initially top post select main day include conflict divide data eight hour segment pick start segment random particular user present user set ten post start segment select use post display title tend change one epoch next employ simple bipartite match algorithm map personalization weight across alternatively one could use recent topic model design work stream data ten mark post like dislike select post change include post subcontinent flaunt tie china particularly notable give post appear relatively infrequently data set thus without personalization rarely select also enough eventually supplant major news top result set disappear list due high prevalence believe precisely behavior one would want personalization set simulate consider case hypothetical sport fan always love read post particular every day present set post popular sport mark like simulate user order examine effect personalization objective function specifically simulate sport fan mark post like specify number personalization update personalization weight epoch next epoch call evaluation epoch compute objective function f three different set post first compute f post epoch hypothesize spend personalize prior evaluation epoch higher value second compute f post another popular sport also expect see higher value objective case finally compute f post popular politics expectation personalize sport post several days f set politics post decrease respect case figure four b show result experiment value five observe precisely hypothesize behavior vertical axis plot show ratio f compute learn personalization weight f uniform weight allow us compare across three thus point plot appear higher along vertical axis one indicate improvement case value one indicate decline respect case figure four c show simulation one aggressive set learn rate thus expect plot show objective function change direction rapidly compare figure four b figure capture important trade deploy system vary learn rate trade speed personalization variety select post five relate work recently increase number index display list popular search four yahoo buzz seven three six one display post without manual intervention search however display post either vote pick post automatically use combination feature link structure two trend search engine query seven number time post share currently use feature derive text post although future hope incorporate link structure post al another key difference lack personalization functionality provide recent paper eight agarwal al address problem similar task select four set sixteen display yahoo sixteen manually pick human hence high quality author use rate learn model article set differ significantly since tackle problem select ten roughly post eight hour segment moreover describe section four data noisy access rat another line relate research area retrieval thirteen eleven retrieval task retrieve document cover many give query traditional information retrieval set assume relevance document independent document however retrieval utility document contingent retrieve document particular newly retrieve document relevant cover cover previous document thus concept relevance retrieval similar notion coverage diminish return characteristic however retrieval intend cover popular discuss two common approach personalization collaborative filter fourteen filter collaborative filter user learn manner correlate user past activity data entire user community approach document recommend user similar document user previously like similarity base document content use approach provide theoretical guarantee personalization moreover currently kind user base need collaborative filter effective al propose solution problem select read order come across important quickly although relate problem fundamental difference instead try select read present user selection post various moreover approach completely content base whereas approach al base link addition also incorporate personalization algorithm also extensive work build model analyze structure example al sixteen present model information flow could potentially leverage analysis future order extract better feature two intend analysis visualization tool unlike us try cover instead present user search interface suggest relate word base search query give preference word whose frequency increase large amount past word high moreover employ personalization six paper describe problem turn noise vast majority post interest average user quantity truly remarkable reason many suffer information overload goal show small set post cover important currently discuss start explore different desire coverage function formalize notion coverage eleven f f f f f f p e k l f r e b n six five four three e z l n r e p n e z l n r e p f f one zero fifteen fourteen thirteen eleven e z l n r e p n e z l n r e p f twelve f one nine zero personalize four five zero one two three five ten fifteen number personalization five ten fifteen number personalization b c figure four result personalization user study measure many post user like ten present epoch personalize line correspond learn rate five b c effect number spend personalize simulate sport fan objective function f respect personalization f evaluate two sport one politics learn rate five b one c optimization problem present efficient algorithm select top next generalize coverage notion personalize case assume user coverage function base personal introduce problem learn coverage function limit user feedback formalize notion feedback illustrate simple personalization method base multiplicative update weight method achieve personalization derive two different base general framework use different feature efficient enough run large feed compare popular aggregation like search yahoo buzz addition post content use feature rate trend search query link structure post use human intervention pick post present result base user study algorithm outperform except yahoo buzz comparable despite access feature furthermore experiment demonstrate algorithm adapt individual result emphasize simple notion coverage introduce successfully capture salient day believe combination coverage personalization prove useful tool battle information overload thank provide us access data grateful helpful useful comment tag cloud paper generate use work partially support career seven reference one two three four search five six seven yahoo buzz eight agarwal p r n j model content optimization nip nine jordan latent allocation ten k r l shi l inference latent allocation eleven j j use reorder document produce twelve n g prediction learn game university press thirteen h less fourteen das news personalization scalable collaborative filter fifteen k g c turn noise tech report sixteen joshi p kale information ecology social media ai magazine seventeen j r finkel c man incorporate nonlocal information information extraction sample eighteen r e adaptive game play use multiplicative weight game economic behavior nineteen l find scientific twenty moss j budget maximum coverage problem information process letter many world need time magazine j c c j n glance outbreak detection network g linden b smith j york collaborative filter compute seven g l fisher analysis maximize set function mathematical program n model discriminative global inference b smith hair still perfect politico sixteen c w w j beyond independent relevance evaluation metrics retrieval b h li l w xi w fan z wy improve web search result use affinity graph