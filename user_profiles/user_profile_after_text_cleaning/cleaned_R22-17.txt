parallel distribute vol sixteen three march complete compiler approach c program embed unsuccessful due widespread use pointer arithmetic complex memory model space digital signal paper develop first time complete approach overcome issue first combine pointer conversion technique new modulo elimination transformation program recovery enable later parallelization stag next integrate novel data transformation technique expose processor location partition data combine new address resolution mechanism generate efficient program run multiple address space without use message pass furthermore possess data cache structure optimization present transform program exploit remote data locality local memory parallelization approach apply suit give average four index embed signal process measurement evaluation model simulation conversion sequential parallel form reverse engineer performance measure array one introduction provide cost effective solution embed require high performance although sophisticate optimize target single one successful parallelize reason simple task complex require combination number overcome particular encounter compile namely program idiom use challenge space architecture write c make extensive use pointer arithmetic three alone prevent attempt parallelization use modulo address prevent standard data dependence analysis also parallelization failure article describe two program recovery translate restrict pointer arithmetic modulo address form suitable optimization memory model globally similar four reduce hardware cost support space eliminate need hardware consistency place pressure compiler either generate code mean ensure correct execution paper describe map address resolution technique allow remote data author university institute compute architecture jam clerk maxwell build road unite kingdom mob manuscript receive march accept two august publish twenty information obtain reprint article please send reference log number need access without achieve develop mechanism similar use generate space code whilst allow exploit architecture cache structure compiler neither rely cache exploit temporal reuse remote data large cache line size exploit spatial locality instead space machine rely effective use direct memory access transfer three article describe space specific locality improve upon approach achieve determine location data transform program exploit locality transfer remote data also exploit increase typically available data guarantee location specific require program correctness case machine four five six allow safe incremental approach improve program performance paper new develop combine previous work manner allow first time efficient map standard write c space embed paper organize follow section two provide motivate example follow four section notation program recovery data parallelization locality section seven provide overall algorithm extensive evaluation approach two seven suit contain full scale embed follow review extensive relate work conclude remark publish computer society complete compiler approach c program fig one example show substitution linear pointer base array explicit array reference loop one elimination modulo array index loop two repeat original code b array recovery c remove two motivation take input sequential code produce parallel code output study scientific compute domain many embed domain recent compilation target first glance seem ideal many static linear access matrices however develop due widespread practice use pointer access two furthermore typically distribute address space remove need expensive memory coherency hardware save hardware level greatly increase complexity compiler task illustrate main point paper two present allow certain separation concern first example demonstrate program recovery use aid later stag parallelization second example demonstrate program contain remote access transform ensure correctness wherever possible exploit space memory model program recovery example code fig typical c program write contain fragment suit use pointer traversal idiom two although underlie algorithm first loop nest linear matrix algebra current form prevent optimize perform aggressive optimization attempt parallelization circular buffer access frequently occur idiom program typically represent modulo expression one array see second loop nest nonlinear defeat data dependence prevent optimization parallelization program recovery scheme first replace array reference base loop modulo access remove apply suitable transformation give new code fig remove pointer arithmetic repeat give code fig new form suitable parallelization although new code contain linear array easily optimize code hoist strength reduction standard native memory model approach exploit fact although machine typically multiple address space part processor memory space visible unlike pure machine processor internal address space purely local reflect external bus addition part processor memory form global address space processor assign certain range address see fig two shade region denote portion global address physically resident processor global address space use access remote data thus processor private internal memory address range refer global address remote however unlike single address space machine global memory allocation processor may allocate data either internal memory part address space visible thus order refer remote data processor must know parallel distribute vol sixteen three march evaluate subset original iteration space refer globally allocate array access remote data multiple address space machine however processor location partition data need explicitly available scheme achieve data nine array form array whose inner index correspond four fig show program partition data apply suitable automatically generate loop recovery nine transformation assume z loop parallelize array partition zero zero seven write processor zero one zero seven write processor one similarly array b c space machine need generate separate program processor ie explicitly enumerate processor id loop z partition code processor zero specify z show fig code one two three identical except define z one two three multiple address space machine require remote data distinct name local thus rename follow zero zero seven become zero seven one zero seven become zero seven similarly array b c processor zero declare variable reside processor declare extern see fig processor one declare local remain array declare extern access local remote data local pointer array set processor simple data structure array contain four pointer assign start address local array four use original name array pointer array initialize pointer array point four distribute array four see fig use original name mean exactly array access form use array fig achieve use property multidimensional array c array array higher dimension array define contain array code generation point view greatly simplify implementation avoid complex difficult message pass program execute correctly provide mean generate code reference remote otherwise space machine introduce overhead exploit locality array reference require pointer native compiler know eventual location data must schedule fit external interconnect network bus greater result available straightforward identify local reference replace indirect pointer array access local array name examine value partition indices see equal local processor id z data reference sometimes local sometimes remote isolate index split program section replace local reference local name show fig access c second loop one otherwise assume private copy two define section c standard paragraph three four fig two logical memory organization address four processor system internal identity remote processor location memory require data value complicate matter internal memory address space processor address space globally visible actually refer physical memory illustrate fig two internal memory address globally visible address correspond local data therefore access directly use local address via globally visible address space access local data via local address space however faster access data via share address space bus require develop novel technique combine space parallelization approach novel address resolution mechanism linear access determine compile time processor memory location data nonlinear access resolve mean simple data structure allow us take advantage higher lower latency local memory whilst maintain correctness overall parallelization scheme parallelization example recover linear access structure program must partition map efficiently onto space memory model machine consider first array recover loop nest fig simple outer vector product use illustrate need space parallelization assume four single address space eight would simply partition iteration space across four show fig processor identical copy program make call determine processor id complete compiler approach c program fig three example contrast standard approach parallelization b novel scheme include data transformation c creation one program per processor address resolution apply first loop fig exclusively local fourteen z thus reference replace local name remote reference c first last loop still access data via c pointer array remote access expensive therefore group access remote data via transfer effective method reduce overhead scheme remote data transfer local temporary storage area achieve insert load loop remote reference show fig space constrain check make load loop insertion guarantee space available transfer perform way exploit temporal spatial locality map potentially distinct multidimensional array reference occur throughout program single dimension temporary area reuse show fig thus method expose processor id partition data generate correct code space use address resolution scheme location data statically determine local memory transfer remote memory exploit three notation describe partition map approach briefly describe notation use loop represent column vector fourteen number enclose loop note loop need perfectly nest occur arbitrarily program loop range describe system define polyhedron iteration space data storage array also view polyhedron introduce array indices fourteen describe array index space space give polyhedron aa assume reference array write integer matrix vector form access pair example notation loop bound first loop nest fig represent parallel distribute vol sixteen three march fig four example show incremental locality optimization isolation local remote array access b introduction separate loop load remote data c substitution remote access transfer apply code fig isolate local remote reference b introduce load loop c remote access two six six four one zero zero one zero one zero one three seven seven five three seven seven five two six six four zero zero one one zero twelve one zero twelve array declaration fig represent similar manner ie index range zero subscript simply discuss program structure introduce notion computation set q fourteen computation set consist loop bound either enclose loop nest one two three four program recovery section describe two program recovery aid later parallelization array recovery array recovery consist two main stag first stage determine whether program form amenable conversion consist number check second stage gather information array pointer pointer loop nest information use replace pointer access correspond explicit array access remove pointer arithmetic completely detail see ten pointer arithmetic pointer assignment arithmetic restrict analysis may initialize array element whose subscript affine expression enclose whose base type scalar simple pointer arithmetic assignment also allow complete compiler approach c program prohibit scheme assignment pointer may side effect relation array difficult identify fortunately rarely find program information program check second stage algorithm gather information pointer usage pointer conversion pointer update capture system solve efficient algorithm ten pointer conversion index array access construct information see first loop fig separate pass pointer access arithmetic replace remove show first loop fig modulo removal modulo address frequently occur idiom program develop new technique remove modulo address transform program equivalent linear form one exist achieve use transformation framework nine manipulate extend linear include thirteen mainly use reason reshape array use program recovery technique restrict attention simple modulo form reference contain modulo expression complex reference highly unlikely may address extend approach include skew let l least common multiple second loop fig fourteen eight fourteen four access g h hence l fourteen eight apply loop transformation base l loop nest particular formulation use base nine benefit fit general linear algebraic transformation framework l fourteen eight fourteen one zero zero fourteen two four one zero zero three five zero eight eight fourteen eight eight zero fourteen new iteration space twelve apply zero fourteen zero zero zero zero nine case fourteen one zero zero zero eight one apply second loop nest show one new iteration space four five six two six six six six six six four one zero zero one zero one zero zero zero zero zero one zero zero zero one zero one three seven seven seven seven seven seven five three five two four three seven seven seven seven seven seven five two six six six six six six four zero zero zero three seven seven loop nest show second loop nest fig new array access find fourteen give new access show second loop fig process repeat modulo remain one modulo expression array g subscript remain second loop fig remove apply transformation give code fig five data parallelism section briefly describe parallelization approach scheme partition attempt partition data along align dimension array may evaluate parallel minimize communication sophisticate approach available eleven twelve beyond scope article partition base alignment four thirteen fourteen fifteen try maximize row equal subscript matrix align row determine index partition along construct partition matrix p define pi fourteen zero fourteen otherwise eight row identity matrix id also construct sequential matrix contain indices partition p fourteen id example fig three one index partition along therefore p fourteen fourteen twelve map array indices partition decide data indices base partition matrix p matrix give new array indices data matrix define two four fourteen one zero zero zero zero zero zero three five k nine twelve p p number fourteen p within embed realistic assume number constant already know compile time example fig three p fourteen four detail transformation framework see nine transformation use expose processor id array reference critical scheme show let map transformation fourteen thus partition indices sequential indices leave alone example fourteen fourteen new array indices give fourteen new array bound find use parallel distribute vol sixteen three march aa transform array bound seven two six six four one zero zero one zero one zero one three seven seven five three seven seven five two six six four zero zero three seven ten eleven ie four eight new array also find zero fourteen general without loop introduce array access example fig three would four b four c j four however framework nine always generate suitable recovery loop transformation transformation apply enclose loop zero fourteen update access matrices array case fourteen ut one fourteen one zero zero one twelve ie z result code show fig expose processor id reference without expensive subscript address resolution array partition across several local partition give new distinct name single address space support therefore introduce new name equal old name follow processor identity number local wish one array reside allocate local processor remainder declare extern allow remote reference address resolve linker complete algorithm give fig function insert insert fig show insert array b c change type declaration whenever array pass function type declaration change must propagate apply transformation code modification require synchronization program data partition across must guarantee program execute correctly synchronization must insert correct data value communicate race condition occur machine achieve synchronize communication however unsynchronized model memory access compiler responsibility insert synchronization appropriate synchronization need enforce exist data dependence sixteen source sink dependence processor need synchronization normal order ensure honor however determine location source sink data dependence generally previous work twenty require array section analysis determine location source sink rely schedule data alignment fig five address resolution overall parallelization address resolution algorithm b overall parallelization algorithm reduce complexity task however take advantage map approach describe make location array access explicit two reference involve dependence processor id partition dependence remove consideration remain consider require synchronization determine equality processor trivial value either reduce macro expansion define z constant fold general function restrict attention constant equality determine function equality general undecidable let sink access matrix vector pair source sink data dependence partition index x correspond nonzero row partition matrix p x fourteen sink x fourteen zero x fourteen x thirteen remove dependence consideration analysis expand synchronization ie x fourteen sink x fourteen zero x x fourteen c fourteen c constant allow synchronization however purpose paper consider barrier synchronization local remove synchronization must insert satisfy use algorithm describe seventeen provably minimal insertion complete compiler approach c program barrier basic block perfect loop certain class imperfect loop nest six locality analysis partition address resolution apply program execute correctly source code generate look similar generate space machine eight addition local pointer array distribute array straightforward code however introduce overhead exploit locality exploit local access embed almost entirely statically schedule ie must make conservative assumption final location element access remote unless prove otherwise greater via external bus hence local local access general determine whether array reference entirely local throughout program however partition scheme explicitly incorporate processor id array reference simply check see equal processor id z simple syntactic transformation give array access pointer array name x syntactic concatenation operator seven x n n apply example fig code fig access statically determine local native compiler locality remote access memory hierarchy single processor relatively straightforward cache exploit locality largely achieve ensure data fit effective register allocation lack cache structure significant impact remote access however repeat reference remote data item incur multiple remote access unless compiler explicitly program exploit available temporal locality approach determine likely remote transfer remote data local temporary use computation remote data transfer code transform exploit temporal spatial locality use underlie engine index split first separate local remote reference split iteration space data either local remote processor id explicit framework need array section analysis perform remote reference original loop partition n separate loop nest use index set split two one n fifteen n fourteen one number dimension partition example fig partition one dimension hence n fourteen three follow zero z one fourteen z z one three sixteen seventeen eighteen give program fig remote data buffer size remote access optimization take place must sufficient storage let storage available remote access simply check remote data fit ie omega calculator determine value use enumerate formulae condition meet currently abandon optimization array contraction loop fusion reduce temporary size explore load loop load loop introduce exploit locality remote access temporary remote reference introduce follow loop distribution always legal dependence cycle transformation form word single loop nest q distribute k loop nest k one load loop example fig one remote array hence k fourteen two transform load loop exploit locality temporal locality load loop correspond invariant access null space access matrix ie n always exist transformation find reduce smith normal form transform iteration space invariant innermost remove elimination loop load loop fig invariant remove show fig stride order allow large amount remote data access one go rather separate access per array element must access order achieve simple loop transformation guarantee access transformation find fourteen example fig identity transformation access load loop already order linearize distinct remote reference may declare vary dimension yet data storage area set aside remote access fix therefore temporary array must linearize throughout program reference update accordingly fourteen lut transform array access case l fourteen eight one twelve temp temp fig convert form format transfer require start address remote data local memory location store plus amount data store achieve effectively inner loop remove loop body place within parallel distribute vol sixteen three march call start address remote element give lower bind innermost loop size equal range thus transform remote array access follow um fourteen zero fourteen temporary array access similarly transform innermost loop eliminate elimination finally replace assignment statement generic transfer call get size give final code fig seven experiment overall parallelization algorithm show fig currently check loop trip count greater eight continue beyond step two algorithm thirteen compiler nineteen select best publicly available c compiler though target evaluate effectiveness parallelization scheme two different set two seven program execute board cluster four share external bus external program compile twenty compiler version full optimization time cycle accurate due emphasis code generation issue frequently consider artificially small data set size therefore evaluate use original small data set well scale version wherever appropriate unlike comprise set well many available original form well form program recovery fig show set program initially compiler fail parallelize program make extensive use pointer arithmetic array show second column however apply array recovery column three program become column four fact program parallelize array conversion contain data dependence permit parallelization program set recover due complexity fifth column fig show whether program profitably parallelize program comprise small loop perform better execute sequentially due overhead associate parallel execution filter stage two algorithm impact modulo removal see fig four program fir convert form scheme modulo removal direct impact parallelizer ability successfully parallelize four program could parallelize application transformation parallelize modulo removal due data although program recovery use largely facilitate parallelization performance impact sequential performance well first two set bar seven eight show original sequential time program recovery three fig six exploitable parallelism b eight benefit transformation whereas single kernel fir experience performance degradation program recovery array recovery enable better data dependence analysis allow schedule case fir small number slight overhead enumerate array disproportionate effect performance fig eight show impact modulo removal performance since computation modulo comparatively expensive operation removal positively influence performance three program wherever applicable data parallelism data distribution address resolution third column set bar seven eight show effect blindly use space approach parallelization without data distribution space machine surprisingly performance universally poor fourth column figure show complete compiler approach c program fig seven incremental performance development profitably program recovery parallelization partition address resolution localization fig eight incremental performance development profitably program recovery parallelization partition address resolution localization appear performance apply data distribution map address resolution although program experience sequential version convolution overall performance still disappoint closer inspection generate assembly cod compiler distinguish local remote data conservatively assume data remote generate slow access ie double word instead quad word access local data generate increase memory access latency account produce schedule addition remote memory occur effectively utilize engine localization final seven eight show performance locality apply partition code access local data make explicit compiler identify local data able generate efficient schedule addition remote memory access group utilize engine case linear achieve program bar one fir number small occur precisely case program recovery give sequential improvement pointer base code overall vary nineteen fir average four overall less dramatic program complex include full greater communication overhead program show average histogram fail give significant due lack sufficient data parallelism inherent program conversely fir parallel distribute vol sixteen three march fig nine novel parallelization approach data set size b number mult large compress filter give due improve sequential performance program parallelization loop shorter parallelization native loop unroll algorithm perform better reduce trip count appear important characteristic parallel program data set size fig show performance program small large data set small data set size default necessarily represent condition image filter example apply four four image obviously little parallelism available artificially reduce data set however parallelization approach still achieve four eight program minimal data set come close linear whereas matrix multiplication experience even ten ten matrices remain four program convolution fir profit parallelization small data set size approximately sixteen data nineteen thirty program parallel nine nineteen determine profitably technique either sequential exhibit little work parallelize fig show approach scale number compress fir contain ten ten array yet distribute data across available case data size full potential parallel execution exploit give linear number eight relate work extremely large body work compile good overview find twenty compile machine largely focus program language six main challenge insert correctly efficient call parallelize program four six without require complex bookkeeping five although compile distribute share memory must incorporate data distribution data locality eight face problem multiple address space compile move primarily parallelization nineteen approach combine data placement loop optimization exploit parallelism benefit extensive work automatic data partition eleven twelve alignment thirteen fourteen fifteen potentially remove need machine reduce memory coherence traffic case work approach parallelize community work successfully examine improve communication performance introduce private copy share data must keep consistent use complex linear memory array access contrast keep copy share data instead use access mean global name data analysis develop communication general communication partition scheme expose processor id eliminate need array section analysis handle general global communication area c nineteen significant work though target space machine modulo recovery consider large highly specialize framework base present solve modulo access however introduce floor div ceiling function effect part program consider large body work develop loop data improve memory access eight data transformation data tile use improve spatial locality representation allow easy integration loop data far parallelization concern early paper describe program may parallelize give detail experimental result similarly interest overall parallelization framework describe mechanism detail parallelization might take place provide impact different parallelization consider however complete compiler approach c program automatic approach provide semiautomatic parallelization method enable exploration different base move architecture present however integrate data partition strategy available data allocate single processor example cod furthermore experiment communication model simulator thus issue map parallelism combine distribute address space address nine conclusion space embed prove challenge compiler due complexity memory model idiomatic program style paper develop integrate approach give average four apply seventeen significant find suggest cost effective solution high performance embed exploit automatically future work consider form parallelism find integrate reference one r novel code optimization second education research four three two v c h methodology l signal process technology synchronization communication seventh l architectural support program operate global communication analysis optimization program language design implementation k compile machine vol eight j v b broom r fowler g k q advance optimization rice compiler experience vol fourteen five six eight seven lee suite canada lam data computation fifth practice parallel program nine integrate loop data global j parallel distribute compute vol four ten b array recovery embed compute vol two two twelve eleven b k automatic data layout use one integer program parallel compiler technology pact j e j framework integrate data alignment distribution redistribution distribute memory parallel distribute vol twelve four thirteen v k p solve alignment use linear algebra seventh l workshop parallel compute fourteen k j g data optimization allocation array reduce communication machine j parallel distribute compute vol eight two fifteen gilbert l r automatic alignment array j parallel distribute compute vol two sixteen f compiler reduction share virtual memory ninth l seventeen ea compile time barrier synchronization minimization parallel distribute vol thirteen six june eighteen w pugh count program design implementation nineteen hall murphy e lam maximize performance compiler computer vol twelve twenty r k v compilation parallel parallel compute vol compile letter program vol two fourteen j improve cache locality combination loop data vol two el da parallelization scalable parallel compiler technology pact f de man transformation nest loop modulo index affine parallel process letter vol four three carr compiler improve data locality sixth l architectural support program operate j l uniform design parallel program l circuit j b singh environment design combine parallelization increase adaptability efficiency ninth workshop first signal process education workshop speed h exploit fine parallelism embed program l parallel compilation pact receive degree computer science university computer science university respectively currently lecturer institute compute architecture university member compiler architecture group main research interest embed receive degree computer science university formerly visit scholar university visit professor barcelona currently advance research fellow reader university main research interest adaptive compilation automatic compilation multicore