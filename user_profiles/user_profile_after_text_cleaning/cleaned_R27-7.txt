report report doucet n j p g p g l de r v e r n de r x abstract investigate focus retrieval structure document provide large test structure document uniform evaluation measure forum compare result paper report evaluation campaign consist wide range track ad book efficiency entity rank interactive link mine run entirely volunteer effort research community anyone idea time spend major impact one introduction traditional search identify whole document relevant user information need task locate relevant information within document leave user next generation search perform task identify relevant part relevant document search engine perform task refer focus discipline know focus retrieval main goal promote evaluation focus retrieval provide large test structure document uniform evaluation measure forum compare result focus retrieval take many form hence evaluation campaign consist wide range track ad track investigate effectiveness passage retrieval four ad retrieval task thorough focus relevant context best context book track investigate support read search full digitize book efficiency track investigate effectiveness efficiency rank retrieval approach real data real query entity rank track investigate entity retrieval rather text retrieval one entity rank two entity list completion interactive track investigate behavior interact document retrieval approach effective question answer track investigate technology access data use address interrogative information need track investigate link discovery document file level element level track investigate structure document mine especially cluster document rest paper discuss aim result track relatively section ad track section two book track section three efficiency track section four entity rank track section five interactive track section six track section seven link track section eight mine track section nine two ad track section briefly discuss aim ad track task setup use measure result try formulate clear find detail three aim task ad track study retrieval general aim system find relevant information give topic request case retrieval article contain relevant information choice whole hierarchy different return hence within regard relevant result result contain relevant information result exhaustively discuss topic contain little information possible result specific topic traditional document retrieval first condition apply measure solely base retrieval highlight text simplify task highlight text retrieval assume return highlight text compare character text retrieve search engine number location character text identify relevant assessor best context discuss use distance best entry point run identify assessor ad track feature four task thorough task result estimate relevance must return evaluate mean average interpolate precision focus task nonoverlapping result must return evaluate early precision relevant context task nonoverlapping result must return group document evaluate mean average generalize precision generalize score per article base retrieve highlight text best context task single start point element start tag passage offset per article must return also evaluate mean average generalize precision generalize score per article base distance assessor point test collection start use new document collection base original syntax convert use general tag layout structure like article section paragraph title list item typographical tag like bold emphatic frequently occur annotation enhance semantic markup article outgo link base semantic knowledge base sixteen fourteen explicitly label class like many pioneer creation since total ad search create assess follow precise use assessment system assist highlight relevant text topic ask mark relevant text pool document assess article relevance separate best entry point decision make assessor relevance freeze ten time fully assess main consist judge specific measure evaluate four task addition every article contain highlight text relevant evaluate document retrieval effectiveness result attractive document retrieval test collection use freely available document genre result receive nineteen participate group discuss detail individual paper four three main research question underlie ad track first main research question impact new collection four time size longer article additional semantic focus retrieval impact collection size impact document length hence complexity structure dom tree saw collection size little impact relevant article much longer mean length increase lead lower fraction highlight text per article mean also reduce correlation article retrieval context task obviously locate right article important enough score well focus retrieval task second main research question impact verbose either structure use phrase impact semantic annotation submit query retrieval effectiveness impact explicitly annotate phrase find task best score run use content query content forty run top ten four task part explanation may low number forty comparison number fifty judge query majority query make reference particular tag structural tag potentially express information need naturally term structural popular query use judge fifty query group better perform run use query although structural hint useful individual lead precision gain also use explicitly annotate phrase phrase query ten total phrase query run competitive several overall top ten result impact phrase seem marginal note exact term present query difference phrase annotation third main research question value internal document structure markup retrieve relevant information document structure help identify relevant information within document number use range low thirteen use range passage result whereas use element result nonelemental competitive run typically base article element run outcome broadly confirm result document structure help select best retrieve research question hope expect result test collection prove value future use main aim initiative create evaluation structure retrieval approach outlook plan currently discussion one main research question impact effort within focus retrieval could study use new measure take read effort account impose length result inspire fit screen explicitly frame focus retrieval snippet teaser generate task three book track section briefly discuss book track detail please refer seven setup aim book track evaluate approach support read search navigate full digitize book track focus four evaluation task book retrieval task frame within user task build read list give topic interest article aim compare traditional document retrieval exploit feature back book index library catalogue information focus book search task aim test value apply focus retrieval approach book expect point directly relevant book part structure extraction se task aim evaluate automatic derive structure layout information build table content active read task art aim explore suitable user enable read annotation review summary across multiple book total register track sixteen take part actively throughout year contribute run relevance test collection test collection test collection consist digitize book total include history book literary study religious teach proceed poetry full text book mark format refer develop document layout team development center contain markup table content book also come associate marc record contain publication author title classification information task build full corpus art could select book use user study se task use different set book page image original file page level structure distribute create sixteen new year contain topic use task full task relevance collect use book search system available develop research allow search browse read annotate book test collection eight gather series read play game game one task find book relevant give topic rank top ten relevant book game two task find page relevant give topic aspect inside book select game one finally game three task review page judge game two game run three weekly prize fifty worth gift share top three proportionate score total book level relevance contribute nine game one page judge two game two three although page also judge game one judge respect overall topic specific topic aspect result book retrieval focus book search task due limit amount page level result publish task main find note since vary greatly across treat preliminary task run submit four group experiment various use table content index well traditional document retrieval best perform run submit university team rank book treat document use indri apply query expansion fifty term top ten result although performance improve across board compare last year best map traditional document retrieval approach ie specific book still dominate top rank suggest still plenty do discover suitable rank book structure extraction task evaluation se task generate compare manually build use structure label tool develop university performance evaluate use like measure different structural level ie different precision define ratio total number correctly recognize total number return recall ratio total number correctly recognize total number eight run submit four group best performance calculate harmonic mean precision recall obtain development center team extract first recognize page book contain print interestingly give best result last year test set book active read task main aim art explore hardware tool read provide support engage variety read relate fact find memory task learn goal investigation derive user consequently design usable tool support active read practice do run comparable individualize set study contribute elicit user usability issue relate task attract two group neither submit result time write outlook book track attract lot interest grow considerably previous however active participation remain challenge reason may high initial setup cost build infrastructure search book time structure extraction task meet great interest create specialist community nine search task although explore around provide reference source article tackle small set group since evaluation task require great deal effort develop relevance assessment system design game collect rethink setup task example plan concentrate focus narrow page corpus may relevant addition improve quality test look ways topic creation provide real value improve test corpus plan run se task goal convert current corpus format contain rich structural semantic markup use subsequent attract plan run art either separate forum combine interactive track four efficiency track section discuss general setup result efficiency track detail refer fifteen overview efficiency track run second time first incarnation seventeen intend provide common forum evaluation effectiveness efficiency rank retrieval approach real data real query efficiency track significantly extend track systematically investigate different type query retrieval classic search query expansion query deeply nest structure available well twenty efficiency track use collection available introduce version article semantic almost million article billion size approximately fifty collection significantly old collection use previous last year efficiency track collection irregular structure many deeply nest turn challenge task metrics one main efficiency track cover range query type typical query hardly structural condition thus addition exist track label type track also consider type b generate run blind feedback result reference run intend simulate query expansion evaluate conjunctive manner pose major challenge kind search engine track also contain type c retrieval set query deeply nest structure condition get type c track task efficiency track particularly encourage use style query thus ask create run result report io cost addition two standard task track ie focus thorough track introduce two extra task article task ask result article level task latter task introduce retrieve result within fix budget simulate interactive retrieval standard easily use constraint may return arbitrarily bad result however get task probably require specific much effort metrics efficiency track apply metrics tool use track assess result quality additionally consider equally important metrics result track receive run submit four participate group use five different comparable include two element retrieval one implement distribute system one system two article retrieval regard efficiency average run time per topic vary fifty second type second type b absolute across hardly comparable due hardware setup become evident dominant part retrieval time spend io activity improve reduce io access could promise way improve efficiency actually one participate already keep index completely memory require additional io result document could store sufficiently large memory well result quality comparable track type best run achieve value one result quality type b generally slightly worse compare type probably cause extreme query expansion efficiency track demonstrate number able achieve good result quality within short process time allow interactive retrieval future efficiency track therefore need introduce new challenge could come much collection complex query structural condition combination five entity rank track section briefly discuss entity rank track detail one overview search support type search return instead web page would facilitate many search task start entity rank track provide forum may compare evaluate return list entity rank entity list completion goal evaluate well rank response user query set rank assume loosely define generic category give query example entity retrieval characterize type search goal evaluate build return instead document compare previous use new collection recent bigger size contain entity text specific case track assign article use define entity type result retrieve compose title query user provide system description narrative natural language explanation information need additionally category field set example contain topic set sixty select develop past candidate correspond name article loosely belong corpus general guideline topic title type explanatory ie human assessor able understand title type retrieve task two task entity rank er list completion concern information need represent triple type query category entity category entity type specify type object retrieve query free text description attempt capture information need entity attribute specify set example instance give entity type er run give input query category attribute run base query entity case system return relevant page page play role entity surrogate test collection final test collection create consist adapt format add strata information nineteen evaluation script test collection available use official evaluation measure perform stratify sample top retrieve run compare last year less aggressive sample strategy use order allow judgment versus sixty create previous specifically track reassess year new collection consistently last year less seven relevant relevant exclude test collection would unstable incomplete respectively three drop task example relevant new collection final test collection consist er additionally create set page page influence evaluation page consider may however useful train data example build page together create set available evaluate entity retrieval two different result notice common behavior year identify entity mention text article query apply different detect entity exploit category information produce rank list article represent retrieve best perform approach exploit probabilistic framework rank use similarity probability obtain estimate average precision er task last year build three set er relevance two different document moreover observe three improvement term effectiveness advance use entity retrieval participate track original research agenda also take web entity rank task introduce six interactive track section briefly discuss interactive track detail thirteen introduction purpose interactive track study interaction information retrieval focus end react exploit potential provide access part document addition full document track run first time repeat although task content focus fundamental premise force throughout common subject recruit procedure common set user task data collection instrument common log procedure interaction understand collect data make available analysis ensure manageable effort participant access rich comparable set data user background user behavior sufficient size level detail allow qualitative quantitative analysis aim task experiment design two task construct track instruct select one three alternative search addition invite perform one task two task intend reflect common purpose searcher would visit primarily bibliographic data broad explorative task category narrower specific task category task category intend force searcher make task two experiment design let searcher assess relevance book could also simulate purchase book add basket broad task design investigate thematic exploration give us data query development type preference navigation pattern example broad task consider start study sociology order prepare course would like get acquaint good recent introductory within field well narrow task design represent relatively narrow topical query purpose allow us study basis relevance compare preference different document example narrow task find trustworthy book discuss conspiracy develop terrorist attack new york test collection document collection use differ use previous crawl two million record book bookseller consolidate correspond bibliographic record book catalogue tool record present book number level formalize author title publisher data subject user tag book cover image full text review content search system develop university base daffodil two partially retrieval component implement use apache result begin experiment ask fill questionnaire task precede conclude questionnaire three work task work questionnaire answer action system record system store track volunteer recruit mostly computer science cognitive communication science library science relate field male seventeen female average age average use experience web search search digital digital give possibility express positive well negative general comment user interface praise well arrange everything fit single screen without need scroll inclusion another document aspect namely review also point positively technical search system sometimes useless relate term due topical data source point criticism also miss highlight query term filter result list also heterogeneity data dislike book extensive scarce also ask indicate five point scale useful five useful different type solve search task find document title book review three popular field also worth note find useful tag seem put trust authoritative source use control vocabulary idiosyncratic tag log analysis show average length session decrease category category category duration search time two query category category category explorative work task longer search perform end session book respectively basket collect book broad task narrow task average query length number term simple query field task result average query length outlook track generate interest data analysis particular pattern interaction different task also would like perform analysis use different type track consider extensive pilot experiment access data base experience scale experiment run part seven question answer track section briefly discuss new propose order compare focus short answer require well summarization aggregate result expect detail ten motivation evaluation campaign example clef aim evaluate retrieve precise answer rather document response question question test set generally compose question question require single precise answer find document collection sometimes complex question list question complex question introduce campaign expect phrase sentence answer question set distinct short find whole collection list question best reach seventy correct answer question forty list question eighteen track aim compare performance retrieval automatic summarization encyclopedic resource track consider two type question question complex question whose answer require aggregation several propose evaluation short answer short part text one word usual way answer factual question question classical basic set expect answer quite well need provide small order set ten non overlap contain possible answer question element passage position answer find system passage answer evaluate compute distance real answer evaluation methodology differ traditional campaign short answer must provide besides support passage major difference term metrics use rank participate assess answer paradigm rather distance indicate answer entry point real one new way evaluate interest side effect allow focus participate task use evaluation even unable extract short answer basic may simply provide relevant short extract retrieve set entry point wherever text first time retrieval focus participate campaign propose evaluation long answer thorough experience evaluate focus retrieval however long answer new context idea propose common task process three different provide list answer automatic summarization extraction focus fifty task answer build aggregation several different document maximal length abstract fix make selection relevant information standard produce list answer support focus return list relevant note task retrieve entire document strongly handicap except combine automatic summarization build abstract relevant document two main result abstract need evaluate readability informative content readability coherence result abstract evaluate accord last point interest answer require human evaluation assessor indicate miss point answer highly incoherent grammatical structure unsolved anaphora redundant informative content answer evaluate accord way overlap relevant follow experiment nine do automatic summarization evaluation data use package university allow directly evaluate base selection relevant general time line available devote fix task overall evaluation methodology base corpus track first list question release test deal hence answer part relevant annotate correct answer among subset short type answer long type along evaluation program write available active order facilitate focus program convert run submission format format also available use corpus task eight link track section briefly discuss link track detail five overview third year link track focus focus link encourage utilize different identify anchor link collection use three randomly select task nominate task assessment tool evaluation tool revise improve efficiency run evaluate run evaluate manual assess set well new metric introduce two new task involve link te ara encyclopedia te ara first task cross link entire collection second te ara task link collection te ara run yet evaluate result suggest exist automatic link discovery produce better quality link presently task task article randomly select filter document size number outgo link ensure suitability document task task total nominate manually assess task assume recommendation task system produce rank list source anchor anchor set target best entry point user choose evaluation consequently base combination score anchor score target description metrics see six group participate task run submit run evaluate grind truth run additionally evaluate several different ways include conventional link focus link discovery te ara encyclopedia contain article collection never link even manually page name indicative content te ara divide contain several page chain together task link entire encyclopedia new approach expect neither link mine page name match likely effective two group participate te ara task seven run submit run validation tool introduce tool display source document anchor highlight target document list link run run pool assess completion pool contain outgo incoming link assessment tool supply facilitate manual assessment assessment slow laborious task log suggest take four per topic effective approach figure one show evaluation do use grind truth manual link generate careful construction user automatically match page name either way link relatively easy find accord experiment link discovery good exhibit high precision level point recall scalable base link mine approach algorithm use divergence topical frequent phrase source document outperform detail description find five discussion focus shift link validation tool introduce metrics adapt new task high quality link figure one evaluation use automatic leave evaluation use manual right discovery achieve use exist approach manual assessment suggest possible automatically identify higher quality link however remain room improvement nine mine track section briefly discuss mine track detail twelve aim task mine document perceive effective solution improve data management facilitate better information retrieval data index data integration query process eleven due inherent flexibility structure semantics mine useful information document face new challenge well benefit aim mine track one study assess potential data mine deal generic task structure domain ie classification cluster document two evaluate cluster approach context information retrieval mine track include two task one unsupervised cluster task two classification task cluster task require group document cluster without knowledge cluster label use unsupervised learn algorithm hand classification task require label document know class use supervise learn algorithm train set cluster task launch explicitly test cluster hypothesis six state document cluster together similar relevance give query use manual query ad track cluster hypothesis hold true suitable cluster achieve cluster solution minimize number cluster need search satisfy give query classification task focus evaluate use external structure collection ie link document along content information internal structure document document multiple precisely track compose multiple label cluster task goal associate document single multiple cluster order determine quality cluster relative optimal collection selection goal give set query multiple label classification task goal find single multiple document task consider context train phase whole graph document know label part give test collection cluster task collection consist million document label set information need ie ad track query answer information need ie manual ad track order enable participation minimal collection provide various document representation term frequent phrase document various structure form tree link name total term collection apply stem removal elimination term occur single document total unique entity tag unique link collection total contain document except document subset contain category information derive use ontology sixteen appear follow power law distribution subset collection contain document also use cluster task team unable process large data collection set document link document also use test collection classification task link correspond link provide author article total unique term subset collection document belong correspond provide label twenty document train set corpus compose direct link correspond document corpus document concern link average result ask submit multiple cluster contain different number cluster cluster task submit cluster index es document collection set cluster evaluate two mean firstly evaluation assume classification document sample know ie document class label submit cluster compute purity measure recall cluster consider cluster belong category majority document important note class label use process cluster purpose evaluation cluster result new mine track cluster also evaluate determine quality cluster relative optimal collection selection goal task evaluate well cluster able group large document collection optimal manner order satisfy query search space total use ad track evaluate quality cluster generate full set collection million document total use evaluate quality cluster generate subset collection document total number document find relevant manual cluster cumulative gain use calculate score best possible collection selection accord give cluster solution total six research team participate cluster task two submit result subset data detail result give twelve expect cluster number divide data set increase performance team base purity score increase base score decrease analysis result various show recall come first cluster confirm hypothesis good cluster solution tend average group together relevant result previously unseen query classification ask submit multiple category document test set evaluate much find correspond real document category compute score measure ability system find relevant also compute average precision score list return document measure ability system rank correctly relevant five different team participate task team except one team able achieve micro macro score obtain range team detail result give twelve ten complete seven track track cover various focus retrieval wide range information retrieval task report touch upon various approach apply task effectiveness formal proceed publish springer series four volume contain track overview paper well paper participate group main result however great number test use future experiment see excite change track continue task study effect change document collection sense longitudinal study three however year innovation range interest news task discussion still ongoing time write entirely volunteer run initiative anyone interest idea time spend make impact reference one g p de overview entity rank track al four two n c p daffodil integrate support search federate digital conference research advance technology digital page three j r j overview ad track al four four j focus retrieval evaluation international workshop initiative evaluation retrieval springer berlin five overview link track al four six n c j van use hierarchical cluster retrieval information storage retrieval seven g doucet overview book track al four eight g n j towards collective gather quality control relevance proceed annual international conference press nine performance confidence estimation automatic summarization page association computer linguistics ten v e x p overview question answer track common task focus automatic summarization al four eleven r data mine process song handbook research text web mine idea group twelve r c de l p overview mine track cluster classification document al four thirteen n r n k n overview interactive track al four fourteen r f g yawn semantically annotate corpus twelve business web page fifteen r overview efficiency track al four sixteen f g g core semantic knowledge international world wide web conference new york press seventeen r overview efficiency track j workshop volume lecture note computer science page springer eighteen e question answer track journal natural language engineer nineteen e e j simple efficient sample method estimate proceed annual international conference page