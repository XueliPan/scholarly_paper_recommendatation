robust decision tree algorithm data set v abstract propose new decision tree algorithm class confidence proportion decision tree robust insensitive size class generate rule statistically order make decision tree robust begin express information gain metric use term confidence rule allow us immediately explain information gain like confidence result rule bias towards majority class overcome bias introduce new measure class confidence proportion form basis generate rule statistically design novel efficient approach use fisher exact test prune branch tree statistically together two change yield classifier perform statistically better traditional decision tree also tree learn data balance well know sample claim confirm extensive experiment cart one introduction several type distinct advantage easily interpretable especially true data mine set high dimensionality data often mean apriori little know underlie mechanism generate data decision tree perhaps popular form fifteen recently however base association rule also become popular nineteen often call associative associative use association rule mine discover interest rule train data set rule discover constitute classifier canonical example associative classifier classification base fourteen distribute high performance compute school information university university dame dame lak use minimum support confidence framework find rule accuracy associative depend quality discover rule however success decision tree associate depend assumption equal amount information class contain train data binary classification similar number instance positive negative class generally perform well hand train data set tend class distribution type classifier bias towards majority class happen accurate prediction typically relate minority class class usually greater interest one way solve imbalance class problem modify class train data minority class majority class instance smite five use increase number minority class instance create synthetic sample smite seven integrate boost sample better model minority class focus difficult sample belong minority majority class nonetheless data sample way deal class specifically design data orient perform well original unmodified data set example variation associative classifier call nineteen show outperform fourteen thirteen data set downside generate large number rule seem feature associative negate many advantage classification eight distance use decision tree split criterion show insensitive towards class distribution skewness compare discuss extensively section suffice state base likelihood difference base likelihood ratio order prevent tree data decision tree use form prune traditional prune base error node prune predict error rate decrease prune technique always perform well data set four show prune detrimental effect learn data unauthorized reproduction article prohibit set since lower error rat achieve remove branch lead minority class leave contrast prune base fisher exact test check path decision tree statistically path prune add advantage every result tree path rule also statistically main insight main insight paper summarize follow let x attribute class let x x two rule confidence p q respectively express information gain term two abstractly f p q f abstract function show split measure base confidence bias towards majority class innovation use class confidence proportion instead confidence abstractly x x r define new split criterion table one example analysis x c b attribute c b b c n x instance performance associative depend quality rule discover train process demonstrate set confidence bias towards majority class suppose train data set consist n record denote x x class form table one rule selection strategy find rule support confidence rule x confidence define x x x c f r stand confidence support similarly main thrust paper show robust class imbalance behave similarly class balance one approach replace conventional split measure generic mechanism traditional decision tree base balance data assumption apply decision tree algorithm check degree impurity inside partition branch cart rest paper structure follow section two analyze factor cause perform poorly data set section three introduce measure split attribute decision tree construction section four present full decision tree algorithm detail incorporate use fisher exact test fet prune wrapper framework utilize sample introduce section five experiment result analysis present section six conclude section seven research two analyze metrics use context data first show rank rule base confidence bias towards majority class express information gain index function confidence show also suffer similar information gain specific mean x x x c c equation suggest select highest confidence rule mean choose frequent class among instance contain antecedent ie x example however data set since size positive class always much smaller negative class always b c suppose positive class give data affect distribution without loss generality assume nearly equally distribute hence data b small c large even suppose occur x frequently c unlikely less positive class size much smaller negative class size thus surprise term equation always tend lower bound term equation appear even though rule x may easy high confidence value hard confidence good rule x significantly bad rule x low confidence classifier build process good rule may rank behind rule higher confidence predict majority class fatal error since class problem often minority class interest unauthorized reproduction article prohibit traditional decision tree decision tree use information gain decide variable split fifteen information gain split node define entropy entropy ni n represent one split assume two ni number instance subnote n stand total number instance classification entropy node define entropy p log p j represent one two class fix train set first term equation fix number instance class ie p equation attribute end challenge maximize information gain equation reduce maximize second term node split two two correspond x x instance node two class denote equation rewrite ni n entropy figure one approximation information gain formula form x x equation information gain x x close five highest x x reach one zero either zero one minimize x x close five note x close zero x close one x close zero x close one therefore information gain achieve highest value either x x highest confidence either x x also highest confidence n n entropy p log p log p entropy entropy ni n p log p log p entropy p log p one p log one p n note probability give x equivalent confidence x q log q one q log one q p log p one p log one p n n n q log q one q log one q n log one p log one q n p p x support x p x support x x denote x p denote x q hence x one p x one q ignore fix term entropy equation obtain relationship equation first approximation step equation ignore second approximation first term entropy transform addition base equation distribution information gain function x x show figure one information gain maximize x x close therefore decision tree split attribute whose partition provide highest confidence strategy similar mechanism association analyze section data set high confidence rule necessarily imply high significance data significant rule may yield high confidence thus assert split criteria suitable balance data set note term p equation poor behavior however p also appear decision unauthorized reproduction article prohibit table two confusion matrix classification two class instance actual positive actual negative predict positive true positive false positive predict negative false negative true negative tree measure example index define cart two express one p two j thus decision tree base cart suffer class problem propose another measure robust data situation three class confidence proportion fisher exact test identify weakness framework factor result poor performance entropy index position propose new measure address problem class confidence proportion previously explain high frequency particular class appear together x necessarily mean x explain class could overwhelm majority class case reasonable instead focus focus class find associate class way instance partition accord class contain consequently instance belong different class impact end define new concept class confidence find interest class x x main difference traditional confidence denominator use instead x focus class notation confusion matrix table two express x f n x f f p f p traditional confidence examine many predict instance actually class balance b class figure two information gain original entropy data set follow different class compare contour line b shift towards precision focus many actual instance predict correctly recall thus even many negative positive instance data set n f affect imbalance consequently rule high regardless whether discover balance data set however obtain high rule still insufficient solve classification necessary ensure class imply rule high confidence interest correspond alternative class therefore propose proportion one class measure interest class call proportion rule x define x x x x rule high mean compare alternative class class rule imply higher consequently likely occur together rule regardless proportion class data set another benefit take proportion ability scale one make possible replace traditional frequency term entropy factor p equation detail replacement entropy introduce section four robustness evaluate robustness use isometric plot propose ten inherently independent class misclassification cost roc space span false positive rate true positive rate contour mark line constant value split criterion unauthorized reproduction article prohibit condition class ratio metrics robust class imbalance similar contour plot different class figure two contour plot information gain show class eleven respectively clear two figure class become contour tend flatter away diagonal thus give true positive rate false positive rate information gain data set figure much lower balance data set figure follow model relative impurity propose ten derive definition impurity measure equation give class balance b class figure three information gain entropy data set follow different class contour line shift data set become x n n f p f f represent positive rate tree construction least two generate one x one x x f f f n n one f two f relative impurity propose ten imp f n f p f p imp f n imp f p f p f n f p f n f n imp p n first term right side represent entropy node split sum second third term represent entropy two split take p example second term f p imp first frequency measure p alternative way interpret confidence rule x similarly p equal confidence second frequency measure rule x show section term inappropriate data learn p f p f p overcome inherent weakness traditional decision tree apply impurity measure thus rewrite information gain definition equation impurity measure imp f n f p f f imp f f two f imp one one f two f two f imp p n still frequency term replace new isometric plot replacement present figure three b comparison two figure tell contour line remain unchanged demonstrate unaffected change class ratio instance contain node belong class entropy minimize zero entropy maximize node contain equal number class take possible confusion matrix table two plot entropy surface function show figure four entropy figure highest equal since equivalent equally split two class hand difference purer smaller entropy however state section data set pattern traditional entropy become distort figure since entropy insensitive class skewness always exhibit fix pattern pattern traditional entropy balance data situation formalize follow use confusion matrix frequency term traditional entropy p entropy class data set evenly distribute unauthorized reproduction article prohibit traditional entropy balance data set c entropy data set b traditional entropy data set positive negative figure four sum entropy split data set entropy surf b entropy c surface always independent imbalance data apply definition obtain f n n f p f f p thus number instance class pattern entropy traditional entropy importantly pattern preserve entropy independent imbalance data set confirm figure always similar pattern figure regardless class distance relationship divergence two absolutely continuous measure distance respect parameter seventeen eleven form figure five attribute selection distance example illustrate complementary situation distance b c distinguish b p q p q distance base decision tree distribution p q assume technique eight normalize feature value x notation across class distance use capture propensity feature separate class algorithm feature select split attribute produce distance two class distance essentially capture relative attribute value two class respectively follow formula derive eight relate true positive rate false positive rate f two one one f two also show insensitive class eight since two formula without dominate class like distance also base show equation however difference distance distance take square root difference f divergence one class distribution take proportion measurement interest graphical difference two measure show figure five draw straight line line three parallel diagonal figure five segment length origin crosspoint line three f pro f pro point line three f proportional distance unauthorized reproduction article prohibit point view select point parallel line segment therefore figure five point line three distance line four thus point line three higher priority selection attribute rewrite f proportional slope line form data point origin consequently favor line highest slope figure five point line one consider better split attribute line two f f analyze distance term line versus reference frame note distance share common problem give example follow suppose three point f b f c f one line one three b line two three c line two four show figure five b line line three parallel diagonal f f b ie c line line two pass origin distance treat ie b better split attribute c explain point line three longer distance line four contrast consider higher split b c since point line one obtain greater line two however point line three b distance fail distinguish since generate difference circumstance may make noneffective decision attribute selection problem become number attribute large many attribute similar f precisely f difference problem occur measurement test point line two b c solution problem straightforward choose split attribute decision tree construction select one highest default attribute possess similar value basis distance thus figure five priority three point b c since point greater value point b c point b higher distance point c detail section four fisher exact test help select branch tree good discriminate class also want evaluate statistical significance branch do fisher exact test fet rule x fet find probability obtain contingency table x positively associate null hypothesis x x independent nineteen p value rule give p b c min b c b c c b n b c implementation definition handle express value logarithmically low p value mean variable independence null hypothesis reject relationship x word positive association cell contingency table true true negative therefore give threshold p value find keep tree branch statistically lower p value discard tree nod four decision tree section provide detail algorithm modify split criterion base entropy replace frequency term due space limit omit cart approach identical factor replace algorithm one creation input train data output decision tree one instance class two return decision tree one node root label instance class find best split attribute assign tree root root value add branch instance attribute add leaf branch three else four five six seven eight nine ten eleven twelve thirteen fourteen fifteen end else end end add branch build tree original definition entropy decision tree present equation explain section factor p equation good criterion learn data set replace define entropy unauthorized reproduction article prohibit algorithm two discover attribute information gain input train data output attribute split one let zero two attribute three four five six seven calculate distance obtain entropy split set sum entropy zero value v mean number instance value x class j attribute eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen j j j j x v j x v j log f f j log f end j else j end nineteen twenty end end return root true true default algorithm three prune prune base fet input unpruned decision tree threshold output prune one leaf two three four end five six end seven obtain root eight child root nine ten eleven twelve thirteen fourteen end fifteen sixteen end child true p child child leaf set child leaf end else false child f algorithm four set status branch node search input branch node n ode threshold output status branch node one child n ode two three end four five end six true seven eight nine ten eleven end twelve n root full tree thirteen fourteen fifteen end calculate p value node n n true true default false end algorithm five prune nod status input branch represent top node n ode output prune branch set n ode leaf one true two three else four five six seven eight nine end child node child leaf child end end j x x restate conclusion make section decision tree achieve highest value either x x high either x x high process create describe algorithm one major difference way select attribute line five process discover attribute highest information gain present algorithm two algorithm two line four obtain entropy attribute split line six eleven obtain new entropy split attribute line thirteen record attribute highest information gain information gain different attribute distance use select attribute whenever value two attribute unauthorized reproduction article prohibit equal line thus overcome inherent drawback distance section decision tree model treat branch node last antecedent rule example three branch nod root leaf leafy assume follow rule exist leaf algorithm two calculate branch node use precede example previous rule rule leaf way attribute select one whose split generate rule tree highest prune tree creation decision tree fisher exact test apply branch node branch node replace leaf node least one descendant node lower p value threshold branch check significance entire branch expensive operation perform efficient prune design strategy show algorithm three first stage check process leaf root node mark process check prune status do via line sixteen algorithm three algorithm four begin branch nod set default status line three algorithm three line thirteen algorithm four check significance node leave root child node node represent rule node reset original unpruned tree n level deep leave time complexity check process check significance conduct second prune stage prune process perform accord status node root leave branch node replace leaf status true line ten algorithm three line one algorithm five original unpruned tree n level deep leave time complexity prune process thus total time complexity prune algorithm prune strategy guarantee completeness correctness prune tree first stage check significance possible rule path tree ensure rule thus complete second stage prune rule prune tree correct five sample another mechanism overcome imbalance class distribution synthetically delete add train instance thus balance class distribution achieve goal various sample propose either remove instance majority class aka introduce new instance minority class aka consider wrapper framework use combination random smite five six wrapper first determine percentage result improvement decision tree train original data number instance majority class stage improve wrapper explore appropriate level smite take level account smite introduce new synthetic minority class continuously optimize point reader six detail wrapper framework performance decision tree train data set optimize wrapper framework evaluate decision tree experiment six experiment experiment compare fifteen cart two eight nineteen binary class data set demonstrate efficiency split criteria performance prune weka cart twenty employ experiment base implement two normalize effect different experiment carry use fold final result average five run first compare purely split criteria without apply prune prune various decision tree present finally compare decision tree sample split criteria data set mostly obtain eight table three include number data set repository source estate contain state series compound us national cancer institute yeast anticancer drug screen ism five highly unbalance source code data set use experiment obtain unauthorized reproduction article prohibit table three information data set list last column proportion minor class data set instance attribute data set boundary breast cam estate german ism letter oil page phoneme segment splice thirty ten twelve two six sixteen fifty ten sixteen five eight twenty sixty four fifty record information calcification oil contain information oil spill relatively small noisy twelve phoneme originate project use distinguish nasal boundary cam oral sound biological data set sixteen german splice available three remain data set originate repository one originally data set convert keep class minority rest majority exception letter vowel become member minority class majority class accuracy consider poor performance measure data set use area roc curve eighteen estimate performance classifier data set learn experiment want compare effect different split criteria thus decision tree cart unpruned use smooth leave since prove efficient data learn nineteen exclude include table four list area roc value data set classifier follow rank present parentheses data set use test confidence level compare among different nine experiment choose best performance classifier base classifier base classifier statistically table four split criteria data set tree unpruned short test classifier sign test statistically outperform base classifier first two test illustrate significantly better cart respectively third test confirm decision tree significantly better area roc data set cart four two five one three six boundary breast five two four one three six three one four two five six cam four one five three one six one two four three five six estate five three one four two six four one five three two six german four three five two one six ism three one four two five six letter six one five two three four oil five one four two three six page four two four two one six five three four two one six phoneme four one four three two six four three five two one six four one five two three six five one four one three six segment splice four one five three two six four one five one one six rank sixteen base cart base base significantly better another classifier comparison ie value test less five put sign respective classifier show table four reveal even though perform better nineteen overall result far less robust decision tree might possible obtain better result repeatedly modify attempt identify optimize manual parameter configuration shortcoming interest replacement original factor p equation three separate test carry first two conventional decision tree propose decision tree third considerable increase traditional decision tree observe statistically confirm first two test small p value mean could confidently reject hypothesis base classifier show current classifier even though unauthorized reproduction article prohibit table five prune strategy short error estimation prune fet significantly better prune error estimation table six prune strategy number leave leave tree prune fet significantly prune error estimation data set three one three four three three three three three four four four four one four four three one four boundary breast cam estate german ism letter oil page phoneme segment splice thirty fet two three two two two four two one one three one three two three one two three four one base four two four two three two four four four two three two three four two three one one two fet one three one one one one one two two one two one one two three one one three two data set boundary breast cam estate german ism letter oil page phoneme segment splice two four two one two three three one one eighty one two three four two four one three three three base fet three two three three four one one three three three one four two three two three four one two twelve one seventy three one two twenty one four four two two two three one three one three two one four four fet four one four four three two two four four four four two one four one four two two one base base statistically better strategy combine analyze section provide improvement higher confidence comparison prune section compare prune prune base error estimation originally propose fifteen reuse data set previous subsection apply prune prune separately tree build confidence level fet set ie threshold set one note tree scope subsection carry separate test respectively compare different prune exclude comparison since provide separate prune table five six present performance two pair prune tree number leave table six average value five two fold cross statistically tree prune fet significantly outperform tree prune error estimation table five without retain significantly leave table six pattern find tree prune sacrifice number leave obtain significantly phenomenon prove completeness fet prune tree since error estimation inappropriately cut number tree hence always smaller number leave lower value sample compare decision tree sample base discuss section five note wrapper optimize train set use five two cross validation determine sample level algorithm evaluate correspond five two cross validate test set three pair decision tree show table seven first pair original modification either data decision tree second pair sample base use wrapper resample train data use original decision tree build classifier base show performance decision tree learn original data test value show although wrap data help improve performance original decision tree use obtain statistically better directly train original data seven conclusion future work address problem design decision tree algorithm classification robust class imbalance data first explain traditional decision tree measure like information gain sensitive class imbalance express information gain term confidence rule information gain like confidence bias towards majority class identify problem propose new measure class confidence proportion use theoretical unauthorized reproduction article prohibit table seven generate original sample mean apply sample train data learn table cart decision tree significantly better tree original sample base original sample base base data set cart three five two two two three one four one one three two four two two one one two four boundary five six four two one six two one four two breast cam three six five four one six four five two one estate three six five four one one five six four two german six two five four three six five three one two ism six four five one one letter six five three four two oil page six five four one one six four five three one phoneme six five two one three six one five four three six four five two one six four five three two segment five two five four two five one six three four splice six five three two one base geometric show insensitive class distribution embed information gain use improvise measure construct decision tree use wide array experiment demonstrate effectiveness data set also propose use fisher exact test method prune decision tree besides improve accuracy add benefit fisher exact test rule find statistically first author paper acknowledge financial support capital market reference one machine learn two l classification regression tree chapman repository three chang lin library support available vector machine four five data set investigate effect sample method probabilistic estimate decision tree structure five bowyer lo hall smite synthetic minority technique journal artificial intelligence research sixteen one six da lo hall joshi automatically counter imbalance empirical relationship cost data mine knowledge discovery seventeen two seven lo hall bowyer improve prediction minority class boost lecture note computer science page eight da learn decision tree unbalance data proceed conference machine learn knowledge discovery page berlin nine j statistical multiple data set journal machine learn research ten pa geometry roc space understand machine learn metrics roc proceed twentieth international conference machine learn page eleven divergence distance measure signal selection communication technology fifteen one twelve machine learn detection oil spill satellite radar image machine learn thirty thirteen w li j han j accurate efficient classification base multiple rule proceed international conference data mine page computer society fourteen b w aa j li integrate classification association rule mine knowledge data engineer fifteen program machine learn morgan san ca sixteen p ak dunker z classification knowledge discovery protein journal four seventeen review canonical alternative correspondence analysis use distance de eighteen signal detection theory roc analysis psychology diagnostics associate nineteen f use positively associate relatively class correlate rule associative classification seventh international conference data mine page twenty e frank data mine practical machine learn tool record one unauthorized reproduction article prohibit