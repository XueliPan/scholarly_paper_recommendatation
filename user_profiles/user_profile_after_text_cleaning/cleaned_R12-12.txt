text stream one engineer engineer management university hong hong china two computer science hong university science technology hong china abstract recently new class application become widely recognize data model best transient stream rather persistent table disk lead new surge research interest call data stream however report work concentrate structural data seldom focus unstructural data textual document paper propose efficient classification approach text stream propose approach base sketch able stream efficiently scan meanwhile consume small bound memory model maintenance operation extensive experiment use news article collection conduct encourage result indicate propose approach highly feasible one introduction rapid growth information become ever pervasive important new class application become widely recognize data model best transient stream rather persistent table disk result lead new surge research interest call data stream data stream nature huge volume impossible one hold entire data stream memory analysis two store disk fast process desire feasible solution reduce number data scan one bound memory space paper focus text stream classification propose novel text stream classification call extend previous work text classification five approach base maintain two sketch one class sketch two document sketch text stream classification classification assign document multiple class whereas previous work deal text classification extensive experiment conduct use two news article collection quality accuracy efficiency g dong al c berlin text stream table one mean symbol mean feature fi feature x x k fi x number feature x x k number document k n total number class frequency fi number document contain fi k k set class contain feature fi relative importance fi x x k x weight fi wi compare exist text classification approach show achieve high accuracy exhibit low cost low memory consumption order sensitivity model update well document sketch class sketch also study two text stream classification text stream classification kind text classification unique add one document queue stream infinite two require continuously model maintenance document stream may change time time model update necessary maintain high quality classifier three limit memory space provide rebuild classifier operate system space efficiency number scan document must time efficiency update operation must conduct fast note first issue cause last two however last two issue conflict traditional text classification rebuild classifier require feature selection feature weight imply large number data scan document archive high computational cost large memory three text stream classification define two sketch class sketch document sketch class document sketch approximator class incoming document let k document class respectively correspond document sketch class sketch respectively high similarity indicate di belong hence quality efficiency rely sketch approximate distinctive feature fast performance information sketch minimize information sketch include feature appear document archive h sketch text stream classification table one show list mean class sketch document sketch k k k k k k give similarity measure coefficient wi wi k w two k wi wi k coefficient choose express degree overlap proportion overlap provide intuitive practical fitness model give n class first compute follow equation one sort class sketch j finally determine top class sketch e e k e e e two two one two class sketch generation weight wi k fi k compute follow wi k k two w c two k w c two two k two two use normalization zero wi k one w k within class coefficient use measure relative importance fi within k w k k one one w k reflect fact feature appear frequently within class critical term classification equation four numerator denominator logarithmic frequency feature appear many document rare similar find also report seven coefficient use measure relative importance fi among class one log n log n one two three four five text stream zero one n w k w k one log n use note give ratio w k class k contain fi maximum w k hence use gather total importance feature across class maximum use average summation value feature regard important many class feature obviously important classification give global view across class suppose two feature fi fi appear class appear n class two case n obviously provide far precious information text classification fi word feature valuable occurrence skew n case argue realistic weight importance feature fi simply count number class fi appear seventeen assign higher weight feature document class contain coefficient use measure average portance fi individual document k six seven eight nine k k k k k one one k one one w k note feature appear n time imply n time important therefore take logarithmic relationship rather linear relationship term within bracket average weight fi among document k average estimator k use determine suitability average instance give two feature fi fi appear document appear document confident declare average fi likely reflect true status unlike w k design class level k handle importance feature within individual document introduce reduce discrepancy among weight increase recall precision model memory consumption need keep k k memory also easily maintain model reflect new feature text stream h document sketch generation handle text stream need figure feature importance usually do consider feature distribution within incoming document practice may need deal text stream document basis one document time thus sample size insufficient document sketch propose estimate true significance feature incoming document ai average weight fi class similar equation nine ai n n k k one one w c n n k one one w however insufficient assume every incoming document contain fi share distribution word risk use equation ten thus geometric distribution formulate one one one ai wi ai finally wi compute combine equation ten fourteen four relate work section two text classification approach naive bay support machine algorithm compute posterior probability unseen document give particular class merit lie little consumption computational build cost linear size train set operation cost also low base independent assumption word store feature appear class fourteen consequently possible update model continuously however require text feature selection eleven else quality poor note feature selection need scan feature document scratch model update ten eleven twelve thirteen fourteen fifteen text stream base statistical learn theory solve pattern recognition problem sixteen attempt generate decision hyperplane maximize margin positive negative train set use quadratic program approach two base knowledge best algorithm term accuracy however build cost quadratic size train set two also require feature weight else performance degrade eight make much appropriate text stream classification memory consumption let n number document average number feature document c total number class furthermore feature digitalize class need store document frequency total term frequency feature memory consumption feature class twelve four feature four document frequency four term frequency recall feature may appear multiple class total memory double twelve c feature associate weight corpus require store build classifier matrix need require additional four hence require c eight n four small bound memory need feature store weight document frequency total eight thus twelve c need half worst case scenario note binary classifier space would reduce significantly five experimental experiment conduct sun physical memory run feature stem punctuation mark number web page address address ignore measure quality classification recall precision use eighteen follow use denote respectively x belong recall precision data set two news collection receive use test table two summarize take select class least one document train test document assign one class document assign multiple class corpus highly skew figure one summarize distribution h ten twenty thirty forty fifty sixty seventy eighty ninety two four six eight ten twelve fourteen sixteen eighteen twenty five ten fifteen twenty thirty forty class class class b c n e c f r e b n zero zero n e c f r e b n zero n e c f r e b n zero zero n e c f r e b n zero n e c f r e b n zero zero n e c f r e b n zero two three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen number class two three four number class two three four five six seven eight nine ten eleven twelve thirteen number class e f fig one document distribution highly skew b evenly distribute c slightly skew f number document multiple class table two summary corpora use train test class feature ninety twenty document assign one class document assign multiple class document evenly distribute among different class figure one summarize distribution large set news article archive directly task assign news article morgan capital international code note already article correspond class figure one summarize distribution multinomial mixture model use feature select use information gain choose base one recall classifier update require feature calculate take computational cost use package weight feature calculate scheme normalize unit length thirteen classifier update base newly receive document text stream one two three four five six seven eight nine one two three four five six seven eight nine one two three four five six batch number batch number batch number b c e n e v c e f f e eight seven n c e e p c ten one one b k r e ten one e n e v c e f f e seven six five n c e e p c ten one b k r e ten one e n e v c e f f e n c e e p c seven six five four three b k r e ten one one two three seven eight nine one two three seven eight nine one two three four five six five four six batch number five four six batch number batch number e f one two three seven eight nine one two three seven eight nine one two three four five six five four six batch number five four six batch number batch number g h fig two classification efficiency b c show accuracy term e f show time g h show memory consumption previously learn support fifteen classifier update require recalculate feature weight lead overhead implement use line implementation neither feature selection weigh necessary thus extra necessary classifier update text stream classification quality document divide n batch build use first batch evaluate use second batch update classifier use second batch evaluate use third batch forth table three show average accuracy three n ten accuracy similar superior particular perform significantly well term recall although outperform term accuracy low h table three result text stream classification method table four result large batch test method cost classifier update operation make highly recommend text stream classification figure two b c show three whenever new batch arrive increase first batch become saturate batch obtain relatively sufficient sample corpus case always perform inferior two accuracy similar show similar omit due limit space figure two e f show cost include classifier rebuild cost use three data set note outperform significantly figure two g h show estimate memory consumption use estimation give previous section large batch test section take whole single large batch word assume large number document come together small time window table four figure three summarize result note cost include cost maintenance cost analysis four detail analysis conduct accuracy different class sketch b average estimator document sketch c classifier rebuild necessity sensitivity document arrival order text stream n c e e p c ten one one two three four five six seven eight nine n c e e p c ten one n c e e p c train classification train classification train classification b c fig three efficiency text classification table five experiment setup table six result test use w k use use k five seven eighteen one two three four five six seven eight nine class sketch nine experiment conduct table five examine significance w k k indicate k use without k table six show result use nine include perform best use either w k one two give unsatisfactory result combination yield improvement three ignore average estimator yield inferior result table seven document sketch test document sketch model document sketch influence entire feature distribution table seven show necessity influence denote set one result show use whereas play important role text stream classification denote classifier rebuild three different case test h table eight order sensitivity test one two three four five six seven eight nine ten seventy table nine accuracy classifier rebuild one two three rebuild classifier whenever document arrive note neither handle perform expensive document rebuild random manner size batch vary experiment simulate situation classifier need rebuild certain time interval rebuild number document batch fix table nine show result average accuracy best suggest continuously classifier rebuild increase accuracy text stream classifier long term difference term average accuracy order sensitivity test whether sensitive order document arrival use place document queue randomly repeat test describe repeat whole process ten time report average table eight show result since standard deviation small conclude may sensitive order document arrival six conclusion future work paper propose novel text stream classification approach need generate sophisticate model main advantage one could achieve high accuracy two model rebuild efficiently regardless size batch three small bound memory require four advance document necessary text stream future work extend research several one study time model rebuild two automatically remove unnecessary feature three model resistance noise four study possibility comb text stream data stream work describe paper partially support grant research grant council hong special administrative region china reference one v fast accurate text classification via multiple linear discriminant proceed large conference two n j support vector machine learn university press three r p e hart g stork pattern classification interscience edition four w b r information retrieval data structure prentice hall five g p c j x w lam automatic stock trend prediction real time news proceed workshop data mine model six w r theory term weight base exploratory data analysis proceed international conference research development information retrieval page seven j holt efficient mine association rule text proceed international conference information knowledge management page eight text categorization support vector machine learn many relevant feature proceed conference machine learn page nine lewis evaluation phrasal cluster text categorization task proceed international conference research development information retrieval page ten lewis naive bay forty independence assumption information retrieval proceed conference machine learn page eleven k comparison event model naive bay text classification workshop learn text categorization twelve h scalable text classification proceed international conference information knowledge management page thirteen g c approach automatic text retrieval information process management five fourteen f machine learn text categorization compute survey one fifteen n h k k sing incremental learn support vector machine proceed international conference knowledge discovery data mine page h sixteen v nature statistical learn theory springer seventeen k automatic text classification method simple approach natural language process pacific rim symposium eighteen yang evaluation statistical approach text categorization retrieval twelve one nineteen yang x text categorization proceed international conference research development information retrieval page