compiler parallelization c program multicore multiple address space institute compute architecture institute compute architecture school university school university mob abstract paper develop new approach compile c program multiple address space integrate novel data transformation technique expose processor location partition data parallelization strategy combine new address resolution mechanism generate efficient program run multiple address space without use message pass approach apply suite evaluate four processor board show outperform exist approach give average parallel subject program optimization general term experimentation measurement performance data partition address resolution multiple address space compilation one introduction although multicore offer method achieve high performance port exist parallel currently complex commercially available take exist sequential program map automatically onto machine seventeen instead typically require rewrite code process network set communicate permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee three thirteen beach copyright sequential process twelve approach well know highly possibly introduce deadlock rewrite application parallel manner highly specialize skill need tool take exist program map automatically onto new architecture efficiently although twenty research compilation scientific compute five six seven two take place embed domain due two main reason program write c rather seven make extensive use pointer arithmetic multiple address space difficult compile overcome first problem use array recovery technique develop work three second problem solve combine novel data transformation scheme expose processor id remote data new address resolution technique use local pointer array track remote data low overhead technique greatly simplify code generation produce code similar produce space parallelization five without introduce complex potentially message pass code embed two overall parallelization strategy develop c compiler outperform exist approach multiple address space embed paper structure follow section two provide motivate example follow section three detail description scheme section four evaluate approach follow short review relate work conclude remark two motivation example approach exploit fact although machine typically multiple address space part processor memory space visible unlike pure machine however unlike single address space machine origin processor must know identity remote processor location memory require data value example figure one one show global memory map system comprise processor processor typically internal address space access local data access purely local reflect external bus addition form global address space processor assign certain range address global address space use access remote data global address equivalently remote processor identity data local address must know develop novel technique combine single address space parallelization approach novel address resolution mechanism base simple data structure determine processor memory location example figure one global memory map one example figure two three illustrate main point paper code figure two box one typical c program write hybrid two program n real update convolution suite use pointer traversal well know idiom nineteen form however prevent many optimize perform aggressive optimization prevent attempt parallelization second box two figure two show program array recovery three replace array reference base loop parallelization partition data computation across processor nod straightforward one array dimension one loop partition number example assume four standard partition five would generate program box three single address space machine would sufficient access remote data space machine however processor loca partition data need explicitly available scheme achieve data fourteen array form two dimensional array whose inner index correspond four allow processor location partition data explicitly available instance array partition zero zero seven reside processor zero one zero seven reside processor one similarly array c b similarly partition iterate work allocate zero seven result code show figure two box four b show simplify presentation later transformation stag need generate separate program processor partition code processor zero specify z show figure two box five code identical except define z twelve three multiple address space machine require remote data distinct name local thus rename follow zero zero seven become zero seven one zero seven become zero seven similarly array c b processor zero declare variable reside processor declare extern see second third line box six figure two processor one declare local remainder declare extern access local remote data local pointer array set processor simple data structure array contain four pointer assign start address local array four use original name array pointer array initialize pointer array point four distribute array four see box six figure two show figure three array b bold line represent data access processor zero thus local access array remote access array processor zero use original name mean exactly array access form use array c b box four achieve use fact multidimensional array c array array higher dimension array define contain array code generation point view greatly simplify implementation avoid complex difficult message pass three overall parallelization overall parallelization algorithm show figure four pointer array conversion first apply enable data dependence analysis detail see three program pointer free form standard data dependence analysis apply determine program parallel check see amount available justify partition map assume private copy define section c standard paragraph three four multiply parallelize loop trip count number check certain threshold continue original code one zero c b array recovery two zero c b standard partition three c b lo hi one one lo hi c b data transformation expose processor id four four eight b four eight z zero three zero seven z c z z b one program per processor five define z zero four eight b four eight zero seven z c z z b address resolution local pointer array six define z zero eight extern eight eight eight remote local eight extern eight eight eight remote local four pointer array b four pointer array zero seven z c z z b figure two example show data transformation address resolution scheme zero seven zero seven b b b b zero one two three figure three data layout array b figure two box five processor local pointer array content pointer array processor zero illustrate bold line represent data access program figure two box six one convert array two pointer free perform data dependence anal determine data partition partition transform data code perform address resolution figure four overall parallelization algorithm exploit data parallelism within program partition data computation across choose best data partition study many five paper use simple method base exploit parallelism reduce communication sixteen later work use sophisticate analysis key point partition approach use data make processor identifier explicit exactly need statically determine whether data local remote notation loop represent column vector j number enclose loop note loop need perfectly nest occur arbitrarily program loop range describe system define polyhedron iteration space b data storage array also view polyhedron introduce array indices describe array index space space give polyhedron ai assume reference array write integer matrix vector thus figure two box two array declaration represent one one zero one ie index range zero loop bound represent similar manner subscript simply one zero partition determine index dimension may evaluate parallel general give number therefore use simple technique reduce communication base data alignment two array reference every element certain dimension correspond index space point say align ie j b k align first index second two array align respect particular index matter individual array partition reference respect index always local partition base alignment try maximize row equal subscript matrix let x define follow x one x x zero zero otherwise function determine whether two non zero equal function h define h one x measure well particular index array read assignment align assign array one index value h calculate index highest value one partition along technique apply across general conflict partition currently consider nest loop dominate execution time calculate value h across different parallel indices highest value determine index partition along construct partition matrix p define pi zero four row identity matrix also construct sequential matrix contain indices partition p example figure two one index partition along therefore p one zero five map two three data matrix define p p p number example figure two p four detail transformation framework see fourteen transformation use expose processor id array reference critical scheme show let map transformation thus partition indices sequential indices leave alone example one zero eight new array indices give new array bound find use transform bound equation one one zero zero one one zero one zero two zero zero three seven six seven six seven six seven six seven two three three four five ie four eight new array also find general without loop introduce array access example figure two would four c four four b four four five four however framework fourteen always generate suitable recovery loop transformation case transformation original loop bound b figure two box two give ie range zero apply enclose loop give new one one zero j j new bound b j b find use b transform bound equation twelve one zero zero one zero one zero one two zero zero three seven six seven six seven six seven six seven two three three ie zero z three zero seven show box four figure two update access matrices array five five four four ut one one zero zero one six seven nine ten eleven twelve thirteen fourteen fifteen sixteen array indices partition decide data indices base partition matrix p matrix give new array indices z result code show figure two box four expose processor id reference without expensive subscript one program one p j one p j insert extern b insert type insert type p j one p one insert insert figure five address resolution address resolution array partition across several local partition give new name single address space support furthermore name must distinct two name distinct assume private therefore introduce new name equal old name follow processor identify number thus four processor system x replace four local array wish one array reside allocate local processor remainder declare extern allow remote reference address resolve linker order minimize impact code generation pointer array size p introduce point start address p unlike pointer array replicate across p initialize array statement begin program complete algorithm give figure five function insert insert figure two box six show insert two array b remain array omit due lack space change type declaration whenever array pass function type declaration change must propagate apply transformation code modification require synchronization beyond scope paper describe synchronization placement though essential correct execution mark use graph base algorithm insert minimal number barrier fifteen four experimental framework best publicly available c compiler produce data parallel c cod sequential c input file require port parallel library target architecture algorithm thirteen compiler evaluate suite eleven contain program execute board cluster four share external bus external program compile compiler version full optimization time cycle accurate addition partition address resolution scheme describe paper locality exploit transfer also apply beyond scope paper detail see four performance parallelism nineteen thirty program parallel nine nineteen determine profitably technique either sequential exhibit little work parallelize reject stage two overall algorithm see figure four figure six show approach scale number three select program compress fir mult ten ten mult ten ten contain ten ten array yet distribute data across available case data size full potential parallel execution exploit give linear number figure six different number due parallelization overview select program sequential show figure seven program benefit approach show substantial range linear close linear achieve four program compress edge detect whereas one program filter experience due better sequential code generation parallelization remain three program scale well number still due parallelization least fifteen average across four five relate work good overview exist parallelization give al six distribute share memory subject al five although machine must seven reference one hardware specification mass two j lam data computation three b array recovery embed compute two two four b combine program recovery locality analysis c program embed appear pact five r r cox de n data distribution support distribute share memory las six r k v compilation parallel parallel compute thirteen seven k compile machine eight eight j b singh environment design conference nine h exploit fine parallelism embed program pact ten compile two fourteen eleven lee suite twelve ea lee process network proceed five may thirteen combine parallelization increase adaptability efficiency hunt fourteen integrate loop data global fifteen ea compile time barrier thirteen six june sixteen ga data alignment transformation reduce communication distribute memory scalable high performance compute conference seventeen e b compilation process network case eighteen j l uniform design parallel program circuit june nineteen v c h methodology signal process technology figure seven four incorporate data distribution data locality increase two face problem multiple address space share memory describe ten al seven method establish share memory however approach assume separate distribute address space require complex bookkeeping two data tile use improve spatial locality representation allow easy integration loop data early paper eighteen describe program may parallelize give detail experimental result similarly eight interest overall parallelization framework describe mechanism detail parallelization might take place provide thirteen impact different parallelization consider however automatic approach provide nine semiautomatic parallelization method enable exploration different base move architecture present however integrate data partition strategy available single address space assume example cod furthermore experiment communication model simulator thus issue map parallelism combine distribute address space address six conclusion paper develop new compiler parallelization approach map c program onto multiple address space use novel data transformation address resolution mechanism achieve average four program contain sufficient parallelism code generate easy read amenable sequential optimization future work consider integration consider integration task pipeline parallelism