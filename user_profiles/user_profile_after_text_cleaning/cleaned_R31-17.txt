bypass cache make scientific good network malik burn department computer science university department computer science engineer university dame dame abstract scientific geographically distribute network bind thus could benefit proxy cache however exist cache suitable compare join large data set exist reduce parallelism conduct distribute query single cache lose data reduction benefit perform develop formulation cache reduce network traffic preserve parallelism data reduction cache altruistic cache minimize overall network traffic generate federation rather focus local performance present adaptive algorithm manage cache also develop make deterministic algorithm randomize algorithm minimal space complexity verify efficacy cache run trace collect sloan digital sky survey prototype implementation one introduction increase number science publish make data available community distribute oceanographic data system fourteen use publish archive comprehensive experiment involve merge join compare scientific increase scale number network constrain performance share network particularly interest network performance work support part award doe award corporation mediation use world wide telescope virtual telescope temporal experiment exemplar scientific federation support query across vast amount data fifteen face impend crisis ten sit network performance limit responsiveness throughput already expect federation expand sit cache principal solution performance exist cache fail meet need scientific cache dangerous technology reduce parallelism data filter benefit thus cache must apply judiciously query federation divide member sit evaluate parallel parallel evaluation bring great computational bear experiment initiate cache reduce parallelism move many cache run query also filter result four produce compact result large table many scientific query operate large amount data bring large data cache compute small result waste arbitrarily large amount network primary goal current cache three eighteen maximize hit rate minimize response time single application minimize network traffic secondary goal direct motivation reduce network traffic charge amount consume however imperative focus good network use share conscientiously generate make unwelcome public network propose cache altruistic cache framework scientific principal goal adopt network citizenship cache data order minimize network traffic cache profile differentiate data object cache save network cache latter rout directly proceed international conference data engineer experiment show framework lead overall network traffic reduction eleven paper model cache problem develop economic satisfy primary goal network traffic reduction also meet proxy cache specifically independence scalable implement telescope introduce concept yield employ cache yield model differ classical cache page model object model cache consider amount data deliver application per request basis page model cache contain object fix size page cache hit occur application read entire object memory operate use page model thirty object model expand upon page model account variable object size nonuniform distance data source cache hit involve access entire object cache object model apply web cache six eighteen distribute file store seventeen yield model follow object model object vary size fetch cost however access object cache see variable benefit depend upon many data request return yield cache query may return partial result base selectivity criteria may return aggregate compute object define metrics cache specifically develop hit rate generalize concept hit rate yield model measure rate cache object reduce network traffic normalize size amount space consume cache use evaluate utility cache cache management employ metric eviction load yield model lead naturally formulation bypass cache cache management make economic minimize network traffic choose load object service query object cache versus bypass cache send query evaluate sit federation network currency economy algorithm invest network traffic load object cache order realize long term save give request load object use network evaluate query ship query result bypass concept closely relate hybrid ship similar employ query base metrics develop algorithm cache management al profile query data object evaluate rate network traffic reduction make bypass versus decision query occur object present cache algorithm compare expect rate save outside object minimum current rate save object cache object outside cache penalize network cost fetch federation query object rate exceed minimum bypass age prune allow algorithm adapt shift keep compact efficient algorithm work well practice lack desirable theoretical particular depend upon degree stability use prior indicator future access also maintain form query profile object federation address theoretical make pattern present algorithm use principle load object cache bypass query generate network traffic equal load cost also present randomize version algorithm minimum space complexity choose load object cache probability proportional yield current query implement experimentally evaluate performance use astronomy collect sloan digital sky survey also compare performance competitive cache without bypass optimal static cache execution without cache result indicate approach performance optimal static cache experimental result also address question cache scientific compare cache table semantic query cache semantic cache attractive preserve filter benefit fact semantic cache lie outside framework ie depend query evaluation within cache however find astronomy exhibit query reuse query containment upon semantic cache rely rather astronomy exhibit schema reuse conduct query similar schema different data example common query iterate sky look object specific thus employ table object cache framework two relate work cache address vital issue cache replacement cache object granularity cache consistency proceed international conference data engineer cache section review relate work cache replacement choice object cache concept bypass cache general page problem page vary size fetch cost goal maintain cache fix size minimize total fetch cost cache miss three page use memory buffer management ten eleven nineteen cache page uniform size cost cache replacement commonly use thirty among use single basic property reference stream order minimize number page fault algorithm introduce variable fetch cost page uniform size algorithm six eighteen extend object model object variable size fetch cost access object assign utility value equal ratio utility value age time keep object high temporal locality cache find work well practice nine eighteen squid web proxy incorporate modify version proxy cache also employ object model variable size fetch cost object may query result object attribute materialize view seven much attention pay cache organization integration query optimization different application sixteen twenty however use simple cache eviction heuristic simple many cache use reference information make better thirty extend incorporate information last k reference popular page disk buffer outperform conventional buffer discriminate frequently infrequently reference page extend include frequency count utility measure result show outperform trace algorithm use frequency count similar object reference stream cache currently eighteen give consider bypass cache object model mention option able bypass request ie serve miss without first bring request object cache help reduce cost request sequence however true cache size query result may much smaller size object al take advantage variable query result size cache admission policy improve performance objective minimize execution cost query sequence neither apply di wan lan server cache client ad figure one network flow pair use bypass cache objective minimize network cost several industry develop cache relational object one forty dynamic materialize view may overlap two cache use simple compare hierarchical squid underline benefit cache paper extend web cache problem incorporate concept yield concern object variable fetch cost size also query object return result variable size cache present extend benefit cache web environment complex environment three cache model formulation bypass cache include query schedule network citizenship federation collocate cache service mediation mediator receive query federation divide query single site federation cache evaluate whether service locally load data cache versus ship evaluate server federation term latter bypass query query result circumvent cache bypass cache semantic cache twelve attempt reuse result individual query cache elect bypass query order minimize total network cost service query assume cache reside near client network try minimize data flow cache client combine cache act independently global problem reduce individual cache figure one time consider cache cache within network traffic minimize wan bypass flow server directly client plus proceed international conference data engineer load object cache client application see query result data da cache ie da traffic query serve local cache local area network share resource scalable lan traffic factor network citizenship develop concept yield measure effectiveness bypass cache environment yield query number query result measure network cost bypass query network save query serve cache compose object size load cost yield lead metrics cache hit rate define one metric measure benefit cache object ie rate network reduction per cache metric evaluate base object thus every object system regardless whether cache define j pi two si one object size si fetch cost fi access query query j occur probability pi j yield j decompose two first arise yield j pi measure different query benefit cache object number j pi j normalize deliver application amount cache space si component prefer object would yield cache thus network save per unit cache space occupy second component describe variable fetch cost fi different object different source normalize object size si component help evict object lower fetch cost reload object cache less expensive often fetch cost proportional object size fi constant c true one cache object single server two cache object multiple collocate three network uniform performance proportional assumption also rely network exhibit linear cost scale object size true network transfer size substantially frame size use simplification call utility define rate evaluate cache management degenerate hit rate object constant size yield equal object size object model degenerate yield equal object size generalization extend also result achieve bound competitive page page object eighteen model key challenge employ propose metrics lie evaluate query one approach estimate base observe access pattern employ estimate algorithm develop age prune age allow estimate adapt change prune limit amount different approach create perform well access pattern rather predict estimate use economic manage cache content take approach four algorithm section develop algorithm use utility metric algorithm trivially extend use hit rate multiple source nonuniform network describe one cache eviction base performance object cache two bypass versus cache load decision base comparison object cache current cache state combine form algorithm manage cache algorithm compare expect network save cache current performance cache express rate save network per unit time per unit cache space occupy time relative measure number query second estimate access use predictor recency frequency important estimate object cache maintain frequency count use probability estimate enforce recency evaluate frequency cache lifetime object cache issue recency complex use divide past represent cluster access data object within episode use frequency count estimate access probability division enforce recency compute rate save estimate cache consider weigh recent heavily utility object cache measure rate network save realize query object cache lifetime construct continuous time metric measure utility call rate profile j pi j si eviction two generalize previous cache utility metrics simpler cache model page model cache use hit proceed international conference data engineer object define j j ti si cache episode rate save episode three lari e e one te e e five object size si access query query j yield j query occur time ti evaluate time object cache ti time object load cache rate profile measure object cache lifetime access object increase time decay rate profile compact easy update simplify complex access pattern average rate network save query service cache track sum yield allow us evaluate rate profile time rate profile different compare select evict cache item rate profile rate save thus create space new object cache discard current time decay unused age cache retain specific time access thus weight towards recency within scope object cache lifetime however lower rate represent lower average save bypass decision object cache compute utility base past performance estimate future network save rate lar construct metric base estimate utility object load cache lar express save rate cache thus lar may use compare directly expect performance object cache current performance cache content lar account query compose age compute lar require intermediate incur network cost load cache reduce total network save account reduce rate profile load cost object construct rate profile individual episode e j j e si fi si e start time current episode time episode e subsequent e quantity profile continuous time metric must single value episode take maximum value represent maximum rate save would realize object te e end time episode e maximum value describe balance point network save overcome initial load cost reduce usage object decrease utility finally consider generate expect rate save lari lari ewe six function weight episode e draw object make bypass decision cache receive query compare lar request object minimum cache enough cache object lower make space request object request object load cache otherwise query bypass employ simple economic invest future save load object incur load penalty expect save object cache exceed current save object cache however use evaluate object already cache include load penalty sink cost ensure cache conservative important aspect model object must reside cache long enough recover load investment decision make base compare expect versus current network save use single currency employ simple divide object disjoint episode represent cluster set access object hazard choose incorrectly long utility object get reduce average long interval short object get use enough overcome load penalty episode begin first access object terminate current episode start new episode either four one e c lari e two object access last k query experiment choose c five k first rule extend long rate increase allow decrease rate order survive short idle burst traffic second rule ensure lightly use object last proceed international conference data engineer long observe rate always increase load penalty overcome ie e zero tune carefully support best technique divide experience dictate mandatory deal burst result robust many five present second third suite cache achieve minimum level performance particular show cost always k time optimal algorithm k ratio size cache size possible object cache achieve performance need train representative however expect practice forgo information space efficient algorithm need store information object federation whether cache may prove impractical use power randomization away need store object however accompany performance guarantee amalgamation two one ski rental problem two cache problem next subsection describe know subsection describe prove bind performance finally subsection describe relate ski rental classical problem thirteen skier ski need decide every ski trip make whether rent ski trip buy decide buy ski rent future trip unfortunately know many ski trip make future lack knowledge future define characteristic five well know algorithm problem rent ski long total pay rental cost match exceed purchase cost buy next trip irrespective number future trip cost incur algorithm twice cost incur optimal algorithm one object cache problem would nearly identical ski rental bypass query correspond rent ski load object cache correspond buy ski one difference rent ski always cost whereas yield query differ however algorithm apply problem cost twice optimal cache view restrict version cache query limit return single object entirety formally cache receive request sequence object vary size let size object si cache service request cost zero otherwise either one bypass request server two first fetch object cache service request case incur cost fi former composition cache change latter cache enough space store evict object currently cache create require space objective respond request manner minimize total cost service sequence without knowledge future request eighteen give k competitive algorithm cache call optional page model recall algorithm say competitive exist constant b every finite input sequence cost cost opt b opt optimal k ratio size cache size possible object extend algorithm cache cache problem run instance ski rental algorithm every object query enough query object arrive cumulative yield match exceed size object treat situation request entire object arrive cache next subsection describe formally show k competitive algorithm cache problem receive query sequence query refer single object yield query result size j cache service query cost zero otherwise either one bypass query server cost c two fetch object cache cost fi service query cache define c equal fi si size former case composition cache change latter case cache evict object necessary create storage space objective respond query manner min proceed international conference data engineer next query input sequence refer object yield j initially set zero algorithm cache next query input sequence refer object yield j algorithm cache one one generate next input cache maintain accord cache service cache else bypass server figure two algorithm total cost service sequence without knowledge future query use algorithm cache generate family cache base different cache describe figure two algorithm employ utility metric section four extend trivially maintain cache accord word load evict object time response object sequence present main result section follow theorem every competitive algorithm cache create correspond algorithm cache four two competitive corollary exist algorithm cache k competitive k ratio size cache size object proof corollary follow algorithm give eighteen towards prove theorem state lemma give input sequence let subsequence consist query refer divide sequence group group consist consecutive query si one seven idea cost bypass query equal fetch cost fi query assign probability generate next input cache maintain accord cache service cache else bypass server figure three algorithm group integrally ie group either contain whole query part may possible satisfy condition seven exactly necessary assign fraction query one group rest next divide yield proportionately group say end last query belong rearrange query query belong group consecutive within group query original order group order accord query end call group sequence denote group query may able form group happen enough query leave yield equal object size query drop group let subsequence query drop drop query drop create trim subsequence denote trim trim sequence contain query order may fractional replace group group object query refer obtain equivalent object sequence denote object object sequence send optimal cache respectively follow lemma state relationship cost term object sequence trim sequence lemma give input sequence query cost object two time cost trim proof lemma appear companion technical report among drop let subsequence drop query refer object object let refer subsequence remain query drop lastly let cost sum cost bypass query server similarly define cost drop proceed international conference data engineer observation cost cost plus cost word benefit fetch object refer query total bypass cost less fetch cost remainder proof theorem appear companion technical report assemble establish division complete bind quite similar figure three instead maintain value decide create simulate similar effect randomly create probability extra space take one six experiment develop cache within federation astronomy cache place within federation sit receive user query resolve member place near network cost communicate mediator sit compare thus mediator sit act proxy cache build prototype implementation system allow us evaluate performance various base implementation algorithm six eighteen widespread use know effectiveness cache binary heap object heap order do base utility value utility value value object utility value equal value heap implementation make log k time heap k object require one time maintain additional hash table cache object cache resolve hit miss one time evaluate yield query trace server case join cache yield individual object calculate decompose yield entire query component part correspond cache object demonstrate yield estimation use typical astronomy query select p one cache table yield table view join query divide proportion table contribution number attribute query query yield divide half table attribute cache yield proportional storage attribute fraction total storage reference query query total storage forty storage eight yield yield entire query cache consistency issue arise respect object take sloan digital sky survey participant federation publish immutable change data administer organization distribute new data release ie new version user query specify version however might arise especially materialize view indices modify use web service server notify mediator cache change cache use event update test effectiveness use trace gather log federate specifically use trace two data release federate node system trace consist request amount network traffic trace include variety access pattern range query spatial search identity query aggregate query trace involve remove query query log compare cache object analyze trace answer question class object perform well cache determine prefer object granularity consider query semantic cache versus cache object relational table attribute materialize view al state imperative show locality containment query cache viable exhibit little query containment render semantic cache ineffective degree query containment number query resolve previous query due refinement determine actual query containment eight upper bind evaluate experimentally query look celestial object region sky object denote unique examine query continuous subsequence trace order evaluate containment necessary sufficient condition containment object subsequent query must satisfy object previous query clarity present data look window query figure four result similar point chart indicate reuse object proceed international conference data engineer fifty query number figure four query containment figure five table locality c e j b zero zero different query thus potential cache hit query cache experiment indicate object experience reuse portion trace large universe object problem candidate celestial object cache indicate candidate query schema locality describe reuse locality data table reuse schema rather specific data figure five six evaluate schema locality trace correspond query enumerate table respectively table enumerate give unique number one maximum number table enumerate x unique table number column belong column number table data point horizontal line indicate reuse column table table show heavy long last reuse reuse localize small fraction total table system indicate table could place cache could service many future query cache hit algorithmic result verify find show large network traffic cache schema performance comparison set experiment compare algorithm two also contrast performance base system without cache system use cache without bypass experiment evaluate use network cost metric total number transmit proxy cache client proxy cache collocate factor traffic network cost outperform significantly cache without cache figure seven eight show network cost algorithm table respectively graph show proceed international conference data engineer figure six column locality network usage query trace cache reduce network load factor five twenty compare cache cache result show sum size query result ship perform poorly cache request load resp table cache generate query result cache include result static table cache comparison cache populate optimal set table cache load eviction occur static table cache optimal dynamic could perform better expect cache relatively stable compare cache model static table cache provide sanity check performance approach performance static table cache result indicate realize benefit cache frequent reuse reduce network time avoid hazard cache preserve data filter benefit evaluate query bypass essential feature cache successfully economic provide frame table one cost breakdown table cache data set number query sequence cost algorithm bypass fetch total cost cost cost data set number query sequence cost algorithm bypass fetch total cost cost cost table two cost breakdown column cache b g c zero zero b g c zero zero query number static table cache query number static column cache figure seven network cost table cache figure eight network cost column cache work make bypass decision also compare performance three table one two show total network cost entire trace divide cost bypass component cost query serve load component cost bring object cache case outperform algorithm indicate observe sound predictor future access pattern however perform surprisingly well promise give reduce state offer competitive bound randomize algorithm always lag behind indicate amount state aid make bypass decision fine granularity column cache offer benefit experiment compare table cache column cache cache much active data fetch cache query bypass result lower overall cost cache coarse granularity table cache also lead poor result cache evict load large table figure eight influence cache size examine variability network cost variety cache size order determine size cache figure nine ten show performance cache size vary ten size proceed international conference data engineer static table static column twenty thirty forty fifty sixty seventy eighty ninety cache size size figure nine performance table cache increase cache size e l c g l b g c e l c g l b g c ten one ten ten ten twenty thirty forty fifty sixty seventy eighty ninety cache size size figure ten performance column cache increase cache size draw two result first algorithm perform poorly small cache size algorithm exchange object higher rat often evict object load cost recover expect artifact remove tune algorithm second bypass cache need relatively large twenty thirty effective attribute partly fact scientific populate large data however find result inconclusive data thus small relative amount data target technology determine need cache size scale size remain issue study expect cache size need grow size rather expect cache size function case static table cache load cost small cache size much bypass cost lead increase total cost cache size load table require sequence show benefit randomize figure nine cache size size algorithm load object randomly probability proportional yield individual query algorithm may load table later compare deterministic case get lucky load large table future overcome load cost experiment run cold cache finite trace exacerbate effect however example show randomize robust seven present architecture altruistic cache network citizenship scientific treatment contain several cache within framework include predictive algorithm competitive algorithm randomize algorithm experimental result show provide benefit cache preserve filter parallelism benefit cache associate well suit scientific exhibit schema locality rather query locality allow cache differentiate query evaluate cache realize network save better ship federation order evaluate parallel data source reference one q c h b g h woo l brown cache web application two k park r data cache web conference information knowledge management three l j r evaluate content management web proxy cache workshop server performance four c chang j distribute process large parallel compute eleven five r computation analysis university press six p proxy cache symposium technology seven b si h v framework cache management mobile design evaluation distribute parallel ten one proceed international conference data engineer n arc low overhead replacement cache file storage conference n z p nain new efficient cache policy world wide web workshop server performance thirty e p g page rein placement algorithm disk buffer plasmodium resource genome r pottinger levy scalable algorithm query use view q h semantic cache query process technical report department southern methodist university n h kang design computer nineteen twelve p j shim r watchman data warehouse intelligent cache manager r illustrate volume one p r w new architecture distribute data forty time ten team data management application tier g g advisor smart enough recommend index k c squid web cache journal select sixteen three r proxy cache estimate page load delay international conference n e young cache cache size vary symposium discrete eight k p merlin optimal implementation conjunctive query relational symposium theory compute nine l g role age frequency high size web cache replacement performance compute network ten e p operate theory prentice hall eleven e h analysis symposium discrete twelve dar j franklin b j tan semantic data cache replacement thirteen h k competitive symposium computation fourteen g data transport within distribute conference graphic data system fifteen j gray science telescope prototype new computational science presentation conference sixteen j practical predicate placement sig seventeen j h l g r n j west scale performance distribute file system computer six one eighteen page replacement page web cache symposium theory compute nineteen j cache replacement computer architecture twenty performance study query optimization alin system support x efficient low recency set replacement policy improve buffer cache performance z greedy web proxy cache conference distribute compute z web cache exploit two source temporal locality web request stream computer two li p web cache protocol draft q j f proxy cache web sit malik r burn bypass cache make scientific good network technical report storage lab university malik r web service approach federate conference innovative data research proceed international conference data engineer