time machine compiler reduce train one university mob two abstract iterative compilation prove popular successful approach achieve high performance however cost many run application machine learn base approach overcome expense large train cost paper present new approach dramatically reduce train time machine learn base compiler achieve focus program best characterize optimization space use unsupervised cluster program feature space able dramatically reduce amount time require train compiler furthermore able learn model dispense iterative search completely allow integration within normal program development cycle evaluate cluster approach suite show reduce number train run factor seven translate average across suite compare default highest optimization level one introduction many particularly embed high performance performance critical iterative compilation successfully apply ten precisely emphasis performance however come cost namely large number need may acceptable final version program program development cycle machine learn one six propose potential overcome move search cost particular program train cost however train cost large must perform whenever underlie platform modify paper propose new technique significantly reduce train cost factor seven allow machine optimize produce new architecture fraction time particularly important embed due inherent performance previous work ten one six show possible obtain considerable improvement execution speed apply learn problem optimization selection however train time system cost run even additionally train time grow linearly number use train necessitate use small suit limit scope lack scope limit previous learn ability properly characterize new program mean frequently rely additional search optimization space achieve good performance five paper one cluster two train three deploy extract feature vector f program p b reduce f principal use c cluster new f n cluster give n typical centroid program space train model use use randomly select n cluster program fig one cluster algorithm present scalable technique allow vastly greater coverage space program fraction cost previous approach although previous work fourteen successfully produce require search target program typically focus one optimization heuristic limit scope approach attempt find good wide space ten inevitably require type feedback via small search target program optimization space significantly reduce cost learn cover program space result compilation feedback particularly useful program development time user want spend additional run program see eventual performance approach program characterize set code feature describe overall structure set feature form feature space examine structure space use unsupervised learn select subset program representative space whole turn reduce number program need evaluate train apply cluster suite reduce number program consider six additionally show gain large scale coverage whole space program train data dispense search altogether produce compiler give excellent performance give average level across suite train way knowledge well compiler train particular domain program new program compile judge confident new program cover previous train whether retrain would beneficial remainder paper structure follow section two give description cluster approach section three describe experimental methodology use section four present experimental result short review relate work finally draw brief conclude remark two approach figure one give overview approach first extract feature program potential train feature extraction way represent essence program later classification feature represent vector value many redundant apply principal analysis determine feature useful redundant reduce feature vector describe program attempt group use cluster determine representative program span space end initial cluster stage next stag standard supervise learn deployment learn model base train data associate n select program feature feature reduction program feature numerical value characterize space program machine learn use feature metric gauge similarity feature use paper assembly level instruction total number execute program ie proportion type instruction use simple feature set easy capture yet later show section four provide excellent performance use arm assembly feature ensure technique properly capture information program facet peculiar particular architecture nine argue use generic architecture capable represent characterize program performance arm use approximation generic core feature extract statically use simple fast profile tool count number type use program predict unbounded value raw count normalize use basic program feature output stage feature vector describe program extract feature reduce number use technique call principal analysis four technique reduce dimensionality feature space examine variance within data preserve much variance possible generate new set feature linear combination original set cluster program cluster use fuzzy cluster method call algorithm eight variation standard fuzzy cluster algorithm four allow detection different geometrical shape one data set algorithm minimize objective function fuzzy distance data point cluster center input algorithm reduce feature set nine principal describe cluster technique determine correct number cluster accurately depict space must supply employ technique suggest ray thirteen consider proportion variance respect variance select first local minimum value k number consider cluster increase train deployment select wish train use standard approach learn model achieve apply random optimization program find best optimization need build model map program feature best find many approach build model since goal approach compilation simply record best optimization flag find train program use nearest neighbor model one fix select program complete train compiler ready deployment firstly feature extraction perform new program input feature input nearest neighbor classifier determine n near use square distance norm assign neighbor compile execute use best perform compiler flag associate point execution time record although simple model later show effective one three experimental methodology evaluate approach seven suite embed domain contain many intensive program utilize embed general purpose program exclude due choice offer default choose experiment carry core two duo processor run machine run strip version kernel version compiler version use allow additional default access via compiler flag select different flag support form optimization space set flag particular value evaluate many distinct cluster approach apply technique suite find correct number cluster six centroid program select give six typical program represent cluster six program use train train program probe optimization space select random flag execute result code best perform flag set record use nearest neighbor model use standard four cluster deployment mean exclude evaluate cluster process program never see compiler standard random train selection give limit amount time available train machine learn compiler standard way select program train simply use random selection fair comparison six randomly choose set select program train process proceed way cluster approach give may high variation performance depend exact six program select repeat random selection time give robust mean performance effect randomly choose particularly good bad train program fig two obtain random selection cluster selection likely upper bind average graph one three generate upper bind finally need measure much available performance actually achieve use machine learn model randomly apply different program record best execution time unlikely model predict good optimization set without feedback information outperform best run provide reasonable upper bind limit four result section evaluate approach term performance gain suite give useful comparison show reasonable upper bind performance program also compare approach standard uniform selection train data figure two four show performance three scheme describe section three figure simply name show relative approach first black bar show result model train data randomly select middle dark grey bar show result use model cluster base selection train data final light gray bar show performance iterative search program optimization space represent best find try different optimization uniform random train consider result first dark bar label random six random program use train nearest neighbor model average across achieve show last set bar fig four use random train data lead average nearest neighbor model although able determine performance fourteen program actually cause slowdown seven three case show although improve performance limit train budget difficult learn model perform well across program space fig three obtain random selection cluster selection likely upper bind average graph two three cluster approach light color bar label cluster figure two four show result model approach train budget six program time however train program select use cluster approach final set bar figure four show average performance average use one evaluation approach yield whole suite compare machine lean compiler use program select random dramatic improvement standard approach give use exactly model train budget approach able avoid large achieve fifteen nineteen likely due nature cod change small section code frequently use large impact result iterative search upper bind final light gray bar label upper figure two four describe best performance achieve try different program unlikely base compiler could ever act useful upper bind every case least good definition hence certain show little room improvement hence poor behavior learn model show room improvement learn cluster base model frequently achieve look average value see achieve average time show large potential performance available sufficient time tune program however average approach achieve half performance improvement attain iterative optimization use run single evaluation additionally achieve factor seven increase additional optimization possible use approach rather standard random selection show viable option search space allow fig four obtain random selection cluster selection likely upper bind average graph two three five relate work base compiler require profile search however machine learn use previously predict effective apply heavily constrain environment without profile al fourteen examine problem parameter selection loop unroll paper consider limit optimization space consist either binary one limit scope code improvement intelligent think specialize example supervise learn search strategy update search traverse space evaluate point space attempt find best result case compiler evaluate space search structure observe previous result use determine space profitable search greedy six iterative progress simple random search evolutionary selection base fitness ten search six complimentary employ probabilistic search also propose model use speed search process one al three use cluster mean reduce input vary compilation cluster profile data allow author characterize input data specialize code transformation different work provide comparison therefore hard say cluster technique better selection process additionally work intend assist estimate performance evaluate paper possible gain similar subset different suite without run whole suite slow simulator profiler six conclusion demonstrate cluster dramatically reduce amount train require achieve good performance factor seven use compiler carefully select train data use better characterize small number point rather randomly select addition show compiler train way give average suite one evaluation achieve train six program finally show instruction effective feature cluster reference one f e j b g marc proceed annual international symposium code generation optimization new york march two r van veen improve estimation cluster international conference fuzzy three nelson ho reduction profile direct optimization proceed annual international symposium code generation optimization four c bishop neural network pattern recognition five j g f e rapidly select good compiler use performance counter march six k cooper reeve l waterman explore structure space compilation sequence use randomize search proceed symposium seven suite eight de fuzzy cluster fuzzy covariance matrix proceed san page nine compare use key ten p kulkarni w h moon k cho j bailey park k find effective optimization phase sequence eleven p kulkarni g j evaluate heuristic optimization phase order search publish international symposium code generation optimization seven march twelve g c e b f e j h leather c milepost machine learn base research compiler proceed summit thirteen ray r determination number cluster cluster application colour image segmentation proceed international conference advance pattern recognition digital fourteen martin meta optimization improve compiler machine learn