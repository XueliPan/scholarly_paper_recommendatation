text classification without label negative document one university hong hong china two hong university science technology hong china three j research abstract k paper present new solution problem build text classifier small set label positive document p large set unlabeled document unlabeled document mix positive negative document word document label negative make task build reliable text classifier challenge general exist approach solve kind problem use approach extract negative document n build classifier base p n however none report study try extract positive document p intuitively extract p increase reliability classifier however extract p difficult document possess feature exhibit p necessarily mean positive document vice sensitive extract positive document extract positive sample may become noise large size high diversity exhibit also contribute difficulty extract positive document paper propose heuristic aim extract positive negative document extensive experiment base three conduct favorable result indicate propose heuristic outperform exist approach significantly especially case size p extremely small one introduction text classification task assign value pair j k domain document k set class task approximate unknown true target function k one zero mean function k one zero coincide much possible nineteen function call classifier coincidence measure effectiveness also know quality classifier classifier build systematically use set train document document la major bottleneck build text classifier cost label train document expensive precision label great impact quality classification label precise enough classifier build strongly bias poor quality result due rapid growth information available task accurately label train document positive negative respect particular class become difficult impossible order overcome bottleneck attempt build use small set positive negative document large set unlabeled document one fifteen five two recently new direction text classification become recognize build use small set positive document p large set unlabeled document p zero eight eleven ten nine document label negative kind classification sometimes know partially supervise text classification nine uniqueness partially supervise text classification large portion train document unlabeled label negative document give make problem challenge important summarize follow one size label positive document p small two often p small reflect true feature distribution within positive class three large portion train document unlabeled four set unlabeled train document mix positive negative document five label negative document give exist approach problem build two stag first stage negative document n extract unlabeled document second stage build apply classification algorithm use give label positive document p extract negative document n generally speak exit approach perform well size p small ie space limit detail iterative select describe paper recommend read original paper eight ten eleven detail proceed international conference data engineer small reflect true feature distribution within positive class note none exist approach attempt enlarge p extract positive document p automatically major difficulty difficult extract proper set positive document p help extract p simply compare feature p reason document possess feature exhibit p necessarily mean positive document large size high diversity also contribute difficulty extract positive document paper propose solve problem one base give label positive document p unlabeled document extract negative document n accord normalize document feature two extract positive document p negative document n n use three use positive train document n n negative train document build binary classifier experimental result indicate approach highly feasible even extremely small size p solution outperform exist approach significantly worth note need build complex classification model iteratively report eight ten eleven rest paper organize follow section two present section three review major approach tackle problem section four evaluate work summarize conclude paper section five two document enlarge classifier document enlarge classifier denote outline algorithm one two input one set label positive document p two set unlabeled document mix positive negative document discuss two main step namely extract reliable negative document line one enlarge set positive negative document line three extract reliable negative document propose approach paper base set core vocabulary core vocabulary class k denote set feature frequently appear class k document p core vocabulary document belong p must possess least one feature list therefore reliable negative document document contain feature list algorithm one p input set positive document p set unlabeled document output classifier one extract n base p algorithm two two n three extract p n algorithm three four build classifier use p p positive train n n negative train core vocabulary positive document important know set reliable negative document subset true negative document base concept reliable negative document emphasize attempt identify possible negative document unlabeled document simply use core vocabulary fact impossible simply compare feature distribution among different class feature distribution text domain sparse eighteen follow focus identify extraction reliable negative document trivial give identification play role identify reliable negative document identify core vocabulary compare feature among feature exhibit small set positive document p specific let h f j function compute feature strength feature f j include feature f j h f j zero discuss determine later section say feature fi effective f j h fi h f j widely use approximate feature strength h f j properly come probabilistic theory information gain two odd ratio mutual information nineteen however none suitable solve problem reason prior knowledge feature distribution positive negative document entire document domain impossible obtain estimate value result obtain feature difficult task use traditional solve paper derive h f j propose utilize information present p let f j nu f j denote number document contain f j p respectively h f j compute measure normalize document frequency p h f j f j nu f j one proceed international conference data engineer maximum value minimum value f j f j p nu f j f j note h f j zero one first second correspond normalize number document contain particular feature f j p respectively although exist work also compute h f j base heuristic function none consider normalization believe normalization critical difference p extremely large p normalization one make possible compare true relative feature set p one robust since capture idea feature important appearance highly skew toward positive set p specific frequency feature p similar feature strength approach zero ie h f j zero skew toward one set feature strength either strictly positive strictly negative depend class skew recall identify extract f j h f j identify adopt idea statistics quality control thirteen one n f h f j two n number f j p standard deviation h f j f j p j idea behind two exist feature f j feature strength h f significantly higher average feature strength document p contain thus document contain f j may necessary belong p recall aim stage extract set reliable negative document thus document contain f j consider reliable negative document two indicate higher value tight control tight control result smaller size paper set six definitely set high value may result small size however experimental result indicate formulation effective efficient even noisy ambiguous environment ie extremely small p algorithm two outline procedure describe section extract reliable negative document contain feature list enlarge set positive negative document previous section discuss extract set reliable negative document n unlabeled document base label positive document p algorithm two p input set positive document p set unlabeled document output set negative document n one f j p two three end four compute use two five n zero six di seven eight f j di satisfy h f j compute h f j use one n n di end nine ten end recall n contain document share feature core vocabulary p section show extract set positive document p set negative document n n enlarge give positive document p previously extract reliable negative document n enlargement motivate follow p small number positive document train inadequate enlarge total number positive document add p p increase recall classifier p large number negative document n extract previous stage smaller large enlarge total number negative document add n n increase precision classifier main difficulty extract p simply compare feature distribution p impossible obtain high quality p document possess feature list may necessarily belong p note also proportion p usually small follow first discuss seemingly possible actually infeasible approach extract p present detail possible infeasible consider three possible infeasible repeat extraction approach approach adopt similar idea propose previous section implement reverse direction first construct core vocabulary extract negative document n extract p n p possess feature proceed international conference data engineer k one k two eighteen thirty table one percentage document nearest neighbor belong different class inadequate reflect true feature distribution positive class p specific let us take example figure one one b show determine decision boundary maximize boundary minimize error figure n cross p black dot different note p figure one smaller figure one b let us refer figure one number positive document inadequate generate wrong decision boundary adjust correct one show figure one b explain kernel base classifier always extract p high recall low precision whereas extract n high precision low recall instance base approach although instance base like twenty relay statistical distribution train data infeasible well recall document contain feature list core vocabulary positive class imply belong p accord probabilistic nature feature distribution two document may share high degree similarity even though belong different class fact observe two document often nearest neighbor belong class table one show percentage document nearest neighbor belong different use three although percentage possibly vary different data set confirm nearest neighbor behavior threshold k never set small value text classification typically k round six twenty obviously large k impossible exist problem set p always small small k obtain poor result obviously profile base approach centroid base nineteen sixteen possibly solve problem however still impractical let us consider two profile denote document p document n respectively suppose use cosine coefficient measure discuss section four small p inadequate b large p adequate figure one decision text classification approach approach first build classifier base p n document positive regard p technique report ten nine eleven extract n however discuss later claim extract p profile base approach approach first construct profile class p n give document order whether belong p n compare distance profile p distance profile n technique adopt nine ten extract n p repeat extraction approach infeasible always consist many different class focus one single topic level diversity usually high since n n inherit property accordingly every topic n contain set core vocabulary large number different n cancel significance core vocabulary eventually none feature n properly select feature select poor quality text classification approach impossible extract high quality p argument base two major classification approach kernel base approach like regression model instance base approach like kernel base approach kernel base approach may wrongly many negative document p reason follow recall n consist diverse cover large region feature space hand p focus one single topic cover much smaller region feature space p small p may proceed international conference data engineer similarity document profile p n p n three note normalize unit length account length different document nineteen ie one simplicity let us use positive class negative class denote correspond profile ie one p dip di one n din di consider whose numerator three become four five one n din di short similarity nothing average similarity document n divide one n large n consist many diverse obviously never similar document n thus extremely small explain precision p extract low observation apply compute similarity coefficient dice coefficient similarly also suffer classification naive bay decision tree neural network share similar describe section well short impossible extract p simply build kind classifier use p n directly feasible section present extract p n recall extract negative document consist diverse diversity make approach discuss far impractical extract p n key issue thus find way deal problem diversity brief approach extend idea profile base approach partition n k cluster document partition share high degree similarity order extract positive negative document compare similarity document centroid cluster ni compare directly advantage partition increase general since one observe one one one ni one n din di di one hand since ni n obtain hand uncertain whether di greater din di however practice observe ni dominate factor feature sparse nature summation always small value confirm observation use therefore take extract negative document comparison approach probability extract wrong negative document decrease precision p increase word negative document extract n use approach extract n use approach document n due problem diversity able extract n two give property one suppose negative document belong n classical classification extract n use solution property two suppose negative belong n classical classification still possible extract n use solution paper adopt classical cluster algorithm three four seven partition set reliable negative document n note cluster adopt partition common question regard cluster algorithm best number cluster number partition large problem appear instance base approach appear number partition small problem appear profile base approach appear optimal number partition thus exist detail analysis would give section four without loss generality assume partition n k cluster cluster order determine whether document belong p n rely decision solely three reason term similarity measurement may similar p ni need extract document significantly similar either p ni document similar p n regard ambiguous proceed international conference data engineer algorithm three partition p n input set positive document p set negative document n set unlabeled document output set positive document p set negative document n one p zero n zero two partition n k cluster three four f satisfy condition nine f true end satisfy condition ten algorithm four p n input set positive document p set negative document n set unlabeled document output set positive document p set negative document n one get p n partition p n two p zero n zero three p p p n n n p n get p n partition p n four five six seven end eight return p n five six seven eight nine ten eleven twelve thirteen fourteen fifteen true end true p p else n n end sixteen end seventeen eighteen end nineteen return p n ignore give detail document belong p determine follow n n p p six deviation sensitive size sample sample size p problem set paper document belong p follow two hypothesis h zero reject p p h h one p zero p nine similar fashion document belong n follow hold ten eleven twelve n one k n one k k k one ni j ni one ni j ni j j j note need apply extract n n always large size algorithm three summarize whole procedure extract p n follow algorithm three obtain set positive train document merge give p extract p ie p p set negative train document merge two set extract negative document ie n n finally use build classification iterative general question regard whether would improvement term choose best report text classification algorithm nowadays six classification could also use however p average similarity document p centroid p p average difference document p ni n di p one p one dip p p dip di k di j seven eight thus six p guarantee similar p p ensure similar p n however practice formulation may result high precision low recall extract p p small p small different feature available represent true core vocabulary positive class thus document regard ambiguous ignore order solve problem use fourteen test whether greater p p respectively evidence mean standard proceed international conference data engineer quality classification classifier train iteratively implement iterative train describe eight ten eleven algorithm four outline idea iterative train general idea run document either positive negative extract base algorithm four propose new call enhance algorithm one replace algorithm three line three algorithm four give section four exist work eight ten eleven nine discuss three relate work propose first build feature set f contain feature f j occur p frequently base follow equation f j f f j p nu f j fourteen f j nu f j number document contain feature f j p respectively document contain feature f n similar idea train iteratively discover n heterogeneous learner use similar sophisticate approach web page classification however include experimental study get satisfactory result use weight logistic regression log propose nine ten first build classifier sixteen p p propose eight use logistic regression weight together performance measure estimate label positive document unlabeled document select regularization parameter validation set author claim apply data set text data thirteen compare document p use cosine coefficient document similar n train iteratively use p n iterative train aim extract n enlarge negative train set enlarge positive train set propose eleven first randomly select set document p put default size fifteen p serve spy document p run algorithm use set p complete result classifier use probabilistic approach base decide threshold identify n em algorithm selection use generate final classifier ten except final classifier generate train iteratively rather use em algorithm ten show perform better number positive document give increase four experimental study implement well log use multinomial version naive bay twelve also implement serve base line approach document feature stem punctuation mark number web page address address remove feature appear one document ignore feature weight use traditional f id f schema seventeen normalize unit length nineteen different f id f schema implement influence none train test model use linear kernel c ten six whereas case use multinomial version prior smooth due space limit evaluate quality classification report nineteen take balance precision recall proceed international conference data engineer experimental setup three use experimental study give follow recent study use split separate data train set evaluation set select ten class train evaluation consist train article test article highly skew class contain positive article contain positive article standard deviation around totally twenty different class class contain message number message nearly evenly distribute class message assign single class message assign multiple class class randomly select eighty message train data remain twenty test data web page web page one seven class skew class contain web page contain web page standard deviation around class randomly select eighty message train remain twenty data pick class k positive class randomly select x document class k form set label positive document p remind train document regard x range one two nine extremely small case ten twenty large case totally nineteen case nine small case nine large case one case partition experiment attempt verify analysis claim section several approach test comparison implement extract positive document ie extract p one two three four five six seven eight nine ten twenty thirty forty fifty sixty seventy eighty ninety ba table two partition importance extract positive document repeat extraction implement first three approach describe section profile base approach implement third three approach describe section use profile base classifier sixteen approach ba build classifier use give percentage p positive train document n negative train document note n mix p thus approach serve approach table two show result use due space limit report result however behave similar way let us discuss interest find table two interest see better ba case ie perfect information situation label negative document actually noise negative train set include kind document negative train set affect consistence model word kind data unhelpful also degrade overall accuracy another interest find general better ba case reason positive document negative document extract also however ba n constant p small n large certainly introduce bias towards negative set thus degrade accuracy classifier certainly set appropriate bias value kind proceed international conference data engineer six five four one f r c two five eight twenty fifty eighty eight one f r c seven three zero ten thirty forty twenty number cluster fifty sixty six zero ten thirty forty twenty number cluster fifty sixty small p b large p figure two effect different cluster number bias could minimize however require complex time consume tune strategy size label positive document ie twenty propose paper perform best particular p small ba perform poorly even though contain positive document negative train set support claim motivation paper quality enhance include positive document train set percentage positive document increase accuracy become better eventually outperform approach since negative document extract main reason perform better certainly negative document incorrectly regard positive document harm quality effect number partition figure two show effect vary number partition stage enlarge document use show two case small p large p small p use two five eight true positive document large p use twenty fifty eighty number partition vary one fifteen thirty sixty size positive document large eighty effectiveness vary number partition small size positive document small two large number cluster significantly decrease accuracy optimum number partition around regardless size label positive document give suggest optimum number partition likely range plan investigate optimum number cluster future work one three five seven ten thirty fifty seventy ninety one f r c seven six five four three two one three four three four three three two two one one three five seven ten table three result iterative train one two three four five six stage figure three result train iteratively result iterative table three show result iterative use bracket average number iteration perform algorithm four table three show number iteration decrease size label positive document increase expect number label document increase number unlabeled document would therefore decrease imply would even difficult extract positive negative document result number minimize consider classification accuracy table three one three seven ten case accuracy iterative slightly better become slightly worse case seem get positive negative sample may necessarily improve accuracy difficult tell whether improve quality classification introduce suggest without iterative achieve reasonable accuracy therefore need make best time terminate iterative train computational cost minimize figure three show detail analysis regard iterative train one three five seven ten case figure stage follow mean stage one result stage two result stage three four five six result two three four five figure three easily see accuracy three seven case iteration proceed international conference data engineer iteration zero one two three four five one earn ten ninety one wheat ten ninety table four heuristic result nearly constant stage two six fluctuation observe suggest accuracy decrease add positive document extract positive document improve accuracy confirm previous argument simple iterative train may lead noticeable improvement table four show detail result one ten case two wheat earn choose wheat usually number whereas earn number table four indicate positive document extract since size category wheat much smaller earn believe major reason earn furthermore notice accuracy earn wheat perform quite stable different therefore conclude suboptimal sense improve noticeably throughout simple iterative extraction method different approach comparison table five table six table seven show result different approach acceptable seventy true positive document give weight logistic regression log outperform inferior best report text classification six however show three table extremely sensitive even perform worse twelve case three replicate previous find ten certainly accuracy may improve change default set different general two noise one noise feature two noise train data former one usually solve apply feature selection information gain two whereas latest one relate precision label positive train negative train paper unless otherwise specify state term noise refer noise train data train error hyperplane use validation set however may require complex time consume parameter tune strategy furthermore validation leave one estimation may become unreliable p small perform better percentage true positive document twenty best percentage twenty sixty perform well two outperform percentage true positive document less twenty three however accuracy show inverse perform best around thirty forty towards end deteriorate desirable show outperform percentage intend large also report ten main difference final classifier generate note use whereas use tolerant noise perform better noisy whereas better noiseless propose significantly outperform especially percentage true positive document extremely small twenty vary much regardless size p three perform best almost case one web page belong one one class hand many article assign one class properly difficult approach dominate size p become large consider note sudden jump ten twenty steadily observation interest investigation do issue future work five conclusion paper present solution text classification problem small set label positive document p large set unlabeled document mix positive negative document give exist focus extract negative document n unlabeled document unable extract positive document unlabeled document therefore fully utilize information present unlabeled document extract positive document unlabeled document important difficult observe difficulty extract proceed international conference data engineer table five result log one two three four five six seven eight nine ten twenty thirty forty fifty sixty seventy eighty ninety one two three four five six seven eight nine ten twenty thirty forty fifty sixty seventy eighty ninety one two three four five six seven eight nine ten twenty thirty forty fifty sixty seventy eighty ninety four zero zero zero zero zero zero zero zero zero zero two zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero three two sixteen fourteen zero two seven six seven nine eleven one zero one two seven eleven one seven seven six seven ten twenty one twelve nineteen three seven log zero zero zero zero ninety zero zero zero zero zero zero zero zero zero two zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero thirteen five thirteen table six result log table seven result proceed international conference data engineer document unlabeled document due diversity exhibit unlabeled document order solve diversity problem first extract negative document n base core vocabulary present p feature normalization second partition n cluster assist us extract document n approach extract positive negative document implement use exist text classification approach conduct extensive test use three test result show approach highly feasible significantly outperform particular number positive document extremely small work describe paper support grant research grant council hong special administrative region reference one support vector machine advance neural information process eleven two j craven exploit among acquire weakly label train data international conference machine learn three p refine initial point cluster proceed international conference machine learn four r cut r j j w approach browse large document proceed international conference research development information retrieval five r combine label unlabeled data text categorization proceed international conference machine learn six text categorization support vector machine learn many relevant feature proceed conference machine learn seven b c fast effective text mine use document cluster proceed international conference knowledge discovery data mine eight w lee b learn positive unlabeled use weight logistics regression international conference machine learn nine x li b learn use positive proceed international unlabeled data joint conference artificial intelligence ten b x li w lee p build text use positive unlabeled international conference data mine eleven b p x li partially supervise classification text document proceed international conference machine learn twelve k comparison event model naive bay text classification national conference artificial intelligence workshop learn text categorization thirteen c introduction statistical quality con text book forth edition fourteen c g c apply statistics sons probability engineer second edition fifteen k text classification label unlabeled document use em machine learn sixteen j relevance feedback information retrieval g editor smart retrieval system experiment automatic document process page prentice hall seventeen g c approach automatic text retrieval information process management five eighteen h hull j comparison document rout problem proceed international conference research development information retrieval nineteen f machine learn text compute survey one twenty yang study text categorization proceed international conference research development information retrieval yang x text categorization proceed international research development information retrieval h k chang j han heterogeneous learner web page classification proceed international conference data mine h j han k chang positive example base learn web page classification use proceed international conference knowledge discovery data mine value unlabeled data classification proceed international con machine learn proceed international conference data engineer