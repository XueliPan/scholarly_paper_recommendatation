fast memory snapshot concurrent program without synchronization research advance development lab corporation computer lab university abstract turn toward provide increase amount parallel commodity however still difficult harness available parallelism user system code propose memory snapshot program without synchronization code port atomic read large since modern support atomic access single word add synchronization code process concurrently environment snapshot read atomically process snapshot image without synchronization code implement use hardware transactional memory reduce storage overhead seven demonstrate usefulness fast snapshot use implement concurrent garbage collection profile without need synchronization code allow system service run parallel user spare core result overhead service minimize approach ideal implementation subject hardware memory general term design snapshot transactional memory one introduction bring abundant parallelism commodity however lack concurrency prevent fully exploit additional hardware core ideally sequential code could easily change use permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee nine june new york copyright multiple core however parallelization trivial practice must deal concurrency management alleviate problem important develop architectural tool help exploit parallelism search tool find memory snapshot particularly useful improve concurrency one two fifteen key benefit snapshot support atomic read produce consistent view large since modern support atomic memory access single word deal complex synchronization issue process concurrently environment snapshot concurrent program read large atomically work consistent snapshot image without synchronization code basic concept use widely file reliable storage eleven several memory snapshot propose applicability performance limit due high overhead since rely pure one two sophisticate allow additional gain cost algorithmic complexity fifteen paper propose memory snapshot concurrent program without synchronization code provide fast memory snapshot hardware acceleration use snapshot read atomically process concurrently environment snapshot image isolate memory update share multiple thread access normal support multiple memory arbitrary lifetime consist multiple disjoint memory fast memory snapshot beneficial need atomic read fast concurrent backup parallel program concurrent garbage collection dynamic fast implement use hardware available transactional memory execute group atomic isolate manner eight seventeen nineteen opportunity share hardware understand intuitively since support sort atomic execution key idea use cache buffer snapshot data additional per cache line snapshot like hardware system transactional data due resource share hardware storage overhead reduce seven evaluate two garbage collection dynamic profile take advantage fact garbage always garbage take memory snapshot perform collection without interfere application thread run parallel profiler take snapshot stack profile read data structure atomically efficiently without synchronization code run task concurrently add negligible overhead rest paper organize follow section two present motivation work section three explain definition program interface hardware implementation fast memory snapshot section four explain implementation hardware section five present quantitative evaluation section six discuss relate work section seven conclude paper two motivation motivate example modern support atomic read write data one two however multiple word need read atomically word write concurrently provide consistent view word read operation let us consider example mimic dynamic memory profile profile thread traverse object reference graph application thread change graph concurrently figure one profile thread visit node determine node b child node read node b application thread detach node node c attach node since node already visit profile thread fail visit node new child node node unobserved problem profile thread read whole graph atomically may miss visit live object due concurrent mutation application thread several address problem profile thread grab global lock graph lose concurrency lock literature grab lock gradually release lock graph traversal complete allow parallelism introduce lock acquisition overhead may profile thread application thread traverse link opposite show figure one b transactional memory seventeen nineteen guarantee atomic isolate execution transaction profile thread read whole graph atomically enclose graph access within transaction however transaction likely much longer usually expect ten may suffer frequent restart due constant conflict application thread show figure one c virtual memory protection five dynamic binary translation use save old value graph profile thread however virtual memory protection suffer page fault exception overhead instruction instrumentation overhead develop specific concurrent graph traversal tricolor mark eighteen three color ie white gray black use present traversal status node scheme solve problem change color node black ie traversal do gray ie traversal ongoing node attach node show figure one since take advantage knowledge data structure child nod number child nod scheme easily generalize atomic read memory snapshot memory snapshot provide support atomic read twelve snapshot memory create provide consistent view p allow execute two update write memory element snapshot scan read memory scan operation atomic read operation memory produce consistent copy moment scan execute memory snapshot solve race problem easily without synchronization show figure one e memory snapshot take ie scan nod graph profile thread read pointer node c node snapshot image application thread modify pointer image unfortunately despite great potential program primitive concurrent program memory implement performance issue update time one two scan time fifteen prevent adopt wide range propose fast memory snapshot hardware assistance easy concurrent program without synchronization code accelerate snapshot hardware provide one update fast single memory write operation p scan space one update time enable application thread write image without performance degradation p scan time allow use large program use cache buffer snapshot data additional per cache line snapshot consistent snapshot image take communication maintain cache coherence protocol support use hardware acceleration memory snapshot apply system include follow fast concurrent process state backward recovery fault tolerance five guest os migration virtual machine fast memory snapshot use create backup memory image concurrently without slow due one update time log thread run parallel application thread log image disk concurrent garbage collection concurrent garbage collection typically incorporate sophisticate deal race increase code management cost twelve fast memory concurrent garbage collection simple garbage collector take snapshot part heap collect garbage snapshot image concurrently concurrent memory profile show example previous section concurrent memory benefit fast memory snapshot traverse consistent object reference graph snapshot image concurrent profile virtual machine c system optimize application code find hot execution profile thirteen fast memory snapshot take snapshot thread stack periodically analyze stack parallel application thread fast cow cow use share virtual page fast fork disk b c b c b c b c b c snapshot image image b c b c profile thread application thread synchronization b deadlock c long transaction tricolor graph specific e memory snapshot figure one concurrency issue synchronization scheme block share virtual machine performance problem cow thread modify share data stall safe copy data make fast memory snapshot thread take snapshot data modify data without stall copy make copy make parallel snapshot image unlike traditional achieve good performance load whole data schema main memory fourteen snapshot image data schema use generate report data usage replicate data cluster support necessary summary fast memory snapshot useful need atomic read obtain recent consistent view various data structure three fast memory snapshot design propose memory snapshot atomic read arbitrary first present definition program interface fast memory snapshot explain hardware mechanism definition interface provide snapshot memory image certain point time multiple disjoint address part snapshot snapshot take ie equivalent scan operation maintain two memory image master image data snapshot image read snapshot image thread first join snapshot number thread join snapshot user thread leave snapshot normal read snapshot return snapshot data separation take snapshot join snapshot make possible take snapshot performance critical section analyze image later use additional core snapshot destroy user thread leave snapshot snapshot image like one two also capture associate core register value moment snapshot take useful analyze image later multiple active moment simplify hardware allow overlap snapshot support granularity specify table one show program interface take join leave destroy snapshot simple data structure call snapshot show figure two use set snapshot unique snapshot id assign newly take snapshot implementation three key hardware implementation show figure three snapshot information table sit show figure three hash table manage snapshot information structure index either virtual address sit call snapshot information block sib similar snapshot structure pass take snapshot join list link list thread join snapshot simple counter atomically generate unique snapshot information buffer show figure three b small hardware cache accelerate retrieval snapshot information sit entry encode information snapshot region access parallel match entry return two field snapshot region belong j join bite indicate current thread join snapshot maintain overflow bite indicate entry evict due capacity issue bite set miss ignore since snapshot region associate memory address bite set miss handle refill handler access sit support handler two add invalidate load similar thirty two snapshot add per cache line data modify since snapshot bite set cache line snapshot region modify since snapshot take read snapshot bite set cache line snapshot region refill old data snapshot image set cache line imply master image snapshot image data line set either bite bite set indicate snapshot set set zero cache line unrelated snapshot take snapshot initiate process take snapshot sib create snapshot copy snapshot block insert sit ensure core aware snapshot information call return guarantee write snapshot snapshot take trigger data build snapshot image gradually avoid scan time snapshot information exchange use handshake show figure four first snapshot request message send via signal thread invoke snapshot signal copy save register value thread data structure point save field snapshot argument load snapshot information next send group snapshot control snapshot share method take snapshot snapshot destroy snapshot join snapshot leave snapshot function take snapshot address specify snapshot new snapshot id set snapshot save register value point snapshot destroy snapshot start use snapshot stop use snapshot table one interface short long two void short tid unique snapshot id array start address end address pair save register value optional thread id isolate selectively figure two snapshot data structure use take snapshot message back thread initiate snapshot wait resume message receive response message initiate thread send resume message terminate resume application thread process p complexity join snapshot update sib remember thread new user snapshot correspond entry reload set j bite thread cache snapshot summarize table two user thread write snapshot exception trigger since user thread allow read snapshot detect case read snapshot hit cache line match address tag bite zero indicate line modify since snapshot take miss bite j bite refill request indicate refill come snapshot image receive refill request cache search cache line match address tag whose bite zero bite cache line match address one bite attach response message notify requester master image deviate snapshot address find match cache line cache request send main memory data refill refill cache line set bite bite bite use later destroy snapshot see line invalidate read write snapshot thread use snapshot ie j bite zero hit cache line match address tag bite zero indicate line contain old data snapshot write hit bite set indicate line longer belong snapshot image indication accomplish simply set bite since line exclusive processor make update one write miss bite set line refill cache miss handle similar case thread join snapshot j bite cache line refill request use cache find cache line whose address tag match whose bite zero since cache capacity limit snapshot may cache overflow snapshot mechanism explain conjunction transactional memory next section leave snapshot call stop use snapshot id thread remove sit reload clear correspond j destroy snapshot destroy snapshot start invalidate cache line snapshot invalidate bite set snapshot complete remove sib snapshot sit four resource share transactional memory astute would already notice similarity implementation typical hardware system similarity come fact use essentially mechanism data use cache buffer multiple data add per cache line version information expect adopt widely near future present base hardware system assume paper explain use hardware end section calculate hardware cost share hardware system hardware use hardware accelerate transactional execution group four eight seventeen hardware assume paper record transactional load store two per cache line r bite load w bite store seventeen use distinguish transactional data data transactional data buffer cache eight seventeen support fast context switch transactional code use transaction per cache line limit capacity hardware cache system use nine deal cache overflow support feature necessary page overflow page reasonable incremental hardware cost show section maintain shadow page table hardware table access logic locate next memory controller nine transactional data evict allocate new page shadow page store last commit version data use old page home page store overflow data shadow page table maintain proper map information index address snapshot information table sit compute core index physical address snapshot information buffer j j data cache v e data short long two short tid counter snapshot information block sib component b hardware component figure three hardware structure snapshot signal handler ready resume wait handler invoke save register value load handler return figure four handshaking snapshot figure five show hardware base system dark gray use hardware key idea support memory snapshot map snapshot transactional properly map memory snapshot transaction never abort end transaction value read snapshot image include transaction write snapshot handle transactional store conflict transaction ignore since snapshot exclude new update moreover regardless conflict cache line snapshot data invalidate eventually abort transaction snapshot destroy realize map use two two snapshot transaction id field shadow page table overflow snapshot shadow page overflow snapshot data table three summarize resource map base map bite w bite use distinguish master image snapshot image similarly bite map r bite mark old data snapshot image invalidate snapshot destroy additional combinational logic use bite control explain sec map transaction id allow multiple share cache use logic reset snapshot destroy shadow page table use deal cache overflow snapshot data use home page store master image shadow page buffer snapshot image cache line bite ie data evict bite line shadow page table perceive transaction id bite w bite snapshot data cache line copy shadow page evict data master image go home page shadow page pool page evict cache line bite ie snapshot data silently part snapshot image main memory reload overflow cache line hardware attach j bite refill request shadow page table use j bite select home page shadow page read bite one shadow page read home page read snapshot destroy send transaction commit message snapshot tell shadow page table end transaction shadow page table release shadow page contain snapshot image figure five show hardware share dark gray hardware light gray snapshot user thread rest write snapshot trigger exception read snapshot hit address tag match bite set set bite refill bite hit address tag match bite set hit address tag match bite set set bite hit refill table two memory snapshot resource transactional per cache line transaction id tid per cache line transactional bite gang clear logic usage snapshot per cache line snapshot id per cache line snapshot bite gang clear logic shadow page table table access logic provide access image page contain image separately table three hardware resource map hardware cost save hardware cost implementation consist combinational logic control storage overhead data since complexity combinational logic hard quantify focus storage overhead paper system assume private cache private cache line size main memory configuration use evaluation section five well storage overhead additional cache per core snapshot storage per core one start address end address j bite storage overhead additional cache sixteen core majority hardware cost shadow page table cache hardware table access logic shadow page table entry consist physical page number home page shadow page valid bite summary physical address length main memory total storage overhead table physical page number valid bite two b summary storage size cache structure system include physical tag nineteen sixteen coherence three four overall total storage overhead hardware use however give additional storage overhead seven sixteen show clearly benefit resource share implementation add shadow page table however total storage overhead go system issue like must reload index virtual address need save since sit always reschedule load either eagerly lazily use eager approach evaluation page flush page cache main memory intentionally flush cause generate shadow page need page home page normally os check associate shadow page reference feature cache cache shadow memory interconnect description core line one cycle hit time private one block write back ten cycle latency private eight bank bite vector four cycle latency tile network link three cycle per hop table four simulate system shadow page table swap shadow page two page page together page access since system two user support use order support user together system need support nest least two pair per cache line dedicate pair user pair five evaluation system implement simulator evaluate table four use nine one micro web browser python interpreter compression tool sixteen three perform continuous fraction factorization six web server vacation mimic system seven radix micro add search delete object tree evaluate garbage collection profile snapshot take advantage fact garbage always garbage garbage object find snapshot image garbage master image well take snapshot memory collect find garbage snapshot image application thread modify master image snapshot profile periodically take snapshot thread stack walk stack obtain information call graph function application code compare snapshot parallel stop world collect garbage multiple parallel snapshot profiler virtual address physical address j snapshot control logic compute core data data cache v e tid data share coherence request data unique interconnect shadow page table memory controller figure five resource share compare profiler stop world analyze stack thirteen garbage collection profile normalize application execution time without run profiler snapshot use seven test observe meaningful behavior within reasonable simulation time heap use smaller real environment would use still exhibit reasonable time total execution time one eighteen also show result run heap figure six present add application execution time without snapshot normalize execution time run without use large heap lower bar better bar stop time spend stop system start mark time mark phase reclaim reclaim phase snapshot time initiate snapshot time time spend application bar represent para bar represent parallel nonconcurrent except snapshot eliminate overhead experience parallel show increase application execution time due increase memory contention average snapshot reduce overhead fifteen make garbage collection essentially snapshot scale well heap table five show memory maintain overflow data use additional physical page shadow page even single cache line overflow within page table snapshot data actual overflow snapshot data cache line granularity shadow page number shadow page time page size except memory snapshot data one worst case still seven moreover page shadow page fit completely shadow page table snapshot profiler figure seven show overhead due profile normalize application execution time without profile prof time run profiler execution application snap time take snapshot profiler trigger cycle previous profile complete experience performance snapshot profile range due deep call graph average depth fourteen result important program write language since typically call depth contrary radix average depth four six regardless call depth snapshot profiler add negligible overhead less three six relate work discuss general memory snapshot section beyond concurrent program several implementation scheme propose recently mostly concurrent parallel garbage twelve twenty competitive term low overhead allow algorithmic simplicity easy code management well recent advance dynamic profile shadow profile however paper report heavyweight clone fork shadow profile show lower much simpler profile count basic block rather actually scan heap stack seven propose memory snapshot parallel application sequential application stop mark reclaim snap thirty twenty fifteen ten five zero e h r e v e n r e z l r n h r p h r p h r p h r p h r p h r p h r p h r p vacation h r p figure six graph show overhead cause parallel para snapshot overhead normalize application execution time without parallel stop run ten core snapshot concurrently run two core sequential run one core parallel stop run two core snapshot run concurrency one core snapshot data shadow page table five memory requirement manage overflow snapshot data appreciate nick insightful ten j h al common case transactional allow read large atomically process snapshot image concurrently without synchronization code environment implement manner use hardware evaluation result show allow garbage run parallel user spare core negligible interference eight comment work nine reference one h al atomic share memory j forty four two j composite register ninety distribute compute three j j p singh parallel decode parallel process four c j al make fast case common uncommon case simple unbounded transactional memory news two five n k processor rollback recovery computer two six r p brent recent progress prospect integer cocoon zero compute seven c al effective hybrid transactional memory system strong isolation guarantee computer architecture june eight l j tuck al bulk speculative thread computer architecture june nine w al unbounded transactional memory architectural support program operate behavior multithreaded program computer architecture eleven k k lazy replication snapshot isolation six large data base twelve c flood al garbage collection four memory management thirteen n j r fowler call path profile unmodified optimize code five fourteen h k main memory overview knowledge data engineer four six fifteen r v k scalable global distribute six sixteen seventeen l v wong al transactional memory coherence consistency prof snap eighty seventy sixty fifty forty thirty twenty ten zero e h r e v e n r e z l r n h q e h q e q e h h q e h q e h q e radix figure seven overhead cause sequential profiler snapshot profiler normalize application execution time without profile sequential call path profiler use one core application profile snapshot profiler concurrently use one core application one core profile computer architecture june eighteen j henry c baker c incremental garbage collection process artificial intelligence program nineteen j e b moss transactional memory architectural support data structure computer architecture may twenty r l j e b moss sapphire copy without stop world one joint p optimal snapshot algorithm five theory compute j al architectural semantics practical transactional memory computer architecture june k e j al transactional memory computer architecture j j al support nest transactional memory architectural support program operate san ca v j r peri shadow profile hide instrumentation cost parallelism seven proceed code generation optimization thirty assembler reference j smith g q effect memory management response time fork compute one three l b soar silva simple mechanism file system consistency three storage network parallel io c memory resource management server rev si k hazelwood parallelize dynamic instrumentation performance seven proceed code generation optimization c woo al program characterization methodological computer architecture june li scalable parallel mark algorithm without synchronization seven parallel distribute process symposium