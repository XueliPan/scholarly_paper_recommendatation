incorporate domain knowledge topic model via forest mark craven department computer department biostatistics medical university wi craven abstract topic model often knowledge composition word high low probability various incorporate domain knowledge use novel forest prior latent allocation framework prior mixture tree special structure present construction inference via collapse sample experiment synthetic real demonstrate model ability follow generalize beyond domain knowledge one introduction topic model use approach latent allocation al enjoy popularity way model hide data however many user may additional knowledge composition word high probability various example biological application one may prefer word termination disassembly release appear high probability topic describe phase biological process furthermore biologist could automatically extract exist ontology gene ontology go gene ontology consortium another example analyst may run topic model corpus people wish inspect result notice college cure cancer appear proceed th international conference machine learn canada copyright author owner pear high probability topic analyst may want express preference two set word appear together rerun topic model incorporate additional base new result case would like guide recovery latent standard lack mechanism incorporate domain knowledge paper propose approach incorporation domain knowledge show many type knowledge express two word pair borrow name constrain cluster literature al call two link although important encode set link associate domain knowledge use forest prior replace prior multinomial p forest prior mixture tree specific tree structure approach several advantage forest encode link something impossible user control strength domain knowledge set parameter allow domain knowledge override data strongly suggest otherwise forest lend efficient inference via collapse sample property inherit conjugacy tree present experiment several synthetic two real demonstrate result successfully incorporate specify domain knowledge also generalize beyond relate word explicitly mention link incorporate domain knowledge topic model two relate work review use notation let let w represent corpus document total n word use di denote document word wi hide topic wi generate let w j p z j document generative model j p j cluster respectively borrow notion topic model informally primitive prefer two word tend generate topic link primitive prefer two word tend generate separate however since topic multinomial word two word general always probability generate topic therefore propose follow definition di multinomial di multinomial one two three four v two word v similar probability within topic ie j one important note large small long similar example biology example could say termination disassembly j v j respectively simplicity assume symmetric asymmetric also possible previous work model use logistic normal distribution dag structure li tree distribution tam addition model al employ domain knowledge special concept particular set word present work complement previous work encode complex domain knowledge word especially arbitrary link flexible efficient prior three topic model forest propose model differ way generate instead three q q q specify tree distribution play role analogous standard one strength parameter domain knowledge discuss q first explain knowledge express use link link link originally propose constrain cluster encourage two instance fall cluster separate link v two word v large probability within topic permissible one large probability small small example one primitive wish example link college cure many type domain knowledge decompose set link demonstrate three type experiment split two set word single topic different place within set link merge two set word different one topic place among set give common set word appear multiple tend appear isolate place within common set place link common set word important note link instead hard encode distribution limit word share common variance parameter mutually independent except normalization constraint however v crucial control two word v differently word tree distribution generalization distribution allow control tree word leaf nod see figure one example let k tree edge weight lead node k let c k immediate node k tree l leave tree internal nod l k incorporate domain knowledge topic model leave k generate sample one first draw multinomial internal node c ie use weight one think redistribute probability mass reach multinomial initially mass one root probability k word k l simply product multinomial edge k root show figure one b show procedure give p l k k one q c k p c k k l k k l k mean standard gamma function function notation k difference internal node difference p zero internal nod tree reduce distribution q q like tree conjugate multinomial possible integrate get distribution word count directly similar distribution p w c k p c k p k k n k c k n k k five n k number word w appear l k encode use tree note definition transitive v v w imply w thus first compute transitive closure express tree simple structure transitive closure one internal node word closure leave weight internal node leave root connect internal nod weight l represent set size addition root directly connect word closure weight example transitive closure b vocabulary b c simply b correspond tree figure one understand encode consider first case domain knowledge strength parameter one equal internal node l tree reduce distribution symmetric prior turn case increase redistribution probability mass govern increase concentration l uniform tend redistribute mass evenly transitive closure represent therefore turn one furthermore mass reach independent still large variance properly encode fact want word similar always large otherwise word would force appear large probability clearly undesirable impossible represent example blue dot figure one c sample tree figure one plot probability simplex dimension three always true p p b total probability mass anywhere zero one similar distribution perhaps one generate sample close five five zero figure one encode link link considerably harder handle first transform alternative form amenable tree note link transitive link b b c entail link c define nod edge correspond link connect graph independent encode link use property factor selection probability later example two b b c form graph figure one e single connect component b c consider connect component r define complement graph flip edge show figure one f let q r maximal r complement graph follow simply call important remember maximal complement graph original example q r two c b follow interpretation clique c maximal subset word connect component occur together one word transitive closure form single node graph incorporate domain knowledge topic model c c c nine b b two nine b c b b c b c c g e f two b h c b c b figure one encode link forest tree encode b one fifty vocabulary b c b sample tree c large set sample tree plot note p p b yet remain flexible actual value desirable contrast sample standard comparable force p p b five encode e link b link b c f complementary graph two maximal c b g clique c h clique b sample mixture model g h encode link one fifty word allow simultaneously large give topic without violate link allow word outside clique b also large probability violate least one link example two discuss encode single connect component r defer discussion complete encode section create mixture model q r one clique topic select exactly one accord probability p q q one q r six conceptually select index q tend redistribute nearly probability mass word within since mass leave impossible word outside clique large probability therefore violate reality soft rather hard link structure follow root connect internal node weight node connect word weight root also directly connect word connect component r weight send probability mass flexibly redistribute among word example figure one g h show c b respectively sample mixture model show figure one rep resent link violate behavior achievable distribution single finally mention although worst case number maximal q r connect component size r grow exponentially al experiment q r three due part word collapse single nod link graph forest prior general domain knowledge express set link first compute transitive closure form node either closure word present note domain knowledge must consistent pair word simultaneously link either explicitly implicitly transitive closure let r number connect q r tree represent consist template figure two tree two small concentration selection effect example beta tend concentrate probability mass one two however weak pseudo count small small concentration posterior dominate data would lose encode domain knowledge q incorporate domain knowledge topic model r branch beneath root one connect component tree differ include branch branch q r possible correspond r therefore tree forest uniquely identify index vector q q one q r q r one q r derive use five p z k j k k p k k p j n k j finally complete generative model j n k k j j connect component one connect component r p w z p z p z p one w q one r w q r word figure two template tree forest tree q prior draw select independently r connect independent respect link p q r p q r q r sample accord six correspond choose solid box q branch figure two structure within solid box define section black nod may single word transitive closure structure show dot box edge weight lead nod k k l k l k set leave k however edge come internal node go link internal node weight multiply strength parameter edge mark figure two define complete forest model integrate collapse let n j number word document assign topic j z generate p z q j n n r p q r one tree per topic j one sample forest prior p j tree implicitly define tree edge weight j use tree let n k number word corpus assign topic j appear node k tree probability generate corpus w give tree topic assignment z j four inference forest forest mixture tree conjugate efficiently perform inference chain monte specifically use collapse sample similar however case state define topic label z tree indices iteration case consist sweep z present conditional collapse sample sample let n j number word document assign topic j exclude word position similarly let n k j number word corpus node k topic j tree exclude word position candidate topic label v one p w n v si v k p v n si v n k k v denote subset internal nod topic v tree leaf wi si unique node immediate child ancestor wi include wi sample q r j since connect independent sample tree factor sample connect component q r candidate q one q r j p q r j q r j w k k j k p k k p j denote internal nod branch tree clique select j n k j j n k k k j incorporate domain knowledge topic model estimate run sufficient follow standard practice use last sample z estimate tree conjugate distribution posterior tree structure update edge weight posterior tree topic post k count n k collect z w estimate j j first moment posterior j k j n k j w j post j w b one post j seven parameter estimate way standard j n j n five experiment b synthetic corpora present result synthetic show forest incorporate different type knowledge recall one equivalent standard verify code previous study often take last sample z discuss one derive sample stochastic nature argue insight gain multiple independent sample consider different run long chain take sample every afterward total sample indication chain observe expect sample label switch ie equivalent label permutation occur near equal frequency sample derive one seven greedily align different sample permute topic label remove label switch effect within perform one project sample result space obtain common visualization row figure three point dither show overlap b c corpus consist six document vocabulary five word document represent twice let two five one produce three one roughly third e time around two two shorthand one one four one two vocabulary another third around two zero zero zero two zero zero one two c two one four one four b four three two two three five three three five three two two six four two zero two four six five one zero five one zero five one one one zero five one one b c b c b c one two one two one two five five b zero five five b zero five five b one one one six four two zero two four six five one zero five one zero five one one one zero five one one three five three zero two two six four two zero two four six five one zero five one zero five one one one zero five one one four five one one four five one one four five one one five zero five one five zero five one five zero five one isolate b isolate b isolate b one three one three one five zero five one five zero five one five zero five one split split split five one five one five one two four six three five seven two four six three five seven two four six three five seven five zero five one five zero five one five zero five one figure three sample four synthetic data experiment four two e c four b four b four two c two final third around four e four correspond cluster twelve three respectively panel figure three add single b c ten data still override somewhat cluster one two disappear completely increase fifty override data cluster one two vanish leave cluster three run take last sample likely obtain four e four want b c present absent together also pull along even though knowledge add b four c four c b two link b corpus four document twice three one one produce six one c evenly b two ad two two correspond cluster fifteen two line add single link b increase cluster two b two disappear involve topic b two violate link cluster become uniformly likely two c two two b two two two two isolate b corpus four document two one one pro thirty incorporate domain knowledge topic model c two b duces three cluster evenly c two two add isolate b compile b link b c concentrate cluster one two b indeed isolate b topic two c b two b two two c four b four split corpus six document present three time five one three produce large portion around four show four add split compile b c link b c increase four however one ie four produce large variety cluster one eight ce cluster two four c two eight simply add one topic clearly separate hand increase eventually concentrate cluster seven satisfy split operation four cluster seven eight eight two c eight eight b c two two eight eight wish corpus consider interactive topic model corpus use collection new year wish submit time square alliance al wish treat document without removal step interactive example set five one run estimate final sample domain knowledge accumulative along step step one run fifteen many probable word conventional wish obscure mean step two manually create list issue isolate preference compile among set link set word top fifty increase sixteen run end two importantly explain two top word become much meaningful step three notice one topic conflate two enter college cure disease top eight word go school cancer well free cure college issue split go school college cancer free cure well separate compile within quadruple link increase eighteen run one clearly take college concept pick relate word explicitly encode prior another topic likewise cure concept many wish like stay cancer free minor change table one wish interactive topic model love lose weight together forever marry meet topic top word sort p merge success health happiness family good prosperity life happy best live time long wish ever someone like much life money make money house work able pay lot people people stop less day every another joy family vote home safe end troop bring war return love true peace happiness dream joy everyone happy healthy family baby safe prosperous better hope president person bush isolate year new god peace call four two three visit one god bless everyone love know heart peace world earth win lottery around save isolate wish split split hope cancer free husband son well dad cure job go great school good college hope move step four notice two correspond romance apply merge love forever marry together love meet marry wed compile word decrease seventeen run one romance disappear remain one correspond merge romance topic lose weight one remain previous survive minor change table one show wish four step place next affect word explicitly specify domain knowledge yeast corpus whereas previous experiment illustrate utility approach interactive set consider case use background knowledge ontology guide topic model prior knowledge base six transcription translation replication characterize three important process carry molecular level initiation elongation termination describe phase three process two set correspond gene ontology translational elongation transcription initiation guide topic model use among small set word concept moreover use link among word specify prefer transcription translation replication represent separate initiation elongation termination represent separate set process phase however incorporate domain knowledge topic model table two yeast leave column show seed word model middle indicate least two seed word among fifty highest probability word column give number share another word right show model one two one two one one two three four five six seven eight one two three four five six seven eight nine ten transcription transcriptional template translation translational replication cycle division initiation start assembly elongation termination disassembly release stop seven one three two corpus use experiment consist abstract select relevance yeast induce topic model use encode link describe use standard control set five one word use seed concept table two show include among fifty probable word make several first concept represent small number word topic occur highly probable word second link obey final third use process phase compositionally example topic four represent transcription initiation topic eight represent replication initiation moreover significantly influence prior typically include highly relevant term among probable word example top word topic four include promoter recruitment specifically germane composite concept transcription initiation case standard seed concept word disperse across greater number highly relate word cycle division often fall topic many induce ordinary semantically coherent specific suggest prior naturally emerge without use work support grant alumni research foundation reference k constrain cluster advance theory chapman j correlate topic advance neural information process els eighteen press jordan latent allocation journal machine learn research three c smyth p model document combine semantic unsupervised statistical learn semantic web springer type one statistics theory twenty n z b x may wish come true study wish recognize human language annual north chapter computational linguistics press l find scientific academy unite state j r c r number maximal independent set connect graph discrete math li w allocation mixture model topic machine learn press p distribution technical report tam correlate latent semantic model unsupervised adaptation acoustics speech signal process gene ontology consortium gene ontology tool unification biology nature genetics