six knowledge data engineer vol eighteen one text classification without negative revisit x member computer society fellow build classifier require two set positive negative paper study problem build text classifier use positive p unlabeled unlabeled mix positive negative since negative example give explicitly task build reliable text classifier become far challenge simply treat unlabeled negative build classifier thereafter undoubtedly poor approach tackle problem generally speak study solve problem heuristic first extract negative n second build classifier base p n surprisingly study try extract positive intuitively enlarge p p zero positive extract build classifier thereafter enhance effectiveness classifier throughout study find extract p zero difficult document possess feature exhibit p necessarily mean positive example vice large size high diversity also contribute extract p zero paper propose label heuristic call tackle problem aim extract high quality positive negative use top exist extensive experiment base several conduct result indicate highly feasible especially situation j extremely small index mine text categorization partially supervise learn label unlabeled data one introduction text classification supervise learn task task assign value pair two k domain document k set task approximate true function k mean function b k b coincide much possible twenty function b call classifier coincidence measure effectiveness also know quality classifier classifier build train systematically use set train document document belong label accord k specifically give category k two k order build classifier k determine document belong k document belong k document belong k label positive train instance p document belong k label negative train instance n fig show typical framework build classifier note effectiveness classifier strongly affect precision label document classifier build thereafter department engineer engineer management university hong new hong china research center thirty saw mill river road new york manuscript receive seven revise nine june accept june publish eighteen information obtain reprint article please send reference log number strongly bias poor quality must result unfortunately label document time consume labor intensive task bottleneck construct reliable classifier due rapid growth information available task accurately label train document positive negative become difficult order overcome bottleneck attempt divide train document three set small set positive small set negative large set unlabeled build classifier use three set document one two six sixteen infeasible however obtain set negative sometimes still expensive recently new direction build recognize build use small set positive p large set unlabeled five nine thirteen eleven ten twelve note negative give kind problem sometimes know partially supervise learn ten partially supervise learn summarize follow one size give positive p small may feature distribution possible represent positive two unlabeled mix positive negative three negative example give generally speak exist approach target solve problem use show fig first step negative n extract unlabeled second publish computer society al text classification without negative revisit seven fig one comparison different learn step classifier use give positive example p extract negative n surprisingly none exist try enlarge give positive p extract positive p one exception previous work five two throughout study find extract p zero difficult reason simply compare feature p help extract p zero document possess feature exhibit p necessarily mean positive example one paper nine eleven thirteen modify traditional classification algorithm build classifier detail discuss later two although twelve also build classifier enlarge positive document work different whole learn process automatic twelve require manual index large size high diversity also contribute extract p zero paper propose label heuristic call positive negative label heuristic extension preliminary work five fig show general framework consist two step extraction enlargement step one extraction step aim extract set negative call reliable negative n unlabeled extraction base concept core vocabulary concept first introduce previous work five core vocabulary contain feature feature greater threshold however contrast five threshold must paper propose new function eight knowledge data engineer vol eighteen one compute feature strength effectively threshold determine automatically help core vocabulary positive follow use positive feature short extract n apart issue relate compute feature effectively determine threshold core vocabulary automatically paper address question whether two document consider positive contain number different positive feature base extensive experiment find two document number positive feature necessarily mean positive paper propose set novel function determine whether document consider positive example base rank new increase number reliable negative meanwhile maintain quality extraction step two enlargement step aim extract set positive p zero another set negative n zero zero zero fourteen n enlarge p p zero n n zero consist two step first partition reliable negative n extract k cluster contrast five number partition k domain paper present determine k dynamically automatically k may vary among even within domain second extract p zero n zero zero enlarge p n contrast five consider similarity among p ni zero perform extraction paper take number feature within domain consideration find reduce number feature enlargement step significantly reduce still retain many positive negative extract reason reduce number feature improve quality extraction due fact p ni reduce number feature p ni possibly diminish noise therefore improve quality extraction one two finally use p p zero positive train instance p n n zero negative train instance n build classifier note apply domain independent classifier build conduct extensive experiment address two issue one much achieve top several representative two much achieve comparison three definition noisy discuss section classifier result indicate approach highly feasible even set positive extremely small need build complex classifier report nine eleven thirteen rest paper organize follow section two present detail section three review major tackle problem partially supervise learn section four evaluate work summarize conclude paper section five two positive negative label overview positive negative label heuristic show fig algorithm one consist two step extraction enlargement objective extraction extract set reliable negative n unlabeled line one algorithm one objective enlargement extract positive p zero negative n zero n line three algorithm one enlarge p n algorithm one p input p positive unlabeled output p positive train n negative train one n p two zero n three obtain p zero n zero call partition p n zero four p p p zero five n p p zero six return p n extract reliable negative absence prior knowledge negative best way extract set negative use feature give positive p unlabeled follow call negative document extract reliable negative denote n show reliable n experimental study section four two main step namely identify positive feature extract reliable negative identify positive feature positive feature feature frequently appear p represent p well identify positive feature base notion call core vocabulary note document belong p must possess feature contain core vocabulary p denote remark accord extract reliable negative n unlabeled remark document belong positive p must possess feature contain core vocabulary p al text classification without negative revisit nine stress try identify negative simply use concept core vocabulary aim identify negative reliable step fact impossible extract negative simply compare feature feature text domain sparse nineteen core vocabulary p identify compare feature among feature exhibit p without loss generality let function compute feature strength particular feature feature assign ie two greater threshold feature higher feature strength indicate effective term classification follow give detail compute function show identify threshold important know need specify explicitly remark let function compute feature strength particular feature fa effective higher discriminative power term classification furthermore two two approximate feature strength find probabilistic theory information gain two odd ratio mutual information twenty unfortunately none suitable solve problem reason prior knowledge feature distribution positive negative entire document domain furthermore impossible obtain estimate value result obtain feature difficult task use exist compute paper approximate propose formula utilize information present p let nu denote number document contain p respectively compute measure normalize document frequency p fourteen nu one maximum value minimum value two p similar formula apply one first component second component correspond normalize number document contain p respectively although exist work also compute base none pay attention significant normalization however normalization critical j extremely large j compute normalization one make possible compare true relative feature p moreover one robust since capture idea feature important distribution highly skew toward p specific normalize frequency feature p similar feature strength approach zero ie fourteen zero skew toward one set feature strength either strictly positive distribution skew p strictly negative distribution skew recall identify extract determine average two p ie fourteen one x two number different feature p extract reliable negative show compute determine select set positive feature formulate core vocabulary positive another important issue give document many positive feature document order consider potential positive example word number positive feature reliable negative example exceed choice number sensitive vary different document exist single number apply document different although precision extract n high recall would low ie size n would small turn make effort step ineffective contrast use large number recall would high precision would low use small number sight propose approach determine whether document regard reliable negative example consider two document di contain number feature document di contain feature higher rank say di likely positive example base rank feature use one assign unique value every feature call positive reference power positive reference power compute base standard exponential distribution fifteen fourteen e one two one j three r rank note zero one give document di let positive reference power di compute simply average di ie feature fourteen two di four one x j four base observation number positive small diversity high large number positive feature di document di consider reliable negative example ten knowledge data engineer vol eighteen one fig two p enlarge recall classifier would increase without enlarge p b enlarge p p zero define average document p enlarge set positive negative fourteen one j j x remark use h g together extract higher quality set reliable negative h alone g allow document contain higher rank feature negative exclude document contain many lower rank feature set negative algorithm aim extract n outline algorithm two take two input small set positive p large set unlabeled document line thirteen compute one feature fi belong p line four compute threshold use two core vocabulary p determine line line eleven compute threshold five finally reliable negative extract line result return line eighteen compute use one algorithm two p input p positive unlabeled output n reliable negative one two p two three end four compute use two five six two p seven eight end nine ten end eleven compute use five twelve n thirteen di two fourteen fifteen end sixteen seventeen end eighteen return n n n five previous section discuss extract reliable negative n unlabeled base positive p section present extract positive p zero another set negative n zero zero zero fourteen n enlarge give p p zero previously extract n n zero enlargement motivate follow two j small number positive train inadequate p reflect true feature distribution positive domain enlarge total number positive increase recall classifier fig two illustrate idea p enlarge situation positive wrongly negative fig possibly correct fig j large j large number reliable negative n extract previous step small enlarge total number negative increase precision classifier fig three illustrate idea n enlarge situation negative wrongly positive fig possibly correct fig text distribution feature sparse typically number feature range common feature among document hence even enlarge p n entire space occupy positive negative may change significantly short enlarge p enlarge n properly improve quality classification boundary positive negative change dramatically unfortunately properly extract p zero n zero zero challenge task difficulty extract proper set p zero zero cause fact document zero possess feature may necessarily positive example addition proportion p zero zero usually small make extract p zero even harder al text classification without negative revisit eleven fig three n enlarge precision classifier increase without enlarge n b enlarge n n zero discuss issue follow section detail follow section first discuss possible inappropriate approach extract p zero present solution base partition strategy possible inappropriate approach section discuss three intuitively possible inappropriate approach extract p zero n zero summarize discuss detail follow section reverse extraction approach approach adopt idea describe previous section implement reverse direction first construct core vocabulary reliable negative n extract p zero zero none document p zero possess feature list simple text classifier construction approach approach first build traditional text classifier base p n zero document positive regard p zero document negative regard n zero approach report eleven ten thirteen extract n zero p zero claim extract high quality p zero approach approach first construct profile class p n give document zero order whether belong p zero n zero compare distance profile p distance profile n technique adopt ten eleven extract n zero p zero reverse extraction approach inappropriate extract p zero n zero note consist many different focus one single topic text classification level diversity always high since n n inherit property every topic n contain set core vocabulary large number different n cancel significance core vocabulary eventually none feature n properly select feature select poor quality simple text classifier construction approach possibly extract high quality n zero impossible extract high quality p zero argument base two major classifier like regression model classifier like classifier approach may wrongly many negative p zero reason follow recall n consist diverse cover large region feature space hand p focus one single topic cover much smaller region feature space j small p may inadequate reflect true feature distribution positive instance domain specific let us take example show construct decision function maximize boundary positive train instance negative instance minimize figure n cross p black dot different note j fig smaller fig let us refer fig first number positive train instance inadequate may generate wrong decision function boundary adjust correct one show fig explain classifier always extract p zero high recall low precision whereas extract n zero high precision low recall classifier although like relay statistical distribution train data extract good set positive p zero recall document contain feature list imply belong p accord probabilistic nature feature two document may share high degree similarity even though belong different two document often nearest neighbor belong category table one show percentage document k nearest neighbor belong different use three five discuss section four use cosine measure compute similarity twelve knowledge data engineer vol eighteen one fig four decision small p inadequate b large p adequate summary although percentage may vary data set data set confirm nearest neighbor behavior threshold k never set small value text classification typically k around seven order construct classifier first define proper k tune proper threshold decision function two issue difficult answer partially supervise learn problem first large k good problem set j always small small k may obtain poor result second j small obtain validation data set impossible hence tune proper difficult occur classifier unfortunately approach twenty seventeen possibly solve problem however still inappropriate extract high quality p zero formalize let us consider two profile denote document p n respectively suppose use cosine coefficient measure similarity document profile two fourteen k k k k two six note normalize unit length account length different document twenty ie k k fourteen one simplicity let us use positive negative denote correspond profile ie fourteen di fourteen seven one x j p j one x j n j di table one percentage document whose first five nearest neighbor belong different let us consider numerator six become fourteen di eight one x j n j short similarity nothing average similarity document n divide k one n large consist many diverse obviously never similar document n thus extremely small explain precision p zero extract zero low observation apply compute similarity coefficient dice coefficient similarly also suffer conclusion difficult extract p zero zero build kind classifier simply use p n directly feasible approach section present second step approach extract p zero n zero zero recall n consist diverse diversity make approach discuss far impractical extract p zero n zero zero key issue therefore find way deal problem diversity follow discuss solve problem use three main namely partition reliable negative assign document p zero n zero iterative enlargement partition reliable negative approach extend idea approach partition n k partition document partition share high degree similarity partition focus smaller set relate feature order extract p zero n zero zero compare similarity document zero centroid p centroid ni fourteen one two k partition reliable negative example n apply kind exist cluster algorithm paper simplicity adopt classical cluster algorithm three eight common question regard cluster best number cluster k k large problem rise four al text classification without negative revisit thirteen approach appear k small problem rise approach appear suppose optimal number partition k exist optimal number vary one domain furthermore even domain different different optimal value p paper determine k base size give positive p reliable negative n k fourteen j idea behind k need control way maintain reasonable number document cluster ni k large j number document cluster ni may small value confirm base extensive test result give consider two x x positive extract reliable negative respectively suppose set extract reliable negative much smaller ie specifically j small small turn make ni become similarly j large large turn make become smaller consequently diversity smaller therefore set small k partition large k partition lead follow remark j large small k small whereas j small large k large equation reflect relationship among j k show effective performance study assign document p zero n zero without loss generality suppose partition set reliable negative n k cluster discuss use cluster algorithm section show assign document zero p zero n zero enlarge total positive train instance p total negative train instance n fig continue discussion let us stress document assign ni document assign n zero assignment document two zero either p zero n zero do simply compare use six recall p ni word say positive example less cluster ni use six say negative example greater cluster ni use six reason term similarity measurement may similar p zero ni need extract document significantly similar either p ni document similar p ni regard ambiguous ignore propose follow strategy document zero belong p zero satisfy follow condition p fourteen p nine ten p average similarity document within p p average difference document p ni n p fourteen one j one j x x p fourteen k eleven twelve note p guarantee similar p p ensure similar p n similar fashion document belong n zero follow condition hold ni fourteen n ni fourteen one x n fourteen one k k x one x sixteen thirteen fourteen fifteen strategy work well size positive p reasonable size however set positive may vary greatly j small say around ten similarity among document p high consider extreme case j fourteen one similarity one imply p one p large result document zero select positive make ni j large p contain many different feature distribution feature always sparse text chance document process positive feature positive example increase moreover similarity within p low turn make p p small many document zero similar p ni eventually ambiguity appear make ni mathematically follow seven six follow fourteen one one one j x one x fourteen seventeen eighteen important notice dominate factor feature always sparse nature result summation always small value thus comparison well likely affect size feature set rather feature exhibit p ni suggest follow seven construct simply average weight feature correspond p ni fourteen knowledge data engineer vol eighteen one paper propose efficient effective approach construct use set representative feature correspond centroid consequently dense make comparison similarity document centroid document centroid efficient strategy give follow remark algorithm three show step construct first line one select top n representative feature p n work set n fourteen three zero base series test select feature range one zero five zero detail issue feature selection would discuss experimental study section centroid top n feature exhibit p line centroid top n feature belong belong ni line algorithm three centroid p input p positive document negative document different partition output centroid positive class negative different partition one l top n feature rank information gain p n two three fi two p four five end six construct weight average weight feature p two l two ni seven ni two eight nine ten eleven twelve thirteen fourteen end end construct weight average weight feature n fifteen end sixteen return iterative enlargement whole enlargement process extract p zero n zero zero outline partition algorithm algorithm four algorithm repeatedly extract negative zero document zero line extract extract n zero begin extract p zero zero line obtain set positive train instance p merge give p extract p zero set negative train instance n merge two set extract negative document n n zero accord p n classifier build algorithm four partition p n zero input p positive document n negative document unlabeled document output p zero positive document n zero negative document one p zero n zero two repeat three four five six seven eight partition n n zero k cluster obtain call centroid p zero n zero fourteen zero two zero satisfy condition nine ten satisfy condition thirteen fourteen n zero n zero end nine ten eleven end twelve thirteen fourteen zero fourteen zero n zero fifteen two zero sixteen p zero p zero seventeen end eighteen nineteen end twenty return p zero n zero three relate work satisfy condition nine ten satisfy condition thirteen fourteen section briefly discuss exist work relate extraction enlargement negative five major approach extract n give p namely ten eleven spy thirteen regression nine naive bay twelve label heuristic propose ten eleven order extract n first build traditional classifier seventeen use give p positive train example negative train example document use classifier document negative form negative train n eleven al also show use iteratively extract set negative n zero zero fourteen n possibly improve effectiveness classifier spy spy propose thirteen first randomly select set document p put default size fifteen percent p serve spy document assume spy document behave similar unknown positive hop infer behavior unknown positive run algorithm use set fourteen p complete result classifier use probabilistic approach base decide threshold identify n eleven al show extract n zero zero use iteratively improve effectiveness classifier propose first build feature set f contain feature occur p frequently document al text classification without negative revisit fifteen contain feature f n eleven al show extract n zero zero use iteratively improve effectiveness classifier j small deteriorate j large however improve effectiveness classifier j large deteriorate j small heterogeneous learner use similar sophisticate approach web page classification however include experimental study get satisfactory result use weight logistic regression heuristic propose nine use logistic regression weight together performance measure estimate label positive document unlabeled document select regularization parameter validation set author claim heuristic apply data set text data naive bay heuristic propose twelve instead label document heuristic label set representative word category human use word extract set document category set unlabeled document form initial train set em apply build final classifier since heuristic require significant human index evaluation subjective include experimental study four experimental study extensive experiment conduct verify effectiveness document punctuation number web page address address remove feature stem convert lower case feature appear one document ignore feature weight use traditional schema eighteen normalize unit length twenty different schema implement influence insignificant none due space limit evaluate effectiveness classifier report value twenty balance precision recall unless otherwise specify use follow default one two three four use linear kernel c fourteen ten naive bay use multinomial version turn value optimal value feature selection apply naive bay report best score experiment setup follow use follow recent trend use split separate data train set evaluation set note highly skew category contain train category contain one train example thus decide follow ten select top ten train evaluation one popular ways text classification evaluation fourteen seven ninety use ninety train evaluation standard method use text mine evaluation task total twenty different category contain message note number message category nearly message assign single category message assign multiple category randomly select eighty percent message train data remain twenty percent test data web page seven web page belong one one category slightly skew category contain web page whereas category contain web page category randomly select eighty percent web page train data remain twenty percent test data conduct follow experiment category k target randomly pick x percent document belong category k use document form positive p remain train document regard unlabeled experiment repeat n number time report average result n thirty x range one two nine small case ten twenty large case note x fourteen percent positive use case design total nineteen case nine small case nine large case one case versus without section evaluate effectiveness label unlabeled train use three popular naive bay give set positive p set unlabeled document classifier compare two case use versus use former positive negative three p n later positive negative three p table table two three four five show result use first column give number percentage total number positive use second three show accuracy three use last three show accuracy three use show table use label train show higher accuracy particular number positive train become three outperform case without x fourteen fifty percent exception table five x fourteen one percent obtain one without comparison zero sixteen knowledge data engineer vol eighteen one table two result ten table four result table three result ninety table five result note capability tolerant much better general perform acceptably large case forty percent accuracy deteriorate significantly even number positive decrease slightly positive interest see table four x fourteen percent accuracy obtain perform even better indicate possibly improve classification result obtain traditional classification traditional classification document belong particular category label negative train however document belong particular category necessary fall negative train set content document six general two noise noise feature noise train data former one usually solve apply feature selection information gain two whereas latest one relate precision label train data positive negative paper unless otherwise specify term noise refer noise train data may similar positive train simply put document negative train set effectiveness classifier possibly affect j large effectiveness use perform marginally well boost effectiveness seldom harm reliability reliable negative step one recall step one extract set reliable negative answer reliable extract reliable negative use two measure precision purity purity mean percentage positive mix reliable negative let p whole set positive note p set x percent sample positive n set extract negative purity fourteen one nineteen al text classification without negative revisit seventeen quality reliable negative document table six table seven significance step two follow recent trend report treat whole compute correspond value table six show result different precision purity high effectiveness enlargement step two recall step two extract set example enlarge give positive reliable negative extraction step two base distance positive negative show significance enlargement compare several case first three case give case one extraction without enlargement case two extraction enlargement positive case three extraction enlargement negative table seven show result ten behave similarly table seven column show result without comparison result well three case obtain use significantly outperform three case small case support claim motivation paper whether quality classifier enhance include positive document train set especially j small ten x fourteen forty percent x fourteen percent degrade slightly comparison case one without enlargement process do possibly introduce however marginal also show table seven j small say one percent influence p zero important n zero case two superior case three small case suggest enlarge p zero important enlarge n zero p small furthermore show significance feature selection algorithm three additional case eighteen knowledge data engineer vol eighteen one table eight necessity feature selection case four without feature selection l algorithm centroid algorithm three set feature p n instead without selection rank experiment three different common feature selection information gain two mutual information twenty find insignificant none result report table eight base information gain refer table eight see feature section important procedure ensure higher accuracy feature selection especially critical two reason number feature significantly total around different feature whereas around feature large number different feature imply level noise present data set higher different label comparison label independent use refer fig one section compare discuss section three use text classifier reason choose threefold one require feature selection require feature selection step do less conclude significance specific label approach result approach may two sensitive noise show use classifier give clear picture precision different label three best report classification algorithm use possibly know best label heuristic follow name follow table nine ten eleven twelve show result table nine result ten table ten result ninety al text classification without negative revisit nineteen table eleven result table twelve result j fourteen eighty percent acceptable f one five j large j fourteen forty percent ten j fourteen sixty percent ninety j fourteen eighty percent outperform inferior perform second best although outperform case outperform significantly even j extremely small forty percent vary much regardless size p addition perform best case finally table thirteen show representative result skew data set ninety usually use evaluate classifier skew data set document evenly distribute among would similar hence report former one evenly distribute latter one slightly skew show table thirteen outperform approach five conclusion present heuristic build text classifier use positive p unlabeled negative example give explicitly exist solve problem focus extract negative n extract high quality positive p zero therefore fully utilize information unfortunately extract p zero difficult difficulty due topic diversity order solve diversity problem propose heuristic contain two step extraction enlargement step one extraction extract negative n base concept call core vocabulary g function core vocabulary construct base concept call feature strength compute base h function threshold g function together threshold use determine whether document consider potential positive example note compute automatically step two enlargement enlarge give p extract n heuristic partition n k cluster assist us extract positive p zero negative n zero zero zero fourteen n k determine dynamically automatically category may different k even domain conduct extensive experiment use three result indicate approach highly feasible significantly outperform j extremely small result ninety table thirteen twenty knowledge data engineer vol eighteen one reference one support vector machine advance neural information process vol eleven j craven exploit among acquire weakly label train data l machine learn p refine initial point cluster l machine learn two three four cut jo approach browse large document l research development information retrieval five h text classification without negative l data six r combine label unlabeled data text categorization l machine learn seven text categorization support vector machine learn many relevant feature machine learn eight b c fast effective text mine use document cluster fifth l knowledge discovery data mine nine lee b learn positive unlabeled use weight logistics regression l machine learn ten x li b learn use positive unlabeled data l joint artificial intelligence eleven b x li lee build text use positive unlabeled third l data mine twelve b x li lee text classification label word nat l artificial intelligence thirteen b x li partially supervise classification text document l machine learn fourteen k comparison event model naive bay text classification nat l artificial intelligence workshop learn text categorization fifteen apply statistics probability engineer second sons sixteen k text classification label unlabeled document use em machine learn vol j relevance feedback information retrieval smart retrieval automatic document process g prentice hall seventeen eighteen g c approach automatic text retrieval information process management vol five nineteen h da hull jo comparison document rout problem l research development information retrieval twenty f machine learn text compute survey vol one yang study text l research development information retrieval yang x text categorization l research development information retrieval h chang j han heterogeneous learner web page classification second l data mine h j han chang positive example base learn web page classification use ninth l knowledge discovery data mine h j han chang web page classification without negative knowledge data vol sixteen value unlabeled data classification l machine learn receive beng degree degree engineer engineer management university hong currently candidate study university research interest include text mine time series mine publish various paper several relate include receive best student paper award five member x receive computer science university japan respectively research fellow faculty member institute information electronics university lecturer department computer science national university currently associate professor department engineer engineer management university hong major research interest include data mine data stream mine process data warehouse analytical process query process optimization member computer society receive degree university china department computer science university join hong university science technology work engineer academy space technology principal research scientist computer science center professor school compute national university research interest data knowledge base management emphasis query process optimization physical design performance recent research work include data quality data warehouse data mine management data also interest development electronic business receive degree electrical engineer national university electrical engineer university degree new york university j research center currently manager tool group research interest include data mine parallel distribute process performance model publish paper referee hold apply us patent associate editor technology member data engineer steer committee also steer committee conference data mine knowledge data engineer editor advisory board member guest coeditor special issue mine also serve associate editor knowledge information program chair committee member many international receive several include two outstanding innovation award outstanding technical achievement award two research division award plateau invention achievement award receive outstanding award international conference data mine also region one award promote perpetuate numerous new electrical engineer master inventor fellow fellow honor