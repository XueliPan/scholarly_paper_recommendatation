replicate heterogeneous timothy r computer science federal de brazil electrical computer engineer state university abstract increase multicore processor parallelism flexibility accelerator turn traditional hierarchical heterogeneous compute fully exploit parallel system design remain open problem moreover current tool development parallel hierarchical concentrate use single processor type several heterogeneous show make use heterogeneous compute significantly improve application performance approach consist optimize efficiently application task execution available process evaluate context replicate propose develop implement integrate system target internode parallelism experimental result complex application show approach nearly double performance implementation distribute heterogeneous accelerator cluster subject processor multiple data stream computer graphics hardware general term design performance one introduction current advance computer architecture traditional distribute fast transform hierarchical compute node may multiple multiple core moreover use modern graphics fast computation become popular significantly improve performance certain condition two trend develop simultaneously need design heterogeneous compute problem discuss paper efficiently execute heterogeneous cluster multicore propose evaluate context program model type decompose filter replicate multiple nod distribute system filter communicate use logical stream capable deliver data instance source filter instance destination filter feel model naturally expose large number independent task execute concurrently multiple filter framework multithreaded different cod execute heterogeneous generalizable similar important facet work performance mean yield depend type computation input data moreover task generate know prior execution therefore decision run task make task create therefore approach assign task device base relative performance device way optimize overall execution time extend previous work consider well internode parallelism also present new performance estimation handle data transfer main performance estimation module task relative base input algorithm efficiently data transfer automatically provide asynchronous data copy back forth novel stream communication policy heterogeneous efficiently use cluster equip multicore accelerator thirteen permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior june figure one flow chart although important active research topic generate code beyond scope paper assume necessary code run application provide programmer focus efficient execution heterogeneous relate work relegate problem programmer either focus accelerator performance assume device constant task program refer mention program research base compiler eighteen specialize seven twelve also though focus easily adapt multiple different rest paper organize follow section two present use case motivate application section three present anthill framework use evaluation propose extend target multiple section four present performance estimation approach section five describe extensive experimental evaluation present section six summarize section seven two motivate application section describe neuroblastoma image analysis system application develop al use motivate example well evaluation platform discuss paper assist determination prognosis neuroblastoma cancer sympathetic nervous system mostly affect prognosis disease currently determine expert base visual examination microscope tissue slide slide different prognostic group condition differentiation grade among issue manual examination time consume process therefore goal assist determination prognosis disease digitize tissue sample different prognostic significance focus application classification stromal development either one morphological criteria disease prognosis contribute categorization histology favorable unfavorable since slide image high resolution x color first decompose image smaller image tile process independently analysis do use strategy construct pyramid representation multiple copy image tile decomposition step one different resolution example analysis could construct fourteen tile size analysis tile proceed start resolution stop resolution level classification satisfy predetermine criterion tile classification achieve base statistical feature characterize texture tissue structure therefore first apply color space conversion lab color space color intensity separate difference two value perceptually uniform enable use distance feature calculation texture information extract use statistics local binary pattern help characterize color intensity tissue structure finally classification confidence particular resolution compute use hypothesis test either classification decision accept analysis resume higher resolution one exist result image analysis classification label assign image tile indicate underlie tissue subtype background figure one show classification stromal development detail find application originally implement program model set five image reader read tile disk color conversion convert image tile color space lab color space statistical feature responsible compute feature vector statistics classifier calculate conduct hypothesis test decide whether classification satisfactory v control process flow computation start read tile resolution restart tile higher resolution classification insufficient accord classifier filter loop continue tile satisfactory compute highest resolution since color conversion statistical feature filter intensive implement experiment start original neuroblastoma except utilize simultaneously difference application code change affect support choose motivate application important complex classification system believe skeleton similar broad class approach divide image several tile independently process pipeline find instance several image process furthermore process strategy concurrently generate task different execution time also present various research computer vision image process medicine signal process thirteen fifteen experience program model bottleneck free number active internal task higher available eight eleven seventeen thus propose approach exploit heterogeneous consist allocate multiple task concurrently perform best detail section three also computation time task dominate overall application execution time cost communication latency offset computation although premise valid broad range also interest work linear algebra computation multicore instance include heterogeneous compute two amount application concurrent task smaller number author improve application performance use schedule take account task order increase number concurrent active task hence allow take advantage multicore three anthill anthill system base model decompose process stag call filter communicate use unidirectional stream application describe multigraph represent logical interconnection filter four run time anthill spawn instance filter multiple nod cluster call transparent copy anthill automatically handle communication state partition among transparent copy filter program abstraction provide anthill programmer provide function match input buffer multiple stream set create event queue anthill control nonblocking io issue necessary approach derive heavily program model six forty programmer also provide handle function filter automatically invoke model amount asynchronous independent task filter multithreaded multiple task spawn provide pending compute feature essential exploit full capability current multicore heterogeneous also use spawn task multiple accomplish anthill allow user provide event multiple specific device figure two filter architecture fifteen figure two illustrate architecture typical filter receive data multiple input stream generate event queue handler function associate show function implement target different type event queue event depict picture responsible consume queue invoke appropriate accord availability compute consume eventually data generate filter need forward next filter do system though depict figure assignment thus queue immediately assign processor rather occur become idle new queue current implementation task assignment policy use default strategy event first decision policy select queue execute decision make fashion provide handle function available processor otherwise next queue select whenever queue select event queue dispatch simple approach assignment different accord relative performance transparent way four performance estimator highlight core propose paper fact relative performance data dependent mind decision assign task delay determine relative performance central whole approach despite fact model performance open challenge nine sixteen believe use relative fitness measure performance algorithm run different accurate enough far easier predict execution time however task leave application programmer rather part system propose performance estimator propose solution depict figure three use strategy first phase new application implement representative execution time store profile generate phase consist application input target execution time construe train use actual performance prediction second phase implement model learn algorithm employ different estimate target relative performance however important notice model behavior base input beyond basic regression model also beyond scope paper study specific problem propose final solution rather propose algorithm validate experimentally show yield sufficient accuracy decision make algorithm use ten model learn algorithm new application task create k nearest profile retrieve base distance metric input execution time average use compute relative task different employ distance metric first normalize divide value highest value dimension use distance attribute hand give distance zero attribute error time error description heart simulation eclat option price simulate body source simulate electrical heart activity find neighbor calculate frequent neuroblastoma section two anthill anthill eleven table one evaluate performance estimator prediction although prediction methodology employ empirical evaluation interest different reason task assignment rely relative performance estimation perform better requirement also use predict execution time application different instance execution time one device available time second processor could calculate utilize relative performance advantage approach estimate execution time error would equal error predict believe relative performance easier predict abstract effect like conditional loop break highly affect execution time model moreover relative performance try model application run program five performance section discuss several improve performance replicate heterogeneous cluster machine first present approach reduce impact data transfer use asynchronous copy mechanism next section present technique better utilization lastly section propose novel stream communication policy improve performance heterogeneous cluster compute nod different task assignment improve data transfer limit critical barrier efficient execution several cost data transfer comparable computation time moreover limitation strongly influence application design increase program challenge reduce collaboration approach reduce idle time data transfer overlap data transfer useful computation similar use double buffer use keep busy data transfer among memory multicore thirty approach employ work also base overlap communication computation double buffer may appropriate technique allow multiple concurrent transfer among thus problem provide efficient data transfer become challenge performance improve saturation point increase number concurrent transfer unfortunately optimal number concurrent transfer vary accord rat task figure three anthill relative performance estimator complete match otherwise one purpose evaluate effectiveness performance estimator evaluate six representative describe table one technique describe evaluation two main purpose understand propose technique perform acceptable estimation discuss relative performance easier predict accurately execution time result show obtain perform use thirty job execute estimator calculate use k two utilize achieve average error application show table one first methodology accuracy high since error higher fourteen average error among use case application whose accuracy average error level impact performance sufficient maintain optimal order task also use approach estimate task execution time use second evaluation simply compute predict execution time average k nearest sample execution time execution time error also show table much higher er sixteen process size transfer data show section solution overlap communication computation reduce processor idle time consist assign multiple concurrent process overlap data transfer computation define number maximize application performance assume data copy data buffer receive filter input stream case kernel input data receive data buffer function rewrite accord filter algorithm one algorithm control data transfer two two zero w zero zero event event zero event zero event zero event send event one two two two one copy kernel input data run kernel typically necessary copy output result back send downstream stage instead copy result programmer use anthill send output data next filter pass address data environment pointer data anthill transparently start asynchronous copy data back send next filter also important highlight asynchronous copy mechanism use stream anthill event associate single stream application performance dramatically influence idle time propose approach dynamically configure number concurrent asynchronous data copy accord task performance solution base approach change number concurrent assign accord throughput application algorithm algorithm one start two concurrent increase number throughput begin decrease previous configuration save next set algorithm continue search better number concurrent data copy start save configuration order quickly find saturation point algorithm increase number concurrent data copy exponentially throughput begin decrease thereafter algorithm simply make change one concurrent data copy time since current allow concurrent transfer one direction algorithm design mind maximize performance give limitation dispatch multiple transfer execute event process finally transfer data back sequentially data transfer direction group asynchronous concurrent data copy mechanism use driver default synchronous copy version although depict algorithm guarantee number never smaller one maximum size bound available memory task assignment problem assign task heterogeneous target research long time one two three five fourteen fifteen nineteen recently increase ubiquity scientific community examine use nod increase detail mar twelve implementation program model nod author evaluate collaborative use map reduce task divide among use fix relative performance system argue process rat system dependent data size generate model process rat system train phase determine best split work among successive application however certain class image analysis application process rat various mean static partition optimal case indeed many internal task exhibit performance experimentally show take account significantly improve application performance heterogeneous process previously study fifteen work author target map schedule task onto heterogeneous parallel task execution time vary accord data show process rate variability leverage give extra performance order exploit task heterogeneity propose implement task assignment policy call dynamic weight anthill event module previously show section three assignment task share among inside single node assign processor become idle first step select stream process do fashion main difference second phase event choose select stream phase choose accord weight may vary execution time value compute estimation event performance process device example value could event likely execution time device compare processor processor system execution weight use order device certain processor available highest weight event queue choose process therefore assign fashion instead use speculative execution reduce negative impact data use sort accord suitability device seventeen also important highlight require exact value task necessary relative order accord performance estimator describe section four sufficient accuracy purpose dynamic selective stream distribute performance heavily dependent upon load balance overall execution time node previous deal receive single instance filter optimize globally however multiple instance filter need consider instance receive process message send present novel stream communication policy optimize distribute heterogeneous multicore compute fully utilize achieve maximum performance satisfy two premise motivate propose policy number data buffer input filter enough keep busy make possible exploit available high create load imbalance among filter instance data buffer send filter maximize performance allocate filter instance base premise propose dynamic selective stream odds policy stream policy implement n direct communication channel n instance producer filter fi instance consumer filter odds implement policy instance receiver filter consume data different rat accord process power instance consume data different rat important determine number data buffer need instance keep fully utilize moreover discuss previously number buffer keep queue short possible avoid load imbalance across compute nod two obviously contradictory pose interest challenge additionally ideal number data buffer filter input queue may different filter instance change application execution progress data stream change data buffer change time communication time vary due load sender filter instance example odds comprise two dynamic queue adaptation algorithm data buffer selection algorithm responsible premise responsible premise next two describe two detail dynamic queue adaptation algorithm solution control queue size receiver side derive develop al twenty transport protocol control flow congestion network continuously measure network response packet round trip time adjust transmission window number transit purpose continuously measure time take request message answer upstream filter instance time take processor process data buffer detail figure four base ratio request response time data buffer process time decide whether length number data buffer assign filter instance include data buffer transfer plus receive queue must increase decrease leave unaltered responsibility thread eighteen algorithm two tid tid one tid zero w zero tid tid tid tid algorithm three tid w tid tid p p p insert tid figure four receiver thread compute target request size finish process data buffer update current request size necessary parallel thread observe change target stream request size whenever fall target value instance upstream filter contact request data buffer receive store filter request occur create single queue receive data buffer buffer reside share receive queue also maintain queue data buffer processor type sort data buffer processor data buffer selection algorithm approach select data buffer send downstream base expect value give data buffer process certain type processor algorithm similar one describe select task give device also rely performance estimator accomplish whenever instance filter demand data input stream request include information processor type cause request issue accord figure four generate specific request message event handler thread upon receipt data request upstream filter instance select among queue data buffer best suit processor type algorithm propose run sender side stream maintain queue data buffer keep sort type processor versus processor instance fi receive request choose send data buffer highest request processor remove buffer sort queue algorithm four w algorithm five w figure five sender thread receiver side state share queue use minimize load imbalance run system figure five show operation thread activate time filter fi instance send data buffer stream insert buffer node compute type processor whenever processor run instance request data buffer filter instance fi thread process request message execute send select data buffer request filter six experimental result carry experiment cluster node equip one core two duo two main memory single cluster interconnect use switch experiment use one core node assign manage available run task run experiment multiple time maximum standard deviation less present average result show section calculate base single version application also fuse filter avoid extra overhead due unnecessary data transfer network communication thus evaluate use already optimize version application effect tile size performance first set experiment analyze performance function input image resolution evaluation generate different use fix number tile vary tile size also assume tile successfully first resolution level compute tile single resolution versus one core show various tile size figure six result label synchronous copy result show high relative performance performance similar tile almost time faster tile small task overhead use proportional analysis execution time make use inefficient figure six also show performance strongly affect input tile size moreover variation show different type process different performance performance heterogeneity could observe nineteen figure six synchronous asynchronous copy linear algebra data mine like eclat employ pyramid thirteen parallel multiple process task process different tile concurrently make performance vary accord task process heterogeneity create demand efficiently use collaboratively discuss section effect data transfer result approach improve data transfer present section sake evaluation use two vector divide vector small chunk copy iterate value six time result computation communication ratio figure seven number stream input data chunk size show figure seven execution time vary number concurrent stream three data chunk size use input vector result show large number stream necessary attain maximum performance please also note number stream require optimum execution time vary size data transfer additionally chance run experiment one type memory network important factor influence optimal number stream minimal application execution time therefore environment mix type optimal single value might exist even application static stream size best static stream size propose dynamic algorithm input chunk size table two static dynamic number stream believe result figure seven strongly motivate need adaptive method control number concurrent transfer propose section performance propose algorithm present table two compare best performance among number fix number stream result achieve dynamic algorithm close best static performance within one standard deviation near one average static performance number improvement execution time due use asynchronous data copy present figure six tile size vary tile first resolution level result show performance improve tile size tile data transfer overhead reduce result gain roughly twenty application performance effect task assignment section show different task assignment affect performance execution application multiple resolution tile concurrently active process pipeline experiment run use fix number tile two resolution level vary tile recalculation rate percent tile recalculate higher resolution show affect performance figure eight assignment figure eight present use various system task assignment experiment significantly improve application performance tile recalculation rate zero tile recalculate process tile size performance therefore recalculation rate roughly two add second device type reference also include execution time version recalculation rate vary table three twenty rate time sec zero thirty four eight twelve sixteen twenty table three execution time version recalculation rate vary tile size sixteen table four number tile process use sixteen tile recalculation increase recalculation rate however policy still almost double pure achieve little improvement instance sixteen tile recalculation version application time faster version use together achieve respectively profile task process policy show table four explain performance gap use process tile schedule majority low resolution tile leave focus high resolution tile far faster overhead due task assignment policy include performance estimation negligible effect section evaluate propose dynamic selective stream task assignment policy odds evaluation conduct use two cluster understand impact assign task sender side capacity odds dynamically adapt number target data buffer necessary keep busy minimum load imbalance cluster homogeneous cluster fourteen machine equip one one describe begin section six heterogeneous cluster fourteen machine turn seven thus cluster heterogeneity among machine seven nod machine seven nod machine table five present three consumer filter get much data request use evaluation schedule maintain minimal queue receiver side processor idle time avoid simpler like random fit paradigm simply push data buffer consumer filter without knowledge whether data buffer process efficiently consider good schedule exclude evaluation policy simply maintain fifo queue data buffer end stream filter instance request data get whatever data buffer next queue use technique sender side sort queue data buffer relative give data buffer processor static value request data buffer execution choose programmer odds discuss section sender receiver queue sort receiver number request data buffer dynamically calculate schedule policy odds area effect queue policy sender unsorted unsorted sort receiver unsorted sort sort size request data buffer static static dynamic table five different schedule use section six homogeneous cluster base case section present result experiment run homogeneous cluster base case consist single machine experiment compare odds one use comparison achieve best performance among task assignment see section experiment use asynchronous copy image tile two resolution level section tile recalculation rate vary figure ten heterogeneous base case evaluation cluster two heterogeneous nod slightly increase performance odds increase understand computation distribute experiment next present table six profile data buffer process use stream eight tile recalculation rate homogeneous base case experiment notable performance difference odds occur scheme significantly collaborate execution low resolution high resolution tile process leave nothing much however odds show preference give vast majority high resolution buffer save majority low resolution buffer profile comparison one two nod also useful understand impact add extra node performance performance gain simply configuration two nod able process slightly higher proportion tile schedule scheme hand could efficiently utilize second node show table six process almost number low resolution tile high resolution tile use one machine odds approach utilize since decision buffer send make initially sender odds able intelligently utilize second additional process remain low resolution tile well high resolution tile important factor precede heterogeneous base case experiment choice number data buffer request maximize performance figure eleven show number request give best execution time stream policy tile recalculation rate value determine via exhaustive search static number request programmer responsible determine parameter stream approach achieve better performance figure nine homogeneous base case evaluation result present figure nine surprisingly show even one process node odds could surpass performance allow gain due transfer odds use twenty tile recalculation instance around obtain odds directly relate ability better select data buffer maximize performance target process occur even one process machine data buffer queue sender side odds select data buffer maximize performance receiver improve ability receiver filter better assign task locally heterogeneous cluster base case stream task assignment evaluate section base case consist two compute nod first equip one one second machine figure ten present stream policy tile recalculation rate vary compare result homogeneous cluster base case heterogeneous cluster base case show figure nine ten respectively notice achieve slightly better performance additional second compute node however performance odds increase significantly instance eight recalculation rate odds achieve respectively performance single homogeneous base case schedule low high homogeneous base case heterogeneous base case odds odds zero table six percent tile process policy utilization odds b odds data request size figure twelve odds execution detail figure eleven best number data buffer request receive filter higher number request important stream policy large number data buffer input queue machine create schedule hand better performance smaller result less load imbalance among compute nod best performance achieve configuration utilization maximum whole execution leave idle may better request high number data buffer generate load imbalance among filter instance end application execution contrast odds adapt number data request accord process rate receiver filter instance provide ability better utilize whole execution see figure twelve show show figure twelve b odds change dynamically one execution application experiment use ten rate expect vary execution proceed adapt slack queue especially notable end execution buildup data buffer compute buildup data buffer longer process time machine cause reduce number request machine reduce load imbalance among compute nod tail end application execution scale homogeneous heterogeneous case finally section show performance number machine increase cluster type scale homogeneous cluster do simply increase number nod heterogeneous cluster fifty compute nod equip fifty without result perform use image tile two resolution level use eight tile recalculation rate calculate accord single version application figure thirteen first present homo figure thirteen scale homogeneous base case cluster four application collaboratively use three stream present table five odds result show could improve application performance much version application hand double performance configuration odds fifteen faster even homogeneous environment better choose data buffer send request due knowledge downstream performance figure fourteen scale heterogeneous base case experimental result increase number machine heterogeneous environment present figure fourteen odds show best performance among stream almost double performance achieve result number machine best among different number buffer request odds automatically adapt achieve odds use fourteen heterogeneous nod also four time higher seven machine show gain achieve mix heterogeneous compute nod odds task assignment policy better due ability adapt available hardware reduce inefficiency due load imbalance processor also target data buffer maximize performance downstream odds dynamically maximize processor performance whole range data buffer application indeed use cluster equip fourteen nod use odds execution application execution time eight tile recalculation rate reduce eleven two second certainly much due version application take nearly five second achieve analysis result many image need analyze together gain performance offer odds bring tangible benefit seven paper present evaluate several heterogeneous capacity improve performance reduce overhead due data transfer appropriate collaborative task execution experimental result complex image analysis application exhibit process show reduce total overhead due data transfer also appropriate performance version application simply add single moreover propose odds stream policy provide developer straightforward way make efficient use homogeneous heterogeneous cluster argue use heterogeneous process nod propose performance estimator evaluate several show good relative performance prediction estimate execution time directly give poor result task assignment policy propose paper allocate entire filter internal task process future work intend consider concurrent execution multiple task exploit filter intrinsic data parallelism may source performance optimization could also ease development since partition task among execution would obviate performance estimator another focus future work plan evaluate sophisticate model learn use current prediction model author would like thank five anonymous comment valuable work partially support cap doe grant research fellowship grant compute time center eight reference one e n culler de k cluster io river make fast case common parallel distribute two c r pa unify platform task schedule heterogeneous multicore nine proceed international conference parallel process three r j g schedule distribute heterogeneous network proceed conference p four r filter large scientific archival storage symposium mass storage five chang c j distribute process large parallel eleven six w coyote system construct communication service sixteen four seven buck horn j k p brook stream compute graphics hardware graph three eight chang c j virtual microscope information technology seven four nine static parameter base performance prediction tool parallel program proceed international conference g electrical activity heart graphic process accept publication eighth international conference parallel process apply mathematics ten fix e j discriminatory analysis image process discrimination consistency computer science technical report school aviation medicine field eleven f r image analysis cluster conference twelve b fang w q wang mar framework graphics parallel compilation thirteen h refinement progressive mesh fourteen li performance effective strategy heterogeneous grid master slave paradigm future fifteen f g parallelize exist distribute heterogeneous environment heterogeneous compute workshop sixteen alme f predictive performance model application one proceed conference seventeen lee f g r j optimize reduction distribute environment three proceed conference p nine eighteen lee min r compiler framework automatic translation optimization nine proceed symposium practice parallel program nineteen collins wang h meng th merge program model heterogeneous multicore three twenty low l wang l understand duality model proceed hong kim h exploit parallelism heterogeneous adaptive map international symposium micro f p comparative evaluation optimization multimodality image registration maximization mutual information medical image analysis three four dynamic network architecture ten two n li g swami r international computer conference zero j toward automatic parallelization affine workshop automatic tune plank g do analysis springer berlin j pathological image analysis use stroma classification neuroblastoma thirty sancho analysis double buffer two different multicore international parallel distribute process symposium j h prognosis neuroblastoma image classification stromal development pattern recognition special issue digital image process pattern recognition detection cancer six h j joshi b terminology morphologic criteria neuroblastic recommendation international neuroblastoma pathology committee cancer two song f j dynamic task schedule linear algebra multicore nine proceed conference high performance compute network storage analysis n st framework efficient scalable execution nine proceed international symposium parallel distribute process g r hastings j efficient reliable scientific system international symposium cluster compute grid zero g fireman r achieve parallelism stream program model international conference parallel process g r fireman r exploit computational distribute heterogeneous international symposium computer architecture high performance compute g r r use improve performance compute intensive cluster performance prediction calibration class eleven forty welsh culler brewer e architecture scalable service rev five b b j parallel implementation texture analysis image four proceed conference