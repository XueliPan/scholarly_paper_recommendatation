little engine could scale social network yang pablo tide research abstract difficulty scale social network introduce new system design challenge often cause costly service like twitter complexity interconnection social network introduce new challenge conventional vertical scale resort full replication costly proposition horizontal scale partition distribute data among use lead costly communication design implement evaluate spar social partition replication transparently leverage social graph structure achieve data locality minimize replication spar guarantee direct neighbor data server gain approach multifold application assume local semantics ie develop would single server achieve add commodity low memory network io redundancy achieve fraction cost detail system design evaluation base twitter work implementation show spar incur minimum overhead help twitter clone reach twitter scale without change line application logic achieve higher throughput base store subject information information storage retrieval distribute data data structure graph network general term experiment performance social network partition replication permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee ten august three new copyright one introduction recently recent unprecedented increase use social network social component popular attract millions sixteen deliver status update high rat twitter seven differ traditional web significantly different ways handle highly personalize content produce nontraditional eleven importantly deal highly interconnect data due presence strong community structure among end twelve factor create new challenge maintenance management scale scale real hard problem particularly acute due interconnect nature astound growth rate twitter instance grow mar thus force redesign architecture several time order keep demand fail virtually disappear two natural traditional solution cope higher demand upgrade exist hardware vertical scale however expensive cost high performance case vertical scale even technically infeasible require multiple memory across machine three cost effective approach rely horizontal scale engage higher number cheap commodity partition load among cloud compute like streamline horizontal scale remove need hardware instead provide ability lease virtual machine dynamically cloud horizontal scale ease scale face traditional web since application logic stateless new demand order meet current load data layer however problematic since maintain state data partition disjoint horizontal scale still hold however last condition hold case existence social hinder partition data clean disjoint eighteen seventeen problem base data user neighbor since belong one community disjoint partition ie server neighbor community structure cause lot traffic resolve query problem become particularly acute random partition standard sixteen thirty hand replicate user data multiple eliminate traffic read increase replication overhead negative impact query execution time network traffic propagate update maintain consistency across indeed difficult beast tame two contribution spar main contribution work design implementation extensive evaluation spar social partition replication social spar solve designer dilemma early stage confront designer dilemma commit scarce developer towards add feature first ensure highly scalable system place handle high traffic volume choose first option lead join attract appeal feature infrastructure support adequate result frustrate leave service story behind demise two hand start highly scalable system similar power establish like twitter require devote scarce complex distribute program management issue come expense build core application attract first place spar avoid dilemma enable transparent spar constrict relevant data user server enforcement local semantics data level allow query resolve locally server create illusion system run one centralize server simplify program enable focus core feature service three nine four eight one two ten four three nine one two two four ten ten eight five seven read traffic zero write traffic ten memory ten six one read traffic ten write traffic zero memory zero one two ten four eight five seven four eight five three seven nine two ten three nine six three nine five seven five seven six six two two four three four ten eight ten nine five seven read traffic zero write traffic eight memory eight two ten four eight three nine five seven six two three two four four ten nine ten read traffic zero write traffic two memory two three six four eight five seven one one b c figure one sketch social network partition two use full replication b partition use random partition c random partition replication neighbor spar socially aware replication neighbor request one use additional decrease per server however decrease number request per server mean usage stay roughly spar reduce impact ensure require data keep local server avoid potential network server bottleneck thus increase throughput avoid performance bottleneck establish minimize effect provider spar avoid potential performance query multiple across network enforce local semantics data instance standard random partition use twitter split data across data query request fetch neighbor data tweet result unpredictable response time determine server problem particularly acute heavy data center load network congestion severe network delay designer dilemma prompt several cloud service develop offer scalable store run top offer transparent expense sacrifice full power establish lose expressive query language like powerful query encapsulation abstraction data relate require use tie particular cloud provider thus suffer poor portability lead architectural nine addition potential network individual cross platform store like could also suffer performance network io disk io bottleneck drive performance system instance could become bound need handle number query request server bind add help serve concern suffer addition performance argue shortly spar implement platform agnostic allow select prefer either store relational spar three problem statement joint partition replication partition side spar ensure underlie community structure preserve much possible replication side spar ensure data neighbor user host particular server server thereby guarantee local semantics data note relevant data user away fig one highlight operation benefit spar top fig one depict social graph ten nod fifteen edge bidirectional friendship social graph contain evident strong community structure two connect bridge nod three four depict physical placement two four different summarize memory term user network cost assume profile read rate one profile random partition b standard store minimize replication overhead zero thus memory either ram disk footprint downside random partition impose highest aggregate network traffic due read ten thus increase network io cost network equipment interconnect replicate neighbor show c eliminate read traffic across increase memory return another widely use approach full replication case network read traffic fall zero memory high ten full replication also result high write traffic maintain consistency propose solution spar perform best overall summary result toy example preview performance result sec five base twitter summarize result spar provide local semantics least overhead reduction random case reduce read traffic zero spar handle node edge dynamics observe minimal overhead system addition spar provide handle gracefully implementation spar serve reduce network traffic factor eight also show substantial gain implement spar spar spar design distribution content picture problem well study area content distribution network spar transaction process system solution storage batch data analysis spar intend characterize aid compute graph although leave future work first describe spar address next formulate problem solve spar finally discuss exist social partition base inadequate meet set set spar must fulfill maintain local semantics relevant data user direct neighbor tweet status update achieve local semantics need ensure every master replica user either master slave replica direct neighbor server use term replica refer copy user data differentiate master replica serve application level slave replica require redundancy guarantee data locality balance load application level read write request user direct master replica write need propagate slave consistency however since master handle much load slave obtain approximately balance load even distribution master among resilient machine cope machine need ensure master least k slave act redundant copy amenable highly dynamic new constantly join process graph densification due new edge form infrastructure host may change addition removal upgrade handle dynamics solution need responsive wide range yet simple quickly converge efficient assignment stable give operate highly dynamic environment need ensure solution stable example addition edge lead cascade change assignment master slave minimize replication overhead overall performance efficiency system strongly correlate number system require solution keep replication overhead define average number slave create per user low possible formulation give formulate solution optimization problem minimize number require purpose use follow notation let g v e denote social graph represent node set v represent edge set e represent friendship among let n v denote total number number available could cast problem integer linear program denote binary decision variable become one primary user assign partition j one j also denote similar decision variable replica user assign partition j finally let one e capture friendship state min replica problem follow x x min j x one j st x pi j j one j pi one j one x x j k v one two three four constraint one formulation ensure exactly one master copy user system constraint two ensure neighbor user master slave machine constraint three try distribute equal number master across machine constraint four encode redundancy requirement lemma one min replica simple reduction graph problem fourteen use prove skip description formal proof lack space partition fall short obvious set use address problem describe previous section include graph partition ten nineteen optimization twelve either aim find equal size partition graph number edge minimize try maximize however several reason inadequate purpose graph partition incremental twelve ten pose problem deal highly dynamic social graph require costly recomputation partition incremental algorithm suit scenario base community detection know extremely sensitive input condition small change graph structure lead different placement nod partition twenty word produce stable argue directly reduce number edge equivalent reduction number however case consider example depict fig two minimize number edge result partition three edge require five maintain locality hand partition result four edge require one less replica show later sec five minimize edge indeed lead worse result motivate exist present solution next section four spar joint partition replication fly describe min replica problem must fulfill present heuristic solution base greedy optimization use local information overview assume represent nod graph edge form create link among algorithm run partition manager module explain sec six average case require information algorithm proportional product average node degree number worst case computational complexity algorithm proportional highest node degree algorithm trigger one six follow possible addition removal either nod edge description node addition new node user assign partition number master addition k slave create assign random partition node removal node remove user delete master slave remove state nod edge update edge addition new edge create nod v algorithm check whether master already master slave action require algorithm calculate number would generate three possible one master maintain two master go partition contain master v three opposite let us start configuration one replica add already exist partition master complementary node may result increase one two depend whether two master already present partition occur nod v already nod partition already exist extra slave v redundancy configuration two slave create v since master partition however node move case create slave replica old partition service master neighbor leave behind partition addition master neighbor create slave replica new partition already one preserve local semantics finally algorithm remove slave old partition serve master since longer need rule also subject maintain minimum number slave due k redundancy old partition slave remove overall system end less k slave particular node configuration three complementary two algorithm greedily choose configuration yield aggregate number subject constraint master across partition specifically configuration two three also need ensure movement load unbalance movement either happen partition master partition save term number best configuration second best one greater current ratio load imbalance partition fig three illustrate step describe example initial configuration subplot contain b c e f g h j b e f c g h b c e f j g h figure two illustrative example minimize edge partition minimize partition result three edge partition five nod replicate e partition result four edge four nod need replicate c f j two three four five one q two three four six five one one five five six one five six one five node five rep node six rep node one rep edge sixteen create six nod rep node one rep node five replica delete six node one move two three four one one five two three four six one five six one five n ode six one five six two three four one node six rep node one rep six node five replica delete figure three spar sketch algorithm ensure master move new server slave replica neighbor guarantee local data semantics mechanism guarantee master across one equally balance however may provide minimum replication overhead reason fraction edge master involve algorithm also trigger edge creation event reduce replication overhead see later evaluation section provide good replication overhead even dynamic server second case algorithm nothing else increase number available arrival take care fill new server new turn attract old edge form lead eventual load balance master across without enforce movement condition continue grow server removal server remove whether intentionally due failure algorithm reallocate n master nod host server remain one equally algorithm decide server slave replica promote master base ratio neighbor already exist server thus highly connect nod potentially many move due local data semantics get first choose server go remain nod place wherever fit follow simple strategy see evaluation section strategy ensure equal repartition fail master maintain small replication overhead six nod three partition current number replicate nod empty circle four edge nod one six create since replica one replica six maintain status quo two additional create maintain local semantics algorithm also evaluate number require two possible node one move would need three new since two five neighbor node one already addition movement would allow remove slave node five longer need consequently movement would increase total number yield new total six worse maintain status quo last step algorithm evaluate number third allow configuration move master node six replica node five remove already exist node link thus replica need create change number one yield total three move six minimize total number however configuration violate load balance condition hence perform thus final action move status quo create additional two edge removal edge v disappear algorithm remove replica partition hold master node v node require algorithm check whether k slave remove node desire redundancy level maintain server addition unlike previous case server addition removal depend social graph trigger externally system detect automatically system management tool two add server one force redistribution master new one balance immediately two let redistribution master result node edge arrival process condition least replicate master server move new server one movement master first case algorithm select n might wonder happen like must replicate server host master direct neighbor user read must might replicate across multiple large audience affect system negatively also impose limit number people follow befriend avoid five measurement drive evaluation computation result section evaluate performance spar term replication overhead replica evaluation methodology metrics main quantitative evaluation metric replication overhead stand number slave need create guarantee local semantics observe condition also analyze number node ie replica operation spar use three different serve different purpose evaluate individual performance metrics twitter collect crawl twitter four comprise two nod edge also contain tweet generate time collection although exist topic specific contain tweet individual best knowledge complete user activity data allow us good approximation twitter use public new network include nod friendship link well wall post collect data consist nod edge include edge creation use first wall post information however available therefore filter retain contain complete information nod edge collect three eleven consist three nod edge information find three comparison compare spar follow partition al random partition store like partition data randomly across random partition standard use commercial thirty graph partition exist several partition graph fix number equal size partition way number edge minimize ten nineteen use metis nineteen know fast yield high quality partition large social graph optimization mo also consider community detection algorithm twelve build around metric modify order able create fix number equal size partition modify version call mo operate group partition sequentially give partition full community size apply mo twelve community evaluation procedure follow input consist graph one describe number desire partition desire minimum number per user profile k partition produce execute input since require local semantics process partition obtain candidate case add master user miss neighbor partition generate edge creation trace use exact twitter available create random edge order order edge creation trace case exact random permutation edge give qualitative result therefore assume apply furthermore virtually quantitative difference across multiple random evaluation replication overhead fig four summarize replication overhead different different k zero two well different number four see spar generate much smaller replication overhead include traditional graph partition metis community detection mo relative rank best worse spar mo metis random look absolute value replication overhead see increase number note logarithmic mean add lead proportional increase resource requirement give spar naturally achieve low replication overhead since optimization objective compete optimize minimal edge therefore end need add miss nod guarantee local semantics partition step redundancy important property spar achieve local semantics discount cost reuse need redundancy anyway way around see let us focus example fault tolerance guarantee k zero replication overhead spar ensure local semantics require profile replicate least k two time new replication overhead rather copy two inevitable due redundancy requirement thus local semantics achieve lower cost instead gain come leverage redundant achieve locality something spar explicitly focus comparison random spar since random standard fig five depict ratio overhead random spar case twitter see vary twelve partition four low k two mean three node twitter number number number spar mo metis random nothing move new user new user e h r e v n c l p e r e h r e v n c l p e r fifteen ten five zero fifteen ten five zero twitter r p r n r r five four three two one sixty forty twenty zero sixty forty twenty zero fifteen ten five zero fifteen ten five zero e g e n p n c f r one eight six four two zero f c one nine eight seven six five four three two one zero e v e n f r e b n zero zero eight sixteen four twitter number four eight sixteen number eight sixteen four number figure four replication overhead spar metis mo random twitter upper graph show result k zero lower graph show result k two k number redundancy four eight sixteen number figure five spar versus random k two figure six ratio action perform spar edge creation occur five one fifteen two three edge creation four five x ten place four case improvement range respectively large number ratio start decrease however likely representative would happen real system since number per server artificially small give number number small allow spar achieve much higher throughput demonstrate run real implementation spar top dynamic spar far show spar outperform exist measure replication overhead turn system state delicate art balance load first focus distribute across take example twitter case average replication overhead three ninety seven less less need replicate across next look impact replication distribution read write read write read conduct master therefore want analyze whether aggregate read load due read master balance load depend distribution master among read pattern spar yield partition fifty zero ten one ten two ten three ten number nod move event five one fifteen two three four edge creation five five x ten figure seven number per edge creation event coefficient variation master nineteen thus heuristic successful balance master across test examine write pattern write per server indicate fairly balance across single server source high proportion write expect read even balance across twitter handle read automatically call via periodic poll ninety twitter traffic generate via low correlation number slave present big problem system therefore show spar handle write read well term balance across per move nod around next turn attention footprint spar term user data fig six show stack bin action take spar client request one eight six four two n e v e n c n f r zero four twitter twitter fifteen two one five n e v e e v e n zero four eight sixteen number eight sixteen number figure eight movement cost various line upon edge arrival event k two sixteen see follow transient phase network build spar enter phase sixty edge create remain forty case data get move fig seven plot number per edge arrival inset see whenever movement occur overwhelm majority ninety case involve data two less movement involve move data nod fig eight summarize movement cost system twitter four leave plot fig eight depict average fraction nothing action involve movement nod data discount transient phase right plot fig eight depict total number divide number edge figure show footprint remain low number add server add use one two one wait new fill server two redistribute exist master new server study overhead term replication cost start first case go sixteen add one server every new strategy yield marginal increase compare would obtain system dimension begin show spar able achieve efficient state independently add although explicit redistribution number master per server remain low end trace four show gracefully add new without extra overhead second experiment test effect extensive upgrade infrastructure double number force redistribution master reduce number master per server half test addition sixteen two case first fifty edge creation trace replay second double number lead expect transmission half master spar however need move additional slave maintain local semantics add server also cause increase final replication overhead instance double initial sixteen fifty trace produce transient increase replication overhead ten overhead stateless application logic library v w x data store figure nine sketch spar architecture progressively reduce new edge add reach two higher start reshuffle edge replay old edge new describe sec four additional overhead cause addition become almost although number higher due reshuffle remove next test happen server remove average number marginal increase reduce cost additional replay edge nod affect server removal one might argue server removal seldom happen usually scale case server due discuss sec overall demonstrate spar able distribute load efficiently cope social network graph dynamics well physical virtual machine dynamics low overhead next describe spar system architecture six spar system architecture describe basic architecture spar fig nine depict spar integrate typical web architecture interaction application spar w w need call application know address server contain user data read write application obtain address use common interface driver application logic write centralize since agnostic number operate scale make scalable transparent task remain spar directory service local directory service partition manager p replication manager system data distribution data distribution handle return server host master replica user key table key additionally also resolve list user v w directory service implement consistent cache distribute across data back end local directory service contain partial table view specifically n table act cache performance reason invalidate whenever location replica change n data partition p run spar algorithm describe sec four perform follow function map user key whether master slave schedule movement redistribute event server addition removal spar algorithm would allow p distribute however simplicity implement centralize version p run multiple mirror act failure main p avoid single point p component update note simplify guarantee global consistency main p write data data take place replica need move migrate one server another movement undergo reconciliation avoid could arise movement ie user write data master change location apply replica schedule remove propose new mechanism handle reconciliation instead rely semantic reconciliation base propose distribute dynamo thirteen data replication consistency replication manager run server data main responsibility propagate write take place user master slave use eventual consistency model guarantee time sync note since spar rely single master multiple slave configuration avoid arise maintain multiple master single master additional benefit arise due due produce edge node server part regular read write note replace different need different adapt interface first iteration spar provide implementation although could equally support ie add remove p also control addition removal cluster demand process describe sec four evaluate five handle failure run data bond happen either server specific disk failure level power network issue spar rely system monitor current centralize version p handle edge per second commodity server health data failure detect p decide course action base failure management policy set administrator consider two type one react transient another one permanent permanent failure treat server removal case slave master go promote master neighbor recreate detail see section four five transient failure short live one potential option promote one slave whose master go without trigger recreation neighbor consequently local data semantics temporally sacrifice case system would suffer graceful degradation since leverage nice property spar property entail server host slave replica one fail master also contain large portion neighbor master therefore promotion slave guarantee local data semantics still provide access user neighborhood better illustrate point let us take example twitter sixteen partition case ninety direct neighbor present server host best possible slave replica candidate promote case failure solution apply extremely short live otherwise user experience would suffer system administrator would better implement permanent failure scenario current spar replication mean redundancy guarantee local data semantics however spar could obtain two ways one modify formulation problem least k master local data semantics maintain minimize min replica approach however leave future work option simple brute force approach mirror spar system k zero many time implementation detail sit top control modify read select forward directly without delay however write insert delete need analyze alter replication performance reason let us illustrate inner work example write operation user want create new event w generate follow command insert insert event wid w wid user event content react command obtain target table event know simple configuration rule event table partition thus insert need replay host slave user query obtain list propagate insert issue insert command local addition instance case twitter server could obtain via mirror three master full local semantics replication overhead one master plus two slave configuration k two would result ally event broadcast neighbor user contact j application generate insert wid replay command appropriate seven spar wild evaluation section study performance spar two traditional use production specifically compare spar random partition full replication reference application scale use call five rely centralize architecture consist cluster sixteen commodity little engine interconnect switch server duo ram single hard drive data twitter end sec load evaluation design run top therefore evaluate spar need reproduce functionality data model specific version fifty define data scheme contain information update tweet list update stream subscribe implement data scheme use different super implement spar top first disable default random partitioner create isolate independent instance nod system communicate full control data placement update implement directory service top nod show underlie infrastructure distribute across avoid bottleneck spar write use thrift interface describe canonical operation retrieve last twenty update tweet give user perform three one randomly select directory service node request location master replica user use get primitive two connect node host master perform operation request list twenty status update show user finally three retrieve content status update return note evaluation compare performance spar standard vanilla random partition interest answer follow two question question impact spar response time compare random partition much spar reduce network traffic answer two question perform follow set experiment randomly select twitter issue request retrieve last twenty status update rate request per second note request primitive spar spar spar random random random ten response time figure ten response time spar top one eight six f four c two zero p b seventy sixty fifty forty thirty twenty ten zero random spar fifty request rate figure eleven network activity spar top application level request user get status update examine response time fig ten show response time spar vanilla use random partition see spar reduce average response time however interest throughput aggregate number request per second give realistic quality service spar support percentile response time random partition support fourteen request rate quality service spar outperform random partition multiple reason discuss one brief first affect delay worst perform server due heavy traffic remote set run switch without background traffic hence network bottleneck fig eleven also show commodity however often hit network io bottleneck try sustain high rat remote read spar relevant data local design remote read necessary second less obvious reason better performance spar improve memory hit ratio come partition master indeed read user bring memory disk data user well give master server high likelihood read direct master good chance read one friend find require data already memory previous read random partition scheme destroy thus suffer lower memory hit ratio consequent disk io analyze network load fig eleven depict aggregate network activity cluster various request rat random partition request spread among multiple nod significantly increase network load compare vanilla implementation spar reduce network traffic factor eight evaluation turn attention traditional system first question want answer spar scale application use important allow continue use familiar framework without worry scale issue specifically want answer make deal demand twitter use version together data scheme provide five schema contain table relate table user profile social graph subscription update notice list update per user notice adapt twitter data scheme contain information status update retrieve last twenty status update tweet give user perform single query use join notice notice table setup use six two use emulate activity concurrent generate application read request retrieve last twenty status update user application write request user generate new status update update experimental evaluation consist multiple four minute sessions query last status update random subset constant request rate make sure every user query per session request spread evenly among comparison full replication first check scheme base full replication work practice would mean load entire twitter sixteen measure number system serve average percentile response time sixteen one request per second per machine expect even higher sixteen hand use spar cluster serve two percentile less show spar use data store able withstand read load small cluster commodity machine whereas full replication system cope note experiment show centralize code know nothing distribute still support load use spar add write system introduce update evaluate effect insertion sixteen update one update per machine part evaluate spar full replication use perform poorly note insertion new status update system generate update since system need insert notice table one entry every user receive status update twitter terminology treat insert crucial overall performance naive implementation perform single insert notice completely saturate system group insert control rate introduce system scenario show achieve percentile response time fifty read read constant rate sixteen update note median response time case low around performance get better add machine section show use spar lead high throughput better compare full replication random partition cost modest replication overhead eight relate work best knowledge first work address problem data section compare contrast approach present paper prior work area relate work literature scale scale web one key feature offer cloud allow effortlessly add compute demand depend current load application four scale however transparent long application stateless case typical web layer data partition independent deal scale application data independent case provide mean ensure local semantics data level store many popular today rely store deal scale data twitter migrate store suffer due distribute design rely random partition data across server lead poor performance case show paper spar improve performance multifold store minimize network io keep relevant data give request local server keep data local help prevent issue like hole observe typical operation store one distribute file distribute data sake performance availability resilience widely study file system community fifteen coda distribute file replicate file high availability distribute file system achieve high availability use replication eight distribute like cluster bayou allow disconnect provide eventual data consistency spar take different approach distribute data maintain locally via replication approach suitable since typical operation require fetch data regular basis nine scale hard problem data highly interconnect hence subject clean partition present spar system base partition social graph combine replication local data semantics guarantee local semantics mean relevant data direct neighbor user server host user enable query resolve locally server consequently break dependency make problematic preserve local semantics many benefit first enable transparent scale low cost second performance benefit throughput request per second serve increase multifold relevant data local network io avoid third network traffic sharply reduce design validate spar use real three different show replication overhead need achieve local data semantics low use spar also demonstrate spar deal dynamics experience gracefully implement application evaluate spar top store use real trace twitter show spar offer gain throughput reduce network traffic thank anonymous shepherd yin valuable feedback special thank weaver early comment feedback ten reference one hole machine capacity two lose lead due failure scale three note scale four five status net six distribute load test tool seven twitter architecture eight w j g r j r j j r r p federate available reliable storage incompletely trust environment two nine fox r r h g lee cloud view cloud compute technical report ten expander flow geometric graph partition j two eleven f cha v characterize user behavior social network nine page new york twelve v r e fast unfold large network page thirteen g jampani g p w dynamo highly available store rev six fourteen r intractability guide theory w h freeman new york fifteen r g guy j w w page g j implementation replicate file system conference proceed sixteen j seventeen j scale eighteen twitter hard scale nineteen g v fast high quality scheme partition irregular graph j twenty one twenty h h moon mine network solution consistency evaluation nine h c lee h park moon twitter social network news media j j c graph evolution densification shrink eleven j k j w community structure large network natural cluster size absence large cluster n media growth twitter mobile k p p b measurement analysis social network seven j park social network different type network rev e e j community structure network j v p divide conquer partition social network j g v p scale social network without pain thirty j high performance massive scale learn coda highly available file system distribute environment f schneider b w understand social network usage network perspective nine b terry k j j c h manage update conflict bayou weakly connect replicate storage system b cha k p evolution user interaction nine