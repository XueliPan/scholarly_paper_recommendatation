optimal outlier removal space abstract slow even mislead algorithm machine study problem subset learn area train data could set point probability distribution algorithm wayward hypothesis even space point x de outlier purely theoretical standpoint remove could lead exist direction w square distance simpler mathematical model mean along w greater time average might constitute phenomenon interest square distance mean along w one main one address question zero exist one fraction answer another precisely outlier original distribution seven distribution space n n paper assume data consist point improve previous bind n b bind show nearly best possible theorem constructive result approximation one one case one could use one de allude point outlier distance mean greater factor time standard low optimization problem give distribution ie deviation follow generalization higher dimension ability sample parameter zero n propose one let p set point point minimum exist subset probability r x p call outlier exist vector w least one square length x along w time average square length p along w ie outlier ellipsoid sample space two two w x w x x p e two one introduction note w x square distance along w two origin problem address follow term outlier familiar one many exist small subset p whose removal ensure several typically remain set precisely quantify far outlier rest data remove subset consist di outlier mean fraction point remain set di outlier point rest data addition di might natural approach might normalize measure scatter set set remove do apply range standard deviation data point linear transformation describe section two result outside threshold label average square length distribution one along identify fundamental ubiquitous prob every unit vector position isotropic isotropic data set might represent experimental position use four show convex set k error case would desirable remove isotropic position contain unit ball contain could performance computer program ball radius n bring distribution isotropic department mathematics support outlier simply square length position allow us identify easily point part career award f g main remain set might still permission make digital hard copy part work personal classroom use grant without fee provide copy make distribute profit commercial advantage copy bear notice full citation first page copy otherwise republish post redistribute list require prior specific permission fee one copyright possible point previously become happen repeatedly perhaps force us throw set main result answer question surprisingly small value present general framework let zero x two b n n b b r f g f two r x two ie subset space outside j j g small ball inside large ball plus origin place point set p discussion probability distribution note contain b n b r r set frequent interest two outlier removal theoretical computer science state probability theorem one main theorem let probability r restriction single step end section three let distribution region space denote b b n n j zero two two j x j x b b b n b r x probability choose accord f g f two r j two one two n ie subset space every exist zero g ordinate j note ax play special b restriction ease exposition remove n b n n role j assume remainder section b log probability distribution interest j n b order detect use linear transformation let x draw accord prob n b e one w two r al l w x x w x x two two two e ability distribution since positive de n f two g two exist matrix consider form space z x transformation preserve one proof theorem section three constructive z outlier direction w transform section two describe two algorithm one space correspond x outlier direction w w untransformed space vice zero lier removal theorem prove use either vari ant although theorem obvious algorithm extremely simple convince reader include transform distribution four position isotropic refer result distribution distribution center implementation algorithm section nine previously use make ge point set point n algorithm run random walk three two full rank still positive instead n time section four show algorithm also use unknown distribution allow center span draw random sample distribution number sample require run time two two five n b b n two direction w also outlier direction x follow center distribution center distribution point x outlier algorithm similar algorithm one e two w x one every w w one inspiration work bind j j pro point x direction w improve previous best bind give w x x thus outlier cation easy center one use crucial component j j algorithm learn linear threshold function algorithm follow simple form presence random noise due high value throw point bind run time learn algorithm close lower dimensional subspace drop lower although polynomial somewhat prohibitive n dimensional subspace stop n b seven five contrast theorem imply improve bind n description c absolute constant learn linear arbitrary algorithm one restriction presence random noise bind let c b log distribution two n n five example bind better one center exist x x let asymptotically best possible show section b log possible n one one two main theorem give bind nat x x retain point j j f j j g ural question whether one achieve best two transform back original space possible particular distribution give axis x two throw parameter want subset point x x two b b whose removal leave set possible question show three repeat neither condition meet even case reduction follow variant algorithm sig section six prove algorithm ni easier analyze whereas previous achieve approximation best possible remove every direction one step one one give algorithm two remove one direction per case may desirable translate data set origin coincide mean rather step c constant algorithm two restriction origin prove follow corollary two n n standard mean section seven let n p probability distribution p q b b q one center exist unit vector w b n two two two two f j j j j two zero one two two one q zero set w x let x w x b f g g f g f g note tain point let c b log distribution outside small ball inside large ball criterion ever possible placement origin except one two transform back original space point corollary zero exist axis x two throw b b one fraction distribution along point x x two direction point away mean b log standard direction three repeat neither condition meet n n p six three proof main theorem construct vector w length belong zero either algorithm terminate clearly dual ellipsoid let w wa since w unit zero outlier free subset remain show throw vector assumption much distribution main idea proof e two two w x x x w w show every step volume associate dual two two ellipsoid increase bound total growth dual ellipsoid volume course algorithm one w w w w zero zero zero two deduce certain fraction original also show every v w also belong w probability mass throw algorithm x hence nates special care take deal possibility step distribution become concentrate v w v v one two x two p two subspace lower dimension towards end need de de ne v v v v x v v e ellipsoid x x one w x ax one refer e f j j g two f j j g x x two w primal inertial ellipsoid dual ellipsoid two respectively subset denote n v v x v x v v one matrix give x x two one r x x x e two boundary ellipsoid lower bound length imply v w length point two two x x two axis since least one axis dual matrix obtain restrict zero length ax length least one point outside denote restrict probability v w f n v w f n two distribution directly useful property attain ply dual volume grow least factor e center respect restriction original dual ellipsoid volume iteration distribution onto region statement still true dual ellipsoid w x x x one every unit vector w volume begin iteration expectation probability respect x conclude proof lemma one two two j j e two draw also need follow elementary lin lemma two dual volume growth let ear algebra volume ellipsoid give distribution let product axis time volume distribution result application either algorithm unit ball denote f n x x one assume let l b log one j f j j g ellipsoid ax give follow v w v e f n function two v w two j n solely dimension although v e zero v w actually prove theorem one w x x one e two x place w x note e two two statement original theorem two proof first lower bind initial dual volume v w consider vector v length two b lemma one relate dual volume growth loss since possible x belong length probability mass lemma two upper bound total dual b two two n v v v x one p e volume growth v belong dual ellipsoid thus dual ellipsoid volume least two n two two b n n b log n lemma one restriction slab let use inscribe cube lower bind volume let center distribution suppose ball w w one nine j j next upper bind v w consider two two two tor v length v v length least v n p w x w x e two two f g axis property full j j j j j let x w x p x dimensional test condition step two either algorithm f g two never thus least fraction v w e v w point x value least two axis b two p two yield proof let w x x x start j j two two e two two b v v v x v n two x two e identity two two two v n two e e e two two two j j w x w x x w x x x two v dual ellipsoid must v v two two x two use w x x get two two log one one v two thus ultimate volume two two one p imply j j n log one dual ellipsoid two two use contain cube upper bind volume two two p two one p e ball v w two use one two prove algorithm two throw fraction probability terminate satisfy theorem one mass since might also drop dimension explicitly proof suppose algorithm terminate sub mass criterion step two algorithm two set throw original zero per bind total amount probability mass throw throw fraction probability probability mass every w without increase dual w x x w x x x three e total number dimension drop two two two two two f two g two two ing three see throw p remind reader normalize j p probability mass life algorithm thus probability distribution point rather bind zero point outside replace zero increase side inequality factor one increase side thus inequality b log n still true even normalize thus achieve c b log simple lemma prove property b follow follow j two n n remain show relation zero lemma three ellipsoid slice n consider dimensional ie throw probability el axis take e one n k mass claim first suppose full dimensional dimensional slice center c e zero let l b log n j th v c f k k n q l suppose iteration algorithm b corollary lemma three axis dual step one p fraction original point throw ellipsoid length two ax total amount throw p lemma w grow throw probability mass one total amount dual volume increase e help f n monotonically decrease two two p p e compare bind total increase q n two p two p dual volume lemma two yield theorem five proof lemma three main tool two two p p e two two choice c three p extend proof case p drop dimension main issue proof one two sup pose step algorithm move x ax min x x zero x x two dimension k dimension k want restrict zero zero minimum al l dimensional sub n one rest algorithm k dimensional zero space theorem two let real metric matrix eigenvalue n n one n th subspace span w e zero zero zero since k dimensional slice subspace k dimensional denote fact zero n k one axis w give consider volume axis slice add subscript v function show two argument k one dimensional subspace c ellipsoid length least apply n upper bind total volume dual zero axis slice ellipsoid ellipsoid decrease two k k l zero b two zero v w k zero k v w k k l next axis length least apply n argument remainder ax conclude thus amount away upper proof lemma three also conclude proof main bind total growth dual volume theorem increase immediate way calculate upper bind dual volume b prove give alternate proof main theorem use shortly conclude argument assume b construction give algorithm one begin prove time drop dimension ie rank decrease analogue lemma one may make progress increase dual volume thus apply lemma one lemma four restriction ellipsoid let two let center distribution mass throw step however take two yield throw fraction f g two let x x x p x total probability mass one step follow v w e v w two p two two two w x one e two proof first establish radially x w x two two metric distribution show radially two metric distribution worst case want since choice already let radially symmetric distribution de ne zero two conclude every time drop dimension p calculate increase zero six v w let w x x x finally prove algorithm one terminate sit zero two two e zero two two w w one center sphere theorem one radius pro sphere j j sharply concentrate around n square p proof lemma two still hold rate increase expectation exactly n use identity two dual volume throw probability mass di two two two e e e w x w x x w x x x x two two two two remain constant clearly still good well ever bind amount probability mass p throw single step longer proof lemma one w deduce stead proof lemma four one thus two two zero one p n lead p choose two two value thus analysis algorithm two n p two two n one e immediately apply analyze algorithm two argue drop proof lemma one observe w include zero dual n possible step dimension vector length direction w since drop could lead overall drop true every w dual ellipsoid volume increase probability mass thus able argue least factor show case n maximum amount dual volume still might radially symmetric distribution grow could increase step successfully analyze algorithm one must prove statement v w e v w throw lot probability mass dimension two p two show radially symmetric distribution drop small amount still make worst case want suppose progress overall growth dual volume center distribution precise let p k k l analysis zero zero statement lemma true construct new center radially symmetric algorithm two prove v w zero k k k p four zero zero two c two e v w k statement also false zero compare b analysis algorithm one begin note every point throw idea proof c point either large also throw rotation follow component subspace vanish large fact center let expectation zero ponent subspace remain dimension random rotation radially zero drop small amount symmetric distribution probability choose many point large component subspace x distance less r origin exactly zero vanish dual volume growth result dis probability choose x distance card point large component subspace less r origin every r let correspond zero zero remain let w k dimensional space span remain zero w k k dimensional subspace toss zero zero consider axis direction w e w one span orthogonal w let p probability remain one w x x x e denote pro x onto w least remain two two e j j zero two two axis length axis also radius e zero similarly let p probability pro x two p two construction zero onto w least one two happen toss n n every point x thus p p p one two two two two one one w x x x e take unit vector w w use favorite toss n n two two two two two x x e e equation w x w x x one way visualize equality take simply x w x x e two two two consider achieve rotate ax onto ax since discrete set clear square axis arithmetic one two k k zero one zero p one p one two two k k zero average square axis make take continuous set without zero factor one k k come expect square pro k k dimensional unit vector run axis consider volume e zero dom direction n n w x x x similar analysis give n two two two e take instead unit vector w w let remain one two two v e f n f n zero one v zero two x n one zero k p two zero two k two zero f n v e zero k n since one lower bind increase vol ume dual ellipsoid subspace w remain inequality arithmetic mean dual volume drop move subspace inequality imply v w v w zero zero k k l conclude proof lemma four k k l p four two two k k zero two zero two increase dual volume least p two x two two two zero zero two e use p p p yield two p one two zero two zero k k l p four k k two two e e imply c express c slightly b four efficiency stead n b r n b r analysis algorithm two independent section describe polynomial time whether drop dimension give iteration th computational model allow suppose throw probability mass p time unit time step one algorithm one c drop dimension take away progress point set towards upper bind dual volume growth suppose distribution explicitly set factor two per dimension drop thus point weight correspond two two p p e two two achieve exactly state value either algorithm run time either al choice c imply two three p give time compute throw less three probability mass step two time center distribution n time p two three two life algorithm thus entire algorithm outlier need repeat whole throw fraction probability process time amount time spend step two mass conclude analysis algorithm one either algorithm negligible yield time bind remove technicality restrict j discussion make worst case n b n two two three one data point throw iter lemma five choice ax let probability dis center look case exist rotation single data point throw center distribution denote subset space b b log zero do distribution initially center v probability p remove new one zero center distribution achieve replace vector two zero j probability distribution n j b one one v p intuitive explanation two v v two v p proof construct randomly rotate zero formula correct inertial direction v use observation choose subset space may retain two scratch center distribution still point small value three two axis equal j set want n b scratch n outlier recenter use formula total time yield improve time restrict consist throw away point two two three n zero j value less two along axis expect amount b probability mass throw away n time amount throw away restrict along one axis ie remove point within slab width two center bind n n arbitrary origin perpendicular one axis consider single point x r x zero follow contain ellipsoid n b two bind probability small pro interest problem give explicitly rather ability sample restriction part random unit vector prove two one apply lemma five necessary get set clean two x four w x w n two two two ax c two get set p x x sample one f g let c b b log since x two three run outlier removal algorithm algorithm b choose value c two two j j x discrete point set p parameter two throw point pro less two four let p outlier free subset p outlier zero choice b imply probability throw free restriction give one e two two point one axis thus probability x x zero accuracy param one x p two zero p least half throw fraction two distribution consider n ax let us consider bind achieve n n main theorem section follow initial distribution j rather r theorem three sample complexity let b b apply transformation lemma five initial step n log log high probability n n two two apply algorithm upper bind two outlier removal algorithm run parameter increase constant factor show two two conclude calculation use b place two one return set satisfy one one b two two one one two achieve deterministic omniscient al two two n n n omniscient know distribution ex b log b log log two n n b log terministic omniscient algorithm parameter remainder section assume de two six subset one occur constant probability two j two two two statement simply show let assume j two since implicit exactly w x x one without loss generality w w x x w x x x e would find two two two f two g two eight f two g two two note take x accurate estimate analysis previous since whenever w w x paragraph case x two two x w x x able conclude imply w x x x two eight f two two two one two one two g f two g two f two g know always imply able case may better bind run time prove arbitrary value two n two lemma seven outlier detection many suppose step estimate e fix let either outlier w log within one every direction let one algorithm restrict parameter produce w two two two two two every point perceive outlier region space x w x value two two least outlier respect true distribution constant probability f g two remove throw away point subset space along two e two deterministic algorithm keep similarly perceive w distribution true distribution along one one w two eight two one remove two two may within one however proof either outlier removal algorithm restrict two two ever wrong one true w simply mean lier respect original distribution throw two consider achieve deterministic even use awe line reason make omniscient version algorithm restrict w since rigorous allow us achieve one outlier free outlier removal algorithm throw away probability region space parameter achieve mass necessary possible restriction deterministic version algorithm lemma six show outlier free de ne x lemma six two particular direction particular iteration lemma six x good approximation lemma seven extend ensure good probability identify proof theorem three extend outlier free prove remain show also show n log log algorithm reason choose substantially sample achieve state goal one region one one eight two two outlier free set high probability de ne x w x suppose n n two two two one one two two two f g nine lemma six outlier detection one iteration fix follow fact w x x two direction let region space let number w e one w x x w x one two two two f two sample consider sample distance x x monotonically increase function g f two g two two direction give let w w x x w x e analysis hold whether consider true two two two x x sample variance f g two underlie distribution sample one two two x two p x w x constant suppose estimate set w x x one two two answer sample also lead us calculate one two f two g two two one one every two w x x x w x two two two show nearby within factor x w x one two f g p two x f two g f one value correctly estimate sample variance g restriction since range possible case algorithm might return two two value two take every value two n k two n n proof property say correctly estimate one integral k union bind variance restriction distribution log two possible value k one property assure us restriction two two two show estimate one distribution probability mass past time good probability actually sample variance ie always throw away six two one respect true distribution mass use sample variance claim reason since within one direction w note find deterministic eight two omniscient algorithm es condition let x random variable represent square one outlier free one show one outlier six two two two sample show least one distance x along direction w x w x without two outlier good probability let x random variable loss generality assume w x x one two two represent square distance x along direction f two g w x w x without loss generality assume one two de ne x lemma six place appropriate scale first show since j apply bind one two assumption condition one six two one sample show least one outlier apply x two two one one x one two determine probability x good estimate two two x e bind p j j two one x e x e two e two g x three argument use w one x one let h h intersection w convexity h space grid zero x two two state bind case w w x x h one min w x one let one yield x h simultaneous lower bind expectation f two g f one e x one two two two g arbitrary w upper bind maximum correct event probability calculation one ten two w yield h one outlier free x probability correctly two two e x three use grid reason one zero two twelve two furthest outlier e one contain h therefore one one outlier two e two two two one x two two e free plicable alternate form bind yield x e one two x three e state corollary run time men still show step two either since di value do lemma section lemma eight assume n carry high probability set p n two two consider log allow us union bind lemma eight follow bind run time possible value show con probability estimate two five two corollary one run time algorithm run case algorithm might return one one imply eight two time two four b n two two two two five b n two four extend analysis lemmata six seven proof section three direction argue correctness never need sam entire algorithm prove theorem three plug value bound dis proof let ellipsoid find entire algorithm run time tic algorithm ie outlier free subset point lie bind refer introduction time ellipsoid rather consider original space achieve one approximation optimal value run time begin section four yield consider transform space e unit sphere consider many w give grid sphere form grid zero zero show carry step two either n choose every w w lie high probability additionally solve two n n zero one choice apply low problem suppose give param f g n n two lemma seven part rather ask appropriate every w grid zero lemma eight show point determine two two w x x w x x ie within factor one much probability mass f two g f two g direction contain show arbitrary direction w one contain two within ellipsoid since one two log two two consider arbitrary unit vector w every axis di one log value consider rection w w grid zero one two loss factor one value w within distance n since therefore simply try estimate e convex minimum distance two one whether require us throw away origin w w give one bound one two one fraction distribution two two maximum decrease every axis direction give thus achievable de least distance one origin direction w terministic algorithm give since within one origin everywhere region space one one zero n two one contain therefore one also contain asymptotic run time increase two two five b n two four conclude proof increase constant consider every w grid zero eight two one one outlier free along w lemma lemma eight probability mass location seven part consider arbitrary unit vector w fix direction let number sample w grid denote w n nearest neighbor within grid zero zero two consider square sample distance w let w vector direction w length give let w w x f g two e zero two f g give w x x one x one fraction sample one w de similarly bound away origin zero constant probability least fraction distribution two two hyperplane form w follow square distance along least w zero convexity e combine space one one w fraction square distance along greater f g two grid previous paragraph zero two two one three one maximum drop move w w one let el let number sample e ie w x x one one min w x x one estimate fraction e e two two one two two two one sample outside constant probability e since one one outlier free along w one fraction outside least fraction one e eight two two h h really slab correspond w w outside e ing w h x w x one w x x w two eight two two e f two proof first show let w x throw point along every axis high let random variable one w x throw along axis n zero event n contradict assumption high high zero g one upper bind probability event symmetric subset could throw achieve two f two p use bind n axis j one e ability distribution achieve along axis j x e ie x x x x x con j j two j constant similar calculation f two g two two take replace choice point zero e two e three one two two two along axis j throw fraction prob j w x one show low throw along axis point throw along probability constant low axis j yield contradiction thus zero f g value restrict attention symmetric proof identical center e direction w along axis pro onto w consider distance origin sample rather point n one ax zero obtain distance along w e e two two one w x x n one consequence section n b two sample size enough estimate inertial ignore factor n rest proof ellipsoid distribution remove strict attention single axis suppose fraction thus bring nearly isotropic position furthest point keep achieve five match lower bind point exponent k choice distribution throw half point one two two factor x x k b zero show twelve exist distribution f two g satisfy one exist calculate expectation w two zero two two one two p e x x x two two two two two zero w x x w x x x two two e f two g two two two k zero e two w x x two two zero b log base comparison n one tween upper lower bound case two one two zero zero two two two zero one two two two b b zero zero zero zero b four throw half distribution yield case thus n n n one b log b log lower bind case b log n one eight describe result asymptotically optimal fix n b log let copy zero b one one two four low distribution along axis let point zero six approximation algorithm distance zero one b one two two two two two two zero zero zero zero b b one one p p p show paper distribution achieve b log question n n naturally arise well particular distribution compare best possible particular distribution formally give seek minimize consider distribution place one two fraction sub probability mass uniformly b point one zero two fraction uniformly remain b point zero e w w x x w x x two two distribution maximum bite length along really approximation problem pa eight f two g two axis log b zero two b two p many ways choose subset probability distribution outlier free normalize quickly restrict set possible exhibit one approximation algorithm task first show never help treat di ax case give distribution explicitly asymmetrically distribution concentrate sample distribution algorithm ax suppose statement true yield one approximation constant zero one one one one gin note distribution concentrate high probability note case look ax w maximize always occur two lemma nine preservation let dis w x x two f two g e w x axis let distribution possible one outlier least outlier one throw fraction distribution achieve pa respect subset satisfy one suppose minimum achievable achieve proof let x outlier original distribution asymmetric let axis axis outlier w w x w x suppose along axis throw w x x x w x x es fraction total distribution let subset w x one w x x zero two two e e e two two one two two two two e two zero zero one approximation algorithm simply either algorithm proof theorem one rely upon show shrinkage describe section four error parameter case primal ellipsoid growth dual ellipsoid algorithm sample could determine step translate origin new mean con binary search suppose least quickly throw probability value achievable restriction mass overall satisfy one algorithm see may point mean element point outlier respect show even case dual volume distribution know bind still hold suppose step criterion outlier restriction lemma nine step two either algorithm meet axis point throw optimal value b double throw x x thus run algorithm force two mean x zero remain us throw away point optimal solution necessarily x zero necessarily also throw away yield achieve x value two n b one one one approximation case explicitly provide distinct value particular di er correctness run time clear less two dimension collapse dual precede volume bind require guarantee every non direct method fact collapse dimension least fraction point proximation one pass every value least two conclude proof n b section two use de ne point outlier order corollary two set namely point outlier second point approximate best possible particular value simply remove initial fraction eight reference point outlier order one frieze r seven standard mean two g r algorithm learn noisy linear threshold function one prove variant theorem show minimum large subset original probability distribution point many standard away symposium theory compute mean appear theoretical computer science special issue corollary two mean three l r random let probability distribution let walk n volume algorithm convex five space denote probability cho x body eleven random structure n b e sen accord let x x x two two two w x x x zero every w e exist two n n b log four l r isoperimetric convex body localization lemma discrete computational geometry thirteen five r horn c matrix analysis one university press w x x x w al l w p n f two g two r nine implementation proof proof corollary much like proof let n matrix whose point x theorem one two additional step distribution let value beta epsilon step show translate origin coincide let variable indicate whether do mean increase volume primal remove case full x inertial ellipsoid second step show run dimensional throughout algorithm common case algorithm value b double preserve complete implementation give follow dual volume bind step take together code previous analysis imply corollary two order analyze volume primal inertial consider radius r primal inertial ellipsoid direction w implicitly take respect restrict probability distribution j do zero do do one let x value pro x onto w w five x center version x r x suppose choose translate one remove current two two e w gin value z along w r x z w two two e beta x zero do zero end single variable calculus show value minimize r end z x mean thus translate w e end origin x minimize radius primal inertial every direction simultaneously since analysis handle dimension drop add line code