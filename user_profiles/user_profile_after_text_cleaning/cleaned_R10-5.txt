dynamic storage cache allocation department computer science engineer state university abstract introduce dynamic efficient share cache management scheme call manage aggregate cache space storage service level concurrently execute satisfy spare cache capacity proportionately allocate accord marginal gain maximize performance use combination algorithm discover require storage cache partition size server every application access server experimental result show algorithm enforce partition provide isolation meet application level even presence dynamically change storage cache improve io latency individual well overall io latency significantly compare two alternate storage cache management scheme single server storage cache management scheme extend architecture one introduction traditional employ single storage server manage multiple connect server initiate request data access heavy load single server potential bottleneck one solution employ solve problem replace single server multiple six eleven recently several research offer storage build intelligent nine minor seven ten high availability distribute nature architecture directly share communicate hence need consolidate resource management scheme handle require effectively utilize distribute share permission make digital hard copy part work personal classroom use grant without fee provide copy permission make digital hard copy part work make distribute profit commercial advantage personal classroom use grant without fee provide copy copy bear notice full citation first page copyright make distribute profit commercial advantage copy work own must honor bear notice full citation first page copy otherwise abstract credit permit copy otherwise republish republish post redistribute list require prior specific post redistribute list require prior specific permission permission fee fee copyright nine c manage share architecture pose several challenge though share storage among significantly reduce operation cost bring forth interference among compete lead unpredictable system behavior therefore consolidate multiple share must address contention various share paper discuss manage aggregate share storage server cache space although scheme extend manage share resource share io also important scheme manage share dynamically able adjust amount cache space devote share use way best match need partition share cache among multiple concurrently execute eliminate possibility destructively interfere demonstrate various study eighteen name however dynamically partition distribute server cache across multiple challenge demand question partition cumulative cache space across compete would cache allocation across available server cache application would cache allocation application adapt dynamic cache particular need effective scheme exploit aggregate memory capacity cumulative storage cache space server cluster minimize io another challenge manage share cache satisfy service level consolidate partition distribute server cache across owe dynamics storage obtain predictable performance pragmatic application access common storage server surely possess specify minimum share server cache space spite presence compete exist explore share storage cache partition design attempt avoid conflict among multiple twenty argon cache partition algorithm use simulator predict cache absorption rate hypothetical cache size al twenty use hint future access partition cache three partition read cache hint block reuse cache unhinted block reuse fourteen twelve partition share file system cache multiple process detect data access pattern common characteristic special treatment data block access belong one server cache work thirteen fifteen two describe share storage proxy cache single server provide hit rate compete class use feedback control exist provide performance guarantee storage throttle data traffic single storage node thirty many propose manage server cache presence multiple use application hint partition cache achieve io response time multiple however discuss data share multiple also guarantee system paper propose evaluate distribute storage cache partition strategy algorithm determine amount storage cache space allocate application access satisfy additionally spare capacity proportionately allocate accord marginal gain overall system performance maximize best knowledge first paper address problem cache partition distribute storage involve multiple share server cache paper make follow first propose use algorithm eight predict storage cache space require compete every application satisfy overall io latency minimize first partition consider storage server cache one big monolithic cache space allocate across use linear program model partition cache allocation application across available server cache second partition determine cache space individual application present experimental evidence show effectiveness approach result obtain use storage cache simulator set io intensive demonstrate partition strategy accomplish performance insulation among multiple compete well improve effective utilization share storage cache space thereby reduce overall io latency continuously provide service level guarantee also compare scheme recently propose approach show scheme perform significantly better overall io latency metric also reduce io latency individual fair share uncontrolled cache management scheme respectively brief description system model provide section two section three present detail discussion storage cache partition scheme present detail experimental setup include simulation platform section four discuss ex client server client client server client server server client client client client client client client figure one architecture architecture client access set server share among access result section five give section six two system model system architecture system architecture consider work consist multiple concurrently execute utilize provide storage system access multiple share access server generic model architecture depict figure one client execute one hence storage server architecture storage share among access server instant time assume cache client level client may page block cache directly attach furthermore assume client information run make homogeneity assume potentially share among share good fit resource consolidation cluster large monolithic pool important resource consolidation help improve resource utilization reduce administration maintenance cost alleviate traffic intensive however scenario server local storage cache need cache management scheme manage allocation cache different across share multiple important adopt design develop holistic view several storage server cache available efficiently utilize overall optimization meet propose distribute cache partition support envision partition scheme incorporate part allocation resource distribution server dynamically receive information storage information well available storage cache capacity provide information use term storage cache refer buffer cache use store access application data server oppose disk cache cache lie disk store recent data access disk resource distribution server compute best allocation storage cache space application best possible distribution cache space across multiple resource distribution server provide compute cache partition size hint storage server system storage adhere hint provide resource distribution server allocate specify amount buffer cache application goal partition share cache consolidate share provide performance guarantee ensure isolation improve application performance goal first provide guarantee service level improve overall system performance usually specify service level agreement sla service provider could specify many metrics io latency cache hit rate resource distribution server describe map specify metric sla amount allocate paper consider distribute share storage server cache accord specify io latency metric every application however important note approach extend share resource general application metric specify io latency hit rate different approach use literature specify specify metric value zero one cache absorption rate achieve application within share available storage server cache example n share storage server cache metric specify fraction zero one hit rate achieve application allocate server cache service provider first execute every application isolation entire infrastructure execute share available infrastructure among order determine possible best utilize service provider maximize paper indicative maximum degradation tolerable access respect access fair share aggregate cache space ie equal partition storage cache space application io latency every one thousand io access fair share case specify maximum five degradation service level guarantee provide term io latency would maximum every thousand io access note propose storage cache partition scheme guarantee minimum specify sla application system every enforcement interval interval cache partition enforce time three partition scheme high level view scheme use process discover require storage cache partition size application g h g two three one four five one seven nine six c e b e f n b c f b e e c r figure two high level view scheme server utilize application first consider aggregate storage server cache space available single large monolithic cache use algorithm predict cache space require application satisfy minimize overall io access latency note general cache space distribute guarantee free space leave use interpolation figure contribute reduce overall io latency reward extra cache space allocation use linear program model partition allocation application across available individual server cache figure two provide high level view storage cache partition scheme set n execute architecture type show figure one partially connect application ai service subset different fully connect application ai service scheme work connectivity configuration simplicity use fully connect configuration description scheme later present result scheme work configuration section consider io latency number specify service level agreement sla use algorithm dynamically predict cache partition size require n concurrently run respectively satisfy constraint specify application ai minimize overall access latency explain algorithm determine cache size section use linear program model distribute allocation application across available server cache detail step use linear program model partition capacity individual server cache distribute cache allocation application give section enforce determine cache partition every enforcement interval cater dynamically change storage cache glimpse e c c r e p c n e l four three two fifteen one five zero zero buffer cache size figure three variation access different vary storage cache size result obtain execute individually single server various storage cache size show motivation dynamic scheme understand behave storage cache space allocate increase conduct experiment use six vary storage cache size measure access obtain detail experimental set describe later section four figure three show access different storage cache size one observe figure different exhibit different storage cache capacity allocate increase example see access almost decrease increase storage cache size decrease certain point beyond curve become flat however saturate point different different case glimpse example access latency curve continue decrease even allocate cache space whereas case curve saturate largely around cache slight decrease access latency allocate close cache space access latency decrease dramatically allocate cache cache space accommodate work set application three see access latency almost saturate allocate cache space close hence may conclude benefit considerably depend work set size give storage cache space compare clearly good dynamic partition policy able recognize difference application allocate storage cache partition accordingly figure four hand plot access latency behavior time see graph access latency per thousand access different vary phase phase course execution hence infer storage cache vary dynamically execution clearly static partition scheme adapt dynamic cache since application behavior change need dynamic scheme change cache allocation carefully satisfy throughout execution period application algorithm base prediction recall figure two propose approach partition storage cache two interpolation module linear program module interpolation module predict total cache space require application meet use algorithm later distribute remain cache space minimize overall io latency first explain algorithm describe cache size n determine throughout execution approach predict cache size versus io latency curve application first step use algorithm call io measure run application storage cache short first enforcement interval one n enforcement interval n number concurrently execute system define enforcement interval interval storage cache partition enforce io latency obtain give initial data point cache size versus io latency curve application determine line pass data point linear interpolation example let line p one p two represent line show figure five accuracy initial data point major concern since algorithm continually update application prediction curve every enforcement interval order experimentally gather sample point increase accuracy second step algorithm use initial data point well additional io latency versus cache size data point experimentally obtain subsequent enforcement interpolate new prediction curve example let p three new data point obtain experimentally second enforcement interval could interpolate data point p one p two p three obtain piecewise linear curve similar two straight line figure five however curve smooth sharp point p two construct smooth curve pass point algorithm apply linear interpolation previously compute linear curve obtain higher degree polynomial quadratic curve show figure five approximate io latency versus cache size curve different instance application execution note step carry application independently every enforce interval obtain cache different size every interval thus accumulate sample point use subsequent enforcement obtain sample point construct higher order polynomial curve pass previously measure data point thereby increase accuracy interpolation way accuracy interpolate curve well cache determine continuously increase execution progress minimum storage glimpse e c c r e p c n e l e c c r e p c n e l four three two fifteen one five zero four three two fifteen one five zero e c c r e p c n e l e c c r e p c n e l four three two fifteen one five zero four three two fifteen one five zero e c c r e p c n e l e c c r e p c n e l four three two fifteen one five zero four three two fifteen one five zero zero ten twenty thirty forty fifty sixty seventy eighty ninety zero ten twenty thirty forty fifty sixty seventy eighty ninety zero ten twenty thirty forty fifty sixty seventy eighty ninety normalize execution time normalize execution time normalize execution time b glimpse c zero ten twenty thirty forty fifty sixty seventy eighty ninety zero ten twenty thirty forty fifty sixty seventy eighty ninety zero ten twenty thirty forty fifty sixty seventy eighty ninety normalize execution time normalize execution time normalize execution time e f figure four variation access latency per thousand access different time execute storage cache capacity case study behavior warm cache execution time application cache capacity require satisfy calculate use interpolate curve specify io latency allocate minimum cache capacity require satisfy free cache space remain distribute among proportion project gain application exhibit term access important point curve predict application use determine cache size allocate satisfy also decide strategy distribute free cache space remain contribute minimize overall io latency two possible distribute free cache space simple solution equally distribute amongst run however recall want minimize overall io latency therefore adopt different strategy distribute free cache among proportion project gain application exhibit term reduce io latency n number concurrently execute predict io latency application j min minimum record access latency application j ree free cache space available distribution piecewise linear interpolation polynomial curve generate algorithm algorithm figure five piecewise linear interpolation three sample point quadratic interpolate curve obtain algorithm curve predict use piecewise linear interpolation sharp point whereas algorithm generate smooth higher order polynomial curve pass sample data point thereby better approximate actual io latency cache size curve block free linear program model cation get ree min min cache proportional individual contribution reduce overall io access latency use min denote potential application exhibit lower access latency allocate extra cache increase probability application access latency likely decrease distribution balance requirement minimize overall io latency well maintain fairness among thus algorithm determine cache size n guarantee also minimize overall io latency let us consider example scenario number system two n two access two storage server cache two cache consider service level objective indicative maximum degradation tolerable access respect access fair share aggregate cache space ie access obtain allocate fair share case one two aggregate cache capacity service level objective term assure io would respectively express one two one example equal indicate tolerate five degradation access latency compare access fair share case suppose algorithm calculate explain minimum cache capacity require satisfy respectively step two distribute free cache space ree proportion project gain application exhibit term io latency get respectively step three algorithm total cache space allocate calculate obtain value guarantee io latency number specify also improve overall performance summarize predict cache size performance curve point observe far execution application although prediction point observe previous enforcement interval application phase typically last several enforcement hence hold good phase application last every enforcement interval decide amount cache allocate application satisfy use curve cache space remain satisfy distribute overall io latency minimize note curve update across enforcement way approach adapt dynamic change cache space ing server cache partition second stage partition scheme involve distribute total cache space require application obtain first stage algorithm use linear program model across available server cache space problem formulate system linear follow suppose server total storage cache capacity partition server cache cache allocation application ai distribute across follow share application ai cache j range one also total cache use concurrently run server less equal total available cache ie linear program solver determine every share application ai cache use mention objective maximize minimum value server j objective function distribute load server uniformly possible across enforce determine cache partition size every enforcement interval cater dynamically change cache important note solver distribute cache application available server cache uniformly possible single server overload distribution also beneficial application application minimal risk requirement event single server failure however requirement enforce one cache allocation particular server could allocate cache size determine first step algorithm server algorithm ensure specify io latency requirement guarantee overall io latency minimize allocation continue example two two distribute obtain curve fit across two server cache partition follow uniformly load determine cache partition size enforce server cache enforce every enforcement interval also every enforcement interval accurate data point accumulate increase precision curve obtain interpolation well quality cache partition four experimental setup simulation platform extend three buffer cache simulator model kernel involve buffer cache management support multiple also support data disk thirty accurate disk simulator since table one description use experiment second column give size io reference stream application third column show total number io access application application description transaction process application trace involve mix five different concurrent sixteen collect trace run decision support suite exercise different query concurrently modify query involve huge volume io read write request business report decision support similar additional base advance knowledge query consist suite business orient query concurrent data locate specify code c source file basically perform source code examination examine source code kernel spec design measure graphics subsystem performance one read entire file render image glimpse stand global implicit search provide index query scheme file use search text string directory seventeen io stream size access twelve accurate hit ratio presence possible without simulator simulate disk model combine simulator extend scenario allow us measure individual cache hit rat io time apart also integrate simulator five linear program solver order efficiently solve simple linear program formulation distribute aggregate cache capacity linear program package default configuration platform consist four execute four fully connect storage cache server give aggregate share cache capacity configuration application client access server system time cache allocation make enforce interval interval default value thirty sec simulation experiment use four cache block table three give default value simulation use experiment every enforcement interval best allocation cache space application best possible distribution cache allocation across multiple compute application satisfy compute new cache partition size communicate hint server run observe experiment communication high nonetheless include result minimize possibility lose lot already warm cache space new cache partition size determine solver different previous one keep track partition size different change every interval new partition size previous one allocation server data block bring new cache space without affect already warm cache however new partition size less previous one choose keep hot block frequently access block application new partition simulator use application trace collect use trace tool trace record follow information io access type time file identifier io size use six large scale trace io intensive size use work comparable three nineteen four important describe table one conduct several experiment concurrently execute different six application trace four experiment table two describe different use experiment choose cover large spectrum data intensiveness exhibit vary access pattern locality three predominantly exhibit random access pattern glimpse exhibit vary sequentiality access pattern three among use exhibit higher degree random access compare dominate sequential application specific io latency requirement specify note sake consistency use application regardless combination use later study impact vary number specification section five experimental result section first describe two base case compare server storage cache allocation distribution scheme explain metrics use study performance scheme follow detail performance analysis scheme lastly present study sensitivity approach various scheme vary one parameter time default configuration see table three base scheme uncontrolled partition scheme server cache share among compete amount cache use application bind aggregate cache capacity system system enforce partition share storage cache table two concurrently execute trace concurrent glimpse glimpse glimpse table three major default simulation number parameter configuration number server cache size disk capacity disk disk seek time enforcement interval page replacement policy cache block size fully connect value four four five thirty sec four uncontrolled uncontrolled uncontrolled uncontrolled c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five overall glimpse overall glimpse overall glimpse overall b c figure six access achieve different scheme different application execute four aggregate storage cache capacity server graph last group bar give overall io latency dot line indicate term maximum tolerable io latency scheme use traditionally due simplicity administration although simple scheme flexible accommodate traffic encourage share buffer data often plague problem interapplication lead unpredictable application behavior lead lack performance guarantee fair share scheme aggregate storage cache divide equally amongst compete application use assign allocation thereby provide isolation note static allocation scheme change cater vary resource demand time also note fair share provide strong insulation static strict form guarantee application guarantee know share cache capacity entire execution however might less useful consider holistic perspective reduce overall io latency much compare uncontrolled partition scheme static scheme contrast scheme dynamically modulate cache order guarantee request performance term io latency apart two base scheme compare approach cache partition approach argon extend single server storage cache management scheme architecture study metrics interest application io latency define average time take data block make available since request block make would like specification directly translate upper bind application io latency number access server upper bind know overall io latency define perspective system rather application sum average data access recall storage cache allocation strategy distribute remain cache capacity satisfy overall io latency minimize improve io utilitarian manner performance analysis perform experiment study performance scheme base scheme describe figure six plot io achieve scheme comparison uncontrolled partition scheme different application see table two execute default configuration see table three access uncontrolled partition scheme normalize respect respective access fair share scheme observe key propose allocation scheme result firstly sla meet regardless combination experiment assume tolerate five degradation access latency fair share aggregate storage cache however uncontrolled partition scheme violate service level many case consider explain approach able handle cache allocation involve two phase interpolation module example particular enforcement interval use algorithm glimpse zero ten twenty thirty forty fifty sixty seventy eighty ninety zero ten twenty thirty forty fifty sixty seventy eighty ninety zero ten twenty thirty forty fifty sixty seventy eighty ninety zero ten twenty thirty forty fifty sixty seventy eighty ninety normalize time normalize time normalize time normalize time glimpse average b average c average average figure seven cache space time one server server one cache uncontrolled uncontrolled uncontrolled uncontrolled b e c p e h c c e c l l zero c n e l e c c e z l r n fifteen thirteen eleven nine seven five b e c p e h c c e c l l zero c n e l e c c e z l r n fifteen thirteen eleven nine seven five b e c p e h c c e c l l zero c n e l e c c e z l r n fifteen thirteen eleven nine seven five b e c p e h c c e c l l zero c n e l e c c e z l r n fifteen thirteen eleven nine seven five overall glimpse overall glimpse overall glimpse overall b c figure eight access achieve different scheme different application execute four aggregate storage cache capacity server dot line indicate term maximum tolerable io latency determine glimpse require amount cache order satisfy allocate aggregate cache space leave need distribute proportion marginal gain explain marginal gain determine proportion three sixteen four lead overall allocation result observe overall io latency improve uncontrolled partition scheme satisfy analyze dynamic nature scheme dynamically allocate vary cache size plot time represent entire execution time application representative combination one figure seven note sum server total storage cache capacity server every instant time result illustrate adaptive nature algorithm track specify value modulate cache sensitivity study performance therefore study impact vary aggregate cache size performance scheme figure eight plot io access achieve scheme comparison uncontrolled partition aggregate cache capacity modify server every parameter remain default configuration result normalize respect access fair share aggregate server cache observe aggregate cache capacity reduce performance affect comparison default configuration obtain large performance benefit beyond satisfy performance barely satisfy remain less similar specifically true like glimpse behavior explain observe aggregate cache capacity reduce would otherwise benefit spare storage cache capacity lose additional share cache would receive would benefit significantly suffer much reduction aggregate cache size amortize lose additional cache without compromise significantly performance sensitivity aggregate cache capacity sensitivity service level aggregate available cache capacity directly influence performance scheme influence fair share section report result experiment study sensitivity approach service uncontrolled uncontrolled uncontrolled uncontrolled c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five overall glimpse overall glimpse overall glimpse overall b c figure nine sensitivity example case execute four simultaneously use four aggregate cache capacity two server dot line indicate term maximum tolerable io latency uncontrolled uncontrolled uncontrolled c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five b c figure ten sensitivity increase number example case execute five simultaneously use four aggregate cache capacity two server dot line indicate term maximum tolerable io latency x st level application overall storage cache io latency achieve scheme sensitive note overall io fair share scheme lower bind scheme achieve regardless service level scheme strive improve overall io latency utilize aggregate storage cache capacity experiment new set service level retain similar default case important characteristic capture new service level differentiate nature different different service level oppose across default case result show comparison performance scheme comparison uncontrolled partition new service level plot figure nine observe overall io latency improve uncontrolled partition sensitivity increase number study scheme perform number default configuration experiment several consist five result show figure ten consist consist glimpse consist glimpse observe c b c c b c c b c c b c ie k figure eleven topology client server cache partially connect scheme able efficiently redistribute cache space increase overall io latency observe overall io latency improve uncontrolled partition scheme three application mix five consider result partial connectivity assume fully connect ie every client connect every server consider case partial connectivity server cache depict figure eleven example application connect two total four server cache therefore client access aggregate cache capacity consider uncontrolled uncontrolled uncontrolled uncontrolled c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five c n e l e c c e z l r n fifteen thirteen eleven nine seven five overall glimpse overall glimpse overall glimpse overall b c figure twelve comparison scheme uncontrolled partition partially connect aggregate cache capacity two dot line indicate term maximum tolerable io latency default configuration partially connect case might possible although frequent total cache allocation application determine linear program step scheme describe section lead solution exceed available cache limit total storage cache size determine interpolation step scheme give client greater total cache size connect client sake correctness case could revert case fair share aggregate cache space available comparison scheme uncontrolled partition scheme case partially connect system show figure twelve apart observe application guarantee important note achieve overall io latency case uncontrolled partition scheme compare default configuration explain occur due reduce interapplication server cache server cache share two partially connect case oppose four default configuration comparison argon extend cache partition scheme propose recent work partition aggregate share cache capacity compare term overall io latency metric obtain scheme argon order discover require cache partition size first request pattern trace deduce relationship cache space require io absorption rate use analytic model cache space require specific io absorption rate predict last step throughput share server calculate extend approach environment predict cache space require allocate server specify requirement term io latency figure thirteen plot overall io latency obtain extend version argon obtain scheme average scheme achieve better overall io latency attribute dynamic adaptability scheme compare argon approach cater change storage cache course execution hence better argon c n e l l l r e v e z l r n one nine eight figure thirteen comparison scheme cache partition scheme argon term overall io latency fulfill cache space requirement six paper propose consolidate storage cache management scheme call efficiently utilize aggregate storage cache space consolidate available cache space across enable access share storage cache scheme also provide performance isolation concurrently execute partition aggregate storage cache space among distribute application cache allocation across application service experimental result show propose strategy perform significantly better improve io individual well overall io latency metric uncontrolled partition scheme fair share scheme satisfy seven author would like gratefully thank codirector scalable scientific compute lab state university kind assistance help us understand different optimization metrics could use linear solver also introduce us linear program solver gratefully acknowledge assistance department state university help us improve paper work support part grant eight reference one performance evaluation implementation global performance measurement computer record seventeen glimpse tool search two p resource management framework predictable quality service web entire file winter technical conference eighteen r l partition cache new type cache research report three r butt c c nineteen n arc low performance impact kernel buffer cache replacement conference model computer four j h l min cho towards characterization block reference case buffer management measurement model computer five j j wright linear program conference mass storage six r wang e cache use remote client memory improve file system performance seven al minor versatile storage conference conference file storage eight al polynomial interpolation extrapolation numerical c art scientific compute edition university press nine w al intelligent project beyond journal research development ten al build distribute enterprise disk array commodity rev h levy c implement global memory management cluster fifteenth symposium operate twelve c r butt c program counter base pattern classification buffer cache symposium operate design implementation thirteen p r provide storage system cache conference model computer fourteen j kim j j kim h l min cho c kim unify buffer management scheme exploit sequential loop reference symposium operate design implementation fifteen b ko k lee k scalable service differentiation share storage cache conference distribute compute sixteen r overhead replacement cache conference file storage twenty r h g e j inform cache symposium operate system g al dynamic partition cache hierarchy share data center endowment j al interactive examination c program winter technical conference g e l new memory monitor scheme schedule partition eighth symposium computer architecture h stone j wolf improve disk cache cache partition global memory management technical report e g r ganger argon performance insulation share storage conference conference file storage schedule distribute storage conference file storage wong j cache make storage exclusive annual technical conference b g r ganger n simulation environment university michigan technical report thirty j provide quality service support file system conference mass storage g factor k li multiple cache conference distribute compute eleven j w e morgan e p r wang merchant