parallel allocation approach base machine learn shun three one department computer science university two member alchemy group university three member institute compute architecture university abstract parallelism one main source performance improvement modern compute environment efficient exploitation available parallelism depend number determine optimum number thread give data parallel loop example difficult problem dependent specific parallel platform paper present approach parallel allocation manner approach use static program feature program decide best allocation scheme base prior experience similar program experimental result twelve test case different total show efficiently allocate parallel among thread achieve efficiency average parallelism allocation cost learn one introduction parallelism one main source performance improvement four eleven modern compute environment particularly true area high performance compute cost parallelism thread creation schedule communication usually negligible compare heavy however rapidly evolve hardware technology enable parallelization modern compute instance embed many cost parallelization become nonnegligible compare moreover inefficient allocation could even degrade performance considerably acceptable therefore vitally important allocate manner order achieve optimal performance widely use program language feature development efficient parallel program require careful consideration avoid performance degradation due cost thread creation schedule communication cost depend many factor cache memory operate system virtual machine interaction factor hard model predict advance search optimal performance give execution environment expect compiler virtual machine use adaptive allocation approach allocate among thread manner achieve statically dynamic manner via speculative parallelization approach specific advantage disadvantage discuss twelve paper present parallel allocation approach base machine learn fifteen learn train allocate parallel among thread new program encounter compiler extract static program feature classification purpose retrieve prior experience similar train use knowledge decide best parallel scheme new program experimental result suggest approach effectively allocate among various number thread achieve optimal suboptimal performance outline paper follow section two present multithreaded framework demonstrate via preliminary experiment demand parallel allocation approach section three present adaptive approach section four evaluate performance analyze result section five briefly review relate work conclude remark section six two motivation first present multithreaded framework use class package parallelize give loop example sequential loop present leave parallelize right modification original code give bold font therefore implement code template advance loop encounter compiler replace loop code template copy loop body run method inner class task embed task class question remain solve parallel scheme ie many thread use share order achieve optimal performance experiment aim evaluate loop performance vary different scheme decide advance framework evaluate platform contain dual two core ram platform standard edition run twelve choose version suite total test case derive contain one loop specific use label test case parallel level use denote scheme allocate evenly among thread simplicity concern experiment consider thread number proportional two evenly distribute among thread discussion general table one sequential loop leave parallel version right via framework public class size public static void main string throw zero size import public class static size static static service public static void main string throw service zero new task private static class task implement runnable private final id private public task id id id size size public void run share load imbalance leave future work impact different parallel scheme summarize demonstrate parallelism significantly improve performance test case take example parallel level increase thread share result rise reach highest share among thread however increase mean thread create imply time spend thread creation schedule diminish performance improvement achieve via parallelism applicable loop program parallelize manner due different cost thread creation schedule communication program performance may vary significantly parallelize different scheme ie case improper scheme may even degrade performance improvement via parallelism twelve different different allocation scheme achieve plot number thread use share performance search higher performance via compiler virtual machine decide scheme manner analysis experimental result reveal program similar likely benefit similar parallel scheme best illustrate although cod different manner actually job result performance curve almost identical good performance improvement achieve use n thread parallelize w likely similar improvement also achieve use n thread vice observation hint compiler could make parallel scheme decision base previous experience program time program parallelize scheme result performance store along description program new program encounter search program similar best scheme similar program consider new one idea long use static compilation analysis usually examine feature program see fit model specific optimization three parallel allocation learn machine learn fifteen natural approach exploit many machine learn approach available vary cost complexity efficiency applicability believe learn fit effectiveness timeliness applicability better therefore parallel allocation approach base learn develop base observation program similar likely benefit similar parallel scheme allocate loop base previous experience similar loop consist two step first learn parallel scheme train set either carefully randomly choose apply knowledge new loop encounter train phase time loop parallelize feature capture classification consider make implicit estimate implicit estimate easier make also sufficient purpose demonstrate later whilst explicit estimate difficult achieve new category create none exist similar loop description store within category together different parallel scheme correspond performance improvement example result metrics new loop encounter capture feature similar loop category within identify base prior parallelization experience loop category select best scheme create thread allocate among implement compiler must correlate loop number parallel thread result performance improvement systematic manner machine learn term input feature problem description program allocation scheme output performance improvement feature reveal important detail program also help compiler classification therefore must formally specify order enable learn use five program feature loop description classification purpose one loop depth two loop size three number array use four number within loop body five number array reference understandable program feature play equal role estimation therefore different weight assign different feature classification higher weight give feature one two four feature better describe loop set feature readily obtain internal representation program four evaluation test environment specify section two experiment aim evaluate impact train set selection performance ie enlarge train set improve predictability carry manner size train set first decide set form select program simplicity size one five ten fifteen thirty sixty consider unnecessary increase train set size one time addition train randomly choose program repeat time test time different train set average obtain apply knowledge learn train set certain size choose scheme improve performance result consider zero let highest achieve performance size define r r r average achieve across different train set size take example train set size achieve average show highest share among thread ie performance train set size summarize performance test case plot train set size take example train set size equivalent random algorithm learn one randomly select example average achieve result performance average performance learn test case equivalent leave one cross validation test best scheme share among sixteen thread one similar select result performance reach similar result find curve show train set size increase performance improve accordingly fig two performance plot size train example set performance define r r r average achieve particular train set size highest achieve give test case regardless train set size learn therefore select better parallel scheme however surge curve test case deserve analysis test case include train set size case surge noise actual variance insignificant indicate train set selection little impact performance case closer look learn process show train set contain many case turn affect performance nevertheless highest relatively modest variance less experimental result convolution show achieve scheme thread scheme improve performance train case randomly choose increase become less likely effectively predict scheme performance decrease instead case higher achieve number thread increase reach highest thread share closer look raw experimental result show besides consider similar enough similar achieve large number thread train set small possibility low program include select scheme smaller number thread result lower therefore performance curve start low start rise train set size reach certain threshold sixty indicate become likely one three program include train set however highest achieve scheme thread whilst highest achieve thread therefore reach highest correctly select one similar otherwise find since share among thread degrade performance suboptimal achieve decide best parallel scheme prior experience brief find optimal parallel scheme test case eighty highest average efficiency test case show train least thirty train achieve reasonable accuracy addition whenever possible train select diverse possible cover also able adapt case worth note current experiment consider test case thread number proportional two evenly distribute among thread leave case arbitrary thread number future work though preliminary result show also able adapt case five relate work parallelism four eleven one main source performance improvement modern compute key automatic detection parallelism discuss five prototype compiler present three loop transformation parallelization use achieve high performance numerical cod two compiler use feedback direct adaptive optimization dynamic optimization present measure cost thread creation parallelize code run time dynamic parallelize compiler seventeen use feedback adapt application operate system chip architecture fly speculative nine sixteen develop exploit parallelism fourteen seventeen eight parallelize speculation support work focus search parallelism opportunity load balance thread migration fourteen give little concern cost parallelism impact performance eighteen consider complement help compiler decide allocate first place accelerate search best parallel scheme machine learn fifteen recently introduce compiler optimization system level various learn approach use iterative optimization one ten explore large optimization space machine learn use six build performance model base small number first test small set sample prior set analyze result order identify characteristic base test run carry target program technique significantly reduce cost evaluate impact compiler logistic regression use seven derive predictive model select suitable apply method base code feature learn use thirteen select suitable loop within optimization space various reorder loop six conclusion paper present fast efficient machine learn base parallel allocation approach use static program feature program decide best allocation base prior experience similar program decide manner best number thread share give loop order achieve optimal performance via experimental result show find best parallel scheme test case eighty best average efficiency test case performance improve train plan use achieve adaptability feature hardware counter read could use estimate program also system portability compiler could achieve via introduction architectural feature addition shall enhance general code block arbitrary could develop select better train identify less representative eliminate necessary reference one use machine learn focus iterative optimization international symposium code generation optimization two optimization notice eleven three automatic loop transformation parallelization international conference four automatic program parallelization proceed two five automatic detection parallelism grand challenge high performance compute parallel distribute technology two three six automatic performance model construction fast exploration new hardware design international conference architecture synthesis embed seven mo dynamic compilation use logistic regression program eight system dynamically parallelize program computer architecture news two nine architectural support scalable speculative parallelization computer architecture ten adaptive optimize century journal one eleven parallel compute morgan us twelve r nim speculative parallelization loop thirteen mo adaptive optimization use learn international conference fourteen speculative multithreaded international conference fifteen machine learn us sixteen search speculative parallelism parallel compilation pact seventeen machine integrate circuit architecture eighteen conservative schedule use predict variance improve schedule dynamic scientific compute nineteen lazy analysis dynamic loop parallelization workshop new twenty loop parallelization international conference parallel distribute compute distribute virtual machine transparent thread migration support international conference cluster compute methodology signal process technology