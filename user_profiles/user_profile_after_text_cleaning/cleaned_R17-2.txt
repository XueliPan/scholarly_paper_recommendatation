international symposium computer architecture high performance compute tree frequent mine multicore department computer science federal de abstract frequent mine core operation several data mine association rule computation document classification many extensively study last moreover become increasingly thus require higher compute power mine reasonable time time advance high performance compute transform hierarchical parallel equip multicore thus fully exploit perform task pose challenge critical problem address paper present efficient multicore accelerate tree projection one competitive experimental result show tree projection implementation scale almost linearly environment careful time faster standard version one introduction frequent mine core operation various data mine attract attention research commercial recent seven twenty give transaction contain set consist find occur give user threshold support work partially support cap national institute science technology web due importance number work focus problem several propose two three eleven twenty may divide two class first class include apriori three variant twenty find frequent base approach follow frequency count second class include tree projection two eleven base projection employ tree construction method base prefix tree one perform mine progressively smaller second class show outperform first class order magnitude faster several current advance computer architecture transform traditional high performance distribute multicore hierarchical thus create efficient fully exploit parallelism important contribution number work also develop parallel find frequent base approach apriori like share ten fourteen eighteen nineteen include single node base implementation apriori eight however receive relatively little attention five thirteen show order magnitude faster base moreover best knowledge exist algorithm paper propose first implementation parallelize tree projection algorithm multicore tree projection choose target implementation compute intensive step suitable data parallelism use multicore careful analysis implementation achieve almost linear multicore share memory machine time faster sequential code two tree projection tree frequent mine algorithm tree projection employ lexicographic tree core data structure use guide mine nod tree frequent assume lexicographic order among tree node depth level k correspond size k root node figure one lexicographic tree absolute support value two order explain algorithm first define node x set generate ie nod whose node x prefix call frequent lexicographic tree example figure one frequent lexicographic node c h active give node subset tree still generate frequent node inactive child generate frequent candidate branch refer set compose last item item set right give node node singleton candidate branch set active candidate branch create thus tree boundary define union active list active nod include active project transaction node intersection transaction active node example transaction active give node b c e result project transaction node transaction correspond project transaction null detail please refer two algorithm one data result frequent f f support ree f k one level one transaction ode one one one ree one k tree projection algorithm extend work base search lexicographic tree nod level k create nod level k one algorithm one present first step algorithm scan compute frequency attribute determine frequent f information use create top level lexicographic tree ree consist root frequent nod initial phase algorithm expand tree create level k one count matrices nod one use count frequency nod level k one node p level k one maintain matrix dimension active p active p row column exist matrix item active p entry exist matrix indicate count p j since matrix symmetric one lower upper triangular part maintain algorithm proceed appropriate exist count matrices level k one execute transaction phase transaction project root node nod level k one project transaction null combine increment matrix show algorithm two process matrix level k one verify node support one new node tree create level k one entry adequate support one level k one delete matrices nod one algorithm two data transaction n ode p level k p one j two ascent combination j else active extension p p transaction onto call last step algorithm prune tree ree one do traverse fashion remove inactive nod update remain nod first singleton nod create level k one remove since one combine continue prune tree update active node remove inactive nod level remain nod active list update follow recursive definition already discuss program finally end level create active nod figure one show example lexicographic tree use absolute support value two generate examine level two nod level three create three parallel tree projection section describe tree projection multicore first section discuss parallelization algorithm adopt strategy section detail implementation multicore several approach avoid race condition update share object study section accelerate version tree projection present parallelization strategy discuss section two tree projection algorithm perform frequent count base lexicographic tree build may construct various paper original tree projection implementation two employ strategy show algorithm one build level tree consecutive algorithm tree build method embed two parallelization process parallelism consist process parallel available hardware update count matrix nod level k one node level parallelism node expand assign different processor process update node matrix may advantage different choose process parallelism due follow reason execution time dominate method process make interest candidate parallelization node parallelism employ hand would load imbalance number nod may enough fully utilize entire execution node level parallelism also demand memory matrices nod concurrently update need memory may unfeasible many although process parallelism several advantage node level parallelism share tree nod expand among parallel process may try concurrently update cell give node matrix require synchronize access section explain different deal problem parallel implementation among thread access different nod section present parallel tree projection propose solution employ transaction process parallelism discuss last section divide among different thread perform independently phase algorithm show inexpensive keep sequential application parallel section build base work queue share among thread six provide dynamic assignment reduce load imbalance among implementation also assign block order minimize overhead apply input operation show algorithm two transaction update proper matrices however process parallel different may need update cell j give node p matrix j demand synchronize access problem although easily solve many critical aspect achieve tree projection share object core algorithm simple way eliminate overhead associate avoid race condition data replication process update local copy reduce end parallel section unfortunately tree projection unacceptable solution size data replicate matrices tree node may large could create unfeasible memory requirement another way avoid undesirable condition create critical section prevent thread update matrices concurrently however solve problem important analyze size critical section type synchronization primitive instance fourteen author assess impact size critical section performance data mine tree base similar structure tree projection base synchronization next discuss three different lock scheme new implementation use hardware base atomic tree level lock create critical section associate level tree thus one thread increment matrices time approach simple inexpensive term overhead create lock impose barrier node level lock employ lock node level k one update despite higher lock management overhead lock node level allow different thread update matrices different nod parallel cell level lock employ lock cell may generate high memory memory use lock need nod matrices moreover cell update may generate extra cache miss read lock structure condition critical section small frequently execute parallel thread use traditional base synchronization may good choice may become bottleneck application thus paper evaluate use twelve implement hardware level available modern multicore update object share among different thread guarantee atomicity update memory eliminate need critical section provide semaphore use add f etch address value include memory address guarantee value even address share object order provide atomic add load old value point address add value update address new value old still avoid aba problem one solution keep counter make sure thread update correct data generation four implementation tree projection application make interest candidate acceleration eleven twenty however irregular several dimension instance different cost process transaction node matrix challenge behavior impose barrier development efficient suitable regular parallel propose parallelization tree projection implement use sixteen multicore implementation exploit process parallelism function implement kernel process several concurrently fifty algorithm three show modify breadth first tree build strategy discuss kernel function execute process set time exploit parallelism assign different different thread since organize line one dimension block enough step set nod deliver always respect memory transaction block process node several one block project active nod various node matrices update simultaneously transaction block process nod bring back matrices use build next tree level important note version projection make directly k one level nod instead project along root fashion like original implementation although strategy go benefit eliminate generate frequent soon possible easier another critical problem address efficiently represent set exist algorithm eight author use n approach represent n number number bite j set one item occur transaction j although representation interest due regularity easy manipulation may acceptable store sparse due memory thus propose novel compact simple representation useful may also example use compute invert index propose structure show figure two store use vector add size transaction addition create auxiliary index identify begin transaction vector store besides simple intuitive way represent sparse approach provide efficient access pattern data successively locate memory compact data representation sparse algorithm three data result frequent f f support ree f k one level one grid st h one one thread h one one grid thread one one one one ree one k figure two sparse data representation four empirical result experiment perform use two machine dual processor main memory operate system second machine equip test algorithm use three present table one synthetic generate use data generator obtain procedure describe three order provide fair comparison among different application program compile use flag result show section average five highest standard deviation execution respectively five multicore analysis section analyze tree projection multicore machine function input figure three present performance propose approach update share length sparse size synthetic synthetic synthetic table one density size tree level lock level node level lock node cell level lock cell base first evaluation average transaction length vary fix support value one show figure three three b three c reference sequential execution time respectively second first figure three propose achieve good scheme slightly better use second hand level node lock strongly fail exploit available parallelism occur average transaction length increase higher pressure critical section project also thus fine grain lock could scale result interest although synchronization section refer small piece code increment variable execute frequently scale poorly coarse grain critical section analyze see figure three c even higher degradation coarse grain lock base observe node scheme still good however order explain behavior measure number cache miss scheme figure three show reduction miss number thread increase observe behavior due share read object among thread projection nod cache miss measure use fifteen event counter finally figure three e evaluate impact support application performance assume follow value five one first show scheme suffer reduction support due result project tree nod critical section show support although scheme efficient configuration achieve near twenty higher performance cell lock average tree projection tree projection implementation evaluate section firstly table one present size three use propose sparse representation base currently use nine show amount memory use representation much higher propose strategy make unfeasible represent low density large problem worsen accelerate data transfer among one critical achieve high performance seventeen also evaluate impact support type tree projection performance compare accelerate version sequential base implementation figure four present sequential version support vary table two show absolute execution time processor figure four result show propose base tree projection efficient compare version also capable scale support decrease achieve maximum sequential tree projection moreover near double performance value support expect represent generation contain stream although gain version tree projection sequential still high b c cache miss e support variation figure three tree projection support fifteen one five table two execution time per processor support decrease relative performance among reduce difference performance result version ability eliminate useful lower level tree evaluate level consequently capable reduce computation cost smaller support less use lower level tree projection relative performance sequential version average transaction length vary present figure five show relative performance higher average transaction length increase behavior due higher process cost consequently pressure parallel section boost tree projection parallel thread work per kernel call finally figure six present relative execution time scale base case show execution time increase almost linearly accord number figure vary length five show good algorithm figure six scale five paper present multicore share memory seven j han p data mine overview perspective knowledge data engineer eight w fang x b q mine graphics nine proceed fifth international workshop data management new hardware page new york nine v g sequence mine parallel thirty four parallel ten eh han g v scalable parallel data mine association rule proceed international conference management data page new york eleven j han j yin mine frequent without candidate generation zero proceed international conference management data page new york twelve synchronization program thirteen one thirteen g approach mine association rule parallel cluster parallel fifteen nine fourteen r g yang g share memory parallelization data mine program interface performance data seventeen one fifteen j manual university sixteen seventeen g r v r replicate heterogeneous international symposium high performance distribute compute eighteen j parallel distribute association mine survey concurrency seven four nineteen j parallel sequence mine parallel memory machine three twenty j w li new fast discovery association rule h r park knowledge discovery data mine page press tree projection algorithm best knowledge first frequent mine implementation also conduct detail analysis algorithm performance multicore provide study several mitigate potential race condition process parallel experimental result show could achieve almost linear multicore share memory use low level atomic traditional base lock fail provide implementation tree projection hand propose new simple sparse data representation achieve compare sequential code compile flag result relevant sequential tree projection already order magnitude faster apriori base implementation several future work intend develop distribute accelerate tree projection utilize concurrently execute parallel section reference one mine sequential pattern efficiently one pattern growth proceed international conference data engineer page computer society two r c agarwal c c v v v tree projection algorithm generation frequent item set j parallel three three r r fast mine association rule large proceed international conference large data base page san ca morgan four j e use synchronization design distribute future twelve five g j toward pattern mine seven solution proceed symposium practice parallel program page new york press six r program thread publish boston