anthill scalable environment data mine abstract data mine become increasingly popular reasonable mean collect rapidly grow many however size raw data increase parallel data mine become necessity paper present support system design allow efficient implementation heterogeneous distribute believe framework suitable class beyond data mine also present example parallelization strategy support system show result two different parallelize use approach support scale close linearly large number nod one introduction large become common place many fact consequence continuous drop cost data storage continuous increase sophistication collect store data analyze huge rapidly become impractical raw form data mine increase popularity lately mean collect meaningful summarize information huge however even sequential may enough volume data therefore development efficient parallel task crucial time grid compute five emerge alternative expensive grid large distribute system create connect together several cluster machine different sit wan cluster set homogeneous machine connect lan potential compute power make available grid large exploit power trivial much work do develop parallel data mine eleven seven main limitation best knowledge show scale well large number recently publish parallel implementation frequent mine problem large heterogeneous distribute show scale really well nine process create implementation generate parallelization strategy class well framework support strategy paper focus two issue framework anthill show two new data mine implement use strategy one frequent mine problem two new cluster classification experiment new show high similar algorithm particular show scale close linearly distribute nod believe framework expose convenient program abstraction suitable design efficient parallel several besides data mine assume eventually run large heterogenous distribute start point anthill three base environment distribute anthill support program model allow wide range efficiently implement remain paper organize follow section two present anthill environment program model section three describe algorithm parallelization section four present experimental result conclude present future section six two framework section describe anthill support framework scalable grid build may efficiently exploit environment maintain good performance challenge scenario usually distribute across several machine grid move data process take place often inefficient usually result data many time smaller input alternative bring computation data reside success approach depend application divide portion may different nod grid execution portion perform part transformation data start input result discussion indicate good parallelization application environment consider data parallelism task parallelism time strategy use two approach together third approach work time dimension allow degree asynchronous execution independent benefit three dimension combine produce high observe experiment base anthill support framework distribute call turn base upon program model describe support anthill program model originally propose active disk one idea create concept little piece application computation could within disk context filter perceive stream data flow computation would generate stream data flow later concept extend program model suitable grid environment system develop support model four system call considerable amount effort put various system eight two stream communication allow fix size data buffer transfer one filter another sense similar concept pip difference pip one stream data come one go propose model arbitrary graph number input output stream possible create application run process refer decomposition filter process application model computation break network filter execution time filter compose application several machine comprise grid stream connect source destination execute application description filter stream connect need provide environment information number copy filter different nod distribute environment refer transparent copy filter anthill paper present parallel program model discuss support highly scalable distribute compute approach base simple observation decompose pipeline represent task parallelism many execution consist multiple pipeline application start initial set possible pass pipeline new possible create experience notice many fit model also strategy allow asynchronous execution sense several possible test simultaneously propose model therefore consist exploit maximum parallelism use three discuss task parallelism data parallelism actual compute copy pipeline stag fine grain parallelism since execution mostly bottleneck free order reduce latency grain parallelism define application designer three important issue arise propose model one transparent copy mechanism allow every stage pipeline distribute across many nod parallel machine data go stage partition across transparent copy represent data time necessary certain data block reach one specific copy stage pipeline two distribute stag often state need maintain globally three nature application decomposition tricky detect computation finish issue discuss label stream label stream abstraction design provide convenient way application allow rout message buffer specific transparent copy receive filter mention stage application pipeline actually execute several different nod distribute environment copy pipeline stage different sense data parallelism mean transparent copy handle distinct independent portion space comprise stage input data copy handle particular data buffer depend upon data label stream add label every message traverse stream thus create l l label original message instead send message stream application send entire l associate label stream also hash function every message traverse stream hash function call parameter output hash function indicate system particular copy message mechanism give application total control message hash function call actual rout decision take individually message change dynamically execution progress feature convenient allow dynamic particularly useful balance load dynamic irregular hash function also little bite relax sense output necessarily return one single filter instead output set filter case message replicate send multiple instance particularly useful one single input data element influence several output data broadcast one instance situation global persistent storage mention stage pipeline distribute across many nod grid environment often time stag stateful mean stage internal state change computation chunk pass pipeline anthill need mechanism allow set transparent copy filter share global state stage partition across nod state transparent copy reside locally many case state may need update different particularly true dynamically balance failure system automatically recover consider fault tolerance scenario add interest feature state require stable sense transactional property change commit state maintain multiple copy state separate host important sake safety describe two feature important data structure maintain global state first several data point within state need migrate conveniently one filter computation progress second stable sense data store need preserve even case individual host run filter copy implement space similar space maintain state structure seem convenient purpose whenever filter copy update data element update space copy maintain filter performance reason copy forward another host safekeeping system allow degree fault tolerance sense copy safely store different host update assume commit termination problem anthill model generic direct graph long graph remain acyclic termination detection application straight forward whenever data stream end filter read notify finish process outstanding task may propagate information flow end outgo stream terminate application graph cycle however problem simple may case filter operate cycle decide whether stream end remember filter may number copy completely independent therefore although filter copy may local information indicate job do process take place another copy may produce new data might travel loop reach first filter situation happen application anthill leave task detect termination condition programmer reasonable solution one system exactly simplify development application would compromise case order avoid anthill implement complete distribute termination detection protocol may rely upon require protocol implement run time system need concern whenever programmer design filter graph cycle require indication stream choose run time system insert notification insertion message break cycle cause filter propagate information accordingly describe detail termination termination detection protocol sake termination protocol filter graph replace graph filter copy copy see connect copy filter connect filter copy connect copy filter since premise original model algorithm work round filter copy suspect computation complete begin contact neighbor filter still compute produce new data round fail algorithm proceed another round sometime future copy filter agree termination single round termination reach final decision leave process leader responsible collect information filter three type message exchange protocol copy suspect termination reach send suspect r neighbor state suspect termination round r copy reach agreement neighbor notify process leader use terminate r message also identify round number leader collect terminate message filter copy round end message back although stream unidirectional application level system use guarantee communication channel reliable message two filter copy always deliver order send besides round counter r filter copy keep list neighbor suspect termination round r reach core protocol illustrate extend finite state machine figure one filter copy may one two state long copy run data still read input stream remain run state filter code block wait data stream empty indication do compute fact wait short interval take decision case data arrive filter compute data input stream still read propagate message termination protocol receive data another filter application message remove sender list neighbor suspect terminate since obviously compute receive suspect r remove list list neighbor leader terminate r run suspect termination update idle tell neighbor suspect r suspect r update r add suspect list suspect r r r update r tell neighbor add suspect list figure one extend termination algorithm sage another node first check whether indicate round r case r round r r update r new value reset list suspect add sender suspect message list system filter copy detect idle time computation take place copy block wait data empty input stream move suspect termination state notify neighbor send suspect message current round number keep list suspect neighbor collect run state since consider termination round suspect state copy keep track neighbor round also reach possible termination state receive suspect message neighbor add list suspect reply message need since copy round r must send suspect message neighbor enter state copy receive suspect message round number r indicate copy may go farther process copy remain wait consensus neighbor must therefore update round counter new value clear list suspect since belong previous round sender message add list point whenever copy collect suspect neighbor give round widespread suspicion termination reach although may true copy point process leader must inform terminate r message suspect state application message may still arrive may case one neighbor compute longer time data send happen arrival data stream bind new computation start filter copy must give termination get back work point must clear list suspect neighbor prepare new future round round counter switch back run state process leader turn must track termination round hear r whenever receive terminate r message filter copy must compare r r r lower message may simply discard since relate round already know pass r r new round start r relevant list terminate copy must clear sender message must add finally equal another filter copy join group process suspect termination reach must add list process termination list complete process leader declare point broadcast end message process take final step toward end filter stream model lead protocol stream select user close deliver notification filter read since filter graph describe application require strongly connect although direct graph create relation filter copy neighbor also specially since neighbor relationship always bidirectional way filter copy reach global termination suspect state since value round counter grow monotonically get add copy switch back run state copy bind converge value r termination agree upon three parallelization section describe parallel implementation decision tree algorithm decision tree leaf nod individual data internal nod contain attribute descend pointer encode possible value attribute map node distinguish depth tree maximum number question attribute value need ask data element order find one single element data basic idea algorithm use greedy search data find discriminate attribute p p q two p p p two attribute v log v two v none none one two three four two p five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen nineteen two attribute disc two v disc p disc p instance v p q p p figure two algorithm level tree sake filter definition distinguish three main task insert node decision tree one value attribute count number instance value two compute information gain attribute three find attribute highest information gain start point algorithm set contain instance n attribute one c possible class attribute may assume value tree generation process base discriminant test specific attribute use divide set two depend number different value occur discriminant initially discriminant partition whole set find new partition set one partition belong class algorithm present figure two line thirteen p first one entire loop line four execute new partition line five select compose partition q line six compute information use entropy metric attribute instance value line line compute information gain attribute final step find attribute yield maximum information gain insert correspond partition p term parallelization two line ten thirteen identify among filter divide process three filter first filter name counter perform associate line four nine responsible count number instance value attribute second filter attribute perform associate line ten twelve correspond compute information gain attribute test previous filter third decision perform remain correspond communicate decision appropriate attribute back first filter process continue select new discriminate attribute produce class algorithm exploit two dimension parallelism base partition decision tree node base partition achieve run several instance counter filter instance process subset one filter process one node tree process nod nod stay busy practically time granularity base partition parallelism fine assign single instance filter change code another source parallelism filter may work multiple partition simultaneously attempt achieve maximum efficiency four experimental result section evaluate implementation two data mine anthill focus efficiency experiment run sixteen node cluster connect use switch fast node main memory run start evaluation implementation describe experiment run decision filter alone separate node nod run counter attribute filter evaluate algorithm use synthetic describe ten particular use two classification function different complexity function two seven function two simpler produce smaller decision tree compare function seven table one show generate two function notation f use denote function x contain attribute z instance size level eight eight table one f x p e e p eighteen sixteen fourteen twelve ten eight six four four p e e p sixteen fourteen twelve ten eight six four four six eight ten twelve fourteen sixteen number figure three start analyze figure three four show respectively observe scale better use show behavior scale better simpler demand less memory seem affect significantly order understand perform detail analysis processor cache usage change number use performance application program interface six measure number cache miss configuration figure five six show substantial drop number cache miss add execution expect explain behavior instance figure five notice increase number four fourteen factor result reduction total number cache miss factor eleven focus analysis three criteria demonstrate various anthill collaborate observe task analysis task algorithm associate analyze determine discriminant f x six eight ten twelve fourteen sixteen number figure four six eight ten twelve fourteen machine figure five total cache miss x number zero fifty zero fifty time sec figure seven active task x execution time cache miss cache miss two l e h c c l two l e h c c l zero four zero four six eight ten twelve fourteen machine figure six total cache miss x number decision tree node arise overlap process several tree nod may belong tree level notice task tree level independent parallelism case trivial exploit interest verify whether may observe task one level execute simultaneously thus exploit potential parallelism present algorithm evaluate level plot number active task tree level across time figure seven show task behavior execution sixteen use input clearly see task one tree level overlap whole experiment execution time explain algorithm efficiency filter analysis table two show breakdown task execution time per filter consider nine twelve nod input observe majority process time task occur counter filter confirm higher demand impose counter filter check message counter present table three observe amount data process decrease go counter decision filter finally interest level one level two level three level four level five level six level seven level eight total k e v c number attribute decision nine ten eleven twelve counter table two percent time filter test notice amount parallelism explore since elapse time execute task usually elapse time processor figure demonstrate number would allow even better result interest note despite high variability demand impose different filter application scale well task parallelization scheme may suffer performance degradation result imbalance application affect filter instance analysis finally evaluate performance filter instance order evaluate imbalance generate data skewness label stream even one basic metric evaluation variability execution time filter instance since execution time task may vary significantly among task compare time quantify load imbalance among filter instance calculate task filter relative standard deviation among execution time filter instance result value present table four observe slight increase variability number nine ten eleven twelve counter attribute decision table three number send message test counter attribute decision total n r fourteen twelve ten eight six four two zero one ten task figure eight elapse time per filter overall nine ten eleven twelve counter attribute table four variability execution time among filter instance number instance expect since load assign instance reduce skewness may impact also observe relative standard quite high although compensate effect take place instance work give task work less later show label stream present good performance load distribution balance association analysis association analysis determine association rule show attribute instance usually call occur frequently together present causality relation divide problem determine association rule two phase determine frequent build rule since first phase much intensive parallelize first phase case input set transaction contain object occur simultaneously output set occur frequency threshold know support association rule base simple powerful principle k frequent k size k one must also frequent base principle may easily build dependency graph among task task divide three partition counter verifier candidate generator counter reduction count number give forward verifier filter verifier filter receive partial count add veri whether frequent consider whole whenever verifier find frequent inform candidate generator candidate generator keep track find frequent able check whether may count verify accord task graph experiment use different synthetic size range generate use procedure describe nine mimic retail environment experiment perform minimum support value one sensitivity analysis conduct data distribution data size degree parallelism better understand effect data distribution distribute among partition two different ways random transaction distribution randomly distribute among partition strategy tend reduce data skewness since partition equal probability take give transaction original transaction distribution simply split partition preserve original data skewness evaluate parallel performance data mine application mean two metrics seem figure nine better number achieve original transaction distribution reason time ie eight counter filter much original skew transaction distribution however increase number counter filter execution time different data tend approach since partition get smaller parallel reduce experiment increase proportion size number counter filter see parallel data mine application show scale well even skew order better understand dynamics define metrics may use understand focus data mine algorithm apply vision application may divide determination support four phase activation various necessary become candidate may arrive time verification filter wait condition consider candidate satisfy execution time execution time original distribution random distribution original distribution random distribution activation eight sixteen contention count check process five ten fifteen twenty thirty five ten fifteen twenty thirty filter filter table five experiment profile process c e l e v seventy sixty fifty forty thirty twenty ten l e v thirty twenty fifteen ten five five c e l e v ninety eighty seventy sixty fifty forty thirty twenty ten eleven l e v one nine five ideal original distribution random distribution ideal original distribution random distribution original distribution random distribution ten fifteen twenty thirty ten fifteen twenty thirty filter filter figure nine parallel performance apriori contention consider good candidate may wait process queue counter filter count counter filter may start simultaneously count phase characterize counter filter calculate support candidate partition check support counter counter filter may arrive time support checker filter check phase time period arrive next go analyze duration phase experiment analysis experiment explain efficiency achieve analysis experiment show table five show duration phase describe employ eight sixteen rightmost column also show average process cost count see cost reduce number increase expect may observe phase except activation phase whose duration seem reach limit around one second problem case number involve high asynchronous nature algorithm make reduction activation time difficult verify time experiment table six verify algorithm see increase number size affect significantly algorithm implementation saturate system mainly communication scale activation eight sixteen contention count check process table six experiment profile process five cluster cluster analysis partition determine group object similar regard similarity criteria section discuss parallelization popular cluster algorithm algorithm base concept represent object compose cluster iteration algorithm assign object centroid update value properly algorithm end object change cluster maximum number reach since single task determine cluster task graph express algorithm use two assigner centroid calculator assigner hold object must cluster base determine centroid object list object assign centroid send centroid calculator recalculate cluster evaluation base two synthetic contain point cluster point fifty dimension perform experiment evaluate linear close one application scale perfectly execution time respective show figure ten see time per algorithm two vary number case almost linear cluster behavior come reduction memory increase number ming operate page press two c chang h r j process multidimensional data parallel distribute parallel compute five three alan h filter large scientific archival storage symposium mass storage page four alan design framework heterogeneous compute workshop page computer society press may five foster c grid blueprint new compute infrastructure morgan six performance white paper university march seven clark f parallel hierarchical cluster ing parallel compute eight eight spencer alan execute multiple data analysis grid proceed conference page computer society press nine w r asynchronous anticipatory stream base parallel algorithm frequent set mine proceed conference practice knowledge discovery ten c ho r parallel classification data mine proceed international conference data engineer page computer society eleven ho parallel data mine linear point point five ten fifteen twenty time per iteration point point p e e p n c e thirty twenty fifteen ten five zero zero four three two fifteen one five zero zero five ten fifteen twenty figure ten parallel performance algorithm six future work paper describe support framework develop support efficient implementation class heterogeneous distribute also show parallelization strategy approach use perform believe approach apply large class experimental result show two design use strategy support scale almost linearly large number nod experiment however limit number compute nod available experimentation fact see reason keep behavior much support system evolution top particular implement label stream termination detection algorithm still actual implementation stable storage distribute state work process incorporate fault tolerance anthill robust implementation distribute state requirement system reference one active disk program model evaluation internation conference architectural support program