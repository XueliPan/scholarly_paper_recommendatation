bootstrap model uncertainty technology square technology square abstract bootstrap become popular method explore model structure uncertainty experiment artificial data demonstrate graph learn bootstrap sample severely bias towards complex graphical model account bias hence essential explore model uncertainty find bias intimately tie spurious induce bootstrap equal one half penalty model complexity demonstrate effect simple experiment also relate bias bias estimator entropy well difference expect test train graphical model asymptotically equal penalty rather one half one introduction bootstrap powerful tool estimate various give statistic commonly bias variance five quickly gain popularity also context model selection learn structure graphical model small data set like data apply explore model structure uncertainty seven six eight twelve however bootstrap procedure also involve various four overview instance bootstrap bootstrap sample b b one b generate draw data point give data replacement bootstrap sample b often contain multiple identical data point typical property discrete data give data fact continuous vanish probability two data point identical data bootstrap procedure introduce spurious discreteness sample b statistic compute discrete bootstrap sample may differ base continuous data note four however effect due induce spurious discreteness typically negligible paper focus spurious induce bootstrap procedure even give discrete data demonstrate institute computational science spurious neglect explore model structure uncertainty mean bootstrap whether parametric graphical model learn bootstrap sample bias towards complex model bias considerably variability graph structure especially interest case limit data result many edge present learn model structure confidence presence edge overestimate suggest bootstrap procedure essential explore model structure uncertainty similarly statistics literature give derivation term amend several popular score function apply bootstrap sample section term asymptotically equal one half penalty term model complexity information criterion section huge effect bias propose illustrate experiment section five maximum likelihood score entropy intimately tie exponential family probability also relate bias towards complex model bias estimator entropy section moreover show section four similarly thirteen one bootstrap use obtain score function whose penalty model complexity asymptotically equal penalty rather one half two bootstrap section introduce relevant notation briefly review bootstrap bias estimation arbitrary statistic well bootstrap also five four commonly use graphical model information criterion information criterion minimum description length posterior probability view special case statistic domain n discrete random x let p x denote unknown true distribution give data sample empirical distribution imply give p x p x n x n n x frequency state x x n x n x sample size statistic number compute give data bias define p denote expectation data set size n sample unknown true distribution p arbitrary statistic p associate possibly slightly different statistic compute normalize distribution since true distribution p typically unknown compute however approximate bootstrap p replace empirical distribution p average data set replace one bootstrap sample b generate p b one b sufficiently large b five b b p one estimator p statistic empirical distribution plug place unknown true one example p ie x two n n ie x two familiar statistic variance unbiased one p unbiased estimator two obviously statistic yield unbiased estimate concern distribution plug consequently empirical distribution plug statistic typically give unbiased estimate concern unknown true distribution statistics linear function p x inherently unbiased arithmetic mean however statistics include score function nonlinear function p x equivalently n x case bias vanish general special case statistic convex concave function p follow immediately inequality bias positive negative example statistic p negative quadratic thus concave function p hence underestimate variance unknown true distribution general procedure use reduce bias bias statistic considerably bootstrap estimator give two bootstrap bias estimate accord eleven typically agree correspond unbiased estimator lead order n five achieve bootstrap five two b b five even though dangerous practice less bias estimator may substantially variance due possibly higher variability estimate bias particularly compute small data set however issue paper since estimate bias turn independent empirical distribution lead order n three section show popular considerably bias towards complex model apply bootstrap sample place give data score function amend additional penalty term account bias use bootstrap slightly nonstandard way simple expression penalty term follow easily section bias estimator entropy review section also eleven two sixteen estimator true entropy entropy define h p x x p x log p x since concave function p estimator h p x tend underestimate true entropy h p x section two bootstrap bias estimate h p x true distribution p x h b b h b x one b b h b b h p x n x bin n p x x n log three x bin n p x denote binomial distribution originate procedure bootstrap n sample size p x probability sample data point x x exact evaluation three prohibitive case monte yield accurate result costly analytical approximation three follow immediately expansion l q x q x log q x p x q x x n two l x n x x h p x one l p x p x two x two x h p x one x one x n one n two one n two four b b statistic approximation apply analogously instead bootstrap estimate term obtain l p x x observe fisher information evaluate empirical value p x x n p x two x n p x one p x variance binomial distribution induce bootstrap four x number joint state x bootstrap estimator entropy unknown true distribution thus give h p x h p x one x one one n two section concern bias popular score function induce bootstrap procedure moment let us focus learn network structure n p xi log log n five n xi p xi p one two maximum likelihood involve summation one n joint state variable xi parent accord graph number independent network give xi one n six xi denote number state variable xi number joint state parent like obviously intend apply give data do optimize yield unbiased estimate true network structure underlie give data however apply bootstrap sample b instead give data expect yield unbiased estimate true graph maximum likelihood term bias compute bootstrap sample b instead give data bias b b differ conceptually one two read ways first exact bias induce bootstrap procedure one bootstrap approximation unknown true bias second one apply statistic general last term one necessarily statistic contrast term involve comprise general statistic since maximum likelihood term intimately tie entropy exponential family probability approximation bias entropy carry four one two n xi one one seven one n one two one n number independent model give six network note bias identical one half penalty model complexity information criterion hence bias due bootstrap neglect compare penalty term inherent popular score function also experiment section five confirm dominate effect bias explore model uncertainty bias maximum likelihood give rise spurious induce bootstrap property paper mainly interest structure learn graphical model context bootstrap procedure obviously give rise considerable bias towards complex model consequence many edge present learn graph structure confidence presence edge overestimate moreover undesirable additional direct edge network tend point towards already large number parent bias proportional number joint state parent variable seven six hence amount induce bias generally vary among different edge graph consequently amend apply bootstrap sample b instead give data read b b one lead order n since bias originate two maximum likelihood term involve apply score moreover approximate log marginal likelihood log p large n seven also expect account bias log p b apply bootstrap sample b four may surprise bias derive seven equal one half penalty section demonstrate indeed consistent score use standard bootstrap procedure section two obtain score function asymptotically equal approach similar one thirteen assume give data sample unknown true distribution p x goal learn network model p x p short graph structure maximum likelihood parameter estimate give data information theoretic measure quality graph divergence unknown true distribution p x one describe network p approach one since entropy true distribution p x irrelevant constant compare different graph minimize equivalent minimize statistic p p p x log p test error learn model use log loss p unknown one evaluate p p approximate train error p p x log p p log p x x assume exponential family note p equal negative maximum log likelihood irrelevant factor n train error underestimate test error however train error p p p ten serve surrogate nearly unbiased estimator unknown test error p p hence score function model selection bias give difference expect train error expect test error p log p p log p eleven x x one n h p one one n two h p one one n two expectation take various data set sample size n sample unknown true distribution p h p unknown conditional entropy true distribution approximation n also section number independent model give six network note expect test error expect train error give rise one half penalty overall eight nine x bias amount n exactly equal penalty model complexity note asymptotically favor model fifteen typically select true model underlie give data complex model bootstrap estimate exact bias eleven insert score function ten result score may view version information criterion thirteen factor two average distribution model natural approach mimic bootstrap approach five experiment experiment artificial data demonstrate crucial effect bias induce bootstrap procedure explore model uncertainty also show penalty term seven compensate possibly large bias structure learn network first experiment use data sample alarm network discrete edge comprise data point respectively generate data set expect entail model structure uncertainty examine two different score function namely posterior probability uniform prior network structure equivalent sample size one ten use search strategy three computational efficiency accuracy structure learn high compare local search even combine simulate anneal ten accuracy due additional input require algorithm namely correct topological order accord true network structure consequently report variability learn network structure tend smaller uncertainty determine local search without additional information however mainly interest bias induce bootstrap expect largely unaffected search strategy although true alarm network know use network structure learn give data reference experiment expect optimal graph learn small data set tend original graph order avoid table one three generate bootstrap sample give data suggest five learn network structure table one show bias induce bootstrap procedure considerable posterior probability neglect compare standard deviation distribution number edge also note despite small data set bootstrap yield graph even edge true alarm network contrast table one illustrate bias towards complex model reduce dramatically outline section however note work perfectly correction n seven jackknife alternative method view approximation bootstrap five delete jackknife procedure generate give data delete data choice one popular lead statistics five inconsistency resolve choose value greedy algorithm yield exactly one graph give data set consequence unlike bootstrap sample jackknife sample contain multiple identical data point generate give continuous data set section one alarm network data n n one zero data boot boot zero jack one nine jack posterior forty zero three zero three posterior zero four n posterior fifteen fifteen table one number edge mean standard deviation network structure learn give data set use various bootstrap boot naive bootstrap boot jackknife jack one delete jackknife jack figure one axis scatter plot show confidence presence edge graph learn data vertical horizontal line indicate threshold value accord mean number edge graph determine three table one n n roughly speak five underestimation bias variance statistic often consider disadvantage jackknife procedure raw jackknife estimate bias variance typically multiply inflation factor usually order sample size n context model selection however one may take advantage extremely small bias raw jackknife estimate determine mean number edge model table one show raw jackknife typically less bias bootstrap experiment however clear context model selection meaningful raw jackknife estimate model variability second experiment essentially confirm result yeast response data contain data point nine data use average optimal number level variable determine fourteen unlike fourteen simply data step conduct experiment base data since correct network structure unknown experiment use local search combine simulate anneal order optimize score posterior probability fourteen reference experiment use network structure learn give data graph find run local search combine simulate procedure also base course accord seven also apply joint optimization graph structure give bootstrap sample anneal suggest ten run simulate anneal result different network structure local optimum practice zero give data give data zero correct data experiment table one qualitatively confirm previous result bias induce bootstrap even suspect difference bias cause rather extreme parameter value original alarm network model lead relatively large ratio even small data set contrast data know extremely noisy another effect spurious induce bootstrap procedure show figure one overestimation confidence presence individual edge network structure confidence individual edge estimate ratio number learn graph edge present overall number learn graph mark figure one correspond edge reflect confidence estimate different obviously naive application bootstrap lead considerable overestimation confidence presence many edge figure one particularly whose absence favor reference bootstrap contrast confidence estimate bootstrap align quite well confidence determine reference figure one lead trustworthy result experiment reference one h information theory extension maximum likelihood principle international symposium information theory two bias information bulletin three g cooper e method construct belief network four bootstrap application five b r j introduction bootstrap six n data analysis network bootstrap approach seven n application bootstrap compute confidence measure feature induce network ai eight n er use network analyze expression data journal computational biology nine j k r young combine location expression data discovery genetic regulatory network pacific symposium ten learn network combination knowledge statistical data machine learn eleven g miller note bias information estimate information theory psychology page twelve er g n infer perturb expression profile thirteen j n g best b p carlin van measure model complexity fit j r soc b fourteen h semi predictive model selection ai memo fifteen stone asymptotic equivalence choice model criterion j r soc b sixteen j victor asymptotic bias information estimate exponential bell neural computation