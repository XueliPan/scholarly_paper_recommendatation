start globally optimize locally predict globally improve performance data department computer science engineer university dame v abstract common approach counter curse imbalance many class imbalance ubiquitous problem supervise learn gain attention literature perhaps prevalent solution apply sample train data order improve classifier performance typical approach apply uniform level sample globally however believe data typically multimodal suggest sample treat locally rather globally purpose paper propose framework first identify meaningful data proceed find optimal sample level within paper demonstrate global classifier train data locally sample produce superior wide range artificial compare contemporary global sample one introduction mine continue pervasive problem large variety eight include medicine finance security traditional objective metrics accuracy often fail describe performance properly trivial classifier may produce high accuracy perform poorly interest rare class instead area roc curve roc typically use measure capability classifier effectively rank minority positive class instance majority negative class instance roc capture true false produce robust metric even direction research focus sample counter problem class imbalance three eight sample focus either add minority positive class point remove majority negative class point improve accuracy roc positive class three fifteen sample x p figure one data example class generate piecewise regional function sample consider class skew whole however contention paper machine learn data mine often face often exhibit local rather global level example consider figure one class probability generate function different within region data dictate relationship class substantially different within discrete area even two exhibit class skew possible exhibit disjoint particular sample level induce strong classifier first region may well induce poor classifier second region suggest typically global task select sample level improve classifier performance many consider locally concern mind primary paper follow one analytic framework support locally optimal sample two effective data partition segmentation method base distribution invariant distance nine ten eleven three identify optimal level type sample within data segment four comprehensive evaluation use twenty consider two popular sample relate work synthetic minority technique smite six random addition include classification use local cluster sample smite inject synthetic positive class emphasize class use cluster partition complex majority class simpler minority class replication two start globally optimize locally supervise learn task label train data sample available model train presumably draw randomly distribution p x xi x single feature vector associate class label addition available unlabeled sample test purpose sake study presume distribution p x goal task train classifier f x estimate probability xi belong class ideal classifier strong performance either minimize loss cost maximize accuracy profit roc thus model maximize objective function e x f x maximize four ideal case f x represent true model data p x practice difficult impossible due limit representation provide p x thus learn often approximate true function due bias inherent learn algorithm representation particular emphasis objective function often case necessarily draw p x actually enhance performance determine classifier task find improve classifier performance call sample primary question employ sample one tune yield optimal result suggest method use heuristic wrapper seven set sample level class wrapper greedily explore search space sample potentially optimal amount discover alternate method stem cost sensitive learn instance discuss simple method calculate optimal sample level base ratio misclassification class fourteen however evaluation typically occur without explicit cost case calculation simply indicate sample class balance point limit often necessary explore much sample space address question data aside class skew affect optimal sample level wrapper exploit data utilize performance learn algorithm guide consider question construct artificial see figure two follow begin exist two class skew ratio fifty positive depict negative class center fix point radius r r respectively since number point class fix product radii reduce radius length increase density final consideration distance higher lower class overlap interest interaction density relative overlap end ability discriminate quantify roc since maintain relative skew desire affect change r resultant region experiment consider three discrete r high medium low determine importance degree r end performance say high r r medium r low r r determine overlap high medium low r overlap high nearly region engulf within region however overlap low half region outside area note therefore r remain fix base address construct affect discriminative ability conserve relative skew ratio class fifty solution would apply level universal optimal solution nine however result show table one indicate otherwise also establish three general trend first increase minority class density improve roc since increase density allow develop confident decision region second increase separation two class center reduce class overlap increase roc since enable construct definitive border class third optimal level sample scenario substantially different generate class balance sample contradiction solution thus observe data aside class skew may substantial effect optimal sample level however often complex figure two perhaps contain multiple point intersection scenario even optimize sample data may represent incomplete solution local perspective thus would like address effect high high high medium high low medium high medium medium medium low low high low medium low low figure two demonstration different radial pair specify level density centroid separation row descend decrease likewise decrease leave right example high medium display high density medium class separation skew situation fifty represent positive example represent negative example consider sample locally global performance consider data highlight figure three note leave middle similar previous represent overlap vary distance center density leave significantly easier learn middle since less overlap reflect determine roc leave middle use use sample leave roc improve use level e fifty find wrapper method seven middle improve use twenty however difficulty leave middle concatenate form rightmost figure default obtain roc improve use fifty somewhat problematic dictate different sample level consider single distinct third sam combination use indicate loss efficiency optimize roc via sample globally confirm since use regional sample level fifty leftmost twenty center improve global performance thus current global sample may insufficient since observe considerable improvement local sample consider rather task find improve performance might divide belief much data operate set guide construction feature associate class label accordance p x presume least data exist modal fashion therefore meaningful derive sample locally procedure begin decompose meaningful segment di figure three two right leave middle segment determine sam figure three two separate two class different skew third reflect combination single table one roc value sample addition optimal sample level nine figure two roc high high high medium high low medium high medium medium medium low low high low medium low low e sixty forty forty fifty sixty thirty thirty eighty optimize locally find recombine use formulate global classifier see example key aspect local optimization construct reliable partition discuss next section three partition data section introduce supervise method split data segment partition build tree maximum height two see algorithm one result four segment initial experimentation demonstrate use height greater two result much segment give high imbalance use paper thus consistent constrain tree height two type segmentation use instead unsupervised learn since know class label provide potentially useful information data also presumably highly want exploit class skew information construct partition well end propose use distance nine ten eleven guide segmentation distance measure distributional divergence let denote measurable space b two continuous measure respect let b b respect definition distance give b b one equivalent b two one two integral note distance depend choice parameter also define countable space b two b distance carry follow one b zero two two symmetric imply b b moreover square distance lower bind divergence b equation one assume normalize feature value across class allow us capture notion affinity probability measure finite event space b distance zero maximal affinity b two zero affinity disjoint distance dictate partition split criterion separability class want select feature carry minimal affinity class assume countable space continuous feature p partition bin essentially interest calculate distance relative class feature value normalize overall class frequency reduce effect class skew thus apply distance algorithm one partition procedure input feature set f current height h binary tree call zero return one h two zero two three end four f five value v f six v h one seven end v p x p x p p two three note bound metric unaffected class skew make distance ideal separate disjoint class distribution within highly data construct partition use procedure give algorithm one distance apply feature highest distance use generate split data generate binary tree initial experimentation indicate produce best result case continuous feature potential feature split explore mean p two procedure apply partition desire maximum depth reach partition method propose generalize framework local sample aim leverage section two practical application four apply local sample local sample provide general framework possible incorporate sort classifier data partition sample mechanism flexible structure allow versatility allow practitioner readily adapt framework many application essence three step extension sample scheme algorithm two outline use propose distance base partition method principle arbitrary partition method p apply divide data phase may apply nearly type supervise unsupervised method set discover data partition z procedure move sample phase choose sample method apply component algorithm check range determine optimal performer accord objective function optimal sample apply component segment merge create new train set algorithm two local sample framework input label train set sample method associate p partition method find partition f learn algorithm objective function output model build z data segment partition discover via p z z one z p two partition z z three four end five six f classifier learn thus go global view data local view partition global view learn classifier follow wrapper framework seven discover potentially optimal amount smite note wrapper framework use use paper combination smite entire without sample level form zero ten zero fifty smite p apply partition method section three wrapper proceed follow first apply train set determine roc performance since use framework apply train set make independent validation framework majority class randomly remove ten time long performance remain level select result new improve performance subsequently smite apply progressive step add synthetic point fifty minority class size repeat long beat performance procedure slightly adapt local sample explore parameter space exhaustively partition sequence order optimization partition majority class point one allow us gradually shift bias towards minority class also likely partition require either smite partition merge optimization determine roc via allow measurement global relevance optimization do partition figure four depict practical application benefit two represent two dimension class disjoint two c c b c two b eight eight eight b eight two two figure four illustrate two dimension represent positive represent negative b use distance tree train sample level explore c component optimize different sample level remove negative inject smite represent concatenation use form single train set juxtapose present effect global smite class overlap also observable use alone yield roc data figure four b distance discuss section three use form split region b entirely comprise exhibit example dominance however c isolate actually form majority use roc region optimize e level forty fifty ninety forty fifty b c respectively see figure four c produce roc outperform global smite combination find roc global sample use seventy local sample effectively apply base number actually add remove class across partition therefore observe practical case similar figure three improve roc case judicious term require better result able focus use smite particular five empirical evaluation section present experimental result validate understand performance particularly comparison contemporary performance analysis use thirteen roc evaluation criterion choose use result independent test set carry sufficient overlap train data across fold make application typical assume independent questionable thirteen note result elevate level type error correct rely idea learn curve rarely cross train set size vary elevate type error come particular concern evaluate false positive false negative significance test describe one confidence note two intrusion possess natural split maintain preserve sanity evaluation result determine also use statistical analysis framework compare classifier rank outline twelve method use holm procedure test establish significance classifier rank across multiple data set specialize procedure test significance multiple mean note test appropriate since assume limit commensurability twelve page parametric test since assume normal homogeneity variance apply classification error measure evaluation twelve page analysis increasingly embrace classifier comparison community compare propose local sample framework contemporary notably combination smite show significantly improve performance recent method counter class imbalance use cluster analysis use local cluster majority class point minority class replication enhance linear separability cluster derive use since algorithm partially dependent initial random seed use ten different cluster cluster result distance centroid preserve sample utilize framework discover potentially optimal allow fair across board vanilla smite combination use train fold first determine level sample determine appropriate number cluster k use top framework finally approach also use determine level sample partition train test set remain across approach ensure approach independently optimize fashion achieve best use two base nineteen four four estate four five boundary table two use paper column intrusion also include number train test feature one two three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen nineteen twenty thirty twenty twelve two sixteen six ten sixteen five eight nineteen sixty four nineteen two four ten ten five ten sixteen segment splice phoneme twelve thirty oil page intrusion letter seven eight five decision tree unpruned use leave keep provost eighteen provide roc train use zero b one build probabilistic estimator since train much higher time complexity decision tree induction exacerbate prohibitive computational cost combine local sample restrict experiment also commonly use classifier sample literature throughout remainder paper abbreviate result include roc five approach completeness repeatability also show parameter value amount sample number cluster minority class describe later use robust statistical measure evaluate performance difference among different table two describe use experimental analysis local sample several acquire various originally consider six estate contain table three roc value average fold along standard statistical significance calculate use confidence use method classifier performance bold imply significantly improve classifier italicize value indicate correspond method significantly improve note achieve higher performance seventeen twenty addition average rank classifier also list show achieve best rank significantly show achieve statistically significantly lower run correspond classifier use test compare rank confidence interval twelve boundary estate intrusion letter oil page phoneme segment splice rank five six fourteen eight six sixteen three five fifteen one two three ten seven twelve four sixteen one twelve nine eight one thirteen seven five three three five fourteen two fourteen one fifteen eleven seven two two eight fourteen four one three fifteen fourteen six two eight one four eight eight nineteen nineteen one five fifteen one one two thirteen six one one eleven one eighteen six six five ten one twelve two one six three ten four four five zero fifteen five four three seven one eight two two five four nine four one six one logical state series compound national cancer institute yeast anticancer drug screen highly record information calcification oil provide seventeen relatively small noisy phoneme originate project use distinguish nasal oral sound north financial publicly hold company accumulate since intrusion subset data acquire challenge intrusion detection minority class normal majority class sixteen boundary various biological twenty splice available five remain originate repository two originally multiple class convert keep class minority rest majority exception letter vowel become member minority class set aside state use result roc value present table three significantly improve classifier classifier value bold classifier value italics significantly improve result follow performance classifier may draw table three note improve categorically indicate identify local rather global sample level fact roc value produce effective strategy quite compel produce best result seventeen twenty one result tie outperform boundary naturally outperform wide margin highly dimensional naturally lend high dimensional despite high dimensionality local sample still able improve base note quite table four optimize classifier give k r k number cluster find majority class r number minority class local sample give e boundary estate intrusion letter oil page phoneme segment splice eighty two eight six two fifty ten forty twenty forty twenty two seven seventy twenty sixty eight ten eighty forty forty forty thirty eighty fourteen nineteen twenty seventy twenty twenty forty seventy thirty fifty thirty sixty fifty thirty large discrepancy base classifier performance note improve local sample since presume quite effective extract subclass digit form majority class effective improve inject synthetic overall eighteen significance test improvement fourteen compare thirteen compare eight compare ten compare ten compare consider conservative nature result noteworthy finally compare performance rank different emerge clear winner confidence per test use holm procedure compel result clearly establish performance achieve local sample also point divide majority class effective improve roc value base classifier often compare statistically significantly improve roc sixteen twenty eighteen use cluster able effectively exploit inherent structure majority class space improve performance base classifier however show decompose minority class space use supervise partition method table four show sample produce optimal result k value select moderately similar r value quite divergent correlation comparison k value show moderate correlation however r value weakly negatively correlate tend select r value opposite end spectrum fisher suggest two significantly different note produce similar determine smite level correlate however sample level highly correlate fisher suggest similar confidence therefore conclude strength local sample derive ability manipulate specific sample allow reproducibility result also readily provide source code interest six various key draw work elucidate key behavior presence class imbalance also help develop thesis propose framework local sample observe data aside class skew affect end classifier performance different sample level may produce thirteen g approximate statistical test compare supervise learn neural computation ten seven fourteen c learn international joint conference artificial intelligence page fifteen n class imbalance problem significance international conference artificial intelligence page sixteen third international knowledge discovery data mine tool competition seventeen r c machine learn detection oil spill satellite radar image machine learn eighteen f provost p tree induction learn three base rank ing nineteen j r induction decision tree machine learn twenty p n v k dunker z classification knowledge discovery journal protein c review canonical alternative analysis use distance g mine rarity unify framework j h p j local international conference rare class analysis knowledge discovery data mine page optimal similar even class skew identical also note classifier improve global sample level may insensitive different data result suboptimal performance demonstrate first discover segment data apply sample locally segment result best global performance start globally optimize locally predict globally main contribution paper general framework local sample class skew insensitive partition method effective application framework able achieve significantly better contemporary per test achieve best performance statistically across twenty derive different reference one e combine f test compare supervise classification learn neural computation eleven eight two machine learn repository three g r study behavior several balance machine learn train data six one four discriminative learn train test page five c chang c lin port vector machine library available six n v k w bowyer l hall w p smite synthetic minority journal artificial intelligence research technique seven n v l hall joshi automatically counter imbalance empirical relationship cost data mine special issue international journal data mine knowledge discovery eight n v n editorial learn six one sixteen nine n v detect fracture international conference performance data mine ten n v framework monitor performance failure occur knowledge information eleven n v learn decision tree unbalance data conference machine learn page twelve j statistical multiple data set journal machine learn research thirty