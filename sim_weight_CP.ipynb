{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.7\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "1. using all publications of researchers with different weight as input to generate user profiles\n",
    "2. pretrain word2vec model window_5.model.bin and candidate_paper.csv are available via google drive link,\n",
    "you can download the files and\n",
    "change the path in this script so as to run the script successfully.\n",
    "3. result saved in rank_result_all_weight/weight_CP.csv\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# load pre-train model on my own corpus\n",
    "model = '/Users/sherry/Downloads/window_5/window_5.model.bin'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "\n",
    "# read all candidate papers info, contain two columns: paper ID and paper content\n",
    "candidate_paper_df = pd.read_csv('/Users/sherry/Downloads/candidate_papers.csv')\n",
    "\n",
    "# define DocSim class to calculate document similarities\n",
    "class DocSim(object):\n",
    "    def __init__(self, w2v_model , stopwords=[]):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def vectorize(self, doc):\n",
    "        \"\"\"Identify the vector values for each word in the given document\"\"\"\n",
    "        doc = str(doc)\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "\n",
    "    def calculate_similarity(self,user_profile,candidate_papers,threshold=0):\n",
    "      # Computing similarity between a given source document in user profile\n",
    "      # and all target documents in candidate papers\n",
    "      # candidate_papers is dataframe, user_profile is a one-line string\n",
    "\n",
    "      # rename columns in user_profile and candidate_papers\n",
    "        candidate_papers.columns = ['paperID', 'paperText']\n",
    "\n",
    "      # convert dataframe to dict\n",
    "        candidate_paper_dict = candidate_papers.set_index('paperID').to_dict()\n",
    "\n",
    "      # for each user profile doc as source doc, calculate similarity with each\n",
    "      # target doc\n",
    "        source_doc = str(user_profile)\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        result = []\n",
    "        i = 1\n",
    "        for paperID,paperText in candidate_paper_dict['paperText'].items():\n",
    "            target_doc = str(paperText)\n",
    "            target_vec = self.vectorize(target_doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                result.append([paperID,sim_score])\n",
    "        # Sort results by similar scores in desc order\n",
    "        result.sort(key=lambda k : k[1] , reverse=True)\n",
    "        return result\n",
    "\n",
    "    def compute_sim_all_pubs(self, user_profile, candidate_papers,threshold=0):\n",
    "        \"\"\"\n",
    "        Computing similarity between several given source documents in user profile (with equal weight) and all target\n",
    "        documents in candidate\n",
    "        papers\n",
    "        :param user_profile: a list, all source docs of a researcher that used to construct one user profile\n",
    "        :param candidate_papers: a dataframe, all target docs that used as candidate recommend doc\n",
    "        :param threshold: filter recommend items according to threshold\n",
    "        :return: Sort rank results by similar scores in desc order\n",
    "        \"\"\"\n",
    "        # rename columns in user_profile and candidate_papers\n",
    "        candidate_papers.columns = ['paperID', 'paperText']\n",
    "\n",
    "        # convert dataframe to dict\n",
    "        candidate_paper_dict = candidate_papers.set_index('paperID').to_dict()\n",
    "\n",
    "        # for each user, source_doc_ls contains all his/her publications\n",
    "        source_docs_vec_ls = []\n",
    "        for pubished_seq,source_doc in enumerate(user_profile):\n",
    "            # weight doc vector based on the order of published year, +2 is because log2(x) <1 when x >2\n",
    "            c = 0.1\n",
    "            source_doc_vec = self.vectorize(source_doc)*(1/np.log2(pubished_seq+2+c))\n",
    "            # add each source doc vector into list source_docs_vec_ls\n",
    "            source_docs_vec_ls.append(source_doc_vec)\n",
    "        # compute user profile vector for each researcher based on all their publications with equal weight\n",
    "        user_profile_vec = np.sum(source_docs_vec_ls,axis = 0)/len(source_docs_vec_ls)\n",
    "\n",
    "        rank_result = []\n",
    "        i = 1\n",
    "        for paperID,paperText in candidate_paper_dict['paperText'].items():\n",
    "            target_doc = str(paperText)\n",
    "            target_vec = self.vectorize(target_doc)\n",
    "            sim_score = self._cosine_sim(user_profile_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                rank_result.append([paperID,sim_score])\n",
    "        # Sort results by similar scores in desc order\n",
    "        rank_result.sort(key=lambda k : k[1] , reverse=True)\n",
    "        return rank_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 16:47:25.833412\n",
      "number of publication for researcher R1 is 5\n",
      "the len of user_profile list for this researcher is: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherry/.pyenv/versions/version376env/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/sherry/.pyenv/versions/version376env/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 17:08:57.404384\n",
      "2020-03-08 17:08:57.582424\n",
      "number of publication for researcher R2 is 12\n",
      "the len of user_profile list for this researcher is: 12\n",
      "2020-03-08 17:41:53.841802\n",
      "2020-03-08 17:41:53.927469\n",
      "number of publication for researcher R3 is 7\n",
      "the len of user_profile list for this researcher is: 7\n",
      "2020-03-08 18:15:14.731825\n",
      "2020-03-08 18:15:14.787761\n",
      "number of publication for researcher R4 is 5\n",
      "the len of user_profile list for this researcher is: 5\n",
      "2020-03-08 18:46:53.840060\n",
      "2020-03-08 18:46:53.903638\n",
      "number of publication for researcher R5 is 2\n",
      "the len of user_profile list for this researcher is: 2\n",
      "2020-03-08 19:19:08.659149\n",
      "2020-03-08 19:19:08.921201\n",
      "number of publication for researcher R6 is 7\n",
      "the len of user_profile list for this researcher is: 7\n",
      "2020-03-08 19:51:06.044567\n",
      "2020-03-08 19:51:06.157019\n",
      "number of publication for researcher R7 is 16\n",
      "the len of user_profile list for this researcher is: 16\n",
      "2020-03-08 20:24:16.054256\n",
      "2020-03-08 20:24:16.137703\n",
      "number of publication for researcher R8 is 7\n",
      "the len of user_profile list for this researcher is: 7\n",
      "2020-03-08 20:59:37.853380\n",
      "2020-03-08 20:59:37.906622\n",
      "number of publication for researcher R9 is 13\n",
      "the len of user_profile list for this researcher is: 13\n",
      "2020-03-08 21:37:53.999782\n",
      "2020-03-08 21:37:54.095151\n",
      "number of publication for researcher R10 is 5\n",
      "the len of user_profile list for this researcher is: 5\n",
      "2020-03-08 22:13:23.695249\n",
      "2020-03-08 22:13:23.760788\n",
      "number of publication for researcher R11 is 12\n",
      "the len of user_profile list for this researcher is: 12\n",
      "2020-03-08 22:52:43.060401\n",
      "2020-03-08 22:52:43.158781\n",
      "number of publication for researcher R12 is 14\n",
      "the len of user_profile list for this researcher is: 14\n",
      "2020-03-08 23:28:39.109377\n",
      "2020-03-08 23:28:39.204377\n",
      "number of publication for researcher R13 is 9\n",
      "the len of user_profile list for this researcher is: 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-73b94011bca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the len of user_profile list for this researcher is: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_profile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# computing sim scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0msim_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_sim_all_pubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_profile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_paper_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'paperID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sim_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-563726176f1d>\u001b[0m in \u001b[0;36mcompute_sim_all_pubs\u001b[0;34m(self, user_profile, candidate_papers, threshold)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpaperID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpaperText\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidate_paper_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paperText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mtarget_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaperText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mtarget_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0msim_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_profile_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msim_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-563726176f1d>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Assuming that document vector is the mean of all the word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/version376env/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3335\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/version376env/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/version376env/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds = DocSim(w2v_model)\n",
    "\n",
    "# get the list of number of publications for each researcher\n",
    "import pandas as pd\n",
    "user_statistics_df = pd.read_csv('user_profiles/user_profiles_statistics.csv')\n",
    "num_pubs_ls = user_statistics_df.iloc[:,1].tolist()\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "ranking = [1,2,3,4,5,6,7,8,9,10]\n",
    "new_df.insert(0,'ranking',ranking)\n",
    "\n",
    "# reverse all researchers publications\n",
    "for i in range(1,51,1):\n",
    "    r = 'R' + str(i)\n",
    "    print(datetime.now())\n",
    "    user_profile = []\n",
    "    # reverse all publications of one researcher, get a list of\n",
    "    print('number of publication for researcher {} is {}'.format(r, num_pubs_ls[i - 1]))\n",
    "    for j in range(1,num_pubs_ls[i-1]+1):\n",
    "        with open('user_profiles/user_profile_after_text_cleaning/cleaned_R{}-{}.txt'.format(i,j), 'r') as f:\n",
    "            each_doc = f.read() # each_doc is a string\n",
    "        # all source docs of a researcher that used to construct his/her user profile\n",
    "        user_profile.append(each_doc)\n",
    "    print('the len of user_profile list for this researcher is: {}'.format(len(user_profile)))\n",
    "    # computing sim scores\n",
    "    sim_scores = ds.compute_sim_all_pubs(user_profile, candidate_paper_df)\n",
    "    df = pd.DataFrame(sim_scores)\n",
    "    df.columns = ['paperID', 'sim_score']\n",
    "    # get the top-10 rank list\n",
    "    df = df.head(10)\n",
    "    new_df[r] = df.iloc[:, 0]\n",
    "    # save ranking results for all researchers\n",
    "    print(datetime.now())\n",
    "    new_df.to_csv('rank_result_weight/weight_CP_{}.csv'.format(r), index=False)\n",
    "new_df.to_csv('rank_result_weight/weight_CP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "version376env",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
