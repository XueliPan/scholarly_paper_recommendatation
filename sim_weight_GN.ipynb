{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.7\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "1. using all publications of researchers with different weight as input to generate user profiles\n",
    "2. pretrain word2vec model window_5.model.bin and candidate_paper.csv are available via google drive link,\n",
    "you can download the files and\n",
    "change the path in this script so as to run the script successfully.\n",
    "3. result saved in rank_result_all_weight/weight_CP.csv\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# load pre-train model on my own corpus\n",
    "model = '/Users/sherry/Downloads/GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "\n",
    "# read all candidate papers info, contain two columns: paper ID and paper content\n",
    "candidate_paper_df = pd.read_csv('/Users/sherry/Downloads/candidate_papers.csv')\n",
    "\n",
    "# define DocSim class to calculate document similarities\n",
    "class DocSim(object):\n",
    "    def __init__(self, w2v_model , stopwords=[]):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def vectorize(self, doc):\n",
    "        \"\"\"Identify the vector values for each word in the given document\"\"\"\n",
    "        doc = str(doc)\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "\n",
    "    def calculate_similarity(self,user_profile,candidate_papers,threshold=0):\n",
    "      # Computing similarity between a given source document in user profile\n",
    "      # and all target documents in candidate papers\n",
    "      # candidate_papers is dataframe, user_profile is a one-line string\n",
    "\n",
    "      # rename columns in user_profile and candidate_papers\n",
    "        candidate_papers.columns = ['paperID', 'paperText']\n",
    "\n",
    "      # convert dataframe to dict\n",
    "        candidate_paper_dict = candidate_papers.set_index('paperID').to_dict()\n",
    "\n",
    "      # for each user profile doc as source doc, calculate similarity with each\n",
    "      # target doc\n",
    "        source_doc = str(user_profile)\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        result = []\n",
    "        i = 1\n",
    "        for paperID,paperText in candidate_paper_dict['paperText'].items():\n",
    "            target_doc = str(paperText)\n",
    "            target_vec = self.vectorize(target_doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                result.append([paperID,sim_score])\n",
    "        # Sort results by similar scores in desc order\n",
    "        result.sort(key=lambda k : k[1] , reverse=True)\n",
    "        return result\n",
    "\n",
    "    def compute_sim_all_pubs(self, user_profile, candidate_papers,threshold=0):\n",
    "        \"\"\"\n",
    "        Computing similarity between several given source documents in user profile (with equal weight) and all target\n",
    "        documents in candidate\n",
    "        papers\n",
    "        :param user_profile: a list, all source docs of a researcher that used to construct one user profile\n",
    "        :param candidate_papers: a dataframe, all target docs that used as candidate recommend doc\n",
    "        :param threshold: filter recommend items according to threshold\n",
    "        :return: Sort rank results by similar scores in desc order\n",
    "        \"\"\"\n",
    "        # rename columns in user_profile and candidate_papers\n",
    "        candidate_papers.columns = ['paperID', 'paperText']\n",
    "\n",
    "        # convert dataframe to dict\n",
    "        candidate_paper_dict = candidate_papers.set_index('paperID').to_dict()\n",
    "\n",
    "        # for each user, source_doc_ls contains all his/her publications\n",
    "        source_docs_vec_ls = []\n",
    "        for pubished_seq,source_doc in enumerate(user_profile):\n",
    "            # weight doc vector based on the order of published year\n",
    "            c = 0.1\n",
    "            source_doc_vec = self.vectorize(source_doc)*(1/np.log2(pubished_seq+1+c))\n",
    "            # add each source doc vector into list source_docs_vec_ls\n",
    "            source_docs_vec_ls.append(source_doc_vec)\n",
    "        # compute user profile vector for each researcher based on all their publications with equal weight\n",
    "        user_profile_vec = np.sum(source_docs_vec_ls,axis = 0)/len(source_docs_vec_ls)\n",
    "\n",
    "        rank_result = []\n",
    "        i = 1\n",
    "        for paperID,paperText in candidate_paper_dict['paperText'].items():\n",
    "            target_doc = str(paperText)\n",
    "            target_vec = self.vectorize(target_doc)\n",
    "            sim_score = self._cosine_sim(user_profile_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                rank_result.append([paperID,sim_score])\n",
    "        # Sort results by similar scores in desc order\n",
    "        rank_result.sort(key=lambda k : k[1] , reverse=True)\n",
    "        return rank_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-09 00:09:10.527048\n",
      "number of publication for researcher R14 is 14\n",
      "the len of user_profile list for this researcher is: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherry/.pyenv/versions/version376env/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/sherry/.pyenv/versions/version376env/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-09 00:28:06.570700\n",
      "2020-03-09 00:28:06.647228\n",
      "number of publication for researcher R15 is 5\n",
      "the len of user_profile list for this researcher is: 5\n"
     ]
    }
   ],
   "source": [
    "ds = DocSim(w2v_model)\n",
    "\n",
    "# get the list of number of publications for each researcher\n",
    "import pandas as pd\n",
    "user_statistics_df = pd.read_csv('user_profiles/user_profiles_statistics.csv')\n",
    "num_pubs_ls = user_statistics_df.iloc[:,1].tolist()\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "ranking = [1,2,3,4,5,6,7,8,9,10]\n",
    "new_df.insert(0,'ranking',ranking)\n",
    "\n",
    "# reverse all researchers publications\n",
    "for i in range(14,51,1):\n",
    "    r = 'R' + str(i)\n",
    "    print(datetime.now())\n",
    "    user_profile = []\n",
    "    # reverse all publications of one researcher, get a list of\n",
    "    print('number of publication for researcher {} is {}'.format(r, num_pubs_ls[i - 1]))\n",
    "    for j in range(1,num_pubs_ls[i-1]+1):\n",
    "        with open('user_profiles/user_profile_after_text_cleaning/cleaned_R{}-{}.txt'.format(i,j), 'r') as f:\n",
    "            each_doc = f.read() # each_doc is a string\n",
    "        # all source docs of a researcher that used to construct his/her user profile\n",
    "        user_profile.append(each_doc)\n",
    "    print('the len of user_profile list for this researcher is: {}'.format(len(user_profile)))\n",
    "    # computing sim scores\n",
    "    sim_scores = ds.compute_sim_all_pubs(user_profile, candidate_paper_df)\n",
    "    df = pd.DataFrame(sim_scores)\n",
    "    df.columns = ['paperID', 'sim_score']\n",
    "    # get the top-10 rank list\n",
    "    df = df.head(10)\n",
    "    new_df[r] = df.iloc[:, 0]\n",
    "    # save ranking results for all researchers\n",
    "    print(datetime.now())\n",
    "    new_df.to_csv('rank_result_weight/weight_GN_{}.csv'.format(r), index=False)\n",
    "new_df.to_csv('rank_result_weight/weight_GN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "version376env",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
